/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 0): env://, gpu 0
[13:13:46.379189] job dir: /home/vision/wonjun/LiVT-main
[13:13:46.379274] [13:13:46.379470] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=16,
batch_size=64,
blr=0.00015,
ckpt_dir='./ckpt/debug/cifar100-LT/debug',
color_jitter=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=800,
gpu=0,
imbf=1,
input_size=224,
local_rank=0,
log_dir='./exp/debug/cifar100-LT/debug',
lr=None,
mask_ratio=0.75,
min_lr=0.0,
model='mae_vit_base_patch16',
norm_pix_loss=True,
num_workers=16,
pin_mem=True,
prit=200,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='None',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=40,
weight_decay=0.05,
world_size=4)
[13:13:46.379549] [13:13:46.710363] Files already downloaded and verified
[13:13:47.620460] Dataset CIFAR100_LT
    Number of datapoints: 50000
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[13:13:47.623678] Dataset CIFAR100_LT
    Number of datapoints: 50000
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[13:13:47.624455] [13:13:47.625163] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f0a30b03e50>
[13:13:47.625231] [13:13:47.626908] log_writer is <torch.utils.tensorboard.writer.SummaryWriter object at 0x7f0a30b03c90>
[13:13:47.626981] [13:13:50.018864] Model = mae_vit_base_patch16
[13:13:50.019046] [13:13:50.019127] base lr: 1.50e-04
[13:13:50.019179] [13:13:50.019221] actual lr: 2.40e-03
[13:13:50.019268] [13:13:50.019304] accumulate grad iterations: 16
[13:13:50.019348] [13:13:50.019385] effective batch size: 4096
[13:13:50.019428] [13:13:50.263749] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.0024
    maximize: False
    weight_decay: 0.01
)
[13:13:50.263883] Traceback (most recent call last):
  File "./main_pretrain.py", line 265, in <module>
    main(args)
  File "./main_pretrain.py", line 221, in main
    misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 349, in load_model
    checkpoint = torch.load(args.resume, map_location='cpu')
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 771, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 270, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 251, in __init__
Traceback (most recent call last):
  File "./main_pretrain.py", line 265, in <module>
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'None'
    main(args)
  File "./main_pretrain.py", line 221, in main
Traceback (most recent call last):
  File "./main_pretrain.py", line 265, in <module>
    misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 349, in load_model
    checkpoint = torch.load(args.resume, map_location='cpu')
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 771, in load
    main(args)
  File "./main_pretrain.py", line 221, in main
    misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)    
with _open_file_like(f, 'rb') as opened_file:
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 349, in load_model
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 270, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 251, in __init__
    checkpoint = torch.load(args.resume, map_location='cpu')
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 771, in load
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'None'
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 270, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 251, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'None'
Traceback (most recent call last):
  File "./main_pretrain.py", line 265, in <module>
    main(args)
  File "./main_pretrain.py", line 221, in main
    misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 349, in load_model
    checkpoint = torch.load(args.resume, map_location='cpu')
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 771, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 270, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/serialization.py", line 251, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'None'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 25229) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./main_pretrain.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-11-20_13:13:52
  host      : user
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 25230)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-11-20_13:13:52
  host      : user
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 25231)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-11-20_13:13:52
  host      : user
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 25232)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-20_13:13:52
  host      : user
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 25229)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 0): env://, gpu 0
[13:15:49.366759] job dir: /home/vision/wonjun/LiVT-main
[13:15:49.366872] [13:15:49.367026] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=16,
batch_size=64,
blr=0.00015,
ckpt_dir='./ckpt/debug/cifar100-LT/debug',
color_jitter=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=800,
gpu=0,
imbf=1,
input_size=224,
local_rank=0,
log_dir='./exp/debug/cifar100-LT/debug',
lr=None,
mask_ratio=0.75,
min_lr=0.0,
model='mae_vit_base_patch16',
norm_pix_loss=True,
num_workers=16,
pin_mem=True,
prit=200,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=40,
weight_decay=0.05,
world_size=4)
[13:15:49.367087] [13:15:49.698508] Files already downloaded and verified
[13:15:50.595638] Dataset CIFAR100_LT
    Number of datapoints: 50000
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[13:15:50.598942] Dataset CIFAR100_LT
    Number of datapoints: 50000
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[13:15:50.599706] [13:15:50.600443] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f12a0ca4050>
[13:15:50.600514] [13:15:50.602142] log_writer is <torch.utils.tensorboard.writer.SummaryWriter object at 0x7f12a0ca4110>
[13:15:50.602216] [13:15:53.036490] Model = mae_vit_base_patch16
[13:15:53.036684] [13:15:53.036774] base lr: 1.50e-04
[13:15:53.036829] [13:15:53.036871] actual lr: 2.40e-03
[13:15:53.036917] [13:15:53.036956] accumulate grad iterations: 16
[13:15:53.037001] [13:15:53.037037] effective batch size: 4096
[13:15:53.037081] [13:15:53.068072] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.0024
    maximize: False
    weight_decay: 0.01
)
[13:15:53.068252] [13:15:53.068542] Save config to: ./exp/debug/cifar100-LT/debug/args.txt
[13:15:53.068584] Start training for 800 epochs
[13:15:53.068637] [13:15:53.070303] log_dir: ./exp/debug/cifar100-LT/debug
[13:15:56.943037] attn initing scaling_factor...
[13:15:56.953427] attn initing scaling_factor...
[13:15:56.954276] mlp initing scaling_factor...
[13:15:56.955822] mlp initing scaling_factor...
[13:15:56.957153] attn initing scaling_factor...
[13:15:56.959345] attn initing scaling_factor...
[13:15:56.959963] mlp initing scaling_factor...
[13:15:56.961537] mlp initing scaling_factor...
[13:15:56.962822] attn initing scaling_factor...
[13:15:56.965082] attn initing scaling_factor...
[13:15:56.965711] mlp initing scaling_factor...
[13:15:56.967226] mlp initing scaling_factor...
[13:15:56.968508] attn initing scaling_factor...
[13:15:56.970814] attn initing scaling_factor...
[13:15:56.971444] mlp initing scaling_factor...
[13:15:56.972986] mlp initing scaling_factor...
[13:15:56.974279] attn initing scaling_factor...
[13:15:56.976506] attn initing scaling_factor...
[13:15:56.977124] mlp initing scaling_factor...
[13:15:56.978609] mlp initing scaling_factor...
[13:15:56.979896] attn initing scaling_factor...
[13:15:56.982209] attn initing scaling_factor...
[13:15:56.982830] mlp initing scaling_factor...
[13:15:56.984324] mlp initing scaling_factor...
[13:15:56.985599] attn initing scaling_factor...
[13:15:56.987985] attn initing scaling_factor...
[13:15:56.988617] mlp initing scaling_factor...
[13:15:56.990156] mlp initing scaling_factor...
[13:15:56.991432] attn initing scaling_factor...
[13:15:56.993614] attn initing scaling_factor...
[13:15:56.994237] mlp initing scaling_factor...
[13:15:56.995728] mlp initing scaling_factor...
[13:15:56.997044] attn initing scaling_factor...
[13:15:56.999356] attn initing scaling_factor...
[13:15:56.999984] mlp initing scaling_factor...
[13:15:57.001524] mlp initing scaling_factor...
[13:15:57.002849] attn initing scaling_factor...
[13:15:57.005098] attn initing scaling_factor...
[13:15:57.005708] mlp initing scaling_factor...
[13:15:57.007300] mlp initing scaling_factor...
[13:15:57.008700] attn initing scaling_factor...
[13:15:57.010956] attn initing scaling_factor...
[13:15:57.011587] mlp initing scaling_factor...
[13:15:57.013367] mlp initing scaling_factor...
[13:15:57.014696] attn initing scaling_factor...
[13:15:57.017244] attn initing scaling_factor...
[13:15:57.017882] mlp initing scaling_factor...
[13:15:57.019466] mlp initing scaling_factor...
[13:15:57.023885] attn initing scaling_factor...
[13:15:57.034035] attn initing scaling_factor...
[13:15:57.035068] mlp initing scaling_factor...
[13:15:57.036739] mlp initing scaling_factor...
[13:15:57.038027] attn initing scaling_factor...
[13:15:57.047190] attn initing scaling_factor...
[13:15:57.048258] mlp initing scaling_factor...
[13:15:57.049831] mlp initing scaling_factor...
[13:15:57.051086] attn initing scaling_factor...
[13:15:57.060102] attn initing scaling_factor...
[13:15:57.061276] mlp initing scaling_factor...
[13:15:57.062861] mlp initing scaling_factor...
[13:15:57.064118] attn initing scaling_factor...
[13:15:57.073129] attn initing scaling_factor...
[13:15:57.074212] mlp initing scaling_factor...
[13:15:57.075790] mlp initing scaling_factor...
[13:15:57.077050] attn initing scaling_factor...
[13:15:57.086065] attn initing scaling_factor...
[13:15:57.087233] mlp initing scaling_factor...
[13:15:57.088819] mlp initing scaling_factor...
[13:15:57.090041] attn initing scaling_factor...
[13:15:57.099057] attn initing scaling_factor...
[13:15:57.100061] mlp initing scaling_factor...
[13:15:57.101672] mlp initing scaling_factor...
[13:15:57.102892] attn initing scaling_factor...
[13:15:57.111897] attn initing scaling_factor...
[13:15:57.112918] mlp initing scaling_factor...
[13:15:57.114513] mlp initing scaling_factor...
[13:15:57.115725] attn initing scaling_factor...
[13:15:57.142468] attn initing scaling_factor...
[13:15:57.143780] mlp initing scaling_factor...
[13:15:57.145386] mlp initing scaling_factor...
[13:15:57.823877] Epoch: [0]  [  0/195]  eta: 0:15:26  lr: 0.000000  loss: 1.8042 (1.8042)  time: 4.7525  data: 1.9419  max mem: 8037
[13:16:08.045094] Epoch: [0]  [ 20/195]  eta: 0:02:04  lr: 0.000005  loss: 1.7997 (1.8006)  time: 0.5110  data: 0.0002  max mem: 9341
[13:16:18.241403] Epoch: [0]  [ 40/195]  eta: 0:01:35  lr: 0.000010  loss: 1.7944 (1.7693)  time: 0.5097  data: 0.0002  max mem: 9341
[13:16:28.437692] Epoch: [0]  [ 60/195]  eta: 0:01:18  lr: 0.000015  loss: 1.6416 (1.7289)  time: 0.5098  data: 0.0002  max mem: 9341
[13:16:38.680449] Epoch: [0]  [ 80/195]  eta: 0:01:04  lr: 0.000025  loss: 1.6017 (1.6983)  time: 0.5121  data: 0.0002  max mem: 9341
[13:16:48.885441] Epoch: [0]  [100/195]  eta: 0:00:52  lr: 0.000030  loss: 1.5607 (1.6694)  time: 0.5102  data: 0.0002  max mem: 9341
[13:16:59.087823] Epoch: [0]  [120/195]  eta: 0:00:40  lr: 0.000034  loss: 1.5105 (1.6415)  time: 0.5101  data: 0.0002  max mem: 9341
[13:17:09.287653] Epoch: [0]  [140/195]  eta: 0:00:29  lr: 0.000039  loss: 1.4382 (1.6145)  time: 0.5099  data: 0.0002  max mem: 9341
[13:17:19.534696] Epoch: [0]  [160/195]  eta: 0:00:18  lr: 0.000049  loss: 1.3951 (1.5876)  time: 0.5123  data: 0.0002  max mem: 9341
[13:17:29.699622] Epoch: [0]  [180/195]  eta: 0:00:08  lr: 0.000054  loss: 1.3380 (1.5598)  time: 0.5082  data: 0.0001  max mem: 9341
[13:17:36.821212] Epoch: [0]  [194/195]  eta: 0:00:00  lr: 0.000059  loss: 1.3089 (1.5411)  time: 0.5101  data: 0.0001  max mem: 9341
[13:17:36.958716] Epoch: [0] Total time: 0:01:43 (0.5328 s / it)
[13:17:36.976260] Averaged stats: lr: 0.000059  loss: 1.3089 (1.5411)
[13:17:41.632238] {"train_lr": 2.7569230769230758e-05, "train_loss": 1.541139717743947, "epoch": 0}
[13:17:41.632551] [13:17:41.632640] Training epoch 0 for 0:01:48
[13:17:41.632698] [13:17:41.637048] log_dir: ./exp/debug/cifar100-LT/debug
[13:17:43.305147] Epoch: [1]  [  0/195]  eta: 0:05:25  lr: 0.000060  loss: 1.2826 (1.2826)  time: 1.6671  data: 1.1592  max mem: 9341
[13:17:53.508620] Epoch: [1]  [ 20/195]  eta: 0:01:38  lr: 0.000065  loss: 1.2566 (1.2509)  time: 0.5101  data: 0.0002  max mem: 9341
[13:18:03.707134] Epoch: [1]  [ 40/195]  eta: 0:01:23  lr: 0.000070  loss: 1.2026 (1.2284)  time: 0.5099  data: 0.0002  max mem: 9341
[13:18:13.914959] Epoch: [1]  [ 60/195]  eta: 0:01:11  lr: 0.000075  loss: 1.1698 (1.2104)  time: 0.5103  data: 0.0002  max mem: 9341
[13:18:24.166782] Epoch: [1]  [ 80/195]  eta: 0:01:00  lr: 0.000085  loss: 1.1312 (1.1916)  time: 0.5125  data: 0.0002  max mem: 9341
[13:18:34.375626] Epoch: [1]  [100/195]  eta: 0:00:49  lr: 0.000090  loss: 1.1059 (1.1740)  time: 0.5104  data: 0.0002  max mem: 9341
[13:18:44.581425] Epoch: [1]  [120/195]  eta: 0:00:39  lr: 0.000094  loss: 1.0793 (1.1580)  time: 0.5102  data: 0.0002  max mem: 9341
[13:18:54.796395] Epoch: [1]  [140/195]  eta: 0:00:28  lr: 0.000099  loss: 1.0430 (1.1424)  time: 0.5107  data: 0.0002  max mem: 9341
[13:19:05.050815] Epoch: [1]  [160/195]  eta: 0:00:18  lr: 0.000109  loss: 1.0257 (1.1283)  time: 0.5127  data: 0.0002  max mem: 9341
[13:19:15.218718] Epoch: [1]  [180/195]  eta: 0:00:07  lr: 0.000114  loss: 1.0166 (1.1162)  time: 0.5083  data: 0.0001  max mem: 9341
[13:19:22.351569] Epoch: [1]  [194/195]  eta: 0:00:00  lr: 0.000119  loss: 1.0071 (1.1080)  time: 0.5106  data: 0.0001  max mem: 9341
[13:19:22.520216] Epoch: [1] Total time: 0:01:40 (0.5173 s / it)
[13:19:22.525970] Averaged stats: lr: 0.000119  loss: 1.0071 (1.1081)
[13:19:27.240256] {"train_lr": 8.756923076923074e-05, "train_loss": 1.108090097629107, "epoch": 1}
[13:19:27.240613] [13:19:27.240712] Training epoch 1 for 0:01:45
[13:19:27.240771] [13:19:27.245222] log_dir: ./exp/debug/cifar100-LT/debug
[13:19:28.926422] Epoch: [2]  [  0/195]  eta: 0:05:27  lr: 0.000120  loss: 1.0021 (1.0021)  time: 1.6797  data: 1.1804  max mem: 9341
[13:19:39.163890] Epoch: [2]  [ 20/195]  eta: 0:01:39  lr: 0.000125  loss: 0.9993 (1.0001)  time: 0.5118  data: 0.0002  max mem: 9341
[13:19:49.396332] Epoch: [2]  [ 40/195]  eta: 0:01:23  lr: 0.000130  loss: 0.9762 (0.9914)  time: 0.5116  data: 0.0002  max mem: 9341
[13:19:59.627105] Epoch: [2]  [ 60/195]  eta: 0:01:11  lr: 0.000135  loss: 0.9766 (0.9865)  time: 0.5115  data: 0.0002  max mem: 9341
[13:20:09.925050] Epoch: [2]  [ 80/195]  eta: 0:01:00  lr: 0.000145  loss: 0.9659 (0.9806)  time: 0.5148  data: 0.0002  max mem: 9341
[13:20:20.163960] Epoch: [2]  [100/195]  eta: 0:00:49  lr: 0.000150  loss: 0.9624 (0.9772)  time: 0.5119  data: 0.0002  max mem: 9341
[13:20:30.403891] Epoch: [2]  [120/195]  eta: 0:00:39  lr: 0.000154  loss: 0.9621 (0.9746)  time: 0.5119  data: 0.0002  max mem: 9341
[13:20:40.637498] Epoch: [2]  [140/195]  eta: 0:00:28  lr: 0.000159  loss: 0.9429 (0.9701)  time: 0.5116  data: 0.0002  max mem: 9341
[13:20:50.936547] Epoch: [2]  [160/195]  eta: 0:00:18  lr: 0.000169  loss: 0.9403 (0.9667)  time: 0.5149  data: 0.0002  max mem: 9341
[13:21:01.129327] Epoch: [2]  [180/195]  eta: 0:00:07  lr: 0.000174  loss: 0.9225 (0.9622)  time: 0.5096  data: 0.0001  max mem: 9341
[13:21:08.279936] Epoch: [2]  [194/195]  eta: 0:00:00  lr: 0.000179  loss: 0.9283 (0.9597)  time: 0.5127  data: 0.0001  max mem: 9341
[13:21:08.450446] Epoch: [2] Total time: 0:01:41 (0.5190 s / it)
[13:21:08.451354] Averaged stats: lr: 0.000179  loss: 0.9283 (0.9581)
[13:21:13.233481] {"train_lr": 0.00014756923076923075, "train_loss": 0.9581346823618963, "epoch": 2}
[13:21:13.233716] [13:21:13.233806] Training epoch 2 for 0:01:45
[13:21:13.233864] [13:21:13.238251] log_dir: ./exp/debug/cifar100-LT/debug
[13:21:14.996464] Epoch: [3]  [  0/195]  eta: 0:05:42  lr: 0.000180  loss: 0.9244 (0.9244)  time: 1.7574  data: 1.2637  max mem: 9341
[13:21:25.230028] Epoch: [3]  [ 20/195]  eta: 0:01:39  lr: 0.000185  loss: 0.9248 (0.9276)  time: 0.5116  data: 0.0002  max mem: 9341
[13:21:35.453203] Epoch: [3]  [ 40/195]  eta: 0:01:23  lr: 0.000190  loss: 0.9093 (0.9197)  time: 0.5111  data: 0.0002  max mem: 9341
[13:21:45.670960] Epoch: [3]  [ 60/195]  eta: 0:01:11  lr: 0.000195  loss: 0.9012 (0.9133)  time: 0.5108  data: 0.0002  max mem: 9341
[13:21:55.926420] Epoch: [3]  [ 80/195]  eta: 0:01:00  lr: 0.000205  loss: 0.8950 (0.9094)  time: 0.5127  data: 0.0002  max mem: 9341
[13:22:06.139194] Epoch: [3]  [100/195]  eta: 0:00:49  lr: 0.000210  loss: 0.8889 (0.9051)  time: 0.5106  data: 0.0002  max mem: 9341
[13:22:16.353083] Epoch: [3]  [120/195]  eta: 0:00:39  lr: 0.000214  loss: 0.8701 (0.8998)  time: 0.5106  data: 0.0002  max mem: 9341
[13:22:26.561830] Epoch: [3]  [140/195]  eta: 0:00:28  lr: 0.000219  loss: 0.8586 (0.8944)  time: 0.5104  data: 0.0002  max mem: 9341
[13:22:36.821839] Epoch: [3]  [160/195]  eta: 0:00:18  lr: 0.000229  loss: 0.8540 (0.8888)  time: 0.5129  data: 0.0002  max mem: 9341
[13:22:46.991572] Epoch: [3]  [180/195]  eta: 0:00:07  lr: 0.000234  loss: 0.8437 (0.8837)  time: 0.5084  data: 0.0001  max mem: 9341
[13:22:54.119043] Epoch: [3]  [194/195]  eta: 0:00:00  lr: 0.000239  loss: 0.8400 (0.8804)  time: 0.5104  data: 0.0001  max mem: 9341
[13:22:54.290495] Epoch: [3] Total time: 0:01:41 (0.5182 s / it)
[13:22:54.295165] Averaged stats: lr: 0.000239  loss: 0.8400 (0.8792)
[13:22:58.990117] {"train_lr": 0.0002075692307692307, "train_loss": 0.8792154143253962, "epoch": 3}
[13:22:58.990388] [13:22:58.990478] Training epoch 3 for 0:01:45
[13:22:58.990534] [13:22:58.995038] log_dir: ./exp/debug/cifar100-LT/debug
[13:23:00.564392] Epoch: [4]  [  0/195]  eta: 0:05:05  lr: 0.000240  loss: 0.7793 (0.7793)  time: 1.5679  data: 1.0573  max mem: 9341
[13:23:10.775822] Epoch: [4]  [ 20/195]  eta: 0:01:38  lr: 0.000245  loss: 0.8263 (0.8212)  time: 0.5105  data: 0.0002  max mem: 9341
[13:23:20.984327] Epoch: [4]  [ 40/195]  eta: 0:01:23  lr: 0.000250  loss: 0.8159 (0.8192)  time: 0.5104  data: 0.0002  max mem: 9341
[13:23:31.195841] Epoch: [4]  [ 60/195]  eta: 0:01:11  lr: 0.000255  loss: 0.8103 (0.8154)  time: 0.5105  data: 0.0002  max mem: 9341
[13:23:41.448221] Epoch: [4]  [ 80/195]  eta: 0:01:00  lr: 0.000265  loss: 0.7988 (0.8117)  time: 0.5126  data: 0.0002  max mem: 9341
[13:23:51.658830] Epoch: [4]  [100/195]  eta: 0:00:49  lr: 0.000270  loss: 0.7909 (0.8086)  time: 0.5105  data: 0.0002  max mem: 9341
[13:24:01.863177] Epoch: [4]  [120/195]  eta: 0:00:38  lr: 0.000274  loss: 0.8134 (0.8091)  time: 0.5102  data: 0.0002  max mem: 9341
[13:24:12.074410] Epoch: [4]  [140/195]  eta: 0:00:28  lr: 0.000279  loss: 0.7845 (0.8071)  time: 0.5105  data: 0.0002  max mem: 9341
[13:24:22.331416] Epoch: [4]  [160/195]  eta: 0:00:18  lr: 0.000289  loss: 0.7958 (0.8054)  time: 0.5128  data: 0.0002  max mem: 9341
[13:24:32.501197] Epoch: [4]  [180/195]  eta: 0:00:07  lr: 0.000294  loss: 0.7904 (0.8049)  time: 0.5084  data: 0.0002  max mem: 9341
[13:24:39.630845] Epoch: [4]  [194/195]  eta: 0:00:00  lr: 0.000299  loss: 0.8063 (0.8051)  time: 0.5103  data: 0.0001  max mem: 9341
[13:24:39.792392] Epoch: [4] Total time: 0:01:40 (0.5169 s / it)
[13:24:39.828971] Averaged stats: lr: 0.000299  loss: 0.8063 (0.8069)
[13:24:44.529975] {"train_lr": 0.00026756923076923066, "train_loss": 0.8069360736088875, "epoch": 4}
[13:24:44.530242] [13:24:44.530332] Training epoch 4 for 0:01:45
[13:24:44.530391] [13:24:44.534890] log_dir: ./exp/debug/cifar100-LT/debug
[13:24:46.293930] Epoch: [5]  [  0/195]  eta: 0:05:42  lr: 0.000300  loss: 0.7476 (0.7476)  time: 1.7578  data: 1.2552  max mem: 9341
[13:24:56.525021] Epoch: [5]  [ 20/195]  eta: 0:01:39  lr: 0.000305  loss: 0.7898 (0.7846)  time: 0.5114  data: 0.0002  max mem: 9341
[13:25:06.763862] Epoch: [5]  [ 40/195]  eta: 0:01:24  lr: 0.000310  loss: 0.8082 (0.7961)  time: 0.5118  data: 0.0002  max mem: 9341
[13:25:16.981577] Epoch: [5]  [ 60/195]  eta: 0:01:11  lr: 0.000315  loss: 0.7933 (0.7934)  time: 0.5108  data: 0.0002  max mem: 9341
[13:25:27.247099] Epoch: [5]  [ 80/195]  eta: 0:01:00  lr: 0.000325  loss: 0.7668 (0.7915)  time: 0.5132  data: 0.0002  max mem: 9341
[13:25:37.451495] Epoch: [5]  [100/195]  eta: 0:00:49  lr: 0.000330  loss: 0.7669 (0.7873)  time: 0.5102  data: 0.0002  max mem: 9341
[13:25:47.661256] Epoch: [5]  [120/195]  eta: 0:00:39  lr: 0.000334  loss: 0.7864 (0.7881)  time: 0.5104  data: 0.0002  max mem: 9341
[13:25:57.869569] Epoch: [5]  [140/195]  eta: 0:00:28  lr: 0.000339  loss: 0.7630 (0.7854)  time: 0.5104  data: 0.0002  max mem: 9341
[13:26:08.128476] Epoch: [5]  [160/195]  eta: 0:00:18  lr: 0.000349  loss: 0.7658 (0.7842)  time: 0.5129  data: 0.0002  max mem: 9341
[13:26:18.294528] Epoch: [5]  [180/195]  eta: 0:00:07  lr: 0.000354  loss: 0.7702 (0.7824)  time: 0.5083  data: 0.0001  max mem: 9341
[13:26:25.424564] Epoch: [5]  [194/195]  eta: 0:00:00  lr: 0.000359  loss: 0.7907 (0.7830)  time: 0.5104  data: 0.0001  max mem: 9341
[13:26:25.605710] Epoch: [5] Total time: 0:01:41 (0.5183 s / it)
[13:26:25.621803] Averaged stats: lr: 0.000359  loss: 0.7907 (0.7831)
[13:26:30.327640] {"train_lr": 0.000327569230769231, "train_loss": 0.7830858044135265, "epoch": 5}
[13:26:30.327900] [13:26:30.327990] Training epoch 5 for 0:01:45
[13:26:30.328044] [13:26:30.332505] log_dir: ./exp/debug/cifar100-LT/debug
[13:26:31.997526] Epoch: [6]  [  0/195]  eta: 0:05:24  lr: 0.000360  loss: 0.7815 (0.7815)  time: 1.6642  data: 1.1597  max mem: 9341
[13:26:42.220976] Epoch: [6]  [ 20/195]  eta: 0:01:39  lr: 0.000365  loss: 0.7681 (0.7729)  time: 0.5111  data: 0.0002  max mem: 9341
[13:26:52.431565] Epoch: [6]  [ 40/195]  eta: 0:01:23  lr: 0.000370  loss: 0.7795 (0.7744)  time: 0.5105  data: 0.0002  max mem: 9341
[13:27:02.642906] Epoch: [6]  [ 60/195]  eta: 0:01:11  lr: 0.000375  loss: 0.7762 (0.7737)  time: 0.5105  data: 0.0002  max mem: 9341
[13:27:12.900633] Epoch: [6]  [ 80/195]  eta: 0:01:00  lr: 0.000385  loss: 0.7683 (0.7732)  time: 0.5128  data: 0.0002  max mem: 9341
[13:27:23.121104] Epoch: [6]  [100/195]  eta: 0:00:49  lr: 0.000390  loss: 0.7560 (0.7717)  time: 0.5110  data: 0.0002  max mem: 9341
[13:27:33.332008] Epoch: [6]  [120/195]  eta: 0:00:39  lr: 0.000394  loss: 0.7504 (0.7693)  time: 0.5105  data: 0.0002  max mem: 9341
[13:27:43.542682] Epoch: [6]  [140/195]  eta: 0:00:28  lr: 0.000399  loss: 0.7609 (0.7691)  time: 0.5105  data: 0.0002  max mem: 9341
[13:27:53.796505] Epoch: [6]  [160/195]  eta: 0:00:18  lr: 0.000409  loss: 0.7720 (0.7694)  time: 0.5126  data: 0.0002  max mem: 9341
[13:28:03.966926] Epoch: [6]  [180/195]  eta: 0:00:07  lr: 0.000414  loss: 0.7739 (0.7693)  time: 0.5085  data: 0.0001  max mem: 9341
[13:28:11.100243] Epoch: [6]  [194/195]  eta: 0:00:00  lr: 0.000419  loss: 0.7696 (0.7689)  time: 0.5106  data: 0.0001  max mem: 9341
[13:28:11.268829] Epoch: [6] Total time: 0:01:40 (0.5176 s / it)
[13:28:11.280168] Averaged stats: lr: 0.000419  loss: 0.7696 (0.7683)
[13:28:15.975630] {"train_lr": 0.0003875692307692304, "train_loss": 0.7683140760813004, "epoch": 6}
[13:28:15.975957] [13:28:15.976049] Training epoch 6 for 0:01:45
[13:28:15.976118] [13:28:15.980742] log_dir: ./exp/debug/cifar100-LT/debug
[13:28:17.695109] Epoch: [7]  [  0/195]  eta: 0:05:34  lr: 0.000420  loss: 0.7978 (0.7978)  time: 1.7131  data: 1.1981  max mem: 9341
[13:28:27.903997] Epoch: [7]  [ 20/195]  eta: 0:01:39  lr: 0.000425  loss: 0.7819 (0.7774)  time: 0.5104  data: 0.0002  max mem: 9341
[13:28:38.113291] Epoch: [7]  [ 40/195]  eta: 0:01:23  lr: 0.000430  loss: 0.7602 (0.7727)  time: 0.5104  data: 0.0002  max mem: 9341
[13:28:48.330102] Epoch: [7]  [ 60/195]  eta: 0:01:11  lr: 0.000435  loss: 0.7636 (0.7705)  time: 0.5107  data: 0.0002  max mem: 9341
[13:28:58.590921] Epoch: [7]  [ 80/195]  eta: 0:01:00  lr: 0.000445  loss: 0.7857 (0.7744)  time: 0.5130  data: 0.0002  max mem: 9341
[13:29:08.803729] Epoch: [7]  [100/195]  eta: 0:00:49  lr: 0.000450  loss: 0.7604 (0.7715)  time: 0.5106  data: 0.0002  max mem: 9341
[13:29:19.016297] Epoch: [7]  [120/195]  eta: 0:00:39  lr: 0.000454  loss: 0.7625 (0.7715)  time: 0.5106  data: 0.0002  max mem: 9341
[13:29:29.234377] Epoch: [7]  [140/195]  eta: 0:00:28  lr: 0.000459  loss: 0.7656 (0.7701)  time: 0.5108  data: 0.0002  max mem: 9341
[13:29:39.489903] Epoch: [7]  [160/195]  eta: 0:00:18  lr: 0.000469  loss: 0.7592 (0.7683)  time: 0.5127  data: 0.0002  max mem: 9341
[13:29:49.662989] Epoch: [7]  [180/195]  eta: 0:00:07  lr: 0.000474  loss: 0.7669 (0.7677)  time: 0.5086  data: 0.0001  max mem: 9341
[13:29:56.790390] Epoch: [7]  [194/195]  eta: 0:00:00  lr: 0.000479  loss: 0.7534 (0.7665)  time: 0.5104  data: 0.0001  max mem: 9341
[13:29:56.975327] Epoch: [7] Total time: 0:01:40 (0.5179 s / it)
[13:29:56.980459] Averaged stats: lr: 0.000479  loss: 0.7534 (0.7689)
[13:30:01.667083] {"train_lr": 0.0004475692307692302, "train_loss": 0.7689054619807464, "epoch": 7}
[13:30:01.667414] [13:30:01.667506] Training epoch 7 for 0:01:45
[13:30:01.667637] [13:30:01.672247] log_dir: ./exp/debug/cifar100-LT/debug
[13:30:03.375463] Epoch: [8]  [  0/195]  eta: 0:05:31  lr: 0.000480  loss: 0.7086 (0.7086)  time: 1.7023  data: 1.1865  max mem: 9341
[13:30:13.578708] Epoch: [8]  [ 20/195]  eta: 0:01:39  lr: 0.000485  loss: 0.7602 (0.7565)  time: 0.5101  data: 0.0002  max mem: 9341
[13:30:23.791487] Epoch: [8]  [ 40/195]  eta: 0:01:23  lr: 0.000490  loss: 0.7537 (0.7523)  time: 0.5106  data: 0.0002  max mem: 9341
[13:30:34.004223] Epoch: [8]  [ 60/195]  eta: 0:01:11  lr: 0.000495  loss: 0.7493 (0.7510)  time: 0.5106  data: 0.0002  max mem: 9341
[13:30:44.258704] Epoch: [8]  [ 80/195]  eta: 0:01:00  lr: 0.000505  loss: 0.7505 (0.7519)  time: 0.5127  data: 0.0002  max mem: 9341
[13:30:54.465624] Epoch: [8]  [100/195]  eta: 0:00:49  lr: 0.000510  loss: 0.7405 (0.7528)  time: 0.5103  data: 0.0002  max mem: 9341
[13:31:04.678564] Epoch: [8]  [120/195]  eta: 0:00:39  lr: 0.000514  loss: 0.7442 (0.7518)  time: 0.5106  data: 0.0002  max mem: 9341
[13:31:14.891424] Epoch: [8]  [140/195]  eta: 0:00:28  lr: 0.000519  loss: 0.7448 (0.7506)  time: 0.5106  data: 0.0002  max mem: 9341
[13:31:25.145018] Epoch: [8]  [160/195]  eta: 0:00:18  lr: 0.000529  loss: 0.7369 (0.7484)  time: 0.5126  data: 0.0002  max mem: 9341
[13:31:35.315013] Epoch: [8]  [180/195]  eta: 0:00:07  lr: 0.000534  loss: 0.7404 (0.7476)  time: 0.5084  data: 0.0001  max mem: 9341
[13:31:42.441938] Epoch: [8]  [194/195]  eta: 0:00:00  lr: 0.000539  loss: 0.7310 (0.7467)  time: 0.5103  data: 0.0001  max mem: 9341
[13:31:42.618916] Epoch: [8] Total time: 0:01:40 (0.5177 s / it)
[13:31:42.623961] Averaged stats: lr: 0.000539  loss: 0.7310 (0.7469)
[13:31:47.300449] {"train_lr": 0.0005075692307692312, "train_loss": 0.7469247126426453, "epoch": 8}
[13:31:47.300710] [13:31:47.300798] Training epoch 8 for 0:01:45
[13:31:47.300855] [13:31:47.305296] log_dir: ./exp/debug/cifar100-LT/debug
[13:31:49.114616] Epoch: [9]  [  0/195]  eta: 0:05:52  lr: 0.000540  loss: 0.7274 (0.7274)  time: 1.8085  data: 1.3092  max mem: 9341
[13:31:59.331307] Epoch: [9]  [ 20/195]  eta: 0:01:40  lr: 0.000545  loss: 0.7305 (0.7286)  time: 0.5108  data: 0.0002  max mem: 9341
[13:32:09.550309] Epoch: [9]  [ 40/195]  eta: 0:01:24  lr: 0.000550  loss: 0.7142 (0.7226)  time: 0.5109  data: 0.0002  max mem: 9341
[13:32:19.754308] Epoch: [9]  [ 60/195]  eta: 0:01:11  lr: 0.000555  loss: 0.7233 (0.7229)  time: 0.5101  data: 0.0002  max mem: 9341
[13:32:30.007342] Epoch: [9]  [ 80/195]  eta: 0:01:00  lr: 0.000565  loss: 0.7248 (0.7221)  time: 0.5126  data: 0.0002  max mem: 9341
[13:32:40.238065] Epoch: [9]  [100/195]  eta: 0:00:49  lr: 0.000570  loss: 0.7211 (0.7234)  time: 0.5115  data: 0.0002  max mem: 9341
[13:32:50.442543] Epoch: [9]  [120/195]  eta: 0:00:39  lr: 0.000574  loss: 0.7131 (0.7217)  time: 0.5102  data: 0.0002  max mem: 9341
[13:33:00.654821] Epoch: [9]  [140/195]  eta: 0:00:28  lr: 0.000579  loss: 0.7115 (0.7199)  time: 0.5106  data: 0.0002  max mem: 9341
[13:33:10.915370] Epoch: [9]  [160/195]  eta: 0:00:18  lr: 0.000589  loss: 0.7185 (0.7210)  time: 0.5130  data: 0.0002  max mem: 9341
[13:33:21.093795] Epoch: [9]  [180/195]  eta: 0:00:07  lr: 0.000594  loss: 0.7258 (0.7217)  time: 0.5089  data: 0.0001  max mem: 9341
[13:33:28.225259] Epoch: [9]  [194/195]  eta: 0:00:00  lr: 0.000599  loss: 0.7041 (0.7204)  time: 0.5108  data: 0.0001  max mem: 9341
[13:33:28.389139] Epoch: [9] Total time: 0:01:41 (0.5184 s / it)
[13:33:28.412504] Averaged stats: lr: 0.000599  loss: 0.7041 (0.7225)
[13:33:33.124899] {"train_lr": 0.0005675692307692311, "train_loss": 0.7224909118352792, "epoch": 9}
[13:33:33.125174] [13:33:33.125270] Training epoch 9 for 0:01:45
[13:33:33.125328] [13:33:33.129892] log_dir: ./exp/debug/cifar100-LT/debug
[13:33:34.855053] Epoch: [10]  [  0/195]  eta: 0:05:36  lr: 0.000600  loss: 0.7930 (0.7930)  time: 1.7244  data: 1.2205  max mem: 9341
[13:33:45.095136] Epoch: [10]  [ 20/195]  eta: 0:01:39  lr: 0.000605  loss: 0.7322 (0.7356)  time: 0.5119  data: 0.0002  max mem: 9341
[13:33:55.331529] Epoch: [10]  [ 40/195]  eta: 0:01:23  lr: 0.000610  loss: 0.7195 (0.7282)  time: 0.5118  data: 0.0002  max mem: 9341
[13:34:05.546073] Epoch: [10]  [ 60/195]  eta: 0:01:11  lr: 0.000615  loss: 0.7105 (0.7240)  time: 0.5107  data: 0.0002  max mem: 9341
[13:34:15.833462] Epoch: [10]  [ 80/195]  eta: 0:01:00  lr: 0.000625  loss: 0.7340 (0.7251)  time: 0.5143  data: 0.0002  max mem: 9341
[13:34:26.073277] Epoch: [10]  [100/195]  eta: 0:00:49  lr: 0.000630  loss: 0.7106 (0.7222)  time: 0.5119  data: 0.0002  max mem: 9341
[13:34:36.310308] Epoch: [10]  [120/195]  eta: 0:00:39  lr: 0.000634  loss: 0.7178 (0.7210)  time: 0.5118  data: 0.0002  max mem: 9341
[13:34:46.545513] Epoch: [10]  [140/195]  eta: 0:00:28  lr: 0.000639  loss: 0.7134 (0.7203)  time: 0.5117  data: 0.0002  max mem: 9341
[13:34:56.807216] Epoch: [10]  [160/195]  eta: 0:00:18  lr: 0.000649  loss: 0.7081 (0.7192)  time: 0.5130  data: 0.0002  max mem: 9341
[13:35:06.978700] Epoch: [10]  [180/195]  eta: 0:00:07  lr: 0.000654  loss: 0.7079 (0.7171)  time: 0.5085  data: 0.0002  max mem: 9341
[13:35:14.114480] Epoch: [10]  [194/195]  eta: 0:00:00  lr: 0.000659  loss: 0.6932 (0.7159)  time: 0.5108  data: 0.0001  max mem: 9341
[13:35:14.294433] Epoch: [10] Total time: 0:01:41 (0.5188 s / it)
[13:35:14.305654] Averaged stats: lr: 0.000659  loss: 0.6932 (0.7136)
[13:35:18.998644] {"train_lr": 0.0006275692307692302, "train_loss": 0.7136386163723775, "epoch": 10}
[13:35:18.998925] [13:35:18.999015] Training epoch 10 for 0:01:45
[13:35:18.999073] [13:35:19.003623] log_dir: ./exp/debug/cifar100-LT/debug
[13:35:20.740598] Epoch: [11]  [  0/195]  eta: 0:05:38  lr: 0.000660  loss: 0.7707 (0.7707)  time: 1.7359  data: 1.2433  max mem: 9341
[13:35:30.947214] Epoch: [11]  [ 20/195]  eta: 0:01:39  lr: 0.000665  loss: 0.7195 (0.7272)  time: 0.5103  data: 0.0002  max mem: 9341
[13:35:41.161340] Epoch: [11]  [ 40/195]  eta: 0:01:23  lr: 0.000670  loss: 0.7224 (0.7284)  time: 0.5106  data: 0.0002  max mem: 9341
[13:35:51.374122] Epoch: [11]  [ 60/195]  eta: 0:01:11  lr: 0.000675  loss: 0.7077 (0.7207)  time: 0.5106  data: 0.0002  max mem: 9341
[13:36:01.673716] Epoch: [11]  [ 80/195]  eta: 0:01:00  lr: 0.000685  loss: 0.6920 (0.7152)  time: 0.5149  data: 0.0002  max mem: 9341
[13:36:11.883919] Epoch: [11]  [100/195]  eta: 0:00:49  lr: 0.000690  loss: 0.6921 (0.7111)  time: 0.5105  data: 0.0002  max mem: 9341
[13:36:22.098050] Epoch: [11]  [120/195]  eta: 0:00:39  lr: 0.000694  loss: 0.6755 (0.7066)  time: 0.5106  data: 0.0002  max mem: 9341
[13:36:32.308997] Epoch: [11]  [140/195]  eta: 0:00:28  lr: 0.000699  loss: 0.6809 (0.7049)  time: 0.5105  data: 0.0002  max mem: 9341
[13:36:42.566680] Epoch: [11]  [160/195]  eta: 0:00:18  lr: 0.000709  loss: 0.7085 (0.7044)  time: 0.5128  data: 0.0002  max mem: 9341
[13:36:52.739458] Epoch: [11]  [180/195]  eta: 0:00:07  lr: 0.000714  loss: 0.6771 (0.7023)  time: 0.5086  data: 0.0001  max mem: 9341
[13:36:59.866127] Epoch: [11]  [194/195]  eta: 0:00:00  lr: 0.000719  loss: 0.6873 (0.7022)  time: 0.5104  data: 0.0001  max mem: 9341
[13:37:00.026710] Epoch: [11] Total time: 0:01:41 (0.5181 s / it)
[13:37:00.052016] Averaged stats: lr: 0.000719  loss: 0.6873 (0.7028)
[13:37:04.843788] {"train_lr": 0.0006875692307692303, "train_loss": 0.7027617011314783, "epoch": 11}
[13:37:04.844337] [13:37:04.844530] Training epoch 11 for 0:01:45
[13:37:04.844644] [13:37:04.854539] log_dir: ./exp/debug/cifar100-LT/debug
[13:37:06.501764] Epoch: [12]  [  0/195]  eta: 0:05:21  lr: 0.000720  loss: 0.6799 (0.6799)  time: 1.6463  data: 1.1307  max mem: 9341
[13:37:16.717377] Epoch: [12]  [ 20/195]  eta: 0:01:38  lr: 0.000725  loss: 0.6817 (0.6884)  time: 0.5107  data: 0.0002  max mem: 9341
[13:37:26.928185] Epoch: [12]  [ 40/195]  eta: 0:01:23  lr: 0.000730  loss: 0.6799 (0.6835)  time: 0.5105  data: 0.0002  max mem: 9341
[13:37:37.144042] Epoch: [12]  [ 60/195]  eta: 0:01:11  lr: 0.000735  loss: 0.6680 (0.6808)  time: 0.5107  data: 0.0002  max mem: 9341
[13:37:47.403086] Epoch: [12]  [ 80/195]  eta: 0:01:00  lr: 0.000745  loss: 0.6971 (0.6854)  time: 0.5129  data: 0.0002  max mem: 9341
[13:37:57.682917] Epoch: [12]  [100/195]  eta: 0:00:49  lr: 0.000750  loss: 0.7273 (0.6957)  time: 0.5139  data: 0.0002  max mem: 9341
[13:38:07.889759] Epoch: [12]  [120/195]  eta: 0:00:39  lr: 0.000754  loss: 0.7069 (0.6974)  time: 0.5103  data: 0.0002  max mem: 9341
[13:38:18.098502] Epoch: [12]  [140/195]  eta: 0:00:28  lr: 0.000759  loss: 0.6990 (0.6963)  time: 0.5104  data: 0.0002  max mem: 9341
[13:38:28.349346] Epoch: [12]  [160/195]  eta: 0:00:18  lr: 0.000769  loss: 0.6775 (0.6942)  time: 0.5125  data: 0.0002  max mem: 9341
[13:38:38.518270] Epoch: [12]  [180/195]  eta: 0:00:07  lr: 0.000774  loss: 0.6654 (0.6916)  time: 0.5084  data: 0.0001  max mem: 9341
[13:38:45.644016] Epoch: [12]  [194/195]  eta: 0:00:00  lr: 0.000779  loss: 0.6751 (0.6917)  time: 0.5104  data: 0.0001  max mem: 9341
[13:38:45.810453] Epoch: [12] Total time: 0:01:40 (0.5177 s / it)
[13:38:45.824843] Averaged stats: lr: 0.000779  loss: 0.6751 (0.6924)
[13:38:50.493086] {"train_lr": 0.0007475692307692313, "train_loss": 0.6923755550995851, "epoch": 12}
[13:38:50.493369] [13:38:50.493461] Training epoch 12 for 0:01:45
[13:38:50.493516] [13:38:50.497953] log_dir: ./exp/debug/cifar100-LT/debug
[13:38:52.294824] Epoch: [13]  [  0/195]  eta: 0:05:50  lr: 0.000780  loss: 0.6423 (0.6423)  time: 1.7958  data: 1.2862  max mem: 9341
[13:39:02.508774] Epoch: [13]  [ 20/195]  eta: 0:01:40  lr: 0.000785  loss: 0.6780 (0.6736)  time: 0.5106  data: 0.0002  max mem: 9341
[13:39:12.721640] Epoch: [13]  [ 40/195]  eta: 0:01:24  lr: 0.000790  loss: 0.6712 (0.6721)  time: 0.5106  data: 0.0002  max mem: 9341
[13:39:22.933790] Epoch: [13]  [ 60/195]  eta: 0:01:11  lr: 0.000795  loss: 0.6811 (0.6743)  time: 0.5105  data: 0.0002  max mem: 9341
[13:39:33.197257] Epoch: [13]  [ 80/195]  eta: 0:01:00  lr: 0.000805  loss: 0.6850 (0.6767)  time: 0.5131  data: 0.0002  max mem: 9341
[13:39:43.411138] Epoch: [13]  [100/195]  eta: 0:00:49  lr: 0.000810  loss: 0.6786 (0.6775)  time: 0.5106  data: 0.0002  max mem: 9341
[13:39:53.624880] Epoch: [13]  [120/195]  eta: 0:00:39  lr: 0.000814  loss: 0.6634 (0.6749)  time: 0.5106  data: 0.0002  max mem: 9341
[13:40:03.837377] Epoch: [13]  [140/195]  eta: 0:00:28  lr: 0.000819  loss: 0.6587 (0.6726)  time: 0.5106  data: 0.0002  max mem: 9341
[13:40:14.096436] Epoch: [13]  [160/195]  eta: 0:00:18  lr: 0.000829  loss: 0.6549 (0.6708)  time: 0.5129  data: 0.0002  max mem: 9341
[13:40:24.266839] Epoch: [13]  [180/195]  eta: 0:00:07  lr: 0.000834  loss: 0.6603 (0.6692)  time: 0.5085  data: 0.0001  max mem: 9341
[13:40:31.393371] Epoch: [13]  [194/195]  eta: 0:00:00  lr: 0.000839  loss: 0.6637 (0.6698)  time: 0.5102  data: 0.0001  max mem: 9341
[13:40:31.572662] Epoch: [13] Total time: 0:01:41 (0.5183 s / it)
[13:40:31.580247] Averaged stats: lr: 0.000839  loss: 0.6637 (0.6690)
[13:40:36.281889] {"train_lr": 0.0008075692307692309, "train_loss": 0.6690403370521008, "epoch": 13}
[13:40:36.282147] [13:40:36.282236] Training epoch 13 for 0:01:45
[13:40:36.282290] [13:40:36.286799] log_dir: ./exp/debug/cifar100-LT/debug
[13:40:38.025859] Epoch: [14]  [  0/195]  eta: 0:05:38  lr: 0.000840  loss: 0.6634 (0.6634)  time: 1.7382  data: 1.2323  max mem: 9341
[13:40:48.256715] Epoch: [14]  [ 20/195]  eta: 0:01:39  lr: 0.000845  loss: 0.6860 (0.6858)  time: 0.5115  data: 0.0002  max mem: 9341
[13:40:58.472476] Epoch: [14]  [ 40/195]  eta: 0:01:23  lr: 0.000850  loss: 0.6820 (0.6826)  time: 0.5107  data: 0.0002  max mem: 9341
[13:41:08.686974] Epoch: [14]  [ 60/195]  eta: 0:01:11  lr: 0.000855  loss: 0.6791 (0.6827)  time: 0.5105  data: 0.0002  max mem: 9341
[13:41:18.940070] Epoch: [14]  [ 80/195]  eta: 0:01:00  lr: 0.000865  loss: 0.6782 (0.6808)  time: 0.5126  data: 0.0002  max mem: 9341
[13:41:29.149065] Epoch: [14]  [100/195]  eta: 0:00:49  lr: 0.000870  loss: 0.6674 (0.6794)  time: 0.5104  data: 0.0002  max mem: 9341
[13:41:39.357859] Epoch: [14]  [120/195]  eta: 0:00:39  lr: 0.000874  loss: 0.6595 (0.6753)  time: 0.5104  data: 0.0002  max mem: 9341
[13:41:49.562281] Epoch: [14]  [140/195]  eta: 0:00:28  lr: 0.000879  loss: 0.6666 (0.6737)  time: 0.5102  data: 0.0002  max mem: 9341
[13:41:59.812441] Epoch: [14]  [160/195]  eta: 0:00:18  lr: 0.000889  loss: 0.6647 (0.6728)  time: 0.5124  data: 0.0002  max mem: 9341
[13:42:09.982988] Epoch: [14]  [180/195]  eta: 0:00:07  lr: 0.000894  loss: 0.6677 (0.6717)  time: 0.5085  data: 0.0001  max mem: 9341
[13:42:17.110011] Epoch: [14]  [194/195]  eta: 0:00:00  lr: 0.000899  loss: 0.7049 (0.6744)  time: 0.5105  data: 0.0001  max mem: 9341
[13:42:17.280218] Epoch: [14] Total time: 0:01:40 (0.5179 s / it)
[13:42:17.287926] Averaged stats: lr: 0.000899  loss: 0.7049 (0.6741)
[13:42:21.977052] {"train_lr": 0.0008675692307692303, "train_loss": 0.6741324390356357, "epoch": 14}
[13:42:21.977547] [13:42:21.977644] Training epoch 14 for 0:01:45
[13:42:21.977700] [13:42:21.983331] log_dir: ./exp/debug/cifar100-LT/debug
[13:42:23.600193] Epoch: [15]  [  0/195]  eta: 0:05:15  lr: 0.000900  loss: 0.7546 (0.7546)  time: 1.6158  data: 1.1017  max mem: 9341
[13:42:33.838123] Epoch: [15]  [ 20/195]  eta: 0:01:38  lr: 0.000905  loss: 0.6863 (0.6909)  time: 0.5118  data: 0.0002  max mem: 9341
[13:42:44.072493] Epoch: [15]  [ 40/195]  eta: 0:01:23  lr: 0.000910  loss: 0.6674 (0.6824)  time: 0.5117  data: 0.0002  max mem: 9341
[13:42:54.287505] Epoch: [15]  [ 60/195]  eta: 0:01:11  lr: 0.000915  loss: 0.6458 (0.6730)  time: 0.5107  data: 0.0002  max mem: 9341
[13:43:04.547174] Epoch: [15]  [ 80/195]  eta: 0:01:00  lr: 0.000925  loss: 0.6770 (0.6728)  time: 0.5129  data: 0.0002  max mem: 9341
[13:43:14.761793] Epoch: [15]  [100/195]  eta: 0:00:49  lr: 0.000930  loss: 0.6662 (0.6723)  time: 0.5107  data: 0.0002  max mem: 9341
[13:43:24.974889] Epoch: [15]  [120/195]  eta: 0:00:39  lr: 0.000934  loss: 0.6536 (0.6699)  time: 0.5106  data: 0.0002  max mem: 9341
[13:43:35.188373] Epoch: [15]  [140/195]  eta: 0:00:28  lr: 0.000939  loss: 0.6451 (0.6680)  time: 0.5106  data: 0.0002  max mem: 9341
[13:43:45.494494] Epoch: [15]  [160/195]  eta: 0:00:18  lr: 0.000949  loss: 0.6583 (0.6652)  time: 0.5152  data: 0.0002  max mem: 9341
[13:43:55.697594] Epoch: [15]  [180/195]  eta: 0:00:07  lr: 0.000954  loss: 0.6361 (0.6627)  time: 0.5101  data: 0.0001  max mem: 9341
[13:44:02.852479] Epoch: [15]  [194/195]  eta: 0:00:00  lr: 0.000959  loss: 0.6479 (0.6618)  time: 0.5129  data: 0.0001  max mem: 9341
[13:44:03.033964] Epoch: [15] Total time: 0:01:41 (0.5182 s / it)
[13:44:03.038130] Averaged stats: lr: 0.000959  loss: 0.6479 (0.6572)
[13:44:07.702118] {"train_lr": 0.0009275692307692311, "train_loss": 0.6572134132568653, "epoch": 15}
[13:44:07.702390] [13:44:07.702477] Training epoch 15 for 0:01:45
[13:44:07.702530] [13:44:07.706995] log_dir: ./exp/debug/cifar100-LT/debug
[13:44:09.226322] Epoch: [16]  [  0/195]  eta: 0:04:55  lr: 0.000960  loss: 0.6020 (0.6020)  time: 1.5168  data: 1.0121  max mem: 9341
[13:44:19.446837] Epoch: [16]  [ 20/195]  eta: 0:01:37  lr: 0.000965  loss: 0.6636 (0.6560)  time: 0.5110  data: 0.0002  max mem: 9341
[13:44:29.676612] Epoch: [16]  [ 40/195]  eta: 0:01:23  lr: 0.000970  loss: 0.6339 (0.6465)  time: 0.5114  data: 0.0002  max mem: 9341
[13:44:39.912056] Epoch: [16]  [ 60/195]  eta: 0:01:11  lr: 0.000975  loss: 0.6460 (0.6482)  time: 0.5117  data: 0.0002  max mem: 9341
[13:44:50.210437] Epoch: [16]  [ 80/195]  eta: 0:01:00  lr: 0.000985  loss: 0.6223 (0.6426)  time: 0.5149  data: 0.0002  max mem: 9341
[13:45:00.444857] Epoch: [16]  [100/195]  eta: 0:00:49  lr: 0.000990  loss: 0.6358 (0.6421)  time: 0.5117  data: 0.0002  max mem: 9341
[13:45:10.688787] Epoch: [16]  [120/195]  eta: 0:00:39  lr: 0.000994  loss: 0.6277 (0.6404)  time: 0.5121  data: 0.0002  max mem: 9341
[13:45:20.899598] Epoch: [16]  [140/195]  eta: 0:00:28  lr: 0.000999  loss: 0.6335 (0.6386)  time: 0.5105  data: 0.0002  max mem: 9341
[13:45:31.155729] Epoch: [16]  [160/195]  eta: 0:00:18  lr: 0.001009  loss: 0.6022 (0.6345)  time: 0.5128  data: 0.0002  max mem: 9341
[13:45:41.323113] Epoch: [16]  [180/195]  eta: 0:00:07  lr: 0.001014  loss: 0.6262 (0.6344)  time: 0.5083  data: 0.0001  max mem: 9341
[13:45:48.450880] Epoch: [16]  [194/195]  eta: 0:00:00  lr: 0.001019  loss: 0.6267 (0.6344)  time: 0.5103  data: 0.0001  max mem: 9341
[13:45:48.663519] Epoch: [16] Total time: 0:01:40 (0.5177 s / it)
[13:45:48.664464] Averaged stats: lr: 0.001019  loss: 0.6267 (0.6353)
[13:45:53.371072] {"train_lr": 0.0009875692307692325, "train_loss": 0.6353003751008939, "epoch": 16}
[13:45:53.371388] [13:45:53.371486] Training epoch 16 for 0:01:45
[13:45:53.371540] [13:45:53.376682] log_dir: ./exp/debug/cifar100-LT/debug
[13:45:55.328175] Epoch: [17]  [  0/195]  eta: 0:06:20  lr: 0.001020  loss: 0.6173 (0.6173)  time: 1.9505  data: 1.4496  max mem: 9341
[13:46:05.539159] Epoch: [17]  [ 20/195]  eta: 0:01:41  lr: 0.001025  loss: 0.6251 (0.6252)  time: 0.5105  data: 0.0002  max mem: 9341
[13:46:15.757334] Epoch: [17]  [ 40/195]  eta: 0:01:24  lr: 0.001030  loss: 0.6220 (0.6263)  time: 0.5108  data: 0.0002  max mem: 9341
[13:46:25.966625] Epoch: [17]  [ 60/195]  eta: 0:01:12  lr: 0.001035  loss: 0.6037 (0.6208)  time: 0.5104  data: 0.0002  max mem: 9341
[13:46:36.224652] Epoch: [17]  [ 80/195]  eta: 0:01:00  lr: 0.001045  loss: 0.6399 (0.6246)  time: 0.5128  data: 0.0002  max mem: 9341
[13:46:46.434362] Epoch: [17]  [100/195]  eta: 0:00:49  lr: 0.001050  loss: 0.6429 (0.6282)  time: 0.5104  data: 0.0002  max mem: 9341
[13:46:56.646129] Epoch: [17]  [120/195]  eta: 0:00:39  lr: 0.001054  loss: 0.6426 (0.6323)  time: 0.5105  data: 0.0002  max mem: 9341
[13:47:06.851472] Epoch: [17]  [140/195]  eta: 0:00:28  lr: 0.001059  loss: 0.6284 (0.6325)  time: 0.5102  data: 0.0002  max mem: 9341
[13:47:17.106726] Epoch: [17]  [160/195]  eta: 0:00:18  lr: 0.001069  loss: 0.6235 (0.6322)  time: 0.5127  data: 0.0002  max mem: 9341
[13:47:27.273910] Epoch: [17]  [180/195]  eta: 0:00:07  lr: 0.001074  loss: 0.6342 (0.6326)  time: 0.5083  data: 0.0001  max mem: 9341
[13:47:34.399803] Epoch: [17]  [194/195]  eta: 0:00:00  lr: 0.001079  loss: 0.6424 (0.6330)  time: 0.5103  data: 0.0001  max mem: 9341
[13:47:34.563299] Epoch: [17] Total time: 0:01:41 (0.5189 s / it)
[13:47:34.577456] Averaged stats: lr: 0.001079  loss: 0.6424 (0.6319)
[13:47:39.308692] {"train_lr": 0.0010475692307692305, "train_loss": 0.6319459725648929, "epoch": 17}
[13:47:39.308940] [13:47:39.309026] Training epoch 17 for 0:01:45
[13:47:39.309091] [13:47:39.313695] log_dir: ./exp/debug/cifar100-LT/debug
[13:47:40.931456] Epoch: [18]  [  0/195]  eta: 0:05:15  lr: 0.001080  loss: 0.6020 (0.6020)  time: 1.6162  data: 1.1072  max mem: 9341
[13:47:51.166780] Epoch: [18]  [ 20/195]  eta: 0:01:38  lr: 0.001085  loss: 0.6380 (0.6436)  time: 0.5117  data: 0.0002  max mem: 9341
[13:48:01.396174] Epoch: [18]  [ 40/195]  eta: 0:01:23  lr: 0.001090  loss: 0.5992 (0.6279)  time: 0.5114  data: 0.0002  max mem: 9341
[13:48:11.607905] Epoch: [18]  [ 60/195]  eta: 0:01:11  lr: 0.001095  loss: 0.6180 (0.6281)  time: 0.5105  data: 0.0002  max mem: 9341
[13:48:21.870139] Epoch: [18]  [ 80/195]  eta: 0:01:00  lr: 0.001105  loss: 0.6117 (0.6248)  time: 0.5131  data: 0.0002  max mem: 9341
[13:48:32.085446] Epoch: [18]  [100/195]  eta: 0:00:49  lr: 0.001110  loss: 0.6065 (0.6221)  time: 0.5107  data: 0.0002  max mem: 9341
[13:48:42.304000] Epoch: [18]  [120/195]  eta: 0:00:39  lr: 0.001114  loss: 0.6092 (0.6219)  time: 0.5109  data: 0.0002  max mem: 9341
[13:48:52.518113] Epoch: [18]  [140/195]  eta: 0:00:28  lr: 0.001119  loss: 0.5967 (0.6194)  time: 0.5106  data: 0.0002  max mem: 9341
[13:49:02.773867] Epoch: [18]  [160/195]  eta: 0:00:18  lr: 0.001129  loss: 0.6046 (0.6176)  time: 0.5127  data: 0.0002  max mem: 9341
[13:49:12.948867] Epoch: [18]  [180/195]  eta: 0:00:07  lr: 0.001134  loss: 0.5944 (0.6161)  time: 0.5087  data: 0.0002  max mem: 9341
[13:49:20.083419] Epoch: [18]  [194/195]  eta: 0:00:00  lr: 0.001139  loss: 0.5927 (0.6151)  time: 0.5107  data: 0.0001  max mem: 9341
[13:49:20.248769] Epoch: [18] Total time: 0:01:40 (0.5176 s / it)
[13:49:20.259197] Averaged stats: lr: 0.001139  loss: 0.5927 (0.6168)
[13:49:24.831080] {"train_lr": 0.0011075692307692304, "train_loss": 0.6168300787607829, "epoch": 18}
[13:49:24.831353] [13:49:24.831442] Training epoch 18 for 0:01:45
[13:49:24.831496] [13:49:24.836010] log_dir: ./exp/debug/cifar100-LT/debug
[13:49:26.521141] Epoch: [19]  [  0/195]  eta: 0:05:28  lr: 0.001140  loss: 0.6031 (0.6031)  time: 1.6838  data: 1.1749  max mem: 9341
[13:49:36.738575] Epoch: [19]  [ 20/195]  eta: 0:01:39  lr: 0.001145  loss: 0.6225 (0.6208)  time: 0.5108  data: 0.0002  max mem: 9341
[13:49:46.954868] Epoch: [19]  [ 40/195]  eta: 0:01:23  lr: 0.001150  loss: 0.6052 (0.6156)  time: 0.5108  data: 0.0002  max mem: 9341
[13:49:57.169902] Epoch: [19]  [ 60/195]  eta: 0:01:11  lr: 0.001155  loss: 0.6073 (0.6136)  time: 0.5107  data: 0.0002  max mem: 9341
[13:50:07.425439] Epoch: [19]  [ 80/195]  eta: 0:01:00  lr: 0.001165  loss: 0.6079 (0.6133)  time: 0.5127  data: 0.0002  max mem: 9341
[13:50:17.631354] Epoch: [19]  [100/195]  eta: 0:00:49  lr: 0.001170  loss: 0.5996 (0.6110)  time: 0.5102  data: 0.0002  max mem: 9341
[13:50:27.851031] Epoch: [19]  [120/195]  eta: 0:00:39  lr: 0.001174  loss: 0.6048 (0.6097)  time: 0.5109  data: 0.0002  max mem: 9341
[13:50:38.061054] Epoch: [19]  [140/195]  eta: 0:00:28  lr: 0.001179  loss: 0.6115 (0.6088)  time: 0.5104  data: 0.0002  max mem: 9341
[13:50:48.317250] Epoch: [19]  [160/195]  eta: 0:00:18  lr: 0.001189  loss: 0.6012 (0.6080)  time: 0.5128  data: 0.0002  max mem: 9341
[13:50:58.488048] Epoch: [19]  [180/195]  eta: 0:00:07  lr: 0.001194  loss: 0.6174 (0.6089)  time: 0.5085  data: 0.0001  max mem: 9341
[13:51:05.620875] Epoch: [19]  [194/195]  eta: 0:00:00  lr: 0.001199  loss: 0.6139 (0.6091)  time: 0.5106  data: 0.0001  max mem: 9341
[13:51:05.814819] Epoch: [19] Total time: 0:01:40 (0.5178 s / it)
[13:51:05.817376] Averaged stats: lr: 0.001199  loss: 0.6139 (0.6111)
[13:51:10.511869] {"train_lr": 0.0011675692307692304, "train_loss": 0.6111152875117767, "epoch": 19}
[13:51:10.512160] [13:51:10.512247] Training epoch 19 for 0:01:45
[13:51:10.512301] [13:51:10.516788] log_dir: ./exp/debug/cifar100-LT/debug
[13:51:12.307325] Epoch: [20]  [  0/195]  eta: 0:05:48  lr: 0.001200  loss: 0.5970 (0.5970)  time: 1.7895  data: 1.2971  max mem: 9341
[13:51:22.544956] Epoch: [20]  [ 20/195]  eta: 0:01:40  lr: 0.001205  loss: 0.5978 (0.6010)  time: 0.5118  data: 0.0002  max mem: 9341
[13:51:32.753256] Epoch: [20]  [ 40/195]  eta: 0:01:24  lr: 0.001210  loss: 0.5931 (0.5970)  time: 0.5104  data: 0.0002  max mem: 9341
[13:51:42.970932] Epoch: [20]  [ 60/195]  eta: 0:01:11  lr: 0.001215  loss: 0.5945 (0.5983)  time: 0.5108  data: 0.0002  max mem: 9341
[13:51:53.227086] Epoch: [20]  [ 80/195]  eta: 0:01:00  lr: 0.001225  loss: 0.6159 (0.6027)  time: 0.5127  data: 0.0002  max mem: 9341
[13:52:03.436295] Epoch: [20]  [100/195]  eta: 0:00:49  lr: 0.001230  loss: 0.6075 (0.6035)  time: 0.5104  data: 0.0002  max mem: 9341
[13:52:13.648824] Epoch: [20]  [120/195]  eta: 0:00:39  lr: 0.001234  loss: 0.5942 (0.6029)  time: 0.5106  data: 0.0002  max mem: 9341
[13:52:23.856725] Epoch: [20]  [140/195]  eta: 0:00:28  lr: 0.001239  loss: 0.5814 (0.6009)  time: 0.5103  data: 0.0002  max mem: 9341
[13:52:34.111684] Epoch: [20]  [160/195]  eta: 0:00:18  lr: 0.001249  loss: 0.6036 (0.6015)  time: 0.5127  data: 0.0002  max mem: 9341
[13:52:44.276896] Epoch: [20]  [180/195]  eta: 0:00:07  lr: 0.001254  loss: 0.5986 (0.6013)  time: 0.5082  data: 0.0001  max mem: 9341
[13:52:51.402612] Epoch: [20]  [194/195]  eta: 0:00:00  lr: 0.001259  loss: 0.5974 (0.6008)  time: 0.5100  data: 0.0001  max mem: 9341
[13:52:51.572935] Epoch: [20] Total time: 0:01:41 (0.5182 s / it)
[13:52:51.582098] Averaged stats: lr: 0.001259  loss: 0.5974 (0.6044)
[13:52:56.310904] {"train_lr": 0.0012275692307692303, "train_loss": 0.6044473946094513, "epoch": 20}
[13:52:56.311160] [13:52:56.311242] Training epoch 20 for 0:01:45
[13:52:56.311296] [13:52:56.318060] log_dir: ./exp/debug/cifar100-LT/debug
[13:52:58.226197] Epoch: [21]  [  0/195]  eta: 0:06:11  lr: 0.001260  loss: 0.6175 (0.6175)  time: 1.9070  data: 1.4057  max mem: 9341
[13:53:08.435033] Epoch: [21]  [ 20/195]  eta: 0:01:40  lr: 0.001265  loss: 0.6141 (0.6152)  time: 0.5104  data: 0.0002  max mem: 9341
[13:53:18.641786] Epoch: [21]  [ 40/195]  eta: 0:01:24  lr: 0.001270  loss: 0.5986 (0.6056)  time: 0.5103  data: 0.0002  max mem: 9341
[13:53:28.856424] Epoch: [21]  [ 60/195]  eta: 0:01:11  lr: 0.001275  loss: 0.5893 (0.6023)  time: 0.5107  data: 0.0002  max mem: 9341
[13:53:39.114003] Epoch: [21]  [ 80/195]  eta: 0:01:00  lr: 0.001285  loss: 0.5974 (0.6006)  time: 0.5128  data: 0.0002  max mem: 9341
[13:53:49.322725] Epoch: [21]  [100/195]  eta: 0:00:49  lr: 0.001290  loss: 0.6079 (0.6025)  time: 0.5104  data: 0.0002  max mem: 9341
[13:53:59.529402] Epoch: [21]  [120/195]  eta: 0:00:39  lr: 0.001294  loss: 0.5925 (0.6021)  time: 0.5103  data: 0.0002  max mem: 9341
[13:54:09.759964] Epoch: [21]  [140/195]  eta: 0:00:28  lr: 0.001299  loss: 0.5804 (0.6008)  time: 0.5115  data: 0.0002  max mem: 9341
[13:54:20.054002] Epoch: [21]  [160/195]  eta: 0:00:18  lr: 0.001309  loss: 0.5890 (0.5996)  time: 0.5146  data: 0.0003  max mem: 9341
[13:54:30.238775] Epoch: [21]  [180/195]  eta: 0:00:07  lr: 0.001314  loss: 0.5850 (0.5983)  time: 0.5092  data: 0.0001  max mem: 9341
[13:54:37.386766] Epoch: [21]  [194/195]  eta: 0:00:00  lr: 0.001319  loss: 0.5812 (0.5967)  time: 0.5122  data: 0.0001  max mem: 9341
[13:54:37.552734] Epoch: [21] Total time: 0:01:41 (0.5192 s / it)
[13:54:37.558536] Averaged stats: lr: 0.001319  loss: 0.5812 (0.5943)
[13:54:42.218539] {"train_lr": 0.0012875692307692307, "train_loss": 0.5942957657269943, "epoch": 21}
[13:54:42.218879] [13:54:42.218964] Training epoch 21 for 0:01:45
[13:54:42.219017] [13:54:42.223476] log_dir: ./exp/debug/cifar100-LT/debug
[13:54:44.024731] Epoch: [22]  [  0/195]  eta: 0:05:50  lr: 0.001320  loss: 0.5523 (0.5523)  time: 1.7998  data: 1.2973  max mem: 9341
[13:54:54.235861] Epoch: [22]  [ 20/195]  eta: 0:01:40  lr: 0.001325  loss: 0.5752 (0.5646)  time: 0.5105  data: 0.0002  max mem: 9341
[13:55:04.441560] Epoch: [22]  [ 40/195]  eta: 0:01:23  lr: 0.001330  loss: 0.5812 (0.5724)  time: 0.5102  data: 0.0002  max mem: 9341
[13:55:14.648440] Epoch: [22]  [ 60/195]  eta: 0:01:11  lr: 0.001335  loss: 0.5900 (0.5788)  time: 0.5103  data: 0.0002  max mem: 9341
[13:55:24.897084] Epoch: [22]  [ 80/195]  eta: 0:01:00  lr: 0.001345  loss: 0.5854 (0.5805)  time: 0.5124  data: 0.0002  max mem: 9341
[13:55:35.105068] Epoch: [22]  [100/195]  eta: 0:00:49  lr: 0.001350  loss: 0.5831 (0.5822)  time: 0.5103  data: 0.0002  max mem: 9341
[13:55:45.310939] Epoch: [22]  [120/195]  eta: 0:00:39  lr: 0.001354  loss: 0.6007 (0.5861)  time: 0.5102  data: 0.0002  max mem: 9341
[13:55:55.519841] Epoch: [22]  [140/195]  eta: 0:00:28  lr: 0.001359  loss: 0.5657 (0.5834)  time: 0.5104  data: 0.0002  max mem: 9341
[13:56:05.769307] Epoch: [22]  [160/195]  eta: 0:00:18  lr: 0.001369  loss: 0.5973 (0.5847)  time: 0.5124  data: 0.0002  max mem: 9341
[13:56:15.930401] Epoch: [22]  [180/195]  eta: 0:00:07  lr: 0.001374  loss: 0.5928 (0.5857)  time: 0.5080  data: 0.0001  max mem: 9341
[13:56:23.055041] Epoch: [22]  [194/195]  eta: 0:00:00  lr: 0.001379  loss: 0.5939 (0.5856)  time: 0.5101  data: 0.0001  max mem: 9341
[13:56:23.233650] Epoch: [22] Total time: 0:01:41 (0.5180 s / it)
[13:56:23.234711] Averaged stats: lr: 0.001379  loss: 0.5939 (0.5854)
[13:56:27.947844] {"train_lr": 0.001347569230769231, "train_loss": 0.585384683081737, "epoch": 22}
[13:56:27.948198] [13:56:27.948286] Training epoch 22 for 0:01:45
[13:56:27.948340] [13:56:27.952836] log_dir: ./exp/debug/cifar100-LT/debug
[13:56:29.638788] Epoch: [23]  [  0/195]  eta: 0:05:28  lr: 0.001380  loss: 0.6046 (0.6046)  time: 1.6847  data: 1.1807  max mem: 9341
[13:56:39.868699] Epoch: [23]  [ 20/195]  eta: 0:01:39  lr: 0.001385  loss: 0.6004 (0.5869)  time: 0.5114  data: 0.0002  max mem: 9341
[13:56:50.080290] Epoch: [23]  [ 40/195]  eta: 0:01:23  lr: 0.001390  loss: 0.5758 (0.5835)  time: 0.5105  data: 0.0002  max mem: 9341
[13:57:00.289700] Epoch: [23]  [ 60/195]  eta: 0:01:11  lr: 0.001395  loss: 0.5782 (0.5849)  time: 0.5104  data: 0.0002  max mem: 9341
[13:57:10.544114] Epoch: [23]  [ 80/195]  eta: 0:01:00  lr: 0.001405  loss: 0.6110 (0.5904)  time: 0.5127  data: 0.0002  max mem: 9341
[13:57:20.748025] Epoch: [23]  [100/195]  eta: 0:00:49  lr: 0.001410  loss: 0.5933 (0.5914)  time: 0.5101  data: 0.0002  max mem: 9341
[13:57:30.949676] Epoch: [23]  [120/195]  eta: 0:00:39  lr: 0.001414  loss: 0.5993 (0.5928)  time: 0.5100  data: 0.0002  max mem: 9341
[13:57:41.150737] Epoch: [23]  [140/195]  eta: 0:00:28  lr: 0.001419  loss: 0.5959 (0.5920)  time: 0.5100  data: 0.0002  max mem: 9341
[13:57:51.394685] Epoch: [23]  [160/195]  eta: 0:00:18  lr: 0.001429  loss: 0.6097 (0.5933)  time: 0.5121  data: 0.0002  max mem: 9341
[13:58:01.554339] Epoch: [23]  [180/195]  eta: 0:00:07  lr: 0.001434  loss: 0.6052 (0.5947)  time: 0.5079  data: 0.0001  max mem: 9341
[13:58:08.674640] Epoch: [23]  [194/195]  eta: 0:00:00  lr: 0.001439  loss: 0.6026 (0.5949)  time: 0.5098  data: 0.0001  max mem: 9341
[13:58:08.841008] Epoch: [23] Total time: 0:01:40 (0.5174 s / it)
[13:58:08.848159] Averaged stats: lr: 0.001439  loss: 0.6026 (0.5938)
[13:58:13.519158] {"train_lr": 0.001407569230769232, "train_loss": 0.5938304266104332, "epoch": 23}
[13:58:13.519497] [13:58:13.519583] Training epoch 23 for 0:01:45
[13:58:13.519636] [13:58:13.524116] log_dir: ./exp/debug/cifar100-LT/debug
[13:58:15.277574] Epoch: [24]  [  0/195]  eta: 0:05:41  lr: 0.001440  loss: 0.5738 (0.5738)  time: 1.7522  data: 1.2574  max mem: 9341
[13:58:25.484589] Epoch: [24]  [ 20/195]  eta: 0:01:39  lr: 0.001445  loss: 0.5837 (0.5858)  time: 0.5103  data: 0.0002  max mem: 9341
[13:58:35.699827] Epoch: [24]  [ 40/195]  eta: 0:01:23  lr: 0.001450  loss: 0.5950 (0.5897)  time: 0.5107  data: 0.0002  max mem: 9341
[13:58:45.907640] Epoch: [24]  [ 60/195]  eta: 0:01:11  lr: 0.001455  loss: 0.5941 (0.5928)  time: 0.5103  data: 0.0002  max mem: 9341
[13:58:56.159307] Epoch: [24]  [ 80/195]  eta: 0:01:00  lr: 0.001465  loss: 0.5779 (0.5900)  time: 0.5125  data: 0.0002  max mem: 9341
[13:59:06.377971] Epoch: [24]  [100/195]  eta: 0:00:49  lr: 0.001470  loss: 0.5796 (0.5881)  time: 0.5109  data: 0.0002  max mem: 9341
[13:59:16.582541] Epoch: [24]  [120/195]  eta: 0:00:39  lr: 0.001474  loss: 0.5938 (0.5895)  time: 0.5102  data: 0.0002  max mem: 9341
[13:59:26.791112] Epoch: [24]  [140/195]  eta: 0:00:28  lr: 0.001479  loss: 0.5749 (0.5871)  time: 0.5104  data: 0.0002  max mem: 9341
[13:59:37.062670] Epoch: [24]  [160/195]  eta: 0:00:18  lr: 0.001489  loss: 0.5689 (0.5855)  time: 0.5135  data: 0.0002  max mem: 9341
[13:59:47.228783] Epoch: [24]  [180/195]  eta: 0:00:07  lr: 0.001494  loss: 0.5781 (0.5851)  time: 0.5083  data: 0.0001  max mem: 9341
[13:59:54.355097] Epoch: [24]  [194/195]  eta: 0:00:00  lr: 0.001499  loss: 0.5781 (0.5842)  time: 0.5104  data: 0.0001  max mem: 9341
[13:59:54.534747] Epoch: [24] Total time: 0:01:41 (0.5180 s / it)
[13:59:54.548582] Averaged stats: lr: 0.001499  loss: 0.5781 (0.5870)
[13:59:59.241850] {"train_lr": 0.0014675692307692324, "train_loss": 0.5870397998736455, "epoch": 24}
[13:59:59.242122] [13:59:59.242210] Training epoch 24 for 0:01:45
[13:59:59.242264] [13:59:59.246684] log_dir: ./exp/debug/cifar100-LT/debug
[14:00:00.916198] Epoch: [25]  [  0/195]  eta: 0:05:25  lr: 0.001500  loss: 0.5821 (0.5821)  time: 1.6683  data: 1.1592  max mem: 9341
[14:00:11.162802] Epoch: [25]  [ 20/195]  eta: 0:01:39  lr: 0.001505  loss: 0.5854 (0.5895)  time: 0.5123  data: 0.0002  max mem: 9341
[14:00:21.368987] Epoch: [25]  [ 40/195]  eta: 0:01:23  lr: 0.001510  loss: 0.5923 (0.5922)  time: 0.5102  data: 0.0002  max mem: 9341
[14:00:31.601740] Epoch: [25]  [ 60/195]  eta: 0:01:11  lr: 0.001515  loss: 0.5817 (0.5911)  time: 0.5116  data: 0.0002  max mem: 9341
[14:00:41.891770] Epoch: [25]  [ 80/195]  eta: 0:01:00  lr: 0.001525  loss: 0.6017 (0.5923)  time: 0.5144  data: 0.0002  max mem: 9341
[14:00:52.104033] Epoch: [25]  [100/195]  eta: 0:00:49  lr: 0.001530  loss: 0.5904 (0.5916)  time: 0.5106  data: 0.0002  max mem: 9341
[14:01:02.306487] Epoch: [25]  [120/195]  eta: 0:00:39  lr: 0.001534  loss: 0.5795 (0.5886)  time: 0.5101  data: 0.0002  max mem: 9341
[14:01:12.510337] Epoch: [25]  [140/195]  eta: 0:00:28  lr: 0.001539  loss: 0.5704 (0.5863)  time: 0.5101  data: 0.0002  max mem: 9341
[14:01:22.756746] Epoch: [25]  [160/195]  eta: 0:00:18  lr: 0.001549  loss: 0.5795 (0.5858)  time: 0.5123  data: 0.0002  max mem: 9341
[14:01:32.919406] Epoch: [25]  [180/195]  eta: 0:00:07  lr: 0.001554  loss: 0.5857 (0.5858)  time: 0.5081  data: 0.0001  max mem: 9341
[14:01:40.044816] Epoch: [25]  [194/195]  eta: 0:00:00  lr: 0.001559  loss: 0.5857 (0.5858)  time: 0.5102  data: 0.0001  max mem: 9341
[14:01:40.211390] Epoch: [25] Total time: 0:01:40 (0.5178 s / it)
[14:01:40.219566] Averaged stats: lr: 0.001559  loss: 0.5857 (0.5856)
[14:01:44.882826] {"train_lr": 0.0015275692307692333, "train_loss": 0.5856282036273908, "epoch": 25}
[14:01:44.883074] [14:01:44.883158] Training epoch 25 for 0:01:45
[14:01:44.883224] [14:01:44.887742] log_dir: ./exp/debug/cifar100-LT/debug
[14:01:46.431726] Epoch: [26]  [  0/195]  eta: 0:05:00  lr: 0.001560  loss: 0.5733 (0.5733)  time: 1.5429  data: 1.0421  max mem: 9341
[14:01:56.640276] Epoch: [26]  [ 20/195]  eta: 0:01:37  lr: 0.001565  loss: 0.5778 (0.5794)  time: 0.5104  data: 0.0002  max mem: 9341
[14:02:06.850730] Epoch: [26]  [ 40/195]  eta: 0:01:23  lr: 0.001570  loss: 0.6132 (0.5972)  time: 0.5105  data: 0.0001  max mem: 9341
[14:02:17.065871] Epoch: [26]  [ 60/195]  eta: 0:01:11  lr: 0.001575  loss: 0.6037 (0.5995)  time: 0.5107  data: 0.0002  max mem: 9341
[14:02:27.322923] Epoch: [26]  [ 80/195]  eta: 0:01:00  lr: 0.001585  loss: 0.5843 (0.5966)  time: 0.5128  data: 0.0002  max mem: 9341
[14:02:37.523912] Epoch: [26]  [100/195]  eta: 0:00:49  lr: 0.001590  loss: 0.5852 (0.5941)  time: 0.5100  data: 0.0002  max mem: 9341
[14:02:47.731726] Epoch: [26]  [120/195]  eta: 0:00:38  lr: 0.001594  loss: 0.5799 (0.5916)  time: 0.5103  data: 0.0002  max mem: 9341
[14:02:57.939445] Epoch: [26]  [140/195]  eta: 0:00:28  lr: 0.001599  loss: 0.5697 (0.5896)  time: 0.5103  data: 0.0002  max mem: 9341
[14:03:08.190594] Epoch: [26]  [160/195]  eta: 0:00:18  lr: 0.001609  loss: 0.5682 (0.5877)  time: 0.5125  data: 0.0002  max mem: 9341
[14:03:18.353559] Epoch: [26]  [180/195]  eta: 0:00:07  lr: 0.001614  loss: 0.5739 (0.5876)  time: 0.5081  data: 0.0001  max mem: 9341
[14:03:25.476092] Epoch: [26]  [194/195]  eta: 0:00:00  lr: 0.001619  loss: 0.5761 (0.5872)  time: 0.5100  data: 0.0001  max mem: 9341
[14:03:25.641836] Epoch: [26] Total time: 0:01:40 (0.5167 s / it)
[14:03:25.653818] Averaged stats: lr: 0.001619  loss: 0.5761 (0.5903)
[14:03:30.333643] {"train_lr": 0.0015875692307692252, "train_loss": 0.5902932805128587, "epoch": 26}
[14:03:30.333910] [14:03:30.333992] Training epoch 26 for 0:01:45
[14:03:30.334045] [14:03:30.338474] log_dir: ./exp/debug/cifar100-LT/debug
[14:03:32.105603] Epoch: [27]  [  0/195]  eta: 0:05:44  lr: 0.001620  loss: 0.5613 (0.5613)  time: 1.7657  data: 1.2637  max mem: 9341
[14:03:42.327050] Epoch: [27]  [ 20/195]  eta: 0:01:39  lr: 0.001625  loss: 0.5688 (0.5663)  time: 0.5110  data: 0.0002  max mem: 9341
[14:03:52.537042] Epoch: [27]  [ 40/195]  eta: 0:01:23  lr: 0.001630  loss: 0.5708 (0.5708)  time: 0.5104  data: 0.0002  max mem: 9341
[14:04:02.753974] Epoch: [27]  [ 60/195]  eta: 0:01:11  lr: 0.001635  loss: 0.5714 (0.5729)  time: 0.5108  data: 0.0002  max mem: 9341
[14:04:13.015466] Epoch: [27]  [ 80/195]  eta: 0:01:00  lr: 0.001645  loss: 0.5755 (0.5725)  time: 0.5130  data: 0.0002  max mem: 9341
[14:04:23.229253] Epoch: [27]  [100/195]  eta: 0:00:49  lr: 0.001650  loss: 0.5649 (0.5715)  time: 0.5106  data: 0.0002  max mem: 9341
[14:04:33.436812] Epoch: [27]  [120/195]  eta: 0:00:39  lr: 0.001654  loss: 0.5685 (0.5708)  time: 0.5103  data: 0.0002  max mem: 9341
[14:04:43.647433] Epoch: [27]  [140/195]  eta: 0:00:28  lr: 0.001659  loss: 0.5691 (0.5705)  time: 0.5105  data: 0.0002  max mem: 9341
[14:04:53.900875] Epoch: [27]  [160/195]  eta: 0:00:18  lr: 0.001669  loss: 0.5555 (0.5693)  time: 0.5126  data: 0.0002  max mem: 9341
[14:05:04.074541] Epoch: [27]  [180/195]  eta: 0:00:07  lr: 0.001674  loss: 0.5578 (0.5683)  time: 0.5086  data: 0.0001  max mem: 9341
[14:05:11.201751] Epoch: [27]  [194/195]  eta: 0:00:00  lr: 0.001679  loss: 0.5507 (0.5673)  time: 0.5104  data: 0.0001  max mem: 9341
[14:05:11.365313] Epoch: [27] Total time: 0:01:41 (0.5181 s / it)
[14:05:11.373216] Averaged stats: lr: 0.001679  loss: 0.5507 (0.5667)
[14:05:16.059552] {"train_lr": 0.0016475692307692288, "train_loss": 0.5667327236670714, "epoch": 27}
[14:05:16.059809] [14:05:16.059902] Training epoch 27 for 0:01:45
[14:05:16.060009] [14:05:16.064498] log_dir: ./exp/debug/cifar100-LT/debug
[14:05:17.853029] Epoch: [28]  [  0/195]  eta: 0:05:48  lr: 0.001680  loss: 0.5567 (0.5567)  time: 1.7878  data: 1.2885  max mem: 9341
[14:05:28.083668] Epoch: [28]  [ 20/195]  eta: 0:01:40  lr: 0.001685  loss: 0.5524 (0.5594)  time: 0.5115  data: 0.0002  max mem: 9341
[14:05:38.298890] Epoch: [28]  [ 40/195]  eta: 0:01:24  lr: 0.001690  loss: 0.5549 (0.5603)  time: 0.5107  data: 0.0002  max mem: 9341
[14:05:48.510021] Epoch: [28]  [ 60/195]  eta: 0:01:11  lr: 0.001695  loss: 0.5784 (0.5648)  time: 0.5105  data: 0.0002  max mem: 9341
[14:05:58.756462] Epoch: [28]  [ 80/195]  eta: 0:01:00  lr: 0.001705  loss: 0.5693 (0.5665)  time: 0.5123  data: 0.0002  max mem: 9341
[14:06:08.966305] Epoch: [28]  [100/195]  eta: 0:00:49  lr: 0.001710  loss: 0.5841 (0.5698)  time: 0.5104  data: 0.0002  max mem: 9341
[14:06:19.174946] Epoch: [28]  [120/195]  eta: 0:00:39  lr: 0.001714  loss: 0.5867 (0.5736)  time: 0.5104  data: 0.0002  max mem: 9341
[14:06:29.384547] Epoch: [28]  [140/195]  eta: 0:00:28  lr: 0.001719  loss: 0.5951 (0.5765)  time: 0.5104  data: 0.0002  max mem: 9341
[14:06:39.645033] Epoch: [28]  [160/195]  eta: 0:00:18  lr: 0.001729  loss: 0.6004 (0.5791)  time: 0.5130  data: 0.0002  max mem: 9341
[14:06:49.815799] Epoch: [28]  [180/195]  eta: 0:00:07  lr: 0.001734  loss: 0.6100 (0.5821)  time: 0.5085  data: 0.0001  max mem: 9341
[14:06:56.947256] Epoch: [28]  [194/195]  eta: 0:00:00  lr: 0.001739  loss: 0.5829 (0.5824)  time: 0.5105  data: 0.0001  max mem: 9341
[14:06:57.137886] Epoch: [28] Total time: 0:01:41 (0.5183 s / it)
[14:06:57.138605] Averaged stats: lr: 0.001739  loss: 0.5829 (0.5845)
[14:07:01.803585] {"train_lr": 0.001707569230769228, "train_loss": 0.5844611266102546, "epoch": 28}
[14:07:01.803842] [14:07:01.803927] Training epoch 28 for 0:01:45
[14:07:01.803980] [14:07:01.808481] log_dir: ./exp/debug/cifar100-LT/debug
[14:07:03.521447] Epoch: [29]  [  0/195]  eta: 0:05:33  lr: 0.001740  loss: 0.5915 (0.5915)  time: 1.7119  data: 1.2176  max mem: 9341
[14:07:13.784316] Epoch: [29]  [ 20/195]  eta: 0:01:39  lr: 0.001745  loss: 0.5969 (0.6026)  time: 0.5131  data: 0.0002  max mem: 9341
[14:07:23.997149] Epoch: [29]  [ 40/195]  eta: 0:01:23  lr: 0.001750  loss: 0.5729 (0.5920)  time: 0.5106  data: 0.0002  max mem: 9341
[14:07:34.208165] Epoch: [29]  [ 60/195]  eta: 0:01:11  lr: 0.001755  loss: 0.5913 (0.5910)  time: 0.5105  data: 0.0002  max mem: 9341
[14:07:44.461702] Epoch: [29]  [ 80/195]  eta: 0:01:00  lr: 0.001765  loss: 0.5838 (0.5885)  time: 0.5126  data: 0.0002  max mem: 9341
[14:07:54.672221] Epoch: [29]  [100/195]  eta: 0:00:49  lr: 0.001770  loss: 0.5789 (0.5867)  time: 0.5105  data: 0.0002  max mem: 9341
[14:08:04.884437] Epoch: [29]  [120/195]  eta: 0:00:39  lr: 0.001774  loss: 0.5778 (0.5851)  time: 0.5106  data: 0.0002  max mem: 9341
[14:08:15.096068] Epoch: [29]  [140/195]  eta: 0:00:28  lr: 0.001779  loss: 0.5794 (0.5842)  time: 0.5105  data: 0.0002  max mem: 9341
[14:08:25.351178] Epoch: [29]  [160/195]  eta: 0:00:18  lr: 0.001789  loss: 0.5742 (0.5821)  time: 0.5127  data: 0.0002  max mem: 9341
[14:08:35.524803] Epoch: [29]  [180/195]  eta: 0:00:07  lr: 0.001794  loss: 0.5589 (0.5790)  time: 0.5086  data: 0.0001  max mem: 9341
[14:08:42.664595] Epoch: [29]  [194/195]  eta: 0:00:00  lr: 0.001799  loss: 0.5614 (0.5786)  time: 0.5111  data: 0.0001  max mem: 9341
[14:08:42.848567] Epoch: [29] Total time: 0:01:41 (0.5182 s / it)
[14:08:42.865765] Averaged stats: lr: 0.001799  loss: 0.5614 (0.5768)
[14:08:47.514204] {"train_lr": 0.0017675692307692295, "train_loss": 0.576842383085153, "epoch": 29}
[14:08:47.514477] [14:08:47.514561] Training epoch 29 for 0:01:45
[14:08:47.514615] [14:08:47.519124] log_dir: ./exp/debug/cifar100-LT/debug
[14:08:49.333926] Epoch: [30]  [  0/195]  eta: 0:05:53  lr: 0.001800  loss: 0.5764 (0.5764)  time: 1.8139  data: 1.3055  max mem: 9341
[14:08:59.547828] Epoch: [30]  [ 20/195]  eta: 0:01:40  lr: 0.001805  loss: 0.5727 (0.5651)  time: 0.5106  data: 0.0002  max mem: 9341
[14:09:09.769511] Epoch: [30]  [ 40/195]  eta: 0:01:24  lr: 0.001810  loss: 0.5699 (0.5649)  time: 0.5110  data: 0.0002  max mem: 9341
[14:09:19.982230] Epoch: [30]  [ 60/195]  eta: 0:01:11  lr: 0.001815  loss: 0.5555 (0.5613)  time: 0.5106  data: 0.0002  max mem: 9341
[14:09:30.241306] Epoch: [30]  [ 80/195]  eta: 0:01:00  lr: 0.001825  loss: 0.5653 (0.5638)  time: 0.5129  data: 0.0002  max mem: 9341
[14:09:40.455587] Epoch: [30]  [100/195]  eta: 0:00:49  lr: 0.001830  loss: 0.5665 (0.5644)  time: 0.5107  data: 0.0002  max mem: 9341
[14:09:50.672684] Epoch: [30]  [120/195]  eta: 0:00:39  lr: 0.001834  loss: 0.5703 (0.5654)  time: 0.5108  data: 0.0002  max mem: 9341
[14:10:00.885585] Epoch: [30]  [140/195]  eta: 0:00:28  lr: 0.001839  loss: 0.5649 (0.5644)  time: 0.5106  data: 0.0002  max mem: 9341
[14:10:11.145306] Epoch: [30]  [160/195]  eta: 0:00:18  lr: 0.001849  loss: 0.5790 (0.5664)  time: 0.5129  data: 0.0002  max mem: 9341
[14:10:21.323195] Epoch: [30]  [180/195]  eta: 0:00:07  lr: 0.001854  loss: 0.5781 (0.5686)  time: 0.5088  data: 0.0001  max mem: 9341
[14:10:28.459839] Epoch: [30]  [194/195]  eta: 0:00:00  lr: 0.001859  loss: 0.5658 (0.5697)  time: 0.5109  data: 0.0001  max mem: 9341
[14:10:28.632188] Epoch: [30] Total time: 0:01:41 (0.5185 s / it)
[14:10:28.643636] Averaged stats: lr: 0.001859  loss: 0.5658 (0.5679)
[14:10:33.500712] {"train_lr": 0.0018275692307692312, "train_loss": 0.5679381406460053, "epoch": 30}
[14:10:33.501029] [14:10:33.501121] Training epoch 30 for 0:01:45
[14:10:33.501176] [14:10:33.506575] log_dir: ./exp/debug/cifar100-LT/debug
[14:10:35.265531] Epoch: [31]  [  0/195]  eta: 0:05:42  lr: 0.001860  loss: 0.5951 (0.5951)  time: 1.7576  data: 1.2557  max mem: 9341
[14:10:45.482125] Epoch: [31]  [ 20/195]  eta: 0:01:39  lr: 0.001865  loss: 0.5705 (0.5771)  time: 0.5107  data: 0.0002  max mem: 9341
[14:10:55.692352] Epoch: [31]  [ 40/195]  eta: 0:01:23  lr: 0.001870  loss: 0.5689 (0.5728)  time: 0.5105  data: 0.0002  max mem: 9341
[14:11:05.906135] Epoch: [31]  [ 60/195]  eta: 0:01:11  lr: 0.001875  loss: 0.5680 (0.5705)  time: 0.5106  data: 0.0002  max mem: 9341
[14:11:16.188245] Epoch: [31]  [ 80/195]  eta: 0:01:00  lr: 0.001885  loss: 0.5579 (0.5686)  time: 0.5140  data: 0.0002  max mem: 9341
[14:11:26.392074] Epoch: [31]  [100/195]  eta: 0:00:49  lr: 0.001890  loss: 0.5607 (0.5688)  time: 0.5101  data: 0.0002  max mem: 9341
[14:11:36.599559] Epoch: [31]  [120/195]  eta: 0:00:39  lr: 0.001894  loss: 0.5669 (0.5694)  time: 0.5103  data: 0.0002  max mem: 9341
[14:11:46.806287] Epoch: [31]  [140/195]  eta: 0:00:28  lr: 0.001899  loss: 0.5793 (0.5710)  time: 0.5103  data: 0.0002  max mem: 9341
[14:11:57.058956] Epoch: [31]  [160/195]  eta: 0:00:18  lr: 0.001909  loss: 0.5758 (0.5715)  time: 0.5126  data: 0.0002  max mem: 9341
[14:12:07.218676] Epoch: [31]  [180/195]  eta: 0:00:07  lr: 0.001914  loss: 0.5676 (0.5713)  time: 0.5079  data: 0.0001  max mem: 9341
[14:12:14.345336] Epoch: [31]  [194/195]  eta: 0:00:00  lr: 0.001919  loss: 0.5609 (0.5709)  time: 0.5101  data: 0.0001  max mem: 9341
[14:12:14.523008] Epoch: [31] Total time: 0:01:41 (0.5180 s / it)
[14:12:14.529071] Averaged stats: lr: 0.001919  loss: 0.5609 (0.5718)
[14:12:19.230835] {"train_lr": 0.0018875692307692325, "train_loss": 0.5717693984508514, "epoch": 31}
[14:12:19.231099] [14:12:19.231185] Training epoch 31 for 0:01:45
[14:12:19.231239] [14:12:19.235705] log_dir: ./exp/debug/cifar100-LT/debug
[14:12:21.005690] Epoch: [32]  [  0/195]  eta: 0:05:44  lr: 0.001920  loss: 0.5521 (0.5521)  time: 1.7687  data: 1.2478  max mem: 9341
[14:12:31.241365] Epoch: [32]  [ 20/195]  eta: 0:01:40  lr: 0.001925  loss: 0.5631 (0.5623)  time: 0.5117  data: 0.0002  max mem: 9341
[14:12:41.460837] Epoch: [32]  [ 40/195]  eta: 0:01:24  lr: 0.001930  loss: 0.5625 (0.5595)  time: 0.5109  data: 0.0002  max mem: 9341
[14:12:51.680487] Epoch: [32]  [ 60/195]  eta: 0:01:11  lr: 0.001935  loss: 0.5522 (0.5585)  time: 0.5109  data: 0.0002  max mem: 9341
[14:13:01.940688] Epoch: [32]  [ 80/195]  eta: 0:01:00  lr: 0.001945  loss: 0.5704 (0.5601)  time: 0.5129  data: 0.0002  max mem: 9341
[14:13:12.157592] Epoch: [32]  [100/195]  eta: 0:00:49  lr: 0.001950  loss: 0.5569 (0.5587)  time: 0.5108  data: 0.0002  max mem: 9341
[14:13:22.374652] Epoch: [32]  [120/195]  eta: 0:00:39  lr: 0.001954  loss: 0.5495 (0.5585)  time: 0.5108  data: 0.0002  max mem: 9341
[14:13:32.587478] Epoch: [32]  [140/195]  eta: 0:00:28  lr: 0.001959  loss: 0.5691 (0.5595)  time: 0.5106  data: 0.0002  max mem: 9341
[14:13:42.844561] Epoch: [32]  [160/195]  eta: 0:00:18  lr: 0.001969  loss: 0.5555 (0.5598)  time: 0.5128  data: 0.0002  max mem: 9341
[14:13:53.016850] Epoch: [32]  [180/195]  eta: 0:00:07  lr: 0.001974  loss: 0.5594 (0.5599)  time: 0.5086  data: 0.0001  max mem: 9341
[14:14:00.150109] Epoch: [32]  [194/195]  eta: 0:00:00  lr: 0.001979  loss: 0.5922 (0.5632)  time: 0.5108  data: 0.0001  max mem: 9341
[14:14:00.319219] Epoch: [32] Total time: 0:01:41 (0.5184 s / it)
[14:14:00.321158] Averaged stats: lr: 0.001979  loss: 0.5922 (0.5641)
[14:14:04.999653] {"train_lr": 0.0019475692307692354, "train_loss": 0.564126431521697, "epoch": 32}
[14:14:04.999979] [14:14:05.000065] Training epoch 32 for 0:01:45
[14:14:05.000145] [14:14:05.004651] log_dir: ./exp/debug/cifar100-LT/debug
[14:14:06.548974] Epoch: [33]  [  0/195]  eta: 0:05:00  lr: 0.001980  loss: 0.5846 (0.5846)  time: 1.5430  data: 1.0400  max mem: 9341
[14:14:16.764521] Epoch: [33]  [ 20/195]  eta: 0:01:37  lr: 0.001985  loss: 0.5594 (0.5646)  time: 0.5107  data: 0.0002  max mem: 9341
[14:14:26.984609] Epoch: [33]  [ 40/195]  eta: 0:01:23  lr: 0.001990  loss: 0.5600 (0.5685)  time: 0.5109  data: 0.0002  max mem: 9341
[14:14:37.210999] Epoch: [33]  [ 60/195]  eta: 0:01:11  lr: 0.001995  loss: 0.5803 (0.5716)  time: 0.5112  data: 0.0002  max mem: 9341
[14:14:47.470901] Epoch: [33]  [ 80/195]  eta: 0:01:00  lr: 0.002005  loss: 0.5635 (0.5699)  time: 0.5129  data: 0.0002  max mem: 9341
[14:14:57.686474] Epoch: [33]  [100/195]  eta: 0:00:49  lr: 0.002010  loss: 0.5526 (0.5666)  time: 0.5107  data: 0.0002  max mem: 9341
[14:15:07.906827] Epoch: [33]  [120/195]  eta: 0:00:38  lr: 0.002014  loss: 0.5717 (0.5657)  time: 0.5110  data: 0.0002  max mem: 9341
[14:15:18.118602] Epoch: [33]  [140/195]  eta: 0:00:28  lr: 0.002019  loss: 0.5631 (0.5664)  time: 0.5105  data: 0.0002  max mem: 9341
[14:15:28.380818] Epoch: [33]  [160/195]  eta: 0:00:18  lr: 0.002029  loss: 0.5573 (0.5656)  time: 0.5131  data: 0.0002  max mem: 9341
[14:15:38.555475] Epoch: [33]  [180/195]  eta: 0:00:07  lr: 0.002034  loss: 0.5678 (0.5652)  time: 0.5087  data: 0.0001  max mem: 9341
[14:15:45.687529] Epoch: [33]  [194/195]  eta: 0:00:00  lr: 0.002039  loss: 0.5590 (0.5651)  time: 0.5106  data: 0.0001  max mem: 9341
[14:15:45.877569] Epoch: [33] Total time: 0:01:40 (0.5173 s / it)
[14:15:45.878400] Averaged stats: lr: 0.002039  loss: 0.5590 (0.5622)
[14:15:50.552278] {"train_lr": 0.0020075692307692317, "train_loss": 0.5621626548660107, "epoch": 33}
[14:15:50.552608] [14:15:50.552693] Training epoch 33 for 0:01:45
[14:15:50.552745] [14:15:50.557221] log_dir: ./exp/debug/cifar100-LT/debug
[14:15:52.297726] Epoch: [34]  [  0/195]  eta: 0:05:38  lr: 0.002040  loss: 0.5624 (0.5624)  time: 1.7381  data: 1.2294  max mem: 9341
[14:16:02.534801] Epoch: [34]  [ 20/195]  eta: 0:01:39  lr: 0.002045  loss: 0.5635 (0.5611)  time: 0.5118  data: 0.0002  max mem: 9341
[14:16:12.757096] Epoch: [34]  [ 40/195]  eta: 0:01:23  lr: 0.002050  loss: 0.5576 (0.5606)  time: 0.5110  data: 0.0002  max mem: 9341
[14:16:22.971829] Epoch: [34]  [ 60/195]  eta: 0:01:11  lr: 0.002055  loss: 0.5473 (0.5597)  time: 0.5107  data: 0.0002  max mem: 9341
[14:16:33.276639] Epoch: [34]  [ 80/195]  eta: 0:01:00  lr: 0.002065  loss: 0.5387 (0.5556)  time: 0.5152  data: 0.0002  max mem: 9341
[14:16:43.506072] Epoch: [34]  [100/195]  eta: 0:00:49  lr: 0.002070  loss: 0.5503 (0.5558)  time: 0.5114  data: 0.0002  max mem: 9341
[14:16:53.735255] Epoch: [34]  [120/195]  eta: 0:00:39  lr: 0.002074  loss: 0.5458 (0.5545)  time: 0.5114  data: 0.0002  max mem: 9341
[14:17:03.969634] Epoch: [34]  [140/195]  eta: 0:00:28  lr: 0.002079  loss: 0.5621 (0.5559)  time: 0.5116  data: 0.0002  max mem: 9341
[14:17:14.269128] Epoch: [34]  [160/195]  eta: 0:00:18  lr: 0.002089  loss: 0.5427 (0.5555)  time: 0.5149  data: 0.0002  max mem: 9341
[14:17:24.459243] Epoch: [34]  [180/195]  eta: 0:00:07  lr: 0.002094  loss: 0.5715 (0.5569)  time: 0.5095  data: 0.0002  max mem: 9341
[14:17:31.603194] Epoch: [34]  [194/195]  eta: 0:00:00  lr: 0.002099  loss: 0.5854 (0.5591)  time: 0.5122  data: 0.0001  max mem: 9341
[14:17:31.770146] Epoch: [34] Total time: 0:01:41 (0.5190 s / it)
[14:17:31.810317] Averaged stats: lr: 0.002099  loss: 0.5854 (0.5574)
[14:17:36.533440] {"train_lr": 0.002067569230769233, "train_loss": 0.5574184291255779, "epoch": 34}
[14:17:36.533741] [14:17:36.533825] Training epoch 34 for 0:01:45
[14:17:36.533943] [14:17:36.538509] log_dir: ./exp/debug/cifar100-LT/debug
[14:17:38.163488] Epoch: [35]  [  0/195]  eta: 0:05:16  lr: 0.002100  loss: 0.5613 (0.5613)  time: 1.6234  data: 1.1096  max mem: 9341
[14:17:48.379225] Epoch: [35]  [ 20/195]  eta: 0:01:38  lr: 0.002105  loss: 0.6069 (0.6070)  time: 0.5107  data: 0.0002  max mem: 9341
[14:17:58.590055] Epoch: [35]  [ 40/195]  eta: 0:01:23  lr: 0.002110  loss: 0.6025 (0.6057)  time: 0.5105  data: 0.0002  max mem: 9341
[14:18:08.804803] Epoch: [35]  [ 60/195]  eta: 0:01:11  lr: 0.002115  loss: 0.5963 (0.6026)  time: 0.5107  data: 0.0002  max mem: 9341
[14:18:19.060420] Epoch: [35]  [ 80/195]  eta: 0:01:00  lr: 0.002125  loss: 0.5699 (0.5961)  time: 0.5127  data: 0.0002  max mem: 9341
[14:18:29.274664] Epoch: [35]  [100/195]  eta: 0:00:49  lr: 0.002130  loss: 0.5869 (0.5954)  time: 0.5107  data: 0.0002  max mem: 9341
[14:18:39.486035] Epoch: [35]  [120/195]  eta: 0:00:39  lr: 0.002134  loss: 0.5858 (0.5928)  time: 0.5105  data: 0.0002  max mem: 9341
[14:18:49.696641] Epoch: [35]  [140/195]  eta: 0:00:28  lr: 0.002139  loss: 0.5609 (0.5892)  time: 0.5105  data: 0.0002  max mem: 9341
[14:18:59.949452] Epoch: [35]  [160/195]  eta: 0:00:18  lr: 0.002149  loss: 0.5783 (0.5873)  time: 0.5126  data: 0.0002  max mem: 9341
[14:19:10.117241] Epoch: [35]  [180/195]  eta: 0:00:07  lr: 0.002154  loss: 0.5557 (0.5849)  time: 0.5083  data: 0.0001  max mem: 9341
[14:19:17.245734] Epoch: [35]  [194/195]  eta: 0:00:00  lr: 0.002159  loss: 0.5526 (0.5836)  time: 0.5102  data: 0.0001  max mem: 9341
[14:19:17.420806] Epoch: [35] Total time: 0:01:40 (0.5173 s / it)
[14:19:17.429602] Averaged stats: lr: 0.002159  loss: 0.5526 (0.5852)
[14:19:22.150211] {"train_lr": 0.002127569230769229, "train_loss": 0.5851558280678896, "epoch": 35}
[14:19:22.150539] [14:19:22.150629] Training epoch 35 for 0:01:45
[14:19:22.150684] [14:19:22.155219] log_dir: ./exp/debug/cifar100-LT/debug
[14:19:23.826529] Epoch: [36]  [  0/195]  eta: 0:05:25  lr: 0.002160  loss: 0.5850 (0.5850)  time: 1.6699  data: 1.1641  max mem: 9341
[14:19:34.037634] Epoch: [36]  [ 20/195]  eta: 0:01:38  lr: 0.002165  loss: 0.5659 (0.5744)  time: 0.5105  data: 0.0003  max mem: 9341
[14:19:44.245427] Epoch: [36]  [ 40/195]  eta: 0:01:23  lr: 0.002170  loss: 0.5735 (0.5757)  time: 0.5103  data: 0.0002  max mem: 9341
[14:19:54.466196] Epoch: [36]  [ 60/195]  eta: 0:01:11  lr: 0.002175  loss: 0.5599 (0.5755)  time: 0.5110  data: 0.0002  max mem: 9341
[14:20:04.720782] Epoch: [36]  [ 80/195]  eta: 0:01:00  lr: 0.002185  loss: 0.5667 (0.5734)  time: 0.5126  data: 0.0002  max mem: 9341
[14:20:14.924714] Epoch: [36]  [100/195]  eta: 0:00:49  lr: 0.002190  loss: 0.5524 (0.5701)  time: 0.5101  data: 0.0002  max mem: 9341
[14:20:25.135166] Epoch: [36]  [120/195]  eta: 0:00:39  lr: 0.002194  loss: 0.5608 (0.5685)  time: 0.5105  data: 0.0002  max mem: 9341
[14:20:35.343293] Epoch: [36]  [140/195]  eta: 0:00:28  lr: 0.002199  loss: 0.5555 (0.5674)  time: 0.5104  data: 0.0002  max mem: 9341
[14:20:45.599323] Epoch: [36]  [160/195]  eta: 0:00:18  lr: 0.002209  loss: 0.5631 (0.5672)  time: 0.5127  data: 0.0002  max mem: 9341
[14:20:55.770818] Epoch: [36]  [180/195]  eta: 0:00:07  lr: 0.002214  loss: 0.5476 (0.5652)  time: 0.5085  data: 0.0001  max mem: 9341
[14:21:02.897951] Epoch: [36]  [194/195]  eta: 0:00:00  lr: 0.002219  loss: 0.5476 (0.5643)  time: 0.5102  data: 0.0001  max mem: 9341
[14:21:03.067111] Epoch: [36] Total time: 0:01:40 (0.5175 s / it)
[14:21:03.084185] Averaged stats: lr: 0.002219  loss: 0.5476 (0.5630)
[14:21:07.792755] {"train_lr": 0.0021875692307692274, "train_loss": 0.5629725831059309, "epoch": 36}
[14:21:07.793090] [14:21:07.793188] Training epoch 36 for 0:01:45
[14:21:07.793243] [14:21:07.797736] log_dir: ./exp/debug/cifar100-LT/debug
[14:21:09.635907] Epoch: [37]  [  0/195]  eta: 0:05:58  lr: 0.002220  loss: 0.5532 (0.5532)  time: 1.8369  data: 1.3231  max mem: 9341
[14:21:19.841352] Epoch: [37]  [ 20/195]  eta: 0:01:40  lr: 0.002225  loss: 0.5561 (0.5516)  time: 0.5102  data: 0.0002  max mem: 9341
[14:21:30.048768] Epoch: [37]  [ 40/195]  eta: 0:01:24  lr: 0.002230  loss: 0.5522 (0.5539)  time: 0.5103  data: 0.0002  max mem: 9341
[14:21:40.260155] Epoch: [37]  [ 60/195]  eta: 0:01:11  lr: 0.002235  loss: 0.5555 (0.5530)  time: 0.5105  data: 0.0002  max mem: 9341
[14:21:50.517424] Epoch: [37]  [ 80/195]  eta: 0:01:00  lr: 0.002245  loss: 0.5479 (0.5516)  time: 0.5128  data: 0.0002  max mem: 9341
[14:22:00.720571] Epoch: [37]  [100/195]  eta: 0:00:49  lr: 0.002250  loss: 0.5514 (0.5519)  time: 0.5101  data: 0.0002  max mem: 9341
[14:22:10.927316] Epoch: [37]  [120/195]  eta: 0:00:39  lr: 0.002254  loss: 0.5561 (0.5536)  time: 0.5103  data: 0.0002  max mem: 9341
[14:22:21.134635] Epoch: [37]  [140/195]  eta: 0:00:28  lr: 0.002259  loss: 0.5432 (0.5530)  time: 0.5102  data: 0.0002  max mem: 9341
[14:22:31.385186] Epoch: [37]  [160/195]  eta: 0:00:18  lr: 0.002269  loss: 0.5466 (0.5529)  time: 0.5124  data: 0.0002  max mem: 9341
[14:22:41.549930] Epoch: [37]  [180/195]  eta: 0:00:07  lr: 0.002274  loss: 0.5408 (0.5524)  time: 0.5082  data: 0.0002  max mem: 9341
[14:22:48.671728] Epoch: [37]  [194/195]  eta: 0:00:00  lr: 0.002279  loss: 0.5348 (0.5517)  time: 0.5099  data: 0.0001  max mem: 9341
[14:22:48.840135] Epoch: [37] Total time: 0:01:41 (0.5182 s / it)
[14:22:48.860481] Averaged stats: lr: 0.002279  loss: 0.5348 (0.5515)
[14:22:53.565432] {"train_lr": 0.0022475692307692284, "train_loss": 0.5515102096857168, "epoch": 37}
[14:22:53.565767] [14:22:53.565851] Training epoch 37 for 0:01:45
[14:22:53.565904] [14:22:53.570377] log_dir: ./exp/debug/cifar100-LT/debug
[14:22:55.338969] Epoch: [38]  [  0/195]  eta: 0:05:44  lr: 0.002280  loss: 0.5009 (0.5009)  time: 1.7671  data: 1.2483  max mem: 9341
[14:23:05.558143] Epoch: [38]  [ 20/195]  eta: 0:01:39  lr: 0.002285  loss: 0.5442 (0.5396)  time: 0.5109  data: 0.0002  max mem: 9341
[14:23:15.769785] Epoch: [38]  [ 40/195]  eta: 0:01:23  lr: 0.002290  loss: 0.5386 (0.5380)  time: 0.5105  data: 0.0002  max mem: 9341
[14:23:25.982200] Epoch: [38]  [ 60/195]  eta: 0:01:11  lr: 0.002295  loss: 0.5424 (0.5424)  time: 0.5105  data: 0.0002  max mem: 9341
[14:23:36.238538] Epoch: [38]  [ 80/195]  eta: 0:01:00  lr: 0.002305  loss: 0.5343 (0.5418)  time: 0.5128  data: 0.0002  max mem: 9341
[14:23:46.449895] Epoch: [38]  [100/195]  eta: 0:00:49  lr: 0.002310  loss: 0.5374 (0.5414)  time: 0.5105  data: 0.0002  max mem: 9341
[14:23:56.667376] Epoch: [38]  [120/195]  eta: 0:00:39  lr: 0.002314  loss: 0.5289 (0.5414)  time: 0.5108  data: 0.0002  max mem: 9341
[14:24:06.878050] Epoch: [38]  [140/195]  eta: 0:00:28  lr: 0.002319  loss: 0.5427 (0.5414)  time: 0.5105  data: 0.0002  max mem: 9341
[14:24:17.138609] Epoch: [38]  [160/195]  eta: 0:00:18  lr: 0.002329  loss: 0.5318 (0.5402)  time: 0.5130  data: 0.0002  max mem: 9341
[14:24:27.319606] Epoch: [38]  [180/195]  eta: 0:00:07  lr: 0.002334  loss: 0.5537 (0.5418)  time: 0.5090  data: 0.0001  max mem: 9341
[14:24:34.445995] Epoch: [38]  [194/195]  eta: 0:00:00  lr: 0.002339  loss: 0.5498 (0.5428)  time: 0.5104  data: 0.0001  max mem: 9341
[14:24:34.626375] Epoch: [38] Total time: 0:01:41 (0.5182 s / it)
[14:24:34.631181] Averaged stats: lr: 0.002339  loss: 0.5498 (0.5457)
[14:24:39.375695] {"train_lr": 0.002307569230769227, "train_loss": 0.545697148793783, "epoch": 38}
[14:24:39.376060] [14:24:39.376167] Training epoch 38 for 0:01:45
[14:24:39.376223] [14:24:39.381218] log_dir: ./exp/debug/cifar100-LT/debug
[14:24:40.871327] Epoch: [39]  [  0/195]  eta: 0:04:50  lr: 0.002340  loss: 0.5511 (0.5511)  time: 1.4885  data: 0.9949  max mem: 9341
[14:24:51.088792] Epoch: [39]  [ 20/195]  eta: 0:01:37  lr: 0.002345  loss: 0.5388 (0.5416)  time: 0.5108  data: 0.0002  max mem: 9341
[14:25:01.299758] Epoch: [39]  [ 40/195]  eta: 0:01:22  lr: 0.002350  loss: 0.5481 (0.5483)  time: 0.5105  data: 0.0002  max mem: 9341
[14:25:11.518979] Epoch: [39]  [ 60/195]  eta: 0:01:11  lr: 0.002355  loss: 0.5383 (0.5464)  time: 0.5109  data: 0.0002  max mem: 9341
[14:25:21.815272] Epoch: [39]  [ 80/195]  eta: 0:01:00  lr: 0.002365  loss: 0.5570 (0.5497)  time: 0.5148  data: 0.0002  max mem: 9341
[14:25:32.044909] Epoch: [39]  [100/195]  eta: 0:00:49  lr: 0.002370  loss: 0.5816 (0.5551)  time: 0.5114  data: 0.0002  max mem: 9341
[14:25:42.279059] Epoch: [39]  [120/195]  eta: 0:00:38  lr: 0.002374  loss: 0.5486 (0.5558)  time: 0.5117  data: 0.0002  max mem: 9341
[14:25:52.511162] Epoch: [39]  [140/195]  eta: 0:00:28  lr: 0.002379  loss: 0.5782 (0.5575)  time: 0.5115  data: 0.0002  max mem: 9341
[14:26:02.787837] Epoch: [39]  [160/195]  eta: 0:00:18  lr: 0.002389  loss: 0.5749 (0.5599)  time: 0.5138  data: 0.0002  max mem: 9341
[14:26:12.963914] Epoch: [39]  [180/195]  eta: 0:00:07  lr: 0.002394  loss: 0.5689 (0.5613)  time: 0.5088  data: 0.0001  max mem: 9341
[14:26:20.095912] Epoch: [39]  [194/195]  eta: 0:00:00  lr: 0.002399  loss: 0.5704 (0.5615)  time: 0.5107  data: 0.0001  max mem: 9341
[14:26:20.269854] Epoch: [39] Total time: 0:01:40 (0.5174 s / it)
[14:26:20.277020] Averaged stats: lr: 0.002399  loss: 0.5704 (0.5620)
[14:26:24.944489] {"train_lr": 0.0023675692307692287, "train_loss": 0.5620487610499064, "epoch": 39}
[14:26:24.944745] [14:26:24.944826] Training epoch 39 for 0:01:45
[14:26:24.944878] [14:26:24.949284] log_dir: ./exp/debug/cifar100-LT/debug
[14:26:26.741398] Epoch: [40]  [  0/195]  eta: 0:05:49  lr: 0.002400  loss: 0.5688 (0.5688)  time: 1.7911  data: 1.2742  max mem: 9341
[14:26:36.983816] Epoch: [40]  [ 20/195]  eta: 0:01:40  lr: 0.002400  loss: 0.5636 (0.5682)  time: 0.5121  data: 0.0002  max mem: 9341
[14:26:47.218839] Epoch: [40]  [ 40/195]  eta: 0:01:24  lr: 0.002400  loss: 0.5527 (0.5628)  time: 0.5117  data: 0.0002  max mem: 9341
[14:26:57.451462] Epoch: [40]  [ 60/195]  eta: 0:01:11  lr: 0.002400  loss: 0.5696 (0.5641)  time: 0.5116  data: 0.0002  max mem: 9341
[14:27:07.706880] Epoch: [40]  [ 80/195]  eta: 0:01:00  lr: 0.002400  loss: 0.5567 (0.5632)  time: 0.5127  data: 0.0002  max mem: 9341
[14:27:17.914636] Epoch: [40]  [100/195]  eta: 0:00:49  lr: 0.002400  loss: 0.5485 (0.5612)  time: 0.5103  data: 0.0002  max mem: 9341
[14:27:28.119132] Epoch: [40]  [120/195]  eta: 0:00:39  lr: 0.002400  loss: 0.5495 (0.5595)  time: 0.5102  data: 0.0002  max mem: 9341
[14:27:38.332544] Epoch: [40]  [140/195]  eta: 0:00:28  lr: 0.002400  loss: 0.5476 (0.5583)  time: 0.5106  data: 0.0002  max mem: 9341
[14:27:48.609672] Epoch: [40]  [160/195]  eta: 0:00:18  lr: 0.002400  loss: 0.5550 (0.5576)  time: 0.5138  data: 0.0002  max mem: 9341
[14:27:58.799143] Epoch: [40]  [180/195]  eta: 0:00:07  lr: 0.002400  loss: 0.5467 (0.5563)  time: 0.5094  data: 0.0001  max mem: 9341
[14:28:05.940200] Epoch: [40]  [194/195]  eta: 0:00:00  lr: 0.002400  loss: 0.5418 (0.5560)  time: 0.5121  data: 0.0001  max mem: 9341
[14:28:06.103287] Epoch: [40] Total time: 0:01:41 (0.5187 s / it)
[14:28:06.114132] Averaged stats: lr: 0.002400  loss: 0.5418 (0.5548)
[14:28:10.777184] {"train_lr": 0.002399996981391489, "train_loss": 0.5547603693145972, "epoch": 40}
[14:28:10.777451] [14:28:10.777537] Training epoch 40 for 0:01:45
[14:28:10.777591] [14:28:10.782125] log_dir: ./exp/debug/cifar100-LT/debug
[14:28:12.323943] Epoch: [41]  [  0/195]  eta: 0:05:00  lr: 0.002400  loss: 0.5685 (0.5685)  time: 1.5409  data: 1.0204  max mem: 9341
[14:28:22.536198] Epoch: [41]  [ 20/195]  eta: 0:01:37  lr: 0.002400  loss: 0.5487 (0.5493)  time: 0.5106  data: 0.0002  max mem: 9341
[14:28:32.750706] Epoch: [41]  [ 40/195]  eta: 0:01:23  lr: 0.002400  loss: 0.5450 (0.5500)  time: 0.5107  data: 0.0002  max mem: 9341
[14:28:42.955361] Epoch: [41]  [ 60/195]  eta: 0:01:11  lr: 0.002400  loss: 0.5446 (0.5512)  time: 0.5102  data: 0.0002  max mem: 9341
[14:28:53.228436] Epoch: [41]  [ 80/195]  eta: 0:01:00  lr: 0.002400  loss: 0.5671 (0.5555)  time: 0.5136  data: 0.0002  max mem: 9341
[14:29:03.436349] Epoch: [41]  [100/195]  eta: 0:00:49  lr: 0.002400  loss: 0.5602 (0.5557)  time: 0.5103  data: 0.0002  max mem: 9341
[14:29:13.649148] Epoch: [41]  [120/195]  eta: 0:00:38  lr: 0.002400  loss: 0.5562 (0.5560)  time: 0.5106  data: 0.0002  max mem: 9341
[14:29:23.851696] Epoch: [41]  [140/195]  eta: 0:00:28  lr: 0.002400  loss: 0.5537 (0.5557)  time: 0.5101  data: 0.0002  max mem: 9341
[14:29:34.099818] Epoch: [41]  [160/195]  eta: 0:00:18  lr: 0.002400  loss: 0.5455 (0.5556)  time: 0.5124  data: 0.0002  max mem: 9341
[14:29:44.261400] Epoch: [41]  [180/195]  eta: 0:00:07  lr: 0.002400  loss: 0.5458 (0.5560)  time: 0.5080  data: 0.0001  max mem: 9341
[14:29:51.379583] Epoch: [41]  [194/195]  eta: 0:00:00  lr: 0.002400  loss: 0.5515 (0.5557)  time: 0.5097  data: 0.0001  max mem: 9341
[14:29:51.563552] Epoch: [41] Total time: 0:01:40 (0.5168 s / it)
[14:29:51.589841] Averaged stats: lr: 0.002400  loss: 0.5515 (0.5575)
[14:29:56.320845] {"train_lr": 0.0023999773074567846, "train_loss": 0.5574881378274698, "epoch": 41}
[14:29:56.321094] [14:29:56.321174] Training epoch 41 for 0:01:45
[14:29:56.321242] [14:29:56.325741] log_dir: ./exp/debug/cifar100-LT/debug
[14:29:58.144261] Epoch: [42]  [  0/195]  eta: 0:05:54  lr: 0.002400  loss: 0.5631 (0.5631)  time: 1.8177  data: 1.3177  max mem: 9341
[14:30:08.356450] Epoch: [42]  [ 20/195]  eta: 0:01:40  lr: 0.002400  loss: 0.5345 (0.5381)  time: 0.5105  data: 0.0002  max mem: 9341
[14:30:18.564652] Epoch: [42]  [ 40/195]  eta: 0:01:24  lr: 0.002400  loss: 0.5523 (0.5451)  time: 0.5104  data: 0.0002  max mem: 9341
[14:30:28.774631] Epoch: [42]  [ 60/195]  eta: 0:01:11  lr: 0.002400  loss: 0.5352 (0.5415)  time: 0.5104  data: 0.0002  max mem: 9341
[14:30:39.073894] Epoch: [42]  [ 80/195]  eta: 0:01:00  lr: 0.002400  loss: 0.5395 (0.5423)  time: 0.5149  data: 0.0002  max mem: 9341
[14:30:49.300845] Epoch: [42]  [100/195]  eta: 0:00:49  lr: 0.002400  loss: 0.5483 (0.5437)  time: 0.5113  data: 0.0002  max mem: 9341
[14:30:59.529701] Epoch: [42]  [120/195]  eta: 0:00:39  lr: 0.002400  loss: 0.5531 (0.5452)  time: 0.5114  data: 0.0002  max mem: 9341
[14:31:09.760506] Epoch: [42]  [140/195]  eta: 0:00:28  lr: 0.002400  loss: 0.5386 (0.5443)  time: 0.5115  data: 0.0002  max mem: 9341
[14:31:20.050270] Epoch: [42]  [160/195]  eta: 0:00:18  lr: 0.002400  loss: 0.5508 (0.5450)  time: 0.5144  data: 0.0002  max mem: 9341
[14:31:30.233322] Epoch: [42]  [180/195]  eta: 0:00:07  lr: 0.002400  loss: 0.5357 (0.5447)  time: 0.5091  data: 0.0001  max mem: 9341
[14:31:37.379951] Epoch: [42]  [194/195]  eta: 0:00:00  lr: 0.002400  loss: 0.5475 (0.5445)  time: 0.5123  data: 0.0001  max mem: 9341
[14:31:37.552279] Epoch: [42] Total time: 0:01:41 (0.5191 s / it)
[14:31:37.567907] Averaged stats: lr: 0.002400  loss: 0.5475 (0.5474)
[14:31:42.259848] {"train_lr": 0.002399937129220747, "train_loss": 0.5473627700255468, "epoch": 42}
[14:31:42.260210] [14:31:42.260295] Training epoch 42 for 0:01:45
[14:31:42.260348] [14:31:42.264837] log_dir: ./exp/debug/cifar100-LT/debug
[14:31:44.000676] Epoch: [43]  [  0/195]  eta: 0:05:38  lr: 0.002400  loss: 0.5654 (0.5654)  time: 1.7348  data: 1.2363  max mem: 9341
[14:31:54.213411] Epoch: [43]  [ 20/195]  eta: 0:01:39  lr: 0.002400  loss: 0.5503 (0.5482)  time: 0.5106  data: 0.0002  max mem: 9341
[14:32:04.441010] Epoch: [43]  [ 40/195]  eta: 0:01:23  lr: 0.002400  loss: 0.5587 (0.5522)  time: 0.5113  data: 0.0002  max mem: 9341
[14:32:14.662575] Epoch: [43]  [ 60/195]  eta: 0:01:11  lr: 0.002400  loss: 0.5501 (0.5500)  time: 0.5110  data: 0.0002  max mem: 9341
[14:32:24.918203] Epoch: [43]  [ 80/195]  eta: 0:01:00  lr: 0.002400  loss: 0.5406 (0.5502)  time: 0.5127  data: 0.0002  max mem: 9341
[14:32:35.126109] Epoch: [43]  [100/195]  eta: 0:00:49  lr: 0.002400  loss: 0.5403 (0.5491)  time: 0.5103  data: 0.0002  max mem: 9341
[14:32:45.337975] Epoch: [43]  [120/195]  eta: 0:00:39  lr: 0.002400  loss: 0.5462 (0.5494)  time: 0.5105  data: 0.0002  max mem: 9341
[14:32:55.552473] Epoch: [43]  [140/195]  eta: 0:00:28  lr: 0.002400  loss: 0.5441 (0.5490)  time: 0.5107  data: 0.0002  max mem: 9341
[14:33:05.803221] Epoch: [43]  [160/195]  eta: 0:00:18  lr: 0.002400  loss: 0.5481 (0.5491)  time: 0.5125  data: 0.0002  max mem: 9341
[14:33:15.973958] Epoch: [43]  [180/195]  eta: 0:00:07  lr: 0.002400  loss: 0.5570 (0.5497)  time: 0.5085  data: 0.0001  max mem: 9341
[14:33:23.104552] Epoch: [43]  [194/195]  eta: 0:00:00  lr: 0.002400  loss: 0.5478 (0.5496)  time: 0.5105  data: 0.0001  max mem: 9341
[14:33:23.281983] Epoch: [43] Total time: 0:01:41 (0.5180 s / it)
[14:33:23.287672] Averaged stats: lr: 0.002400  loss: 0.5478 (0.5468)
[14:33:28.093974] {"train_lr": 0.002399876447369909, "train_loss": 0.546791127591561, "epoch": 43}
[14:33:28.094263] [14:33:28.094352] Training epoch 43 for 0:01:45
[14:33:28.094406] [14:33:28.099517] log_dir: ./exp/debug/cifar100-LT/debug
[14:33:29.591314] Epoch: [44]  [  0/195]  eta: 0:04:50  lr: 0.002400  loss: 0.5326 (0.5326)  time: 1.4907  data: 0.9767  max mem: 9341
[14:33:39.804253] Epoch: [44]  [ 20/195]  eta: 0:01:37  lr: 0.002400  loss: 0.5496 (0.5492)  time: 0.5106  data: 0.0002  max mem: 9341
[14:33:50.027178] Epoch: [44]  [ 40/195]  eta: 0:01:22  lr: 0.002400  loss: 0.5430 (0.5474)  time: 0.5111  data: 0.0002  max mem: 9341
[14:34:00.237619] Epoch: [44]  [ 60/195]  eta: 0:01:11  lr: 0.002400  loss: 0.5386 (0.5462)  time: 0.5104  data: 0.0002  max mem: 9341
[14:34:10.489022] Epoch: [44]  [ 80/195]  eta: 0:01:00  lr: 0.002400  loss: 0.5437 (0.5450)  time: 0.5125  data: 0.0002  max mem: 9341
[14:34:20.700203] Epoch: [44]  [100/195]  eta: 0:00:49  lr: 0.002400  loss: 0.5384 (0.5444)  time: 0.5105  data: 0.0002  max mem: 9341
[14:34:30.908701] Epoch: [44]  [120/195]  eta: 0:00:38  lr: 0.002400  loss: 0.5503 (0.5447)  time: 0.5104  data: 0.0002  max mem: 9341
[14:34:41.114279] Epoch: [44]  [140/195]  eta: 0:00:28  lr: 0.002400  loss: 0.5360 (0.5430)  time: 0.5102  data: 0.0002  max mem: 9341
[14:34:51.366955] Epoch: [44]  [160/195]  eta: 0:00:18  lr: 0.002400  loss: 0.5333 (0.5426)  time: 0.5126  data: 0.0002  max mem: 9341
[14:35:01.537255] Epoch: [44]  [180/195]  eta: 0:00:07  lr: 0.002400  loss: 0.5432 (0.5427)  time: 0.5085  data: 0.0001  max mem: 9341
[14:35:08.664656] Epoch: [44]  [194/195]  eta: 0:00:00  lr: 0.002400  loss: 0.5432 (0.5431)  time: 0.5103  data: 0.0001  max mem: 9341
[14:35:08.840465] Epoch: [44] Total time: 0:01:40 (0.5166 s / it)
[14:35:08.847580] Averaged stats: lr: 0.002400  loss: 0.5432 (0.5440)
[14:35:13.555973] {"train_lr": 0.002399795262941156, "train_loss": 0.544045898089042, "epoch": 44}
[14:35:13.559645] [14:35:13.559758] Training epoch 44 for 0:01:45
[14:35:13.559820] [14:35:13.564743] log_dir: ./exp/debug/cifar100-LT/debug
[14:35:15.210497] Epoch: [45]  [  0/195]  eta: 0:05:20  lr: 0.002400  loss: 0.5468 (0.5468)  time: 1.6441  data: 1.1380  max mem: 9341
[14:35:25.431727] Epoch: [45]  [ 20/195]  eta: 0:01:38  lr: 0.002400  loss: 0.5240 (0.5360)  time: 0.5109  data: 0.0002  max mem: 9341
[14:35:35.644217] Epoch: [45]  [ 40/195]  eta: 0:01:23  lr: 0.002400  loss: 0.5336 (0.5392)  time: 0.5106  data: 0.0002  max mem: 9341
[14:35:45.855364] Epoch: [45]  [ 60/195]  eta: 0:01:11  lr: 0.002400  loss: 0.5497 (0.5407)  time: 0.5105  data: 0.0002  max mem: 9341
[14:35:56.114704] Epoch: [45]  [ 80/195]  eta: 0:01:00  lr: 0.002400  loss: 0.5532 (0.5430)  time: 0.5129  data: 0.0002  max mem: 9341
[14:36:06.330169] Epoch: [45]  [100/195]  eta: 0:00:49  lr: 0.002400  loss: 0.5459 (0.5421)  time: 0.5107  data: 0.0002  max mem: 9341
[14:36:16.544388] Epoch: [45]  [120/195]  eta: 0:00:39  lr: 0.002400  loss: 0.5387 (0.5424)  time: 0.5107  data: 0.0002  max mem: 9341
[14:36:26.763458] Epoch: [45]  [140/195]  eta: 0:00:28  lr: 0.002400  loss: 0.5238 (0.5409)  time: 0.5109  data: 0.0002  max mem: 9341
[14:36:37.020163] Epoch: [45]  [160/195]  eta: 0:00:18  lr: 0.002400  loss: 0.5414 (0.5408)  time: 0.5128  data: 0.0002  max mem: 9341
[14:36:47.187558] Epoch: [45]  [180/195]  eta: 0:00:07  lr: 0.002400  loss: 0.5508 (0.5413)  time: 0.5083  data: 0.0001  max mem: 9341
[14:36:54.314962] Epoch: [45]  [194/195]  eta: 0:00:00  lr: 0.002400  loss: 0.5562 (0.5418)  time: 0.5102  data: 0.0001  max mem: 9341
[14:36:54.495100] Epoch: [45] Total time: 0:01:40 (0.5176 s / it)
[14:36:54.516426] Averaged stats: lr: 0.002400  loss: 0.5562 (0.5431)
[14:36:59.312411] {"train_lr": 0.002399693577321708, "train_loss": 0.5430550337984011, "epoch": 45}
[14:36:59.312639] [14:36:59.312721] Training epoch 45 for 0:01:45
[14:36:59.312773] [14:36:59.317215] log_dir: ./exp/debug/cifar100-LT/debug
[14:37:01.057667] Epoch: [46]  [  0/195]  eta: 0:05:39  lr: 0.002400  loss: 0.5803 (0.5803)  time: 1.7388  data: 1.2179  max mem: 9341
[14:37:11.280706] Epoch: [46]  [ 20/195]  eta: 0:01:39  lr: 0.002400  loss: 0.5320 (0.5346)  time: 0.5111  data: 0.0002  max mem: 9341
[14:37:21.506469] Epoch: [46]  [ 40/195]  eta: 0:01:23  lr: 0.002400  loss: 0.5402 (0.5355)  time: 0.5112  data: 0.0002  max mem: 9341
[14:37:31.747439] Epoch: [46]  [ 60/195]  eta: 0:01:11  lr: 0.002400  loss: 0.5403 (0.5369)  time: 0.5120  data: 0.0002  max mem: 9341
[14:37:42.045179] Epoch: [46]  [ 80/195]  eta: 0:01:00  lr: 0.002400  loss: 0.5311 (0.5360)  time: 0.5148  data: 0.0002  max mem: 9341
[14:37:52.276893] Epoch: [46]  [100/195]  eta: 0:00:49  lr: 0.002400  loss: 0.5669 (0.5403)  time: 0.5115  data: 0.0002  max mem: 9341
[14:38:02.508779] Epoch: [46]  [120/195]  eta: 0:00:39  lr: 0.002400  loss: 0.5474 (0.5410)  time: 0.5115  data: 0.0002  max mem: 9341
[14:38:12.720638] Epoch: [46]  [140/195]  eta: 0:00:28  lr: 0.002400  loss: 0.5450 (0.5411)  time: 0.5105  data: 0.0002  max mem: 9341
[14:38:22.977330] Epoch: [46]  [160/195]  eta: 0:00:18  lr: 0.002400  loss: 0.5273 (0.5407)  time: 0.5128  data: 0.0002  max mem: 9341
[14:38:33.151707] Epoch: [46]  [180/195]  eta: 0:00:07  lr: 0.002400  loss: 0.5287 (0.5395)  time: 0.5087  data: 0.0001  max mem: 9341
[14:38:40.278888] Epoch: [46]  [194/195]  eta: 0:00:00  lr: 0.002399  loss: 0.5237 (0.5387)  time: 0.5104  data: 0.0001  max mem: 9341
[14:38:40.450691] Epoch: [46] Total time: 0:01:41 (0.5186 s / it)
[14:38:40.460299] Averaged stats: lr: 0.002399  loss: 0.5237 (0.5366)
[14:38:45.181565] {"train_lr": 0.0023995713922490844, "train_loss": 0.5366249146751868, "epoch": 46}
[14:38:45.181867] [14:38:45.181957] Training epoch 46 for 0:01:45
[14:38:45.182011] [14:38:45.186547] log_dir: ./exp/debug/cifar100-LT/debug
[14:38:47.078557] Epoch: [47]  [  0/195]  eta: 0:06:08  lr: 0.002399  loss: 0.5313 (0.5313)  time: 1.8906  data: 1.3850  max mem: 9341
[14:38:57.322832] Epoch: [47]  [ 20/195]  eta: 0:01:41  lr: 0.002399  loss: 0.5212 (0.5264)  time: 0.5122  data: 0.0002  max mem: 9341
[14:39:07.540446] Epoch: [47]  [ 40/195]  eta: 0:01:24  lr: 0.002399  loss: 0.5242 (0.5238)  time: 0.5108  data: 0.0002  max mem: 9341
[14:39:17.758452] Epoch: [47]  [ 60/195]  eta: 0:01:12  lr: 0.002399  loss: 0.5356 (0.5299)  time: 0.5108  data: 0.0002  max mem: 9341
[14:39:28.016383] Epoch: [47]  [ 80/195]  eta: 0:01:00  lr: 0.002399  loss: 0.5394 (0.5318)  time: 0.5128  data: 0.0003  max mem: 9341
[14:39:38.231263] Epoch: [47]  [100/195]  eta: 0:00:49  lr: 0.002399  loss: 0.5276 (0.5305)  time: 0.5107  data: 0.0002  max mem: 9341
[14:39:48.441212] Epoch: [47]  [120/195]  eta: 0:00:39  lr: 0.002399  loss: 0.5251 (0.5305)  time: 0.5104  data: 0.0002  max mem: 9341
[14:39:58.657171] Epoch: [47]  [140/195]  eta: 0:00:28  lr: 0.002399  loss: 0.5167 (0.5300)  time: 0.5107  data: 0.0002  max mem: 9341
[14:40:08.920617] Epoch: [47]  [160/195]  eta: 0:00:18  lr: 0.002399  loss: 0.5468 (0.5312)  time: 0.5131  data: 0.0002  max mem: 9341
[14:40:19.096032] Epoch: [47]  [180/195]  eta: 0:00:07  lr: 0.002399  loss: 0.5347 (0.5309)  time: 0.5087  data: 0.0001  max mem: 9341
[14:40:26.229071] Epoch: [47]  [194/195]  eta: 0:00:00  lr: 0.002399  loss: 0.5251 (0.5307)  time: 0.5107  data: 0.0001  max mem: 9341
[14:40:26.398622] Epoch: [47] Total time: 0:01:41 (0.5190 s / it)
[14:40:26.409950] Averaged stats: lr: 0.002399  loss: 0.5251 (0.5346)
[14:40:31.128401] {"train_lr": 0.0023994287098110987, "train_loss": 0.5346271346394832, "epoch": 47}
[14:40:31.128735] [14:40:31.128820] Training epoch 47 for 0:01:45
[14:40:31.128871] [14:40:31.133412] log_dir: ./exp/debug/cifar100-LT/debug
[14:40:32.863687] Epoch: [48]  [  0/195]  eta: 0:05:37  lr: 0.002399  loss: 0.5481 (0.5481)  time: 1.7289  data: 1.2147  max mem: 9341
[14:40:43.072994] Epoch: [48]  [ 20/195]  eta: 0:01:39  lr: 0.002399  loss: 0.5235 (0.5257)  time: 0.5104  data: 0.0002  max mem: 9341
[14:40:53.288275] Epoch: [48]  [ 40/195]  eta: 0:01:23  lr: 0.002399  loss: 0.5368 (0.5324)  time: 0.5107  data: 0.0002  max mem: 9341
[14:41:03.505563] Epoch: [48]  [ 60/195]  eta: 0:01:11  lr: 0.002399  loss: 0.5261 (0.5303)  time: 0.5108  data: 0.0002  max mem: 9341
[14:41:13.763864] Epoch: [48]  [ 80/195]  eta: 0:01:00  lr: 0.002399  loss: 0.5299 (0.5320)  time: 0.5129  data: 0.0002  max mem: 9341
[14:41:23.975827] Epoch: [48]  [100/195]  eta: 0:00:49  lr: 0.002399  loss: 0.5380 (0.5340)  time: 0.5105  data: 0.0002  max mem: 9341
[14:41:34.187784] Epoch: [48]  [120/195]  eta: 0:00:39  lr: 0.002399  loss: 0.5527 (0.5358)  time: 0.5105  data: 0.0002  max mem: 9341
[14:41:44.404357] Epoch: [48]  [140/195]  eta: 0:00:28  lr: 0.002399  loss: 0.5255 (0.5347)  time: 0.5108  data: 0.0002  max mem: 9341
[14:41:54.656341] Epoch: [48]  [160/195]  eta: 0:00:18  lr: 0.002399  loss: 0.5139 (0.5323)  time: 0.5125  data: 0.0002  max mem: 9341
[14:42:04.825308] Epoch: [48]  [180/195]  eta: 0:00:07  lr: 0.002399  loss: 0.5300 (0.5324)  time: 0.5084  data: 0.0001  max mem: 9341
[14:42:11.953552] Epoch: [48]  [194/195]  eta: 0:00:00  lr: 0.002399  loss: 0.5300 (0.5326)  time: 0.5103  data: 0.0001  max mem: 9341
[14:42:12.117451] Epoch: [48] Total time: 0:01:40 (0.5179 s / it)
[14:42:12.123915] Averaged stats: lr: 0.002399  loss: 0.5300 (0.5333)
[14:42:16.842107] {"train_lr": 0.0023992655324458064, "train_loss": 0.5333115614759616, "epoch": 48}
[14:42:16.842652] [14:42:16.842755] Training epoch 48 for 0:01:45
[14:42:16.842810] [14:42:16.847394] log_dir: ./exp/debug/cifar100-LT/debug
[14:42:18.460250] Epoch: [49]  [  0/195]  eta: 0:05:14  lr: 0.002399  loss: 0.5243 (0.5243)  time: 1.6114  data: 1.1020  max mem: 9341
[14:42:28.673654] Epoch: [49]  [ 20/195]  eta: 0:01:38  lr: 0.002399  loss: 0.5184 (0.5207)  time: 0.5106  data: 0.0002  max mem: 9341
[14:42:38.883643] Epoch: [49]  [ 40/195]  eta: 0:01:23  lr: 0.002399  loss: 0.5223 (0.5237)  time: 0.5104  data: 0.0002  max mem: 9341
[14:42:49.098717] Epoch: [49]  [ 60/195]  eta: 0:01:11  lr: 0.002399  loss: 0.5387 (0.5270)  time: 0.5107  data: 0.0002  max mem: 9341
[14:42:59.353500] Epoch: [49]  [ 80/195]  eta: 0:01:00  lr: 0.002399  loss: 0.5477 (0.5291)  time: 0.5127  data: 0.0002  max mem: 9341
[14:43:09.566022] Epoch: [49]  [100/195]  eta: 0:00:49  lr: 0.002399  loss: 0.5218 (0.5276)  time: 0.5106  data: 0.0002  max mem: 9341
[14:43:19.770463] Epoch: [49]  [120/195]  eta: 0:00:38  lr: 0.002399  loss: 0.5383 (0.5306)  time: 0.5102  data: 0.0002  max mem: 9341
[14:43:29.982192] Epoch: [49]  [140/195]  eta: 0:00:28  lr: 0.002399  loss: 0.5377 (0.5310)  time: 0.5105  data: 0.0002  max mem: 9341
[14:43:40.236229] Epoch: [49]  [160/195]  eta: 0:00:18  lr: 0.002399  loss: 0.5132 (0.5294)  time: 0.5126  data: 0.0002  max mem: 9341
[14:43:50.409793] Epoch: [49]  [180/195]  eta: 0:00:07  lr: 0.002399  loss: 0.5283 (0.5294)  time: 0.5086  data: 0.0001  max mem: 9341
[14:43:57.537619] Epoch: [49]  [194/195]  eta: 0:00:00  lr: 0.002399  loss: 0.5275 (0.5290)  time: 0.5104  data: 0.0001  max mem: 9341
[14:43:57.714359] Epoch: [49] Total time: 0:01:40 (0.5173 s / it)
[14:43:57.720851] Averaged stats: lr: 0.002399  loss: 0.5275 (0.5276)
[14:44:02.448059] {"train_lr": 0.002399081862941443, "train_loss": 0.5275720995206099, "epoch": 49}
[14:44:02.448352] [14:44:02.448437] Training epoch 49 for 0:01:45
[14:44:02.448491] [14:44:02.452957] log_dir: ./exp/debug/cifar100-LT/debug
[14:44:04.234260] Epoch: [50]  [  0/195]  eta: 0:05:47  lr: 0.002399  loss: 0.5901 (0.5901)  time: 1.7798  data: 1.2753  max mem: 9341
[14:44:14.451184] Epoch: [50]  [ 20/195]  eta: 0:01:39  lr: 0.002399  loss: 0.5187 (0.5230)  time: 0.5108  data: 0.0002  max mem: 9341
[14:44:24.665070] Epoch: [50]  [ 40/195]  eta: 0:01:23  lr: 0.002399  loss: 0.5235 (0.5247)  time: 0.5106  data: 0.0002  max mem: 9341
[14:44:34.876738] Epoch: [50]  [ 60/195]  eta: 0:01:11  lr: 0.002399  loss: 0.5217 (0.5236)  time: 0.5105  data: 0.0002  max mem: 9341
[14:44:45.128028] Epoch: [50]  [ 80/195]  eta: 0:01:00  lr: 0.002399  loss: 0.5187 (0.5223)  time: 0.5125  data: 0.0002  max mem: 9341
[14:44:55.339693] Epoch: [50]  [100/195]  eta: 0:00:49  lr: 0.002399  loss: 0.5110 (0.5210)  time: 0.5105  data: 0.0002  max mem: 9341
[14:45:05.552599] Epoch: [50]  [120/195]  eta: 0:00:39  lr: 0.002399  loss: 0.5289 (0.5227)  time: 0.5106  data: 0.0002  max mem: 9341
[14:45:15.765711] Epoch: [50]  [140/195]  eta: 0:00:28  lr: 0.002399  loss: 0.5396 (0.5248)  time: 0.5106  data: 0.0002  max mem: 9341
[14:45:26.014922] Epoch: [50]  [160/195]  eta: 0:00:18  lr: 0.002399  loss: 0.5287 (0.5256)  time: 0.5124  data: 0.0002  max mem: 9341
[14:45:36.185080] Epoch: [50]  [180/195]  eta: 0:00:07  lr: 0.002399  loss: 0.5424 (0.5271)  time: 0.5085  data: 0.0002  max mem: 9341
[14:45:43.315559] Epoch: [50]  [194/195]  eta: 0:00:00  lr: 0.002399  loss: 0.5456 (0.5276)  time: 0.5105  data: 0.0001  max mem: 9341
[14:45:43.475940] Epoch: [50] Total time: 0:01:41 (0.5181 s / it)
[14:45:43.494343] Averaged stats: lr: 0.002399  loss: 0.5456 (0.5284)
[14:45:48.140983] {"train_lr": 0.002398877704436421, "train_loss": 0.5283950520631594, "epoch": 50}
[14:45:48.141338] [14:45:48.141421] Training epoch 50 for 0:01:45
[14:45:48.141476] [14:45:48.145989] log_dir: ./exp/debug/cifar100-LT/debug
[14:45:49.749474] Epoch: [51]  [  0/195]  eta: 0:05:12  lr: 0.002399  loss: 0.5643 (0.5643)  time: 1.6016  data: 1.0930  max mem: 9341
[14:45:59.988292] Epoch: [51]  [ 20/195]  eta: 0:01:38  lr: 0.002399  loss: 0.5247 (0.5309)  time: 0.5119  data: 0.0002  max mem: 9341
[14:46:10.237336] Epoch: [51]  [ 40/195]  eta: 0:01:23  lr: 0.002399  loss: 0.5320 (0.5342)  time: 0.5124  data: 0.0002  max mem: 9341
[14:46:20.480130] Epoch: [51]  [ 60/195]  eta: 0:01:11  lr: 0.002399  loss: 0.5263 (0.5324)  time: 0.5121  data: 0.0002  max mem: 9341
[14:46:30.785527] Epoch: [51]  [ 80/195]  eta: 0:01:00  lr: 0.002399  loss: 0.5337 (0.5319)  time: 0.5152  data: 0.0002  max mem: 9341
[14:46:41.022470] Epoch: [51]  [100/195]  eta: 0:00:49  lr: 0.002399  loss: 0.5155 (0.5299)  time: 0.5118  data: 0.0002  max mem: 9341
[14:46:51.262575] Epoch: [51]  [120/195]  eta: 0:00:39  lr: 0.002399  loss: 0.5401 (0.5320)  time: 0.5119  data: 0.0002  max mem: 9341
[14:47:01.508199] Epoch: [51]  [140/195]  eta: 0:00:28  lr: 0.002399  loss: 0.5294 (0.5317)  time: 0.5122  data: 0.0002  max mem: 9341
[14:47:11.776877] Epoch: [51]  [160/195]  eta: 0:00:18  lr: 0.002399  loss: 0.5233 (0.5310)  time: 0.5134  data: 0.0002  max mem: 9341
[14:47:21.955977] Epoch: [51]  [180/195]  eta: 0:00:07  lr: 0.002399  loss: 0.5328 (0.5313)  time: 0.5089  data: 0.0001  max mem: 9341
[14:47:29.091699] Epoch: [51]  [194/195]  eta: 0:00:00  lr: 0.002399  loss: 0.5251 (0.5312)  time: 0.5109  data: 0.0001  max mem: 9341
[14:47:29.270346] Epoch: [51] Total time: 0:01:41 (0.5186 s / it)
[14:47:29.284460] Averaged stats: lr: 0.002399  loss: 0.5251 (0.5311)
[14:47:34.024713] {"train_lr": 0.0023986530604192574, "train_loss": 0.5310736936254379, "epoch": 51}
[14:47:34.024988] [14:47:34.025076] Training epoch 51 for 0:01:45
[14:47:34.025147] [14:47:34.029645] log_dir: ./exp/debug/cifar100-LT/debug
[14:47:35.622647] Epoch: [52]  [  0/195]  eta: 0:05:10  lr: 0.002399  loss: 0.4834 (0.4834)  time: 1.5918  data: 1.0875  max mem: 9341
[14:47:45.835146] Epoch: [52]  [ 20/195]  eta: 0:01:38  lr: 0.002399  loss: 0.5138 (0.5187)  time: 0.5106  data: 0.0002  max mem: 9341
[14:47:56.080535] Epoch: [52]  [ 40/195]  eta: 0:01:23  lr: 0.002398  loss: 0.5253 (0.5211)  time: 0.5122  data: 0.0002  max mem: 9341
[14:48:06.320274] Epoch: [52]  [ 60/195]  eta: 0:01:11  lr: 0.002398  loss: 0.5174 (0.5207)  time: 0.5119  data: 0.0002  max mem: 9341
[14:48:16.617090] Epoch: [52]  [ 80/195]  eta: 0:01:00  lr: 0.002398  loss: 0.5227 (0.5216)  time: 0.5148  data: 0.0002  max mem: 9341
[14:48:26.851707] Epoch: [52]  [100/195]  eta: 0:00:49  lr: 0.002398  loss: 0.5118 (0.5209)  time: 0.5117  data: 0.0002  max mem: 9341
[14:48:37.090936] Epoch: [52]  [120/195]  eta: 0:00:39  lr: 0.002398  loss: 0.5082 (0.5198)  time: 0.5119  data: 0.0002  max mem: 9341
[14:48:47.324433] Epoch: [52]  [140/195]  eta: 0:00:28  lr: 0.002398  loss: 0.5226 (0.5202)  time: 0.5116  data: 0.0002  max mem: 9341
[14:48:57.631217] Epoch: [52]  [160/195]  eta: 0:00:18  lr: 0.002398  loss: 0.5098 (0.5202)  time: 0.5153  data: 0.0002  max mem: 9341
[14:49:07.819396] Epoch: [52]  [180/195]  eta: 0:00:07  lr: 0.002398  loss: 0.5272 (0.5207)  time: 0.5094  data: 0.0001  max mem: 9341
[14:49:14.971662] Epoch: [52]  [194/195]  eta: 0:00:00  lr: 0.002398  loss: 0.5213 (0.5215)  time: 0.5124  data: 0.0001  max mem: 9341
[14:49:15.138550] Epoch: [52] Total time: 0:01:41 (0.5185 s / it)
[14:49:15.153839] Averaged stats: lr: 0.002398  loss: 0.5213 (0.5225)
[14:49:19.893906] {"train_lr": 0.0023984079347284795, "train_loss": 0.5224815883315527, "epoch": 52}
[14:49:19.894195] [14:49:19.894294] Training epoch 52 for 0:01:45
[14:49:19.894360] [14:49:19.898779] log_dir: ./exp/debug/cifar100-LT/debug
[14:49:21.500892] Epoch: [53]  [  0/195]  eta: 0:05:12  lr: 0.002398  loss: 0.5569 (0.5569)  time: 1.6012  data: 1.1064  max mem: 9341
[14:49:31.738516] Epoch: [53]  [ 20/195]  eta: 0:01:38  lr: 0.002398  loss: 0.5169 (0.5192)  time: 0.5118  data: 0.0002  max mem: 9341
[14:49:41.958890] Epoch: [53]  [ 40/195]  eta: 0:01:23  lr: 0.002398  loss: 0.5154 (0.5179)  time: 0.5110  data: 0.0002  max mem: 9341
[14:49:52.177959] Epoch: [53]  [ 60/195]  eta: 0:01:11  lr: 0.002398  loss: 0.5182 (0.5171)  time: 0.5109  data: 0.0002  max mem: 9341
[14:50:02.447891] Epoch: [53]  [ 80/195]  eta: 0:01:00  lr: 0.002398  loss: 0.5214 (0.5184)  time: 0.5134  data: 0.0002  max mem: 9341
[14:50:12.669385] Epoch: [53]  [100/195]  eta: 0:00:49  lr: 0.002398  loss: 0.5258 (0.5202)  time: 0.5110  data: 0.0002  max mem: 9341
[14:50:22.892487] Epoch: [53]  [120/195]  eta: 0:00:39  lr: 0.002398  loss: 0.5214 (0.5200)  time: 0.5111  data: 0.0002  max mem: 9341
[14:50:33.121620] Epoch: [53]  [140/195]  eta: 0:00:28  lr: 0.002398  loss: 0.5324 (0.5215)  time: 0.5114  data: 0.0002  max mem: 9341
[14:50:43.380013] Epoch: [53]  [160/195]  eta: 0:00:18  lr: 0.002398  loss: 0.5240 (0.5218)  time: 0.5129  data: 0.0002  max mem: 9341
[14:50:53.564765] Epoch: [53]  [180/195]  eta: 0:00:07  lr: 0.002398  loss: 0.5263 (0.5228)  time: 0.5092  data: 0.0002  max mem: 9341
[14:51:00.704055] Epoch: [53]  [194/195]  eta: 0:00:00  lr: 0.002398  loss: 0.5186 (0.5228)  time: 0.5110  data: 0.0001  max mem: 9341
[14:51:00.871874] Epoch: [53] Total time: 0:01:40 (0.5178 s / it)
[14:51:00.888253] Averaged stats: lr: 0.002398  loss: 0.5186 (0.5229)
[14:51:05.557182] {"train_lr": 0.002398142331552621, "train_loss": 0.5229446976230695, "epoch": 53}
[14:51:05.557487] [14:51:05.557607] Training epoch 53 for 0:01:45
[14:51:05.557695] [14:51:05.562207] log_dir: ./exp/debug/cifar100-LT/debug
[14:51:07.116113] Epoch: [54]  [  0/195]  eta: 0:05:02  lr: 0.002398  loss: 0.5143 (0.5143)  time: 1.5523  data: 1.0500  max mem: 9341
[14:51:17.399288] Epoch: [54]  [ 20/195]  eta: 0:01:38  lr: 0.002398  loss: 0.5174 (0.5292)  time: 0.5141  data: 0.0002  max mem: 9341
[14:51:27.618563] Epoch: [54]  [ 40/195]  eta: 0:01:23  lr: 0.002398  loss: 0.5367 (0.5286)  time: 0.5109  data: 0.0002  max mem: 9341
[14:51:37.843766] Epoch: [54]  [ 60/195]  eta: 0:01:11  lr: 0.002398  loss: 0.5301 (0.5270)  time: 0.5112  data: 0.0002  max mem: 9341
[14:51:48.107878] Epoch: [54]  [ 80/195]  eta: 0:01:00  lr: 0.002398  loss: 0.5242 (0.5262)  time: 0.5131  data: 0.0002  max mem: 9341
[14:51:58.322331] Epoch: [54]  [100/195]  eta: 0:00:49  lr: 0.002398  loss: 0.5155 (0.5251)  time: 0.5107  data: 0.0002  max mem: 9341
[14:52:08.541853] Epoch: [54]  [120/195]  eta: 0:00:39  lr: 0.002398  loss: 0.5275 (0.5262)  time: 0.5109  data: 0.0002  max mem: 9341
[14:52:18.759136] Epoch: [54]  [140/195]  eta: 0:00:28  lr: 0.002398  loss: 0.5146 (0.5247)  time: 0.5108  data: 0.0002  max mem: 9341
[14:52:29.017929] Epoch: [54]  [160/195]  eta: 0:00:18  lr: 0.002398  loss: 0.5048 (0.5222)  time: 0.5129  data: 0.0002  max mem: 9341
[14:52:39.195709] Epoch: [54]  [180/195]  eta: 0:00:07  lr: 0.002398  loss: 0.5152 (0.5223)  time: 0.5088  data: 0.0001  max mem: 9341
[14:52:46.332373] Epoch: [54]  [194/195]  eta: 0:00:00  lr: 0.002398  loss: 0.5314 (0.5235)  time: 0.5108  data: 0.0001  max mem: 9341
[14:52:46.505055] Epoch: [54] Total time: 0:01:40 (0.5177 s / it)
[14:52:46.522041] Averaged stats: lr: 0.002398  loss: 0.5314 (0.5212)
[14:52:51.193629] {"train_lr": 0.0023978562554300975, "train_loss": 0.5212493308843711, "epoch": 54}
[14:52:51.193891] [14:52:51.193979] Training epoch 54 for 0:01:45
[14:52:51.194036] [14:52:51.198485] log_dir: ./exp/debug/cifar100-LT/debug
[14:52:52.865388] Epoch: [55]  [  0/195]  eta: 0:05:24  lr: 0.002398  loss: 0.5053 (0.5053)  time: 1.6661  data: 1.1622  max mem: 9341
[14:53:03.113789] Epoch: [55]  [ 20/195]  eta: 0:01:39  lr: 0.002398  loss: 0.5227 (0.5231)  time: 0.5124  data: 0.0002  max mem: 9341
[14:53:13.333956] Epoch: [55]  [ 40/195]  eta: 0:01:23  lr: 0.002398  loss: 0.5166 (0.5231)  time: 0.5110  data: 0.0002  max mem: 9341
[14:53:23.579249] Epoch: [55]  [ 60/195]  eta: 0:01:11  lr: 0.002398  loss: 0.5098 (0.5197)  time: 0.5122  data: 0.0002  max mem: 9341
[14:53:33.881438] Epoch: [55]  [ 80/195]  eta: 0:01:00  lr: 0.002398  loss: 0.5164 (0.5178)  time: 0.5150  data: 0.0002  max mem: 9341
[14:53:44.117185] Epoch: [55]  [100/195]  eta: 0:00:49  lr: 0.002398  loss: 0.5147 (0.5182)  time: 0.5117  data: 0.0002  max mem: 9341
[14:53:54.357452] Epoch: [55]  [120/195]  eta: 0:00:39  lr: 0.002398  loss: 0.5211 (0.5192)  time: 0.5120  data: 0.0002  max mem: 9341
[14:54:04.593980] Epoch: [55]  [140/195]  eta: 0:00:28  lr: 0.002397  loss: 0.5208 (0.5193)  time: 0.5118  data: 0.0002  max mem: 9341
[14:54:14.852003] Epoch: [55]  [160/195]  eta: 0:00:18  lr: 0.002397  loss: 0.5068 (0.5181)  time: 0.5128  data: 0.0002  max mem: 9341
[14:54:25.027285] Epoch: [55]  [180/195]  eta: 0:00:07  lr: 0.002397  loss: 0.5107 (0.5176)  time: 0.5087  data: 0.0001  max mem: 9341
[14:54:32.157713] Epoch: [55]  [194/195]  eta: 0:00:00  lr: 0.002397  loss: 0.4994 (0.5169)  time: 0.5106  data: 0.0001  max mem: 9341
[14:54:32.330176] Epoch: [55] Total time: 0:01:41 (0.5186 s / it)
[14:54:32.342813] Averaged stats: lr: 0.002397  loss: 0.4994 (0.5183)
[14:54:37.064425] {"train_lr": 0.002397549711249174, "train_loss": 0.5183134867594792, "epoch": 55}
[14:54:37.064690] [14:54:37.064778] Training epoch 55 for 0:01:45
[14:54:37.064833] [14:54:37.069270] log_dir: ./exp/debug/cifar100-LT/debug
[14:54:38.821559] Epoch: [56]  [  0/195]  eta: 0:05:41  lr: 0.002397  loss: 0.5290 (0.5290)  time: 1.7511  data: 1.2385  max mem: 9341
[14:54:49.043541] Epoch: [56]  [ 20/195]  eta: 0:01:39  lr: 0.002397  loss: 0.5009 (0.5141)  time: 0.5110  data: 0.0002  max mem: 9341
[14:54:59.255552] Epoch: [56]  [ 40/195]  eta: 0:01:23  lr: 0.002397  loss: 0.5182 (0.5181)  time: 0.5105  data: 0.0002  max mem: 9341
[14:55:09.490679] Epoch: [56]  [ 60/195]  eta: 0:01:11  lr: 0.002397  loss: 0.5149 (0.5162)  time: 0.5117  data: 0.0002  max mem: 9341
[14:55:19.797240] Epoch: [56]  [ 80/195]  eta: 0:01:00  lr: 0.002397  loss: 0.5296 (0.5204)  time: 0.5153  data: 0.0002  max mem: 9341
[14:55:30.036251] Epoch: [56]  [100/195]  eta: 0:00:49  lr: 0.002397  loss: 0.5366 (0.5228)  time: 0.5119  data: 0.0002  max mem: 9341
[14:55:40.280487] Epoch: [56]  [120/195]  eta: 0:00:39  lr: 0.002397  loss: 0.5274 (0.5239)  time: 0.5121  data: 0.0002  max mem: 9341
[14:55:50.505802] Epoch: [56]  [140/195]  eta: 0:00:28  lr: 0.002397  loss: 0.5172 (0.5236)  time: 0.5112  data: 0.0002  max mem: 9341
[14:56:00.757158] Epoch: [56]  [160/195]  eta: 0:00:18  lr: 0.002397  loss: 0.5239 (0.5233)  time: 0.5125  data: 0.0002  max mem: 9341
[14:56:10.919951] Epoch: [56]  [180/195]  eta: 0:00:07  lr: 0.002397  loss: 0.5214 (0.5237)  time: 0.5081  data: 0.0001  max mem: 9341
[14:56:18.055224] Epoch: [56]  [194/195]  eta: 0:00:00  lr: 0.002397  loss: 0.5230 (0.5243)  time: 0.5105  data: 0.0001  max mem: 9341
[14:56:18.233734] Epoch: [56] Total time: 0:01:41 (0.5188 s / it)
[14:56:18.244112] Averaged stats: lr: 0.002397  loss: 0.5230 (0.5245)
[14:56:22.944525] {"train_lr": 0.0023972227042478264, "train_loss": 0.5245157775206444, "epoch": 56}
[14:56:22.944803] [14:56:22.944891] Training epoch 56 for 0:01:45
[14:56:22.944946] [14:56:22.949426] log_dir: ./exp/debug/cifar100-LT/debug
[14:56:24.598389] Epoch: [57]  [  0/195]  eta: 0:05:21  lr: 0.002397  loss: 0.5257 (0.5257)  time: 1.6481  data: 1.1456  max mem: 9341
[14:56:34.811742] Epoch: [57]  [ 20/195]  eta: 0:01:38  lr: 0.002397  loss: 0.5229 (0.5317)  time: 0.5106  data: 0.0002  max mem: 9341
[14:56:45.055056] Epoch: [57]  [ 40/195]  eta: 0:01:23  lr: 0.002397  loss: 0.5167 (0.5300)  time: 0.5121  data: 0.0002  max mem: 9341
[14:56:55.273465] Epoch: [57]  [ 60/195]  eta: 0:01:11  lr: 0.002397  loss: 0.5073 (0.5237)  time: 0.5108  data: 0.0002  max mem: 9341
[14:57:05.537090] Epoch: [57]  [ 80/195]  eta: 0:01:00  lr: 0.002397  loss: 0.5232 (0.5233)  time: 0.5131  data: 0.0002  max mem: 9341
[14:57:15.756002] Epoch: [57]  [100/195]  eta: 0:00:49  lr: 0.002397  loss: 0.5124 (0.5215)  time: 0.5109  data: 0.0002  max mem: 9341
[14:57:25.977941] Epoch: [57]  [120/195]  eta: 0:00:39  lr: 0.002397  loss: 0.5190 (0.5213)  time: 0.5110  data: 0.0002  max mem: 9341
[14:57:36.193856] Epoch: [57]  [140/195]  eta: 0:00:28  lr: 0.002397  loss: 0.5230 (0.5221)  time: 0.5107  data: 0.0002  max mem: 9341
[14:57:46.452986] Epoch: [57]  [160/195]  eta: 0:00:18  lr: 0.002397  loss: 0.5098 (0.5212)  time: 0.5129  data: 0.0002  max mem: 9341
[14:57:56.635515] Epoch: [57]  [180/195]  eta: 0:00:07  lr: 0.002397  loss: 0.5188 (0.5205)  time: 0.5091  data: 0.0001  max mem: 9341
[14:58:03.772267] Epoch: [57]  [194/195]  eta: 0:00:00  lr: 0.002397  loss: 0.5158 (0.5203)  time: 0.5110  data: 0.0001  max mem: 9341
[14:58:03.946431] Epoch: [57] Total time: 0:01:40 (0.5179 s / it)
[14:58:03.947375] Averaged stats: lr: 0.002397  loss: 0.5158 (0.5204)
[14:58:08.621576] {"train_lr": 0.002396875240013723, "train_loss": 0.520384050714664, "epoch": 57}
[14:58:08.622046] [14:58:08.622144] Training epoch 57 for 0:01:45
[14:58:08.622201] [14:58:08.626728] log_dir: ./exp/debug/cifar100-LT/debug
[14:58:10.274620] Epoch: [58]  [  0/195]  eta: 0:05:21  lr: 0.002397  loss: 0.5369 (0.5369)  time: 1.6470  data: 1.1323  max mem: 9341
[14:58:20.492383] Epoch: [58]  [ 20/195]  eta: 0:01:38  lr: 0.002397  loss: 0.5184 (0.5242)  time: 0.5108  data: 0.0003  max mem: 9341
[14:58:30.713011] Epoch: [58]  [ 40/195]  eta: 0:01:23  lr: 0.002397  loss: 0.5241 (0.5272)  time: 0.5110  data: 0.0002  max mem: 9341
[14:58:40.928528] Epoch: [58]  [ 60/195]  eta: 0:01:11  lr: 0.002397  loss: 0.5149 (0.5256)  time: 0.5107  data: 0.0002  max mem: 9341
[14:58:51.226149] Epoch: [58]  [ 80/195]  eta: 0:01:00  lr: 0.002397  loss: 0.5312 (0.5274)  time: 0.5148  data: 0.0002  max mem: 9341
[14:59:01.470277] Epoch: [58]  [100/195]  eta: 0:00:49  lr: 0.002396  loss: 0.5178 (0.5249)  time: 0.5121  data: 0.0002  max mem: 9341
[14:59:11.710544] Epoch: [58]  [120/195]  eta: 0:00:39  lr: 0.002396  loss: 0.5184 (0.5235)  time: 0.5120  data: 0.0002  max mem: 9341
[14:59:21.950006] Epoch: [58]  [140/195]  eta: 0:00:28  lr: 0.002396  loss: 0.4996 (0.5219)  time: 0.5119  data: 0.0002  max mem: 9341
[14:59:32.249608] Epoch: [58]  [160/195]  eta: 0:00:18  lr: 0.002396  loss: 0.5120 (0.5219)  time: 0.5149  data: 0.0002  max mem: 9341
[14:59:42.442579] Epoch: [58]  [180/195]  eta: 0:00:07  lr: 0.002396  loss: 0.5122 (0.5214)  time: 0.5096  data: 0.0002  max mem: 9341
[14:59:49.599956] Epoch: [58]  [194/195]  eta: 0:00:00  lr: 0.002396  loss: 0.5168 (0.5210)  time: 0.5129  data: 0.0001  max mem: 9341
[14:59:49.769076] Epoch: [58] Total time: 0:01:41 (0.5187 s / it)
[14:59:49.786266] Averaged stats: lr: 0.002396  loss: 0.5168 (0.5221)
[14:59:54.495422] {"train_lr": 0.0023965073244840533, "train_loss": 0.5220816042942878, "epoch": 58}
[14:59:54.495775] [14:59:54.495865] Training epoch 58 for 0:01:45
[14:59:54.495923] [14:59:54.500440] log_dir: ./exp/debug/cifar100-LT/debug
[14:59:56.198672] Epoch: [59]  [  0/195]  eta: 0:05:30  lr: 0.002396  loss: 0.5452 (0.5452)  time: 1.6966  data: 1.1968  max mem: 9341
[15:00:06.415616] Epoch: [59]  [ 20/195]  eta: 0:01:39  lr: 0.002396  loss: 0.5278 (0.5318)  time: 0.5108  data: 0.0002  max mem: 9341
[15:00:16.640914] Epoch: [59]  [ 40/195]  eta: 0:01:23  lr: 0.002396  loss: 0.5148 (0.5252)  time: 0.5112  data: 0.0002  max mem: 9341
[15:00:26.853348] Epoch: [59]  [ 60/195]  eta: 0:01:11  lr: 0.002396  loss: 0.5195 (0.5215)  time: 0.5106  data: 0.0002  max mem: 9341
[15:00:37.119566] Epoch: [59]  [ 80/195]  eta: 0:01:00  lr: 0.002396  loss: 0.5260 (0.5239)  time: 0.5133  data: 0.0002  max mem: 9341
[15:00:47.344465] Epoch: [59]  [100/195]  eta: 0:00:49  lr: 0.002396  loss: 0.5151 (0.5234)  time: 0.5112  data: 0.0002  max mem: 9341
[15:00:57.563323] Epoch: [59]  [120/195]  eta: 0:00:39  lr: 0.002396  loss: 0.5182 (0.5232)  time: 0.5109  data: 0.0002  max mem: 9341
[15:01:07.781587] Epoch: [59]  [140/195]  eta: 0:00:28  lr: 0.002396  loss: 0.5200 (0.5225)  time: 0.5109  data: 0.0002  max mem: 9341
[15:01:18.049709] Epoch: [59]  [160/195]  eta: 0:00:18  lr: 0.002396  loss: 0.5281 (0.5235)  time: 0.5133  data: 0.0002  max mem: 9341
[15:01:28.238382] Epoch: [59]  [180/195]  eta: 0:00:07  lr: 0.002396  loss: 0.5253 (0.5238)  time: 0.5094  data: 0.0002  max mem: 9341
[15:01:35.376621] Epoch: [59]  [194/195]  eta: 0:00:00  lr: 0.002396  loss: 0.5245 (0.5247)  time: 0.5110  data: 0.0001  max mem: 9341
[15:01:35.562147] Epoch: [59] Total time: 0:01:41 (0.5183 s / it)
[15:01:35.568844] Averaged stats: lr: 0.002396  loss: 0.5245 (0.5226)
[15:01:40.234350] {"train_lr": 0.002396118963945493, "train_loss": 0.5225722139462446, "epoch": 59}
[15:01:40.234628] [15:01:40.234716] Training epoch 59 for 0:01:45
[15:01:40.234832] [15:01:40.239281] log_dir: ./exp/debug/cifar100-LT/debug
[15:01:41.962159] Epoch: [60]  [  0/195]  eta: 0:05:35  lr: 0.002396  loss: 0.4895 (0.4895)  time: 1.7211  data: 1.2099  max mem: 9341
[15:01:52.186754] Epoch: [60]  [ 20/195]  eta: 0:01:39  lr: 0.002396  loss: 0.5250 (0.5219)  time: 0.5112  data: 0.0002  max mem: 9341
[15:02:02.400320] Epoch: [60]  [ 40/195]  eta: 0:01:23  lr: 0.002396  loss: 0.5086 (0.5161)  time: 0.5106  data: 0.0002  max mem: 9341
[15:02:12.617757] Epoch: [60]  [ 60/195]  eta: 0:01:11  lr: 0.002396  loss: 0.5231 (0.5221)  time: 0.5108  data: 0.0003  max mem: 9341
[15:02:22.879754] Epoch: [60]  [ 80/195]  eta: 0:01:00  lr: 0.002396  loss: 0.5243 (0.5214)  time: 0.5130  data: 0.0003  max mem: 9341
[15:02:33.093805] Epoch: [60]  [100/195]  eta: 0:00:49  lr: 0.002396  loss: 0.5081 (0.5196)  time: 0.5106  data: 0.0003  max mem: 9341
[15:02:43.310290] Epoch: [60]  [120/195]  eta: 0:00:39  lr: 0.002396  loss: 0.5207 (0.5197)  time: 0.5108  data: 0.0003  max mem: 9341
[15:02:53.529360] Epoch: [60]  [140/195]  eta: 0:00:28  lr: 0.002396  loss: 0.5108 (0.5188)  time: 0.5109  data: 0.0003  max mem: 9341
[15:03:03.792962] Epoch: [60]  [160/195]  eta: 0:00:18  lr: 0.002396  loss: 0.5045 (0.5178)  time: 0.5131  data: 0.0002  max mem: 9341
[15:03:13.968454] Epoch: [60]  [180/195]  eta: 0:00:07  lr: 0.002396  loss: 0.5017 (0.5160)  time: 0.5087  data: 0.0002  max mem: 9341
[15:03:21.104602] Epoch: [60]  [194/195]  eta: 0:00:00  lr: 0.002395  loss: 0.5032 (0.5154)  time: 0.5108  data: 0.0001  max mem: 9341
[15:03:21.274199] Epoch: [60] Total time: 0:01:41 (0.5181 s / it)
[15:03:21.288668] Averaged stats: lr: 0.002395  loss: 0.5032 (0.5199)
[15:03:25.981544] {"train_lr": 0.002395710165034032, "train_loss": 0.5198980839206622, "epoch": 60}
[15:03:25.981899] [15:03:25.982002] Training epoch 60 for 0:01:45
[15:03:25.982058] [15:03:25.986554] log_dir: ./exp/debug/cifar100-LT/debug
[15:03:27.596753] Epoch: [61]  [  0/195]  eta: 0:05:13  lr: 0.002395  loss: 0.4889 (0.4889)  time: 1.6085  data: 1.0902  max mem: 9341
[15:03:37.824840] Epoch: [61]  [ 20/195]  eta: 0:01:38  lr: 0.002395  loss: 0.5210 (0.5215)  time: 0.5113  data: 0.0002  max mem: 9341
[15:03:48.039455] Epoch: [61]  [ 40/195]  eta: 0:01:23  lr: 0.002395  loss: 0.5185 (0.5196)  time: 0.5107  data: 0.0002  max mem: 9341
[15:03:58.259655] Epoch: [61]  [ 60/195]  eta: 0:01:11  lr: 0.002395  loss: 0.5095 (0.5177)  time: 0.5109  data: 0.0002  max mem: 9341
[15:04:08.521045] Epoch: [61]  [ 80/195]  eta: 0:01:00  lr: 0.002395  loss: 0.5165 (0.5177)  time: 0.5130  data: 0.0002  max mem: 9341
[15:04:18.744198] Epoch: [61]  [100/195]  eta: 0:00:49  lr: 0.002395  loss: 0.5018 (0.5151)  time: 0.5111  data: 0.0002  max mem: 9341
[15:04:28.963656] Epoch: [61]  [120/195]  eta: 0:00:39  lr: 0.002395  loss: 0.5242 (0.5155)  time: 0.5109  data: 0.0002  max mem: 9341
[15:04:39.175431] Epoch: [61]  [140/195]  eta: 0:00:28  lr: 0.002395  loss: 0.5277 (0.5158)  time: 0.5105  data: 0.0002  max mem: 9341
[15:04:49.456704] Epoch: [61]  [160/195]  eta: 0:00:18  lr: 0.002395  loss: 0.5153 (0.5152)  time: 0.5140  data: 0.0002  max mem: 9341
[15:04:59.656134] Epoch: [61]  [180/195]  eta: 0:00:07  lr: 0.002395  loss: 0.5166 (0.5149)  time: 0.5099  data: 0.0001  max mem: 9341
[15:05:06.813460] Epoch: [61]  [194/195]  eta: 0:00:00  lr: 0.002395  loss: 0.5186 (0.5148)  time: 0.5131  data: 0.0001  max mem: 9341
[15:05:06.991994] Epoch: [61] Total time: 0:01:41 (0.5180 s / it)
[15:05:06.998336] Averaged stats: lr: 0.002395  loss: 0.5186 (0.5155)
[15:05:11.593679] {"train_lr": 0.002395280934734927, "train_loss": 0.5155219620619065, "epoch": 61}
[15:05:11.593946] [15:05:11.594036] Training epoch 61 for 0:01:45
[15:05:11.594092] [15:05:11.598551] log_dir: ./exp/debug/cifar100-LT/debug
[15:05:13.196069] Epoch: [62]  [  0/195]  eta: 0:05:11  lr: 0.002395  loss: 0.4621 (0.4621)  time: 1.5964  data: 1.0885  max mem: 9341
[15:05:23.416278] Epoch: [62]  [ 20/195]  eta: 0:01:38  lr: 0.002395  loss: 0.5134 (0.5136)  time: 0.5109  data: 0.0002  max mem: 9341
[15:05:33.633544] Epoch: [62]  [ 40/195]  eta: 0:01:23  lr: 0.002395  loss: 0.5138 (0.5130)  time: 0.5108  data: 0.0002  max mem: 9341
[15:05:43.857017] Epoch: [62]  [ 60/195]  eta: 0:01:11  lr: 0.002395  loss: 0.5124 (0.5135)  time: 0.5111  data: 0.0002  max mem: 9341
[15:05:54.138100] Epoch: [62]  [ 80/195]  eta: 0:01:00  lr: 0.002395  loss: 0.5161 (0.5146)  time: 0.5140  data: 0.0002  max mem: 9341
[15:06:04.375752] Epoch: [62]  [100/195]  eta: 0:00:49  lr: 0.002395  loss: 0.5058 (0.5128)  time: 0.5118  data: 0.0002  max mem: 9341
[15:06:14.616160] Epoch: [62]  [120/195]  eta: 0:00:39  lr: 0.002395  loss: 0.5038 (0.5127)  time: 0.5120  data: 0.0002  max mem: 9341
[15:06:24.854942] Epoch: [62]  [140/195]  eta: 0:00:28  lr: 0.002395  loss: 0.4965 (0.5119)  time: 0.5119  data: 0.0002  max mem: 9341
[15:06:35.157553] Epoch: [62]  [160/195]  eta: 0:00:18  lr: 0.002395  loss: 0.5057 (0.5121)  time: 0.5151  data: 0.0002  max mem: 9341
[15:06:45.355218] Epoch: [62]  [180/195]  eta: 0:00:07  lr: 0.002395  loss: 0.5095 (0.5113)  time: 0.5098  data: 0.0001  max mem: 9341
[15:06:52.514791] Epoch: [62]  [194/195]  eta: 0:00:00  lr: 0.002395  loss: 0.5116 (0.5113)  time: 0.5131  data: 0.0001  max mem: 9341
[15:06:52.682569] Epoch: [62] Total time: 0:01:41 (0.5184 s / it)
[15:06:52.697814] Averaged stats: lr: 0.002395  loss: 0.5116 (0.5122)
[15:06:57.329035] {"train_lr": 0.0023948312803825467, "train_loss": 0.5122119271602386, "epoch": 62}
[15:06:57.329298] [15:06:57.329389] Training epoch 62 for 0:01:45
[15:06:57.329444] [15:06:57.333942] log_dir: ./exp/debug/cifar100-LT/debug
[15:06:59.104583] Epoch: [63]  [  0/195]  eta: 0:05:45  lr: 0.002395  loss: 0.5097 (0.5097)  time: 1.7697  data: 1.2564  max mem: 9341
[15:07:09.360961] Epoch: [63]  [ 20/195]  eta: 0:01:40  lr: 0.002395  loss: 0.5192 (0.5135)  time: 0.5128  data: 0.0002  max mem: 9341
[15:07:19.579349] Epoch: [63]  [ 40/195]  eta: 0:01:24  lr: 0.002395  loss: 0.5233 (0.5178)  time: 0.5109  data: 0.0002  max mem: 9341
[15:07:29.793622] Epoch: [63]  [ 60/195]  eta: 0:01:11  lr: 0.002394  loss: 0.5434 (0.5251)  time: 0.5107  data: 0.0002  max mem: 9341
[15:07:40.059284] Epoch: [63]  [ 80/195]  eta: 0:01:00  lr: 0.002394  loss: 0.5241 (0.5273)  time: 0.5132  data: 0.0002  max mem: 9341
[15:07:50.285094] Epoch: [63]  [100/195]  eta: 0:00:49  lr: 0.002394  loss: 0.5457 (0.5330)  time: 0.5112  data: 0.0002  max mem: 9341
[15:08:00.506264] Epoch: [63]  [120/195]  eta: 0:00:39  lr: 0.002394  loss: 0.5671 (0.5381)  time: 0.5110  data: 0.0002  max mem: 9341
[15:08:10.718145] Epoch: [63]  [140/195]  eta: 0:00:28  lr: 0.002394  loss: 0.5661 (0.5417)  time: 0.5105  data: 0.0002  max mem: 9341
[15:08:20.975432] Epoch: [63]  [160/195]  eta: 0:00:18  lr: 0.002394  loss: 0.5510 (0.5429)  time: 0.5128  data: 0.0002  max mem: 9341
[15:08:31.151690] Epoch: [63]  [180/195]  eta: 0:00:07  lr: 0.002394  loss: 0.5473 (0.5445)  time: 0.5088  data: 0.0001  max mem: 9341
[15:08:38.280757] Epoch: [63]  [194/195]  eta: 0:00:00  lr: 0.002394  loss: 0.5500 (0.5447)  time: 0.5106  data: 0.0001  max mem: 9341
[15:08:38.452814] Epoch: [63] Total time: 0:01:41 (0.5186 s / it)
[15:08:38.461633] Averaged stats: lr: 0.002394  loss: 0.5500 (0.5452)
[15:08:43.176237] {"train_lr": 0.0023943612096602317, "train_loss": 0.5451980716907061, "epoch": 63}
[15:08:43.176604] [15:08:43.176696] Training epoch 63 for 0:01:45
[15:08:43.176775] [15:08:43.181243] log_dir: ./exp/debug/cifar100-LT/debug
[15:08:44.901030] Epoch: [64]  [  0/195]  eta: 0:05:34  lr: 0.002394  loss: 0.5395 (0.5395)  time: 1.7168  data: 1.2004  max mem: 9341
[15:08:55.117772] Epoch: [64]  [ 20/195]  eta: 0:01:39  lr: 0.002394  loss: 0.5500 (0.5609)  time: 0.5108  data: 0.0002  max mem: 9341
[15:09:05.329553] Epoch: [64]  [ 40/195]  eta: 0:01:23  lr: 0.002394  loss: 0.5486 (0.5555)  time: 0.5105  data: 0.0002  max mem: 9341
[15:09:15.543378] Epoch: [64]  [ 60/195]  eta: 0:01:11  lr: 0.002394  loss: 0.5421 (0.5521)  time: 0.5105  data: 0.0002  max mem: 9341
[15:09:25.794585] Epoch: [64]  [ 80/195]  eta: 0:01:00  lr: 0.002394  loss: 0.5377 (0.5478)  time: 0.5125  data: 0.0003  max mem: 9341
[15:09:36.005088] Epoch: [64]  [100/195]  eta: 0:00:49  lr: 0.002394  loss: 0.5286 (0.5449)  time: 0.5105  data: 0.0002  max mem: 9341
[15:09:46.214328] Epoch: [64]  [120/195]  eta: 0:00:39  lr: 0.002394  loss: 0.5295 (0.5433)  time: 0.5104  data: 0.0002  max mem: 9341
[15:09:56.423627] Epoch: [64]  [140/195]  eta: 0:00:28  lr: 0.002394  loss: 0.5393 (0.5426)  time: 0.5104  data: 0.0002  max mem: 9341
[15:10:06.676327] Epoch: [64]  [160/195]  eta: 0:00:18  lr: 0.002394  loss: 0.5221 (0.5402)  time: 0.5126  data: 0.0002  max mem: 9341
[15:10:16.845475] Epoch: [64]  [180/195]  eta: 0:00:07  lr: 0.002394  loss: 0.5291 (0.5387)  time: 0.5084  data: 0.0002  max mem: 9341
[15:10:23.974991] Epoch: [64]  [194/195]  eta: 0:00:00  lr: 0.002394  loss: 0.5224 (0.5379)  time: 0.5105  data: 0.0001  max mem: 9341
[15:10:24.137888] Epoch: [64] Total time: 0:01:40 (0.5177 s / it)
[15:10:24.153034] Averaged stats: lr: 0.002394  loss: 0.5224 (0.5384)
[15:10:28.842991] {"train_lr": 0.002393870730600194, "train_loss": 0.538358617019959, "epoch": 64}
[15:10:28.843318] [15:10:28.843416] Training epoch 64 for 0:01:45
[15:10:28.843470] [15:10:28.847949] log_dir: ./exp/debug/cifar100-LT/debug
[15:10:30.572630] Epoch: [65]  [  0/195]  eta: 0:05:36  lr: 0.002394  loss: 0.5386 (0.5386)  time: 1.7233  data: 1.2174  max mem: 9341
[15:10:40.825652] Epoch: [65]  [ 20/195]  eta: 0:01:39  lr: 0.002394  loss: 0.5329 (0.5247)  time: 0.5126  data: 0.0002  max mem: 9341
[15:10:51.045391] Epoch: [65]  [ 40/195]  eta: 0:01:23  lr: 0.002394  loss: 0.5126 (0.5203)  time: 0.5109  data: 0.0002  max mem: 9341
[15:11:01.264033] Epoch: [65]  [ 60/195]  eta: 0:01:11  lr: 0.002393  loss: 0.5360 (0.5229)  time: 0.5109  data: 0.0002  max mem: 9341
[15:11:11.528905] Epoch: [65]  [ 80/195]  eta: 0:01:00  lr: 0.002393  loss: 0.5334 (0.5240)  time: 0.5132  data: 0.0002  max mem: 9341
[15:11:21.744862] Epoch: [65]  [100/195]  eta: 0:00:49  lr: 0.002393  loss: 0.5153 (0.5223)  time: 0.5107  data: 0.0002  max mem: 9341
[15:11:31.964244] Epoch: [65]  [120/195]  eta: 0:00:39  lr: 0.002393  loss: 0.5222 (0.5238)  time: 0.5109  data: 0.0002  max mem: 9341
[15:11:42.181976] Epoch: [65]  [140/195]  eta: 0:00:28  lr: 0.002393  loss: 0.5101 (0.5226)  time: 0.5108  data: 0.0002  max mem: 9341
[15:11:52.445060] Epoch: [65]  [160/195]  eta: 0:00:18  lr: 0.002393  loss: 0.5232 (0.5218)  time: 0.5131  data: 0.0002  max mem: 9341
[15:12:02.619639] Epoch: [65]  [180/195]  eta: 0:00:07  lr: 0.002393  loss: 0.5054 (0.5210)  time: 0.5087  data: 0.0001  max mem: 9341
[15:12:09.751964] Epoch: [65]  [194/195]  eta: 0:00:00  lr: 0.002393  loss: 0.5194 (0.5210)  time: 0.5106  data: 0.0001  max mem: 9341
[15:12:09.928579] Epoch: [65] Total time: 0:01:41 (0.5184 s / it)
[15:12:09.938361] Averaged stats: lr: 0.002393  loss: 0.5194 (0.5204)
[15:12:14.639582] {"train_lr": 0.0023933598515833822, "train_loss": 0.5204033113060853, "epoch": 65}
[15:12:14.639879] [15:12:14.639969] Training epoch 65 for 0:01:45
[15:12:14.640023] [15:12:14.644606] log_dir: ./exp/debug/cifar100-LT/debug
[15:12:16.205627] Epoch: [66]  [  0/195]  eta: 0:05:04  lr: 0.002393  loss: 0.5538 (0.5538)  time: 1.5600  data: 1.0535  max mem: 9341
[15:12:26.417734] Epoch: [66]  [ 20/195]  eta: 0:01:38  lr: 0.002393  loss: 0.5059 (0.5094)  time: 0.5105  data: 0.0002  max mem: 9341
[15:12:36.633044] Epoch: [66]  [ 40/195]  eta: 0:01:23  lr: 0.002393  loss: 0.5181 (0.5111)  time: 0.5107  data: 0.0002  max mem: 9341
[15:12:46.846159] Epoch: [66]  [ 60/195]  eta: 0:01:11  lr: 0.002393  loss: 0.5260 (0.5153)  time: 0.5106  data: 0.0002  max mem: 9341
[15:12:57.103458] Epoch: [66]  [ 80/195]  eta: 0:01:00  lr: 0.002393  loss: 0.5088 (0.5145)  time: 0.5128  data: 0.0002  max mem: 9341
[15:13:07.315141] Epoch: [66]  [100/195]  eta: 0:00:49  lr: 0.002393  loss: 0.5162 (0.5161)  time: 0.5105  data: 0.0002  max mem: 9341
[15:13:17.532774] Epoch: [66]  [120/195]  eta: 0:00:38  lr: 0.002393  loss: 0.5204 (0.5173)  time: 0.5108  data: 0.0002  max mem: 9341
[15:13:27.755360] Epoch: [66]  [140/195]  eta: 0:00:28  lr: 0.002393  loss: 0.5299 (0.5183)  time: 0.5111  data: 0.0002  max mem: 9341
[15:13:38.017821] Epoch: [66]  [160/195]  eta: 0:00:18  lr: 0.002393  loss: 0.5367 (0.5204)  time: 0.5130  data: 0.0002  max mem: 9341
[15:13:48.191821] Epoch: [66]  [180/195]  eta: 0:00:07  lr: 0.002393  loss: 0.5200 (0.5202)  time: 0.5087  data: 0.0002  max mem: 9341
[15:13:55.325624] Epoch: [66]  [194/195]  eta: 0:00:00  lr: 0.002393  loss: 0.5338 (0.5205)  time: 0.5109  data: 0.0001  max mem: 9341
[15:13:55.484658] Epoch: [66] Total time: 0:01:40 (0.5171 s / it)
[15:13:55.498016] Averaged stats: lr: 0.002393  loss: 0.5338 (0.5187)
[15:14:00.229069] {"train_lr": 0.0023928285813393057, "train_loss": 0.5187088222839893, "epoch": 66}
[15:14:00.229400] [15:14:00.229485] Training epoch 66 for 0:01:45
[15:14:00.229539] [15:14:00.234013] log_dir: ./exp/debug/cifar100-LT/debug
[15:14:02.009548] Epoch: [67]  [  0/195]  eta: 0:05:45  lr: 0.002393  loss: 0.5142 (0.5142)  time: 1.7741  data: 1.2741  max mem: 9341
[15:14:12.222365] Epoch: [67]  [ 20/195]  eta: 0:01:39  lr: 0.002392  loss: 0.5246 (0.5238)  time: 0.5106  data: 0.0002  max mem: 9341
[15:14:22.438909] Epoch: [67]  [ 40/195]  eta: 0:01:23  lr: 0.002392  loss: 0.5051 (0.5121)  time: 0.5107  data: 0.0002  max mem: 9341
[15:14:32.652525] Epoch: [67]  [ 60/195]  eta: 0:01:11  lr: 0.002392  loss: 0.5089 (0.5131)  time: 0.5106  data: 0.0002  max mem: 9341
[15:14:42.908307] Epoch: [67]  [ 80/195]  eta: 0:01:00  lr: 0.002392  loss: 0.4985 (0.5121)  time: 0.5127  data: 0.0002  max mem: 9341
[15:14:53.126964] Epoch: [67]  [100/195]  eta: 0:00:49  lr: 0.002392  loss: 0.5128 (0.5138)  time: 0.5109  data: 0.0002  max mem: 9341
[15:15:03.333256] Epoch: [67]  [120/195]  eta: 0:00:39  lr: 0.002392  loss: 0.5180 (0.5141)  time: 0.5103  data: 0.0002  max mem: 9341
[15:15:13.542393] Epoch: [67]  [140/195]  eta: 0:00:28  lr: 0.002392  loss: 0.5095 (0.5143)  time: 0.5104  data: 0.0002  max mem: 9341
[15:15:23.794024] Epoch: [67]  [160/195]  eta: 0:00:18  lr: 0.002392  loss: 0.5082 (0.5140)  time: 0.5125  data: 0.0002  max mem: 9341
[15:15:33.962954] Epoch: [67]  [180/195]  eta: 0:00:07  lr: 0.002392  loss: 0.5116 (0.5136)  time: 0.5084  data: 0.0001  max mem: 9341
[15:15:41.088482] Epoch: [67]  [194/195]  eta: 0:00:00  lr: 0.002392  loss: 0.5113 (0.5139)  time: 0.5103  data: 0.0001  max mem: 9341
[15:15:41.256110] Epoch: [67] Total time: 0:01:41 (0.5181 s / it)
[15:15:41.262495] Averaged stats: lr: 0.002392  loss: 0.5113 (0.5157)
[15:15:45.972283] {"train_lr": 0.002392276928945878, "train_loss": 0.515663853707986, "epoch": 67}
[15:15:45.972550] [15:15:45.972632] Training epoch 67 for 0:01:45
[15:15:45.972684] [15:15:45.977150] log_dir: ./exp/debug/cifar100-LT/debug
[15:15:47.769335] Epoch: [68]  [  0/195]  eta: 0:05:49  lr: 0.002392  loss: 0.4988 (0.4988)  time: 1.7913  data: 1.2880  max mem: 9341
[15:15:57.986144] Epoch: [68]  [ 20/195]  eta: 0:01:40  lr: 0.002392  loss: 0.5181 (0.5144)  time: 0.5108  data: 0.0002  max mem: 9341
[15:16:08.196397] Epoch: [68]  [ 40/195]  eta: 0:01:23  lr: 0.002392  loss: 0.5222 (0.5189)  time: 0.5105  data: 0.0002  max mem: 9341
[15:16:18.410336] Epoch: [68]  [ 60/195]  eta: 0:01:11  lr: 0.002392  loss: 0.5113 (0.5182)  time: 0.5106  data: 0.0002  max mem: 9341
[15:16:28.656930] Epoch: [68]  [ 80/195]  eta: 0:01:00  lr: 0.002392  loss: 0.5137 (0.5163)  time: 0.5123  data: 0.0002  max mem: 9341
[15:16:38.882309] Epoch: [68]  [100/195]  eta: 0:00:49  lr: 0.002392  loss: 0.5085 (0.5153)  time: 0.5112  data: 0.0002  max mem: 9341
[15:16:49.093232] Epoch: [68]  [120/195]  eta: 0:00:39  lr: 0.002392  loss: 0.5185 (0.5161)  time: 0.5105  data: 0.0002  max mem: 9341
[15:16:59.308897] Epoch: [68]  [140/195]  eta: 0:00:28  lr: 0.002392  loss: 0.5012 (0.5143)  time: 0.5107  data: 0.0002  max mem: 9341
[15:17:09.586874] Epoch: [68]  [160/195]  eta: 0:00:18  lr: 0.002391  loss: 0.5136 (0.5147)  time: 0.5138  data: 0.0002  max mem: 9341
[15:17:19.773499] Epoch: [68]  [180/195]  eta: 0:00:07  lr: 0.002391  loss: 0.5180 (0.5149)  time: 0.5093  data: 0.0001  max mem: 9341
[15:17:26.921038] Epoch: [68]  [194/195]  eta: 0:00:00  lr: 0.002391  loss: 0.5180 (0.5153)  time: 0.5123  data: 0.0001  max mem: 9341
[15:17:27.077490] Epoch: [68] Total time: 0:01:41 (0.5185 s / it)
[15:17:27.094830] Averaged stats: lr: 0.002391  loss: 0.5180 (0.5147)
[15:17:31.734145] {"train_lr": 0.0023917049038293513, "train_loss": 0.5146813089648883, "epoch": 68}
[15:17:31.734466] [15:17:31.734552] Training epoch 68 for 0:01:45
[15:17:31.734606] [15:17:31.739132] log_dir: ./exp/debug/cifar100-LT/debug
[15:17:33.491963] Epoch: [69]  [  0/195]  eta: 0:05:41  lr: 0.002391  loss: 0.5326 (0.5326)  time: 1.7514  data: 1.2407  max mem: 9341
[15:17:43.725239] Epoch: [69]  [ 20/195]  eta: 0:01:39  lr: 0.002391  loss: 0.5035 (0.5126)  time: 0.5116  data: 0.0002  max mem: 9341
[15:17:53.940638] Epoch: [69]  [ 40/195]  eta: 0:01:23  lr: 0.002391  loss: 0.5134 (0.5138)  time: 0.5107  data: 0.0002  max mem: 9341
[15:18:04.153388] Epoch: [69]  [ 60/195]  eta: 0:01:11  lr: 0.002391  loss: 0.5183 (0.5142)  time: 0.5106  data: 0.0002  max mem: 9341
[15:18:14.409851] Epoch: [69]  [ 80/195]  eta: 0:01:00  lr: 0.002391  loss: 0.5043 (0.5112)  time: 0.5128  data: 0.0003  max mem: 9341
[15:18:24.623780] Epoch: [69]  [100/195]  eta: 0:00:49  lr: 0.002391  loss: 0.5089 (0.5112)  time: 0.5106  data: 0.0002  max mem: 9341
[15:18:34.831194] Epoch: [69]  [120/195]  eta: 0:00:39  lr: 0.002391  loss: 0.4977 (0.5098)  time: 0.5103  data: 0.0003  max mem: 9341
[15:18:45.041292] Epoch: [69]  [140/195]  eta: 0:00:28  lr: 0.002391  loss: 0.4943 (0.5089)  time: 0.5105  data: 0.0002  max mem: 9341
[15:18:55.291605] Epoch: [69]  [160/195]  eta: 0:00:18  lr: 0.002391  loss: 0.5093 (0.5089)  time: 0.5125  data: 0.0002  max mem: 9341
[15:19:05.459450] Epoch: [69]  [180/195]  eta: 0:00:07  lr: 0.002391  loss: 0.5003 (0.5081)  time: 0.5083  data: 0.0002  max mem: 9341
[15:19:12.592768] Epoch: [69]  [194/195]  eta: 0:00:00  lr: 0.002391  loss: 0.4992 (0.5084)  time: 0.5106  data: 0.0001  max mem: 9341
[15:19:12.756820] Epoch: [69] Total time: 0:01:41 (0.5180 s / it)
[15:19:12.772360] Averaged stats: lr: 0.002391  loss: 0.4992 (0.5083)
[15:19:17.628188] {"train_lr": 0.002391112515764049, "train_loss": 0.5082641037992942, "epoch": 69}
[15:19:17.628467] [15:19:17.628553] Training epoch 69 for 0:01:45
[15:19:17.628606] [15:19:17.633147] log_dir: ./exp/debug/cifar100-LT/debug
[15:19:19.444935] Epoch: [70]  [  0/195]  eta: 0:05:52  lr: 0.002391  loss: 0.5426 (0.5426)  time: 1.8096  data: 1.3015  max mem: 9341
[15:19:29.660174] Epoch: [70]  [ 20/195]  eta: 0:01:40  lr: 0.002391  loss: 0.4969 (0.5033)  time: 0.5107  data: 0.0002  max mem: 9341
[15:19:39.875670] Epoch: [70]  [ 40/195]  eta: 0:01:24  lr: 0.002391  loss: 0.5023 (0.5035)  time: 0.5107  data: 0.0002  max mem: 9341
[15:19:50.088404] Epoch: [70]  [ 60/195]  eta: 0:01:11  lr: 0.002391  loss: 0.4974 (0.5039)  time: 0.5106  data: 0.0002  max mem: 9341
[15:20:00.345748] Epoch: [70]  [ 80/195]  eta: 0:01:00  lr: 0.002391  loss: 0.5125 (0.5064)  time: 0.5128  data: 0.0002  max mem: 9341
[15:20:10.559515] Epoch: [70]  [100/195]  eta: 0:00:49  lr: 0.002390  loss: 0.5279 (0.5102)  time: 0.5106  data: 0.0002  max mem: 9341
[15:20:20.797920] Epoch: [70]  [120/195]  eta: 0:00:39  lr: 0.002390  loss: 0.5002 (0.5100)  time: 0.5119  data: 0.0002  max mem: 9341
[15:20:31.039519] Epoch: [70]  [140/195]  eta: 0:00:28  lr: 0.002390  loss: 0.5234 (0.5121)  time: 0.5120  data: 0.0002  max mem: 9341
[15:20:41.340719] Epoch: [70]  [160/195]  eta: 0:00:18  lr: 0.002390  loss: 0.5105 (0.5128)  time: 0.5150  data: 0.0002  max mem: 9341
[15:20:51.533802] Epoch: [70]  [180/195]  eta: 0:00:07  lr: 0.002390  loss: 0.5228 (0.5137)  time: 0.5096  data: 0.0001  max mem: 9341
[15:20:58.688062] Epoch: [70]  [194/195]  eta: 0:00:00  lr: 0.002390  loss: 0.5081 (0.5132)  time: 0.5127  data: 0.0001  max mem: 9341
[15:20:58.864985] Epoch: [70] Total time: 0:01:41 (0.5191 s / it)
[15:20:58.871040] Averaged stats: lr: 0.002390  loss: 0.5081 (0.5156)
[15:21:03.584695] {"train_lr": 0.0023904997748722344, "train_loss": 0.5155593402874775, "epoch": 70}
[15:21:03.585033] [15:21:03.585117] Training epoch 70 for 0:01:45
[15:21:03.585171] [15:21:03.589648] log_dir: ./exp/debug/cifar100-LT/debug
[15:21:05.341867] Epoch: [71]  [  0/195]  eta: 0:05:41  lr: 0.002390  loss: 0.5326 (0.5326)  time: 1.7508  data: 1.2550  max mem: 9341
[15:21:15.562996] Epoch: [71]  [ 20/195]  eta: 0:01:39  lr: 0.002390  loss: 0.5131 (0.5102)  time: 0.5110  data: 0.0002  max mem: 9341
[15:21:25.775828] Epoch: [71]  [ 40/195]  eta: 0:01:23  lr: 0.002390  loss: 0.5058 (0.5110)  time: 0.5106  data: 0.0002  max mem: 9341
[15:21:35.997390] Epoch: [71]  [ 60/195]  eta: 0:01:11  lr: 0.002390  loss: 0.5212 (0.5147)  time: 0.5110  data: 0.0002  max mem: 9341
[15:21:46.276790] Epoch: [71]  [ 80/195]  eta: 0:01:00  lr: 0.002390  loss: 0.5140 (0.5148)  time: 0.5139  data: 0.0002  max mem: 9341
[15:21:56.494567] Epoch: [71]  [100/195]  eta: 0:00:49  lr: 0.002390  loss: 0.5131 (0.5129)  time: 0.5108  data: 0.0002  max mem: 9341
[15:22:06.709362] Epoch: [71]  [120/195]  eta: 0:00:39  lr: 0.002390  loss: 0.5011 (0.5115)  time: 0.5107  data: 0.0002  max mem: 9341
[15:22:16.948072] Epoch: [71]  [140/195]  eta: 0:00:28  lr: 0.002390  loss: 0.5160 (0.5121)  time: 0.5119  data: 0.0002  max mem: 9341
[15:22:27.242401] Epoch: [71]  [160/195]  eta: 0:00:18  lr: 0.002390  loss: 0.5057 (0.5114)  time: 0.5147  data: 0.0002  max mem: 9341
[15:22:37.434774] Epoch: [71]  [180/195]  eta: 0:00:07  lr: 0.002390  loss: 0.4915 (0.5095)  time: 0.5096  data: 0.0001  max mem: 9341
[15:22:44.581370] Epoch: [71]  [194/195]  eta: 0:00:00  lr: 0.002390  loss: 0.5157 (0.5104)  time: 0.5124  data: 0.0001  max mem: 9341
[15:22:44.739754] Epoch: [71] Total time: 0:01:41 (0.5187 s / it)
[15:22:44.762138] Averaged stats: lr: 0.002390  loss: 0.5157 (0.5098)
[15:22:49.457369] {"train_lr": 0.002389866691623969, "train_loss": 0.5097936247785886, "epoch": 71}
[15:22:49.457645] [15:22:49.457733] Training epoch 71 for 0:01:45
[15:22:49.457786] [15:22:49.462788] log_dir: ./exp/debug/cifar100-LT/debug
[15:22:51.131649] Epoch: [72]  [  0/195]  eta: 0:05:25  lr: 0.002390  loss: 0.4427 (0.4427)  time: 1.6679  data: 1.1579  max mem: 9341
[15:23:01.373685] Epoch: [72]  [ 20/195]  eta: 0:01:39  lr: 0.002389  loss: 0.5101 (0.5030)  time: 0.5120  data: 0.0002  max mem: 9341
[15:23:11.611411] Epoch: [72]  [ 40/195]  eta: 0:01:23  lr: 0.002389  loss: 0.4963 (0.5017)  time: 0.5118  data: 0.0002  max mem: 9341
[15:23:21.823197] Epoch: [72]  [ 60/195]  eta: 0:01:11  lr: 0.002389  loss: 0.5103 (0.5043)  time: 0.5105  data: 0.0002  max mem: 9341
[15:23:32.077092] Epoch: [72]  [ 80/195]  eta: 0:01:00  lr: 0.002389  loss: 0.5074 (0.5052)  time: 0.5126  data: 0.0002  max mem: 9341
[15:23:42.285439] Epoch: [72]  [100/195]  eta: 0:00:49  lr: 0.002389  loss: 0.4951 (0.5035)  time: 0.5104  data: 0.0002  max mem: 9341
[15:23:52.491492] Epoch: [72]  [120/195]  eta: 0:00:39  lr: 0.002389  loss: 0.4965 (0.5026)  time: 0.5102  data: 0.0002  max mem: 9341
[15:24:02.721260] Epoch: [72]  [140/195]  eta: 0:00:28  lr: 0.002389  loss: 0.4971 (0.5027)  time: 0.5114  data: 0.0002  max mem: 9341
[15:24:13.023012] Epoch: [72]  [160/195]  eta: 0:00:18  lr: 0.002389  loss: 0.4906 (0.5018)  time: 0.5150  data: 0.0002  max mem: 9341
[15:24:23.209961] Epoch: [72]  [180/195]  eta: 0:00:07  lr: 0.002389  loss: 0.5053 (0.5023)  time: 0.5093  data: 0.0001  max mem: 9341
[15:24:30.359175] Epoch: [72]  [194/195]  eta: 0:00:00  lr: 0.002389  loss: 0.5057 (0.5029)  time: 0.5123  data: 0.0001  max mem: 9341
[15:24:30.509233] Epoch: [72] Total time: 0:01:41 (0.5182 s / it)
[15:24:30.541475] Averaged stats: lr: 0.002389  loss: 0.5057 (0.5050)
[15:24:35.205372] {"train_lr": 0.0023892132768369018, "train_loss": 0.5049722956923338, "epoch": 72}
[15:24:35.205632] [15:24:35.205714] Training epoch 72 for 0:01:45
[15:24:35.205767] [15:24:35.210223] log_dir: ./exp/debug/cifar100-LT/debug
[15:24:36.843703] Epoch: [73]  [  0/195]  eta: 0:05:18  lr: 0.002389  loss: 0.5321 (0.5321)  time: 1.6323  data: 1.1159  max mem: 9341
[15:24:47.080892] Epoch: [73]  [ 20/195]  eta: 0:01:38  lr: 0.002389  loss: 0.5034 (0.5008)  time: 0.5118  data: 0.0002  max mem: 9341
[15:24:57.296445] Epoch: [73]  [ 40/195]  eta: 0:01:23  lr: 0.002389  loss: 0.5059 (0.5049)  time: 0.5107  data: 0.0002  max mem: 9341
[15:25:07.509567] Epoch: [73]  [ 60/195]  eta: 0:01:11  lr: 0.002389  loss: 0.5029 (0.5039)  time: 0.5106  data: 0.0002  max mem: 9341
[15:25:17.769208] Epoch: [73]  [ 80/195]  eta: 0:01:00  lr: 0.002389  loss: 0.5066 (0.5049)  time: 0.5129  data: 0.0002  max mem: 9341
[15:25:27.988418] Epoch: [73]  [100/195]  eta: 0:00:49  lr: 0.002389  loss: 0.4983 (0.5045)  time: 0.5109  data: 0.0002  max mem: 9341
[15:25:38.203256] Epoch: [73]  [120/195]  eta: 0:00:39  lr: 0.002388  loss: 0.5032 (0.5039)  time: 0.5107  data: 0.0002  max mem: 9341
[15:25:48.415096] Epoch: [73]  [140/195]  eta: 0:00:28  lr: 0.002388  loss: 0.4965 (0.5029)  time: 0.5105  data: 0.0002  max mem: 9341
[15:25:58.661750] Epoch: [73]  [160/195]  eta: 0:00:18  lr: 0.002388  loss: 0.4990 (0.5030)  time: 0.5123  data: 0.0002  max mem: 9341
[15:26:08.831443] Epoch: [73]  [180/195]  eta: 0:00:07  lr: 0.002388  loss: 0.4924 (0.5024)  time: 0.5084  data: 0.0002  max mem: 9341
[15:26:15.962567] Epoch: [73]  [194/195]  eta: 0:00:00  lr: 0.002388  loss: 0.4947 (0.5018)  time: 0.5104  data: 0.0001  max mem: 9341
[15:26:16.136722] Epoch: [73] Total time: 0:01:40 (0.5176 s / it)
[15:26:16.160798] Averaged stats: lr: 0.002388  loss: 0.4947 (0.4998)
[15:26:20.841515] {"train_lr": 0.002388539541676081, "train_loss": 0.4997995056020908, "epoch": 73}
[15:26:20.841788] [15:26:20.841874] Training epoch 73 for 0:01:45
[15:26:20.841927] [15:26:20.846410] log_dir: ./exp/debug/cifar100-LT/debug
[15:26:22.657100] Epoch: [74]  [  0/195]  eta: 0:05:52  lr: 0.002388  loss: 0.5446 (0.5446)  time: 1.8091  data: 1.2987  max mem: 9341
[15:26:32.939546] Epoch: [74]  [ 20/195]  eta: 0:01:40  lr: 0.002388  loss: 0.5107 (0.5087)  time: 0.5141  data: 0.0002  max mem: 9341
[15:26:43.159092] Epoch: [74]  [ 40/195]  eta: 0:01:24  lr: 0.002388  loss: 0.5089 (0.5134)  time: 0.5109  data: 0.0002  max mem: 9341
[15:26:53.372333] Epoch: [74]  [ 60/195]  eta: 0:01:11  lr: 0.002388  loss: 0.5125 (0.5140)  time: 0.5106  data: 0.0002  max mem: 9341
[15:27:03.630444] Epoch: [74]  [ 80/195]  eta: 0:01:00  lr: 0.002388  loss: 0.5081 (0.5120)  time: 0.5129  data: 0.0002  max mem: 9341
[15:27:13.869151] Epoch: [74]  [100/195]  eta: 0:00:49  lr: 0.002388  loss: 0.5141 (0.5128)  time: 0.5119  data: 0.0002  max mem: 9341
[15:27:24.105913] Epoch: [74]  [120/195]  eta: 0:00:39  lr: 0.002388  loss: 0.5061 (0.5111)  time: 0.5118  data: 0.0002  max mem: 9341
[15:27:34.341830] Epoch: [74]  [140/195]  eta: 0:00:28  lr: 0.002388  loss: 0.5096 (0.5109)  time: 0.5117  data: 0.0002  max mem: 9341
[15:27:44.647328] Epoch: [74]  [160/195]  eta: 0:00:18  lr: 0.002388  loss: 0.5054 (0.5117)  time: 0.5152  data: 0.0002  max mem: 9341
[15:27:54.845041] Epoch: [74]  [180/195]  eta: 0:00:07  lr: 0.002388  loss: 0.5057 (0.5109)  time: 0.5098  data: 0.0001  max mem: 9341
[15:28:01.999506] Epoch: [74]  [194/195]  eta: 0:00:00  lr: 0.002387  loss: 0.5078 (0.5113)  time: 0.5128  data: 0.0001  max mem: 9341
[15:28:02.174488] Epoch: [74] Total time: 0:01:41 (0.5196 s / it)
[15:28:02.181019] Averaged stats: lr: 0.002387  loss: 0.5078 (0.5108)
[15:28:06.950960] {"train_lr": 0.0023878454976537828, "train_loss": 0.5107963799666135, "epoch": 74}
[15:28:06.951229] [15:28:06.951318] Training epoch 74 for 0:01:46
[15:28:06.951371] [15:28:06.955997] log_dir: ./exp/debug/cifar100-LT/debug
[15:28:08.714409] Epoch: [75]  [  0/195]  eta: 0:05:42  lr: 0.002387  loss: 0.5702 (0.5702)  time: 1.7575  data: 1.2479  max mem: 9341
[15:28:18.949325] Epoch: [75]  [ 20/195]  eta: 0:01:39  lr: 0.002387  loss: 0.5028 (0.5104)  time: 0.5117  data: 0.0002  max mem: 9341
[15:28:29.163752] Epoch: [75]  [ 40/195]  eta: 0:01:23  lr: 0.002387  loss: 0.5243 (0.5153)  time: 0.5106  data: 0.0002  max mem: 9341
[15:28:39.379060] Epoch: [75]  [ 60/195]  eta: 0:01:11  lr: 0.002387  loss: 0.5332 (0.5202)  time: 0.5107  data: 0.0004  max mem: 9341
[15:28:49.637361] Epoch: [75]  [ 80/195]  eta: 0:01:00  lr: 0.002387  loss: 0.5057 (0.5164)  time: 0.5129  data: 0.0002  max mem: 9341
[15:28:59.851094] Epoch: [75]  [100/195]  eta: 0:00:49  lr: 0.002387  loss: 0.5078 (0.5136)  time: 0.5106  data: 0.0002  max mem: 9341
[15:29:10.063458] Epoch: [75]  [120/195]  eta: 0:00:39  lr: 0.002387  loss: 0.5058 (0.5127)  time: 0.5106  data: 0.0002  max mem: 9341
[15:29:20.279724] Epoch: [75]  [140/195]  eta: 0:00:28  lr: 0.002387  loss: 0.5074 (0.5131)  time: 0.5108  data: 0.0002  max mem: 9341
[15:29:30.537918] Epoch: [75]  [160/195]  eta: 0:00:18  lr: 0.002387  loss: 0.5060 (0.5121)  time: 0.5129  data: 0.0002  max mem: 9341
[15:29:40.711265] Epoch: [75]  [180/195]  eta: 0:00:07  lr: 0.002387  loss: 0.4999 (0.5106)  time: 0.5086  data: 0.0001  max mem: 9341
[15:29:47.850781] Epoch: [75]  [194/195]  eta: 0:00:00  lr: 0.002387  loss: 0.4942 (0.5101)  time: 0.5108  data: 0.0001  max mem: 9341
[15:29:48.011156] Epoch: [75] Total time: 0:01:41 (0.5182 s / it)
[15:29:48.026721] Averaged stats: lr: 0.002387  loss: 0.4942 (0.5072)
[15:29:52.744314] {"train_lr": 0.0023871311566293113, "train_loss": 0.5071590967285328, "epoch": 75}
[15:29:52.744662] [15:29:52.744748] Training epoch 75 for 0:01:45
[15:29:52.744802] [15:29:52.749277] log_dir: ./exp/debug/cifar100-LT/debug
[15:29:54.350152] Epoch: [76]  [  0/195]  eta: 0:05:11  lr: 0.002387  loss: 0.4812 (0.4812)  time: 1.5993  data: 1.0951  max mem: 9341
[15:30:04.582137] Epoch: [76]  [ 20/195]  eta: 0:01:38  lr: 0.002387  loss: 0.5007 (0.5066)  time: 0.5115  data: 0.0002  max mem: 9341
[15:30:14.799657] Epoch: [76]  [ 40/195]  eta: 0:01:23  lr: 0.002387  loss: 0.4996 (0.5061)  time: 0.5108  data: 0.0002  max mem: 9341
[15:30:25.012286] Epoch: [76]  [ 60/195]  eta: 0:01:11  lr: 0.002387  loss: 0.5068 (0.5048)  time: 0.5105  data: 0.0002  max mem: 9341
[15:30:35.271353] Epoch: [76]  [ 80/195]  eta: 0:01:00  lr: 0.002386  loss: 0.4968 (0.5031)  time: 0.5129  data: 0.0002  max mem: 9341
[15:30:45.505641] Epoch: [76]  [100/195]  eta: 0:00:49  lr: 0.002386  loss: 0.4878 (0.5013)  time: 0.5116  data: 0.0002  max mem: 9341
[15:30:55.714802] Epoch: [76]  [120/195]  eta: 0:00:39  lr: 0.002386  loss: 0.4849 (0.4985)  time: 0.5104  data: 0.0002  max mem: 9341
[15:31:05.925194] Epoch: [76]  [140/195]  eta: 0:00:28  lr: 0.002386  loss: 0.4850 (0.4979)  time: 0.5105  data: 0.0002  max mem: 9341
[15:31:16.179871] Epoch: [76]  [160/195]  eta: 0:00:18  lr: 0.002386  loss: 0.4990 (0.4991)  time: 0.5127  data: 0.0002  max mem: 9341
[15:31:26.352594] Epoch: [76]  [180/195]  eta: 0:00:07  lr: 0.002386  loss: 0.4932 (0.4982)  time: 0.5086  data: 0.0001  max mem: 9341
[15:31:33.481420] Epoch: [76]  [194/195]  eta: 0:00:00  lr: 0.002386  loss: 0.4859 (0.4980)  time: 0.5103  data: 0.0001  max mem: 9341
[15:31:33.661158] Epoch: [76] Total time: 0:01:40 (0.5175 s / it)
[15:31:33.672929] Averaged stats: lr: 0.002386  loss: 0.4859 (0.4982)
[15:31:38.280867] {"train_lr": 0.0023863965308087718, "train_loss": 0.4981877696055632, "epoch": 76}
[15:31:38.281217] [15:31:38.281308] Training epoch 76 for 0:01:45
[15:31:38.281363] [15:31:38.285816] log_dir: ./exp/debug/cifar100-LT/debug
[15:31:40.027873] Epoch: [77]  [  0/195]  eta: 0:05:39  lr: 0.002386  loss: 0.5067 (0.5067)  time: 1.7407  data: 1.2263  max mem: 9341
[15:31:50.306226] Epoch: [77]  [ 20/195]  eta: 0:01:40  lr: 0.002386  loss: 0.4982 (0.4978)  time: 0.5139  data: 0.0002  max mem: 9341
[15:32:00.525523] Epoch: [77]  [ 40/195]  eta: 0:01:24  lr: 0.002386  loss: 0.4797 (0.4906)  time: 0.5109  data: 0.0002  max mem: 9341
[15:32:10.740541] Epoch: [77]  [ 60/195]  eta: 0:01:11  lr: 0.002386  loss: 0.4970 (0.4911)  time: 0.5107  data: 0.0002  max mem: 9341
[15:32:21.006661] Epoch: [77]  [ 80/195]  eta: 0:01:00  lr: 0.002386  loss: 0.4881 (0.4914)  time: 0.5132  data: 0.0002  max mem: 9341
[15:32:31.223108] Epoch: [77]  [100/195]  eta: 0:00:49  lr: 0.002386  loss: 0.5018 (0.4933)  time: 0.5108  data: 0.0002  max mem: 9341
[15:32:41.437968] Epoch: [77]  [120/195]  eta: 0:00:39  lr: 0.002386  loss: 0.4931 (0.4943)  time: 0.5107  data: 0.0002  max mem: 9341
[15:32:51.655240] Epoch: [77]  [140/195]  eta: 0:00:28  lr: 0.002385  loss: 0.4884 (0.4938)  time: 0.5108  data: 0.0002  max mem: 9341
[15:33:01.912071] Epoch: [77]  [160/195]  eta: 0:00:18  lr: 0.002385  loss: 0.5055 (0.4953)  time: 0.5128  data: 0.0002  max mem: 9341
[15:33:12.086122] Epoch: [77]  [180/195]  eta: 0:00:07  lr: 0.002385  loss: 0.5014 (0.4955)  time: 0.5086  data: 0.0001  max mem: 9341
[15:33:19.216919] Epoch: [77]  [194/195]  eta: 0:00:00  lr: 0.002385  loss: 0.4923 (0.4947)  time: 0.5106  data: 0.0001  max mem: 9341
[15:33:19.381806] Epoch: [77] Total time: 0:01:41 (0.5184 s / it)
[15:33:19.392390] Averaged stats: lr: 0.002385  loss: 0.4923 (0.4947)
[15:33:24.090205] {"train_lr": 0.002385641632744912, "train_loss": 0.4947165261858549, "epoch": 77}
[15:33:24.090467] [15:33:24.090553] Training epoch 77 for 0:01:45
[15:33:24.090608] [15:33:24.095089] log_dir: ./exp/debug/cifar100-LT/debug
[15:33:25.674259] Epoch: [78]  [  0/195]  eta: 0:05:07  lr: 0.002385  loss: 0.4900 (0.4900)  time: 1.5777  data: 1.0755  max mem: 9341
[15:33:35.893526] Epoch: [78]  [ 20/195]  eta: 0:01:38  lr: 0.002385  loss: 0.4864 (0.4857)  time: 0.5109  data: 0.0002  max mem: 9341
[15:33:46.113546] Epoch: [78]  [ 40/195]  eta: 0:01:23  lr: 0.002385  loss: 0.4870 (0.4840)  time: 0.5109  data: 0.0002  max mem: 9341
[15:33:56.331411] Epoch: [78]  [ 60/195]  eta: 0:01:11  lr: 0.002385  loss: 0.4889 (0.4863)  time: 0.5108  data: 0.0002  max mem: 9341
[15:34:06.591995] Epoch: [78]  [ 80/195]  eta: 0:01:00  lr: 0.002385  loss: 0.4866 (0.4874)  time: 0.5130  data: 0.0002  max mem: 9341
[15:34:16.805587] Epoch: [78]  [100/195]  eta: 0:00:49  lr: 0.002385  loss: 0.4914 (0.4890)  time: 0.5106  data: 0.0002  max mem: 9341
[15:34:27.016411] Epoch: [78]  [120/195]  eta: 0:00:38  lr: 0.002385  loss: 0.4949 (0.4909)  time: 0.5105  data: 0.0002  max mem: 9341
[15:34:37.235719] Epoch: [78]  [140/195]  eta: 0:00:28  lr: 0.002385  loss: 0.4924 (0.4923)  time: 0.5109  data: 0.0002  max mem: 9341
[15:34:47.488054] Epoch: [78]  [160/195]  eta: 0:00:18  lr: 0.002385  loss: 0.4906 (0.4927)  time: 0.5126  data: 0.0002  max mem: 9341
[15:34:57.675626] Epoch: [78]  [180/195]  eta: 0:00:07  lr: 0.002385  loss: 0.5036 (0.4932)  time: 0.5093  data: 0.0001  max mem: 9341
[15:35:04.804980] Epoch: [78]  [194/195]  eta: 0:00:00  lr: 0.002384  loss: 0.5067 (0.4944)  time: 0.5107  data: 0.0001  max mem: 9341
[15:35:04.992683] Epoch: [78] Total time: 0:01:40 (0.5174 s / it)
[15:35:04.993383] Averaged stats: lr: 0.002384  loss: 0.5067 (0.4946)
[15:35:09.596970] {"train_lr": 0.0023848664753368383, "train_loss": 0.4945544156890649, "epoch": 78}
[15:35:09.597239] [15:35:09.597326] Training epoch 78 for 0:01:45
[15:35:09.597380] [15:35:09.602282] log_dir: ./exp/debug/cifar100-LT/debug
[15:35:11.291595] Epoch: [79]  [  0/195]  eta: 0:05:29  lr: 0.002384  loss: 0.5260 (0.5260)  time: 1.6883  data: 1.1760  max mem: 9341
[15:35:21.515399] Epoch: [79]  [ 20/195]  eta: 0:01:39  lr: 0.002384  loss: 0.4882 (0.4909)  time: 0.5111  data: 0.0002  max mem: 9341
[15:35:31.739635] Epoch: [79]  [ 40/195]  eta: 0:01:23  lr: 0.002384  loss: 0.4924 (0.4924)  time: 0.5111  data: 0.0002  max mem: 9341
[15:35:41.960377] Epoch: [79]  [ 60/195]  eta: 0:01:11  lr: 0.002384  loss: 0.4843 (0.4939)  time: 0.5110  data: 0.0002  max mem: 9341
[15:35:52.215905] Epoch: [79]  [ 80/195]  eta: 0:01:00  lr: 0.002384  loss: 0.5023 (0.4950)  time: 0.5127  data: 0.0002  max mem: 9341
[15:36:02.428606] Epoch: [79]  [100/195]  eta: 0:00:49  lr: 0.002384  loss: 0.4953 (0.4953)  time: 0.5106  data: 0.0002  max mem: 9341
[15:36:12.639039] Epoch: [79]  [120/195]  eta: 0:00:39  lr: 0.002384  loss: 0.4973 (0.4957)  time: 0.5105  data: 0.0002  max mem: 9341
[15:36:22.852174] Epoch: [79]  [140/195]  eta: 0:00:28  lr: 0.002384  loss: 0.4975 (0.4960)  time: 0.5106  data: 0.0002  max mem: 9341
[15:36:33.128458] Epoch: [79]  [160/195]  eta: 0:00:18  lr: 0.002384  loss: 0.5113 (0.4976)  time: 0.5138  data: 0.0002  max mem: 9341
[15:36:43.317782] Epoch: [79]  [180/195]  eta: 0:00:07  lr: 0.002384  loss: 0.5020 (0.4987)  time: 0.5094  data: 0.0001  max mem: 9341
[15:36:50.467004] Epoch: [79]  [194/195]  eta: 0:00:00  lr: 0.002384  loss: 0.4973 (0.4999)  time: 0.5124  data: 0.0001  max mem: 9341
[15:36:50.650478] Epoch: [79] Total time: 0:01:41 (0.5182 s / it)
[15:36:50.660054] Averaged stats: lr: 0.002384  loss: 0.4973 (0.4975)
[15:36:55.391985] {"train_lr": 0.002384071071829865, "train_loss": 0.4975479953182049, "epoch": 79}
[15:36:55.392380] [15:36:55.392472] Training epoch 79 for 0:01:45
[15:36:55.392525] [15:36:55.397127] log_dir: ./exp/debug/cifar100-LT/debug
[15:36:57.216541] Epoch: [80]  [  0/195]  eta: 0:05:54  lr: 0.002384  loss: 0.5243 (0.5243)  time: 1.8173  data: 1.3028  max mem: 9341
[15:37:07.425623] Epoch: [80]  [ 20/195]  eta: 0:01:40  lr: 0.002384  loss: 0.4936 (0.5021)  time: 0.5104  data: 0.0002  max mem: 9341
[15:37:17.633104] Epoch: [80]  [ 40/195]  eta: 0:01:24  lr: 0.002383  loss: 0.4926 (0.4992)  time: 0.5103  data: 0.0002  max mem: 9341
[15:37:27.851638] Epoch: [80]  [ 60/195]  eta: 0:01:11  lr: 0.002383  loss: 0.5035 (0.5016)  time: 0.5109  data: 0.0002  max mem: 9341
[15:37:38.107548] Epoch: [80]  [ 80/195]  eta: 0:01:00  lr: 0.002383  loss: 0.4944 (0.5023)  time: 0.5127  data: 0.0002  max mem: 9341
[15:37:48.322841] Epoch: [80]  [100/195]  eta: 0:00:49  lr: 0.002383  loss: 0.4875 (0.4996)  time: 0.5107  data: 0.0002  max mem: 9341
[15:37:58.537933] Epoch: [80]  [120/195]  eta: 0:00:39  lr: 0.002383  loss: 0.5026 (0.5001)  time: 0.5107  data: 0.0002  max mem: 9341
[15:38:08.742627] Epoch: [80]  [140/195]  eta: 0:00:28  lr: 0.002383  loss: 0.5064 (0.5016)  time: 0.5102  data: 0.0002  max mem: 9341
[15:38:19.002750] Epoch: [80]  [160/195]  eta: 0:00:18  lr: 0.002383  loss: 0.4994 (0.5008)  time: 0.5130  data: 0.0002  max mem: 9341
[15:38:29.174454] Epoch: [80]  [180/195]  eta: 0:00:07  lr: 0.002383  loss: 0.5025 (0.5015)  time: 0.5085  data: 0.0001  max mem: 9341
[15:38:36.306569] Epoch: [80]  [194/195]  eta: 0:00:00  lr: 0.002383  loss: 0.5075 (0.5019)  time: 0.5107  data: 0.0001  max mem: 9341
[15:38:36.467316] Epoch: [80] Total time: 0:01:41 (0.5183 s / it)
[15:38:36.474054] Averaged stats: lr: 0.002383  loss: 0.5075 (0.5007)
[15:38:41.151667] {"train_lr": 0.0023832554358152297, "train_loss": 0.5007028277103718, "epoch": 80}
[15:38:41.151980] [15:38:41.152063] Training epoch 80 for 0:01:45
[15:38:41.152147] [15:38:41.156658] log_dir: ./exp/debug/cifar100-LT/debug
[15:38:42.906083] Epoch: [81]  [  0/195]  eta: 0:05:40  lr: 0.002383  loss: 0.5346 (0.5346)  time: 1.7480  data: 1.2304  max mem: 9341
[15:38:53.120914] Epoch: [81]  [ 20/195]  eta: 0:01:39  lr: 0.002383  loss: 0.5035 (0.5007)  time: 0.5107  data: 0.0002  max mem: 9341
[15:39:03.335656] Epoch: [81]  [ 40/195]  eta: 0:01:23  lr: 0.002383  loss: 0.4781 (0.4949)  time: 0.5107  data: 0.0002  max mem: 9341
[15:39:13.561136] Epoch: [81]  [ 60/195]  eta: 0:01:11  lr: 0.002383  loss: 0.4880 (0.4921)  time: 0.5112  data: 0.0002  max mem: 9341
[15:39:23.818650] Epoch: [81]  [ 80/195]  eta: 0:01:00  lr: 0.002382  loss: 0.4894 (0.4927)  time: 0.5128  data: 0.0002  max mem: 9341
[15:39:34.030239] Epoch: [81]  [100/195]  eta: 0:00:49  lr: 0.002382  loss: 0.4932 (0.4932)  time: 0.5105  data: 0.0002  max mem: 9341
[15:39:44.241417] Epoch: [81]  [120/195]  eta: 0:00:39  lr: 0.002382  loss: 0.4839 (0.4925)  time: 0.5105  data: 0.0002  max mem: 9341
[15:39:54.454915] Epoch: [81]  [140/195]  eta: 0:00:28  lr: 0.002382  loss: 0.5010 (0.4941)  time: 0.5106  data: 0.0002  max mem: 9341
[15:40:04.709966] Epoch: [81]  [160/195]  eta: 0:00:18  lr: 0.002382  loss: 0.4836 (0.4933)  time: 0.5127  data: 0.0002  max mem: 9341
[15:40:14.884543] Epoch: [81]  [180/195]  eta: 0:00:07  lr: 0.002382  loss: 0.4868 (0.4927)  time: 0.5087  data: 0.0001  max mem: 9341
[15:40:22.013443] Epoch: [81]  [194/195]  eta: 0:00:00  lr: 0.002382  loss: 0.4853 (0.4931)  time: 0.5105  data: 0.0001  max mem: 9341
[15:40:22.179786] Epoch: [81] Total time: 0:01:41 (0.5181 s / it)
[15:40:22.192984] Averaged stats: lr: 0.002382  loss: 0.4853 (0.4919)
[15:40:26.898050] {"train_lr": 0.0023824195812299125, "train_loss": 0.4918657238666828, "epoch": 81}
[15:40:26.898386] [15:40:26.898480] Training epoch 81 for 0:01:45
[15:40:26.898535] [15:40:26.903165] log_dir: ./exp/debug/cifar100-LT/debug
[15:40:28.728108] Epoch: [82]  [  0/195]  eta: 0:05:55  lr: 0.002382  loss: 0.5116 (0.5116)  time: 1.8239  data: 1.3136  max mem: 9341
[15:40:38.943503] Epoch: [82]  [ 20/195]  eta: 0:01:40  lr: 0.002382  loss: 0.4938 (0.5012)  time: 0.5107  data: 0.0002  max mem: 9341
[15:40:49.164648] Epoch: [82]  [ 40/195]  eta: 0:01:24  lr: 0.002382  loss: 0.4751 (0.4935)  time: 0.5110  data: 0.0002  max mem: 9341
[15:40:59.376856] Epoch: [82]  [ 60/195]  eta: 0:01:11  lr: 0.002382  loss: 0.4840 (0.4920)  time: 0.5105  data: 0.0002  max mem: 9341
[15:41:09.643739] Epoch: [82]  [ 80/195]  eta: 0:01:00  lr: 0.002382  loss: 0.4846 (0.4902)  time: 0.5133  data: 0.0002  max mem: 9341
[15:41:19.860876] Epoch: [82]  [100/195]  eta: 0:00:49  lr: 0.002382  loss: 0.4883 (0.4905)  time: 0.5108  data: 0.0002  max mem: 9341
[15:41:30.068038] Epoch: [82]  [120/195]  eta: 0:00:39  lr: 0.002381  loss: 0.4885 (0.4908)  time: 0.5103  data: 0.0002  max mem: 9341
[15:41:40.277487] Epoch: [82]  [140/195]  eta: 0:00:28  lr: 0.002381  loss: 0.4814 (0.4903)  time: 0.5104  data: 0.0002  max mem: 9341
[15:41:50.537267] Epoch: [82]  [160/195]  eta: 0:00:18  lr: 0.002381  loss: 0.4892 (0.4909)  time: 0.5129  data: 0.0002  max mem: 9341
[15:42:00.714031] Epoch: [82]  [180/195]  eta: 0:00:07  lr: 0.002381  loss: 0.4808 (0.4902)  time: 0.5088  data: 0.0001  max mem: 9341
[15:42:07.845738] Epoch: [82]  [194/195]  eta: 0:00:00  lr: 0.002381  loss: 0.4862 (0.4903)  time: 0.5107  data: 0.0001  max mem: 9341
[15:42:08.011181] Epoch: [82] Total time: 0:01:41 (0.5185 s / it)
[15:42:08.012118] Averaged stats: lr: 0.002381  loss: 0.4862 (0.4868)
[15:42:12.716037] {"train_lr": 0.0023815635223563532, "train_loss": 0.4867895292930114, "epoch": 82}
[15:42:12.716383] [15:42:12.716481] Training epoch 82 for 0:01:45
[15:42:12.716535] [15:42:12.721452] log_dir: ./exp/debug/cifar100-LT/debug
[15:42:14.458412] Epoch: [83]  [  0/195]  eta: 0:05:38  lr: 0.002381  loss: 0.4842 (0.4842)  time: 1.7356  data: 1.2371  max mem: 9341
[15:42:24.675724] Epoch: [83]  [ 20/195]  eta: 0:01:39  lr: 0.002381  loss: 0.4677 (0.4765)  time: 0.5108  data: 0.0002  max mem: 9341
[15:42:34.897598] Epoch: [83]  [ 40/195]  eta: 0:01:23  lr: 0.002381  loss: 0.4834 (0.4828)  time: 0.5110  data: 0.0002  max mem: 9341
[15:42:45.135373] Epoch: [83]  [ 60/195]  eta: 0:01:11  lr: 0.002381  loss: 0.4938 (0.4863)  time: 0.5118  data: 0.0002  max mem: 9341
[15:42:55.431147] Epoch: [83]  [ 80/195]  eta: 0:01:00  lr: 0.002381  loss: 0.4908 (0.4872)  time: 0.5147  data: 0.0002  max mem: 9341
[15:43:05.648443] Epoch: [83]  [100/195]  eta: 0:00:49  lr: 0.002381  loss: 0.5024 (0.4899)  time: 0.5108  data: 0.0002  max mem: 9341
[15:43:15.861238] Epoch: [83]  [120/195]  eta: 0:00:39  lr: 0.002381  loss: 0.4929 (0.4904)  time: 0.5106  data: 0.0002  max mem: 9341
[15:43:26.073778] Epoch: [83]  [140/195]  eta: 0:00:28  lr: 0.002381  loss: 0.4760 (0.4893)  time: 0.5105  data: 0.0002  max mem: 9341
[15:43:36.331929] Epoch: [83]  [160/195]  eta: 0:00:18  lr: 0.002380  loss: 0.4832 (0.4879)  time: 0.5128  data: 0.0002  max mem: 9341
[15:43:46.502501] Epoch: [83]  [180/195]  eta: 0:00:07  lr: 0.002380  loss: 0.4897 (0.4879)  time: 0.5085  data: 0.0002  max mem: 9341
[15:43:53.631256] Epoch: [83]  [194/195]  eta: 0:00:00  lr: 0.002380  loss: 0.4897 (0.4886)  time: 0.5105  data: 0.0001  max mem: 9341
[15:43:53.801905] Epoch: [83] Total time: 0:01:41 (0.5184 s / it)
[15:43:53.812847] Averaged stats: lr: 0.002380  loss: 0.4897 (0.4897)
[15:43:58.494776] {"train_lr": 0.002380687273822253, "train_loss": 0.4896614491557464, "epoch": 83}
[15:43:58.495041] [15:43:58.495124] Training epoch 83 for 0:01:45
[15:43:58.495177] [15:43:58.499704] log_dir: ./exp/debug/cifar100-LT/debug
[15:44:00.372733] Epoch: [84]  [  0/195]  eta: 0:06:05  lr: 0.002380  loss: 0.4958 (0.4958)  time: 1.8721  data: 1.3768  max mem: 9341
[15:44:10.595780] Epoch: [84]  [ 20/195]  eta: 0:01:40  lr: 0.002380  loss: 0.4839 (0.4921)  time: 0.5111  data: 0.0002  max mem: 9341
[15:44:20.810951] Epoch: [84]  [ 40/195]  eta: 0:01:24  lr: 0.002380  loss: 0.4808 (0.4871)  time: 0.5107  data: 0.0002  max mem: 9341
[15:44:31.044296] Epoch: [84]  [ 60/195]  eta: 0:01:12  lr: 0.002380  loss: 0.4916 (0.4893)  time: 0.5116  data: 0.0002  max mem: 9341
[15:44:41.347293] Epoch: [84]  [ 80/195]  eta: 0:01:00  lr: 0.002380  loss: 0.4695 (0.4863)  time: 0.5151  data: 0.0002  max mem: 9341
[15:44:51.583388] Epoch: [84]  [100/195]  eta: 0:00:49  lr: 0.002380  loss: 0.4913 (0.4870)  time: 0.5118  data: 0.0002  max mem: 9341
[15:45:01.816760] Epoch: [84]  [120/195]  eta: 0:00:39  lr: 0.002380  loss: 0.4893 (0.4865)  time: 0.5116  data: 0.0002  max mem: 9341
[15:45:12.051853] Epoch: [84]  [140/195]  eta: 0:00:28  lr: 0.002380  loss: 0.4751 (0.4853)  time: 0.5117  data: 0.0002  max mem: 9341
[15:45:22.346844] Epoch: [84]  [160/195]  eta: 0:00:18  lr: 0.002379  loss: 0.4847 (0.4857)  time: 0.5147  data: 0.0002  max mem: 9341
[15:45:32.521235] Epoch: [84]  [180/195]  eta: 0:00:07  lr: 0.002379  loss: 0.4930 (0.4862)  time: 0.5087  data: 0.0001  max mem: 9341
[15:45:39.653178] Epoch: [84]  [194/195]  eta: 0:00:00  lr: 0.002379  loss: 0.4784 (0.4850)  time: 0.5108  data: 0.0001  max mem: 9341
[15:45:39.814014] Epoch: [84] Total time: 0:01:41 (0.5196 s / it)
[15:45:39.833998] Averaged stats: lr: 0.002379  loss: 0.4784 (0.4877)
[15:45:44.517649] {"train_lr": 0.0023797908506002596, "train_loss": 0.4877037171752025, "epoch": 84}
[15:45:44.517901] [15:45:44.517982] Training epoch 84 for 0:01:46
[15:45:44.518038] [15:45:44.522515] log_dir: ./exp/debug/cifar100-LT/debug
[15:45:46.095939] Epoch: [85]  [  0/195]  eta: 0:05:06  lr: 0.002379  loss: 0.4736 (0.4736)  time: 1.5718  data: 1.0699  max mem: 9341
[15:45:56.314354] Epoch: [85]  [ 20/195]  eta: 0:01:38  lr: 0.002379  loss: 0.4866 (0.4922)  time: 0.5109  data: 0.0002  max mem: 9341
[15:46:06.534455] Epoch: [85]  [ 40/195]  eta: 0:01:23  lr: 0.002379  loss: 0.4903 (0.4888)  time: 0.5110  data: 0.0002  max mem: 9341
[15:46:16.755790] Epoch: [85]  [ 60/195]  eta: 0:01:11  lr: 0.002379  loss: 0.4799 (0.4876)  time: 0.5110  data: 0.0002  max mem: 9341
[15:46:27.011780] Epoch: [85]  [ 80/195]  eta: 0:01:00  lr: 0.002379  loss: 0.4825 (0.4871)  time: 0.5127  data: 0.0002  max mem: 9341
[15:46:37.219248] Epoch: [85]  [100/195]  eta: 0:00:49  lr: 0.002379  loss: 0.4904 (0.4877)  time: 0.5103  data: 0.0002  max mem: 9341
[15:46:47.447637] Epoch: [85]  [120/195]  eta: 0:00:38  lr: 0.002379  loss: 0.4902 (0.4890)  time: 0.5114  data: 0.0002  max mem: 9341
[15:46:57.660205] Epoch: [85]  [140/195]  eta: 0:00:28  lr: 0.002379  loss: 0.4723 (0.4872)  time: 0.5106  data: 0.0002  max mem: 9341
[15:47:07.910123] Epoch: [85]  [160/195]  eta: 0:00:18  lr: 0.002379  loss: 0.4738 (0.4860)  time: 0.5124  data: 0.0002  max mem: 9341
[15:47:18.075235] Epoch: [85]  [180/195]  eta: 0:00:07  lr: 0.002378  loss: 0.4783 (0.4851)  time: 0.5082  data: 0.0001  max mem: 9341
[15:47:25.203919] Epoch: [85]  [194/195]  eta: 0:00:00  lr: 0.002378  loss: 0.4803 (0.4847)  time: 0.5104  data: 0.0001  max mem: 9341
[15:47:25.375592] Epoch: [85] Total time: 0:01:40 (0.5172 s / it)
[15:47:25.386637] Averaged stats: lr: 0.002378  loss: 0.4803 (0.4836)
[15:47:30.088458] {"train_lr": 0.002378874268007788, "train_loss": 0.4836318870767569, "epoch": 85}
[15:47:30.088782] [15:47:30.088866] Training epoch 85 for 0:01:45
[15:47:30.088919] [15:47:30.093250] log_dir: ./exp/debug/cifar100-LT/debug
[15:47:31.760447] Epoch: [86]  [  0/195]  eta: 0:05:24  lr: 0.002378  loss: 0.4563 (0.4563)  time: 1.6658  data: 1.1713  max mem: 9341
[15:47:42.023586] Epoch: [86]  [ 20/195]  eta: 0:01:39  lr: 0.002378  loss: 0.4888 (0.4892)  time: 0.5131  data: 0.0002  max mem: 9341
[15:47:52.238766] Epoch: [86]  [ 40/195]  eta: 0:01:23  lr: 0.002378  loss: 0.4772 (0.4865)  time: 0.5107  data: 0.0002  max mem: 9341
[15:48:02.459248] Epoch: [86]  [ 60/195]  eta: 0:01:11  lr: 0.002378  loss: 0.4829 (0.4844)  time: 0.5109  data: 0.0002  max mem: 9341
[15:48:12.719539] Epoch: [86]  [ 80/195]  eta: 0:01:00  lr: 0.002378  loss: 0.4774 (0.4829)  time: 0.5129  data: 0.0002  max mem: 9341
[15:48:22.927518] Epoch: [86]  [100/195]  eta: 0:00:49  lr: 0.002378  loss: 0.4615 (0.4802)  time: 0.5103  data: 0.0002  max mem: 9341
[15:48:33.143058] Epoch: [86]  [120/195]  eta: 0:00:39  lr: 0.002378  loss: 0.4852 (0.4821)  time: 0.5107  data: 0.0002  max mem: 9341
[15:48:43.361436] Epoch: [86]  [140/195]  eta: 0:00:28  lr: 0.002378  loss: 0.4848 (0.4828)  time: 0.5109  data: 0.0002  max mem: 9341
[15:48:53.621415] Epoch: [86]  [160/195]  eta: 0:00:18  lr: 0.002378  loss: 0.4825 (0.4830)  time: 0.5129  data: 0.0002  max mem: 9341
[15:49:03.798902] Epoch: [86]  [180/195]  eta: 0:00:07  lr: 0.002378  loss: 0.4873 (0.4840)  time: 0.5088  data: 0.0001  max mem: 9341
[15:49:10.930762] Epoch: [86]  [194/195]  eta: 0:00:00  lr: 0.002377  loss: 0.4936 (0.4848)  time: 0.5107  data: 0.0001  max mem: 9341
[15:49:11.098078] Epoch: [86] Total time: 0:01:41 (0.5180 s / it)
[15:49:11.100964] Averaged stats: lr: 0.002377  loss: 0.4936 (0.4834)
[15:49:15.817182] {"train_lr": 0.0023779375417067048, "train_loss": 0.4834466596444448, "epoch": 86}
[15:49:15.817515] [15:49:15.817600] Training epoch 86 for 0:01:45
[15:49:15.817653] [15:49:15.822217] log_dir: ./exp/debug/cifar100-LT/debug
[15:49:17.419994] Epoch: [87]  [  0/195]  eta: 0:05:11  lr: 0.002377  loss: 0.4688 (0.4688)  time: 1.5963  data: 1.0961  max mem: 9341
[15:49:27.643004] Epoch: [87]  [ 20/195]  eta: 0:01:38  lr: 0.002377  loss: 0.4862 (0.4797)  time: 0.5111  data: 0.0002  max mem: 9341
[15:49:37.860973] Epoch: [87]  [ 40/195]  eta: 0:01:23  lr: 0.002377  loss: 0.4863 (0.4844)  time: 0.5108  data: 0.0002  max mem: 9341
[15:49:48.070665] Epoch: [87]  [ 60/195]  eta: 0:01:11  lr: 0.002377  loss: 0.4889 (0.4849)  time: 0.5104  data: 0.0002  max mem: 9341
[15:49:58.331681] Epoch: [87]  [ 80/195]  eta: 0:01:00  lr: 0.002377  loss: 0.4880 (0.4848)  time: 0.5130  data: 0.0002  max mem: 9341
[15:50:08.552053] Epoch: [87]  [100/195]  eta: 0:00:49  lr: 0.002377  loss: 0.4751 (0.4840)  time: 0.5110  data: 0.0002  max mem: 9341
[15:50:18.762110] Epoch: [87]  [120/195]  eta: 0:00:39  lr: 0.002377  loss: 0.4912 (0.4845)  time: 0.5104  data: 0.0002  max mem: 9341
[15:50:28.977634] Epoch: [87]  [140/195]  eta: 0:00:28  lr: 0.002377  loss: 0.4736 (0.4838)  time: 0.5107  data: 0.0002  max mem: 9341
[15:50:39.248668] Epoch: [87]  [160/195]  eta: 0:00:18  lr: 0.002377  loss: 0.4900 (0.4844)  time: 0.5135  data: 0.0002  max mem: 9341
[15:50:49.420302] Epoch: [87]  [180/195]  eta: 0:00:07  lr: 0.002377  loss: 0.4742 (0.4836)  time: 0.5085  data: 0.0001  max mem: 9341
[15:50:56.553886] Epoch: [87]  [194/195]  eta: 0:00:00  lr: 0.002376  loss: 0.4940 (0.4845)  time: 0.5107  data: 0.0001  max mem: 9341
[15:50:56.735638] Epoch: [87] Total time: 0:01:40 (0.5175 s / it)
[15:50:56.736417] Averaged stats: lr: 0.002376  loss: 0.4940 (0.4857)
[15:51:01.430969] {"train_lr": 0.0023769806877030654, "train_loss": 0.48567323046617017, "epoch": 87}
[15:51:01.431256] [15:51:01.431344] Training epoch 87 for 0:01:45
[15:51:01.431397] [15:51:01.436019] log_dir: ./exp/debug/cifar100-LT/debug
[15:51:03.295541] Epoch: [88]  [  0/195]  eta: 0:06:02  lr: 0.002376  loss: 0.5102 (0.5102)  time: 1.8581  data: 1.3618  max mem: 9341
[15:51:13.511416] Epoch: [88]  [ 20/195]  eta: 0:01:40  lr: 0.002376  loss: 0.4777 (0.4843)  time: 0.5107  data: 0.0002  max mem: 9341
[15:51:23.789456] Epoch: [88]  [ 40/195]  eta: 0:01:24  lr: 0.002376  loss: 0.4843 (0.4825)  time: 0.5138  data: 0.0002  max mem: 9341
[15:51:34.002167] Epoch: [88]  [ 60/195]  eta: 0:01:12  lr: 0.002376  loss: 0.4900 (0.4857)  time: 0.5106  data: 0.0002  max mem: 9341
[15:51:44.265627] Epoch: [88]  [ 80/195]  eta: 0:01:00  lr: 0.002376  loss: 0.4775 (0.4839)  time: 0.5131  data: 0.0002  max mem: 9341
[15:51:54.530404] Epoch: [88]  [100/195]  eta: 0:00:49  lr: 0.002376  loss: 0.4818 (0.4835)  time: 0.5132  data: 0.0002  max mem: 9341
[15:52:04.745205] Epoch: [88]  [120/195]  eta: 0:00:39  lr: 0.002376  loss: 0.4739 (0.4811)  time: 0.5107  data: 0.0002  max mem: 9341
[15:52:14.963174] Epoch: [88]  [140/195]  eta: 0:00:28  lr: 0.002376  loss: 0.4708 (0.4805)  time: 0.5108  data: 0.0002  max mem: 9341
[15:52:25.259105] Epoch: [88]  [160/195]  eta: 0:00:18  lr: 0.002376  loss: 0.4767 (0.4803)  time: 0.5147  data: 0.0002  max mem: 9341
[15:52:35.437315] Epoch: [88]  [180/195]  eta: 0:00:07  lr: 0.002376  loss: 0.4796 (0.4808)  time: 0.5089  data: 0.0001  max mem: 9341
[15:52:42.568969] Epoch: [88]  [194/195]  eta: 0:00:00  lr: 0.002375  loss: 0.4741 (0.4800)  time: 0.5106  data: 0.0001  max mem: 9341
[15:52:42.740303] Epoch: [88] Total time: 0:01:41 (0.5195 s / it)
[15:52:42.755780] Averaged stats: lr: 0.002375  loss: 0.4741 (0.4796)
[15:52:47.425054] {"train_lr": 0.0023760037223468766, "train_loss": 0.4796049544826532, "epoch": 88}
[15:52:47.425384] [15:52:47.425471] Training epoch 88 for 0:01:45
[15:52:47.425526] [15:52:47.430015] log_dir: ./exp/debug/cifar100-LT/debug
[15:52:49.042233] Epoch: [89]  [  0/195]  eta: 0:05:14  lr: 0.002375  loss: 0.4550 (0.4550)  time: 1.6110  data: 1.1122  max mem: 9341
[15:52:59.253137] Epoch: [89]  [ 20/195]  eta: 0:01:38  lr: 0.002375  loss: 0.4713 (0.4704)  time: 0.5105  data: 0.0002  max mem: 9341
[15:53:09.462597] Epoch: [89]  [ 40/195]  eta: 0:01:23  lr: 0.002375  loss: 0.4796 (0.4754)  time: 0.5104  data: 0.0002  max mem: 9341
[15:53:19.680073] Epoch: [89]  [ 60/195]  eta: 0:01:11  lr: 0.002375  loss: 0.4705 (0.4736)  time: 0.5108  data: 0.0002  max mem: 9341
[15:53:29.938885] Epoch: [89]  [ 80/195]  eta: 0:01:00  lr: 0.002375  loss: 0.4785 (0.4765)  time: 0.5129  data: 0.0002  max mem: 9341
[15:53:40.151200] Epoch: [89]  [100/195]  eta: 0:00:49  lr: 0.002375  loss: 0.4663 (0.4765)  time: 0.5106  data: 0.0002  max mem: 9341
[15:53:50.370083] Epoch: [89]  [120/195]  eta: 0:00:39  lr: 0.002375  loss: 0.4657 (0.4761)  time: 0.5109  data: 0.0002  max mem: 9341
[15:54:00.584412] Epoch: [89]  [140/195]  eta: 0:00:28  lr: 0.002375  loss: 0.4759 (0.4761)  time: 0.5107  data: 0.0002  max mem: 9341
[15:54:10.845716] Epoch: [89]  [160/195]  eta: 0:00:18  lr: 0.002375  loss: 0.4625 (0.4749)  time: 0.5130  data: 0.0002  max mem: 9341
[15:54:21.023848] Epoch: [89]  [180/195]  eta: 0:00:07  lr: 0.002375  loss: 0.4729 (0.4742)  time: 0.5089  data: 0.0001  max mem: 9341
[15:54:28.159681] Epoch: [89]  [194/195]  eta: 0:00:00  lr: 0.002374  loss: 0.4599 (0.4735)  time: 0.5109  data: 0.0001  max mem: 9341
[15:54:28.333884] Epoch: [89] Total time: 0:01:40 (0.5175 s / it)
[15:54:28.347190] Averaged stats: lr: 0.002374  loss: 0.4599 (0.4760)
[15:54:33.067340] {"train_lr": 0.002375006662331776, "train_loss": 0.47601586286074077, "epoch": 89}
[15:54:33.067707] [15:54:33.067792] Training epoch 89 for 0:01:45
[15:54:33.067845] [15:54:33.072315] log_dir: ./exp/debug/cifar100-LT/debug
[15:54:34.738729] Epoch: [90]  [  0/195]  eta: 0:05:24  lr: 0.002374  loss: 0.4869 (0.4869)  time: 1.6650  data: 1.1653  max mem: 9341
[15:54:44.954005] Epoch: [90]  [ 20/195]  eta: 0:01:38  lr: 0.002374  loss: 0.4837 (0.4858)  time: 0.5107  data: 0.0002  max mem: 9341
[15:54:55.170413] Epoch: [90]  [ 40/195]  eta: 0:01:23  lr: 0.002374  loss: 0.4694 (0.4800)  time: 0.5108  data: 0.0002  max mem: 9341
[15:55:05.388604] Epoch: [90]  [ 60/195]  eta: 0:01:11  lr: 0.002374  loss: 0.4830 (0.4819)  time: 0.5108  data: 0.0002  max mem: 9341
[15:55:15.653087] Epoch: [90]  [ 80/195]  eta: 0:01:00  lr: 0.002374  loss: 0.4883 (0.4834)  time: 0.5131  data: 0.0002  max mem: 9341
[15:55:25.866863] Epoch: [90]  [100/195]  eta: 0:00:49  lr: 0.002374  loss: 0.4965 (0.4860)  time: 0.5106  data: 0.0002  max mem: 9341
[15:55:36.078528] Epoch: [90]  [120/195]  eta: 0:00:39  lr: 0.002374  loss: 0.4959 (0.4883)  time: 0.5105  data: 0.0002  max mem: 9341
[15:55:46.289604] Epoch: [90]  [140/195]  eta: 0:00:28  lr: 0.002374  loss: 0.4788 (0.4875)  time: 0.5105  data: 0.0002  max mem: 9341
[15:55:56.549802] Epoch: [90]  [160/195]  eta: 0:00:18  lr: 0.002374  loss: 0.4795 (0.4875)  time: 0.5130  data: 0.0002  max mem: 9341
[15:56:06.723334] Epoch: [90]  [180/195]  eta: 0:00:07  lr: 0.002374  loss: 0.4788 (0.4868)  time: 0.5086  data: 0.0001  max mem: 9341
[15:56:13.854731] Epoch: [90]  [194/195]  eta: 0:00:00  lr: 0.002373  loss: 0.4691 (0.4854)  time: 0.5106  data: 0.0001  max mem: 9341
[15:56:14.019853] Epoch: [90] Total time: 0:01:40 (0.5177 s / it)
[15:56:14.040128] Averaged stats: lr: 0.002373  loss: 0.4691 (0.4872)
[15:56:18.707859] {"train_lr": 0.0023739895246947655, "train_loss": 0.4871594834786195, "epoch": 90}
[15:56:18.708230] [15:56:18.708317] Training epoch 90 for 0:01:45
[15:56:18.708370] [15:56:18.712909] log_dir: ./exp/debug/cifar100-LT/debug
[15:56:20.382186] Epoch: [91]  [  0/195]  eta: 0:05:24  lr: 0.002373  loss: 0.4829 (0.4829)  time: 1.6658  data: 1.1516  max mem: 9341
[15:56:30.608462] Epoch: [91]  [ 20/195]  eta: 0:01:39  lr: 0.002373  loss: 0.4737 (0.4822)  time: 0.5112  data: 0.0002  max mem: 9341
[15:56:40.851380] Epoch: [91]  [ 40/195]  eta: 0:01:23  lr: 0.002373  loss: 0.4826 (0.4799)  time: 0.5121  data: 0.0002  max mem: 9341
[15:56:51.099444] Epoch: [91]  [ 60/195]  eta: 0:01:11  lr: 0.002373  loss: 0.4923 (0.4840)  time: 0.5123  data: 0.0002  max mem: 9341
[15:57:01.404106] Epoch: [91]  [ 80/195]  eta: 0:01:00  lr: 0.002373  loss: 0.4790 (0.4841)  time: 0.5152  data: 0.0002  max mem: 9341
[15:57:11.638791] Epoch: [91]  [100/195]  eta: 0:00:49  lr: 0.002373  loss: 0.4906 (0.4836)  time: 0.5117  data: 0.0002  max mem: 9341
[15:57:21.873486] Epoch: [91]  [120/195]  eta: 0:00:39  lr: 0.002373  loss: 0.4823 (0.4830)  time: 0.5117  data: 0.0002  max mem: 9341
[15:57:32.103926] Epoch: [91]  [140/195]  eta: 0:00:28  lr: 0.002373  loss: 0.4805 (0.4829)  time: 0.5115  data: 0.0002  max mem: 9341
[15:57:42.399577] Epoch: [91]  [160/195]  eta: 0:00:18  lr: 0.002373  loss: 0.4836 (0.4829)  time: 0.5147  data: 0.0002  max mem: 9341
[15:57:52.588679] Epoch: [91]  [180/195]  eta: 0:00:07  lr: 0.002372  loss: 0.4903 (0.4835)  time: 0.5094  data: 0.0001  max mem: 9341
[15:57:59.735574] Epoch: [91]  [194/195]  eta: 0:00:00  lr: 0.002372  loss: 0.4727 (0.4823)  time: 0.5122  data: 0.0001  max mem: 9341
[15:57:59.916476] Epoch: [91] Total time: 0:01:41 (0.5190 s / it)
[15:57:59.922826] Averaged stats: lr: 0.002372  loss: 0.4727 (0.4836)
[15:58:04.648036] {"train_lr": 0.0023729523268159326, "train_loss": 0.4836444317530363, "epoch": 91}
[15:58:04.648404] [15:58:04.648491] Training epoch 91 for 0:01:45
[15:58:04.648543] [15:58:04.652978] log_dir: ./exp/debug/cifar100-LT/debug
[15:58:06.156018] Epoch: [92]  [  0/195]  eta: 0:04:52  lr: 0.002372  loss: 0.4647 (0.4647)  time: 1.5015  data: 1.0059  max mem: 9341
[15:58:16.369582] Epoch: [92]  [ 20/195]  eta: 0:01:37  lr: 0.002372  loss: 0.4842 (0.4887)  time: 0.5106  data: 0.0002  max mem: 9341
[15:58:26.591493] Epoch: [92]  [ 40/195]  eta: 0:01:22  lr: 0.002372  loss: 0.4895 (0.4894)  time: 0.5110  data: 0.0002  max mem: 9341
[15:58:36.814576] Epoch: [92]  [ 60/195]  eta: 0:01:11  lr: 0.002372  loss: 0.4767 (0.4857)  time: 0.5111  data: 0.0002  max mem: 9341
[15:58:47.119688] Epoch: [92]  [ 80/195]  eta: 0:01:00  lr: 0.002372  loss: 0.4678 (0.4826)  time: 0.5152  data: 0.0002  max mem: 9341
[15:58:57.360847] Epoch: [92]  [100/195]  eta: 0:00:49  lr: 0.002372  loss: 0.4799 (0.4823)  time: 0.5120  data: 0.0002  max mem: 9341
[15:59:07.583124] Epoch: [92]  [120/195]  eta: 0:00:39  lr: 0.002372  loss: 0.4765 (0.4824)  time: 0.5110  data: 0.0002  max mem: 9341
[15:59:17.793570] Epoch: [92]  [140/195]  eta: 0:00:28  lr: 0.002372  loss: 0.4697 (0.4811)  time: 0.5105  data: 0.0002  max mem: 9341
[15:59:28.052705] Epoch: [92]  [160/195]  eta: 0:00:18  lr: 0.002372  loss: 0.4793 (0.4811)  time: 0.5129  data: 0.0002  max mem: 9341
[15:59:38.223212] Epoch: [92]  [180/195]  eta: 0:00:07  lr: 0.002371  loss: 0.4665 (0.4796)  time: 0.5085  data: 0.0001  max mem: 9341
[15:59:45.356063] Epoch: [92]  [194/195]  eta: 0:00:00  lr: 0.002371  loss: 0.4693 (0.4793)  time: 0.5106  data: 0.0001  max mem: 9341
[15:59:45.526489] Epoch: [92] Total time: 0:01:40 (0.5173 s / it)
[15:59:45.538196] Averaged stats: lr: 0.002371  loss: 0.4693 (0.4781)
[15:59:50.255054] {"train_lr": 0.002371895086418129, "train_loss": 0.47812701028126936, "epoch": 92}
[15:59:50.255381] [15:59:50.255464] Training epoch 92 for 0:01:45
[15:59:50.255517] [15:59:50.260009] log_dir: ./exp/debug/cifar100-LT/debug
[15:59:51.930321] Epoch: [93]  [  0/195]  eta: 0:05:25  lr: 0.002371  loss: 0.4911 (0.4911)  time: 1.6689  data: 1.1616  max mem: 9341
[16:00:02.170835] Epoch: [93]  [ 20/195]  eta: 0:01:39  lr: 0.002371  loss: 0.4746 (0.4868)  time: 0.5119  data: 0.0002  max mem: 9341
[16:00:12.429002] Epoch: [93]  [ 40/195]  eta: 0:01:23  lr: 0.002371  loss: 0.4683 (0.4803)  time: 0.5129  data: 0.0002  max mem: 9341
[16:00:22.641962] Epoch: [93]  [ 60/195]  eta: 0:01:11  lr: 0.002371  loss: 0.4758 (0.4790)  time: 0.5106  data: 0.0002  max mem: 9341
[16:00:32.892719] Epoch: [93]  [ 80/195]  eta: 0:01:00  lr: 0.002371  loss: 0.4730 (0.4763)  time: 0.5125  data: 0.0002  max mem: 9341
[16:00:43.104029] Epoch: [93]  [100/195]  eta: 0:00:49  lr: 0.002371  loss: 0.4687 (0.4761)  time: 0.5105  data: 0.0002  max mem: 9341
[16:00:53.443809] Epoch: [93]  [120/195]  eta: 0:00:39  lr: 0.002371  loss: 0.4652 (0.4748)  time: 0.5169  data: 0.0002  max mem: 9341
[16:01:03.660916] Epoch: [93]  [140/195]  eta: 0:00:28  lr: 0.002371  loss: 0.4677 (0.4745)  time: 0.5108  data: 0.0002  max mem: 9341
[16:01:13.913267] Epoch: [93]  [160/195]  eta: 0:00:18  lr: 0.002370  loss: 0.4806 (0.4744)  time: 0.5126  data: 0.0002  max mem: 9341
[16:01:24.078643] Epoch: [93]  [180/195]  eta: 0:00:07  lr: 0.002370  loss: 0.4752 (0.4745)  time: 0.5082  data: 0.0001  max mem: 9341
[16:01:31.206573] Epoch: [93]  [194/195]  eta: 0:00:00  lr: 0.002370  loss: 0.4630 (0.4747)  time: 0.5104  data: 0.0001  max mem: 9341
[16:01:31.368304] Epoch: [93] Total time: 0:01:41 (0.5185 s / it)
[16:01:31.385752] Averaged stats: lr: 0.002370  loss: 0.4630 (0.4724)
[16:01:35.991799] {"train_lr": 0.0023708178215666553, "train_loss": 0.4724347139780338, "epoch": 93}
[16:01:35.992167] [16:01:35.992253] Training epoch 93 for 0:01:45
[16:01:35.992307] [16:01:35.996748] log_dir: ./exp/debug/cifar100-LT/debug
[16:01:37.670162] Epoch: [94]  [  0/195]  eta: 0:05:26  lr: 0.002370  loss: 0.4928 (0.4928)  time: 1.6720  data: 1.1578  max mem: 9341
[16:01:47.885636] Epoch: [94]  [ 20/195]  eta: 0:01:39  lr: 0.002370  loss: 0.4773 (0.4737)  time: 0.5107  data: 0.0002  max mem: 9341
[16:01:58.098834] Epoch: [94]  [ 40/195]  eta: 0:01:23  lr: 0.002370  loss: 0.4550 (0.4710)  time: 0.5106  data: 0.0002  max mem: 9341
[16:02:08.310635] Epoch: [94]  [ 60/195]  eta: 0:01:11  lr: 0.002370  loss: 0.4613 (0.4694)  time: 0.5105  data: 0.0002  max mem: 9341
[16:02:18.573216] Epoch: [94]  [ 80/195]  eta: 0:01:00  lr: 0.002370  loss: 0.4827 (0.4720)  time: 0.5131  data: 0.0002  max mem: 9341
[16:02:28.785879] Epoch: [94]  [100/195]  eta: 0:00:49  lr: 0.002370  loss: 0.4725 (0.4730)  time: 0.5106  data: 0.0002  max mem: 9341
[16:02:39.001131] Epoch: [94]  [120/195]  eta: 0:00:39  lr: 0.002370  loss: 0.4687 (0.4742)  time: 0.5107  data: 0.0002  max mem: 9341
[16:02:49.233589] Epoch: [94]  [140/195]  eta: 0:00:28  lr: 0.002370  loss: 0.4644 (0.4747)  time: 0.5116  data: 0.0002  max mem: 9341
[16:02:59.529345] Epoch: [94]  [160/195]  eta: 0:00:18  lr: 0.002369  loss: 0.4770 (0.4747)  time: 0.5147  data: 0.0002  max mem: 9341
[16:03:09.713453] Epoch: [94]  [180/195]  eta: 0:00:07  lr: 0.002369  loss: 0.4622 (0.4755)  time: 0.5092  data: 0.0001  max mem: 9341
[16:03:16.860044] Epoch: [94]  [194/195]  eta: 0:00:00  lr: 0.002369  loss: 0.4869 (0.4751)  time: 0.5122  data: 0.0001  max mem: 9341
[16:03:17.027366] Epoch: [94] Total time: 0:01:41 (0.5181 s / it)
[16:03:17.037687] Averaged stats: lr: 0.002369  loss: 0.4869 (0.4729)
[16:03:21.670366] {"train_lr": 0.0023697205506690096, "train_loss": 0.47293819452707586, "epoch": 94}
[16:03:21.670693] [16:03:21.670778] Training epoch 94 for 0:01:45
[16:03:21.670832] [16:03:21.675392] log_dir: ./exp/debug/cifar100-LT/debug
[16:03:23.481998] Epoch: [95]  [  0/195]  eta: 0:05:52  lr: 0.002369  loss: 0.4540 (0.4540)  time: 1.8054  data: 1.2907  max mem: 9341
[16:03:33.690986] Epoch: [95]  [ 20/195]  eta: 0:01:40  lr: 0.002369  loss: 0.4786 (0.4798)  time: 0.5104  data: 0.0002  max mem: 9341
[16:03:43.902614] Epoch: [95]  [ 40/195]  eta: 0:01:24  lr: 0.002369  loss: 0.4695 (0.4741)  time: 0.5105  data: 0.0002  max mem: 9341
[16:03:54.119314] Epoch: [95]  [ 60/195]  eta: 0:01:11  lr: 0.002369  loss: 0.4846 (0.4773)  time: 0.5108  data: 0.0002  max mem: 9341
[16:04:04.365906] Epoch: [95]  [ 80/195]  eta: 0:01:00  lr: 0.002369  loss: 0.4725 (0.4779)  time: 0.5123  data: 0.0002  max mem: 9341
[16:04:14.575039] Epoch: [95]  [100/195]  eta: 0:00:49  lr: 0.002369  loss: 0.4744 (0.4788)  time: 0.5104  data: 0.0002  max mem: 9341
[16:04:24.787699] Epoch: [95]  [120/195]  eta: 0:00:39  lr: 0.002368  loss: 0.4625 (0.4769)  time: 0.5106  data: 0.0002  max mem: 9341
[16:04:35.000891] Epoch: [95]  [140/195]  eta: 0:00:28  lr: 0.002368  loss: 0.4610 (0.4747)  time: 0.5106  data: 0.0002  max mem: 9341
[16:04:45.257627] Epoch: [95]  [160/195]  eta: 0:00:18  lr: 0.002368  loss: 0.4753 (0.4741)  time: 0.5128  data: 0.0002  max mem: 9341
[16:04:55.422977] Epoch: [95]  [180/195]  eta: 0:00:07  lr: 0.002368  loss: 0.4610 (0.4726)  time: 0.5082  data: 0.0002  max mem: 9341
[16:05:02.554422] Epoch: [95]  [194/195]  eta: 0:00:00  lr: 0.002368  loss: 0.4640 (0.4720)  time: 0.5105  data: 0.0001  max mem: 9341
[16:05:02.715734] Epoch: [95] Total time: 0:01:41 (0.5182 s / it)
[16:05:02.736483] Averaged stats: lr: 0.002368  loss: 0.4640 (0.4705)
[16:05:07.414754] {"train_lr": 0.002368603292474515, "train_loss": 0.47046170983559044, "epoch": 95}
[16:05:07.415078] [16:05:07.415161] Training epoch 95 for 0:01:45
[16:05:07.415213] [16:05:07.419682] log_dir: ./exp/debug/cifar100-LT/debug
[16:05:09.057223] Epoch: [96]  [  0/195]  eta: 0:05:19  lr: 0.002368  loss: 0.4249 (0.4249)  time: 1.6361  data: 1.1338  max mem: 9341
[16:05:19.290652] Epoch: [96]  [ 20/195]  eta: 0:01:38  lr: 0.002368  loss: 0.4635 (0.4688)  time: 0.5116  data: 0.0002  max mem: 9341
[16:05:29.504458] Epoch: [96]  [ 40/195]  eta: 0:01:23  lr: 0.002368  loss: 0.4742 (0.4707)  time: 0.5106  data: 0.0002  max mem: 9341
[16:05:39.742174] Epoch: [96]  [ 60/195]  eta: 0:01:11  lr: 0.002368  loss: 0.4682 (0.4701)  time: 0.5118  data: 0.0002  max mem: 9341
[16:05:50.013436] Epoch: [96]  [ 80/195]  eta: 0:01:00  lr: 0.002368  loss: 0.4561 (0.4664)  time: 0.5135  data: 0.0002  max mem: 9341
[16:06:00.224949] Epoch: [96]  [100/195]  eta: 0:00:49  lr: 0.002367  loss: 0.4599 (0.4657)  time: 0.5105  data: 0.0002  max mem: 9341
[16:06:10.440493] Epoch: [96]  [120/195]  eta: 0:00:39  lr: 0.002367  loss: 0.4723 (0.4671)  time: 0.5107  data: 0.0002  max mem: 9341
[16:06:20.691530] Epoch: [96]  [140/195]  eta: 0:00:28  lr: 0.002367  loss: 0.4765 (0.4684)  time: 0.5125  data: 0.0002  max mem: 9341
[16:06:30.947301] Epoch: [96]  [160/195]  eta: 0:00:18  lr: 0.002367  loss: 0.4765 (0.4694)  time: 0.5127  data: 0.0002  max mem: 9341
[16:06:41.116047] Epoch: [96]  [180/195]  eta: 0:00:07  lr: 0.002367  loss: 0.4767 (0.4702)  time: 0.5084  data: 0.0001  max mem: 9341
[16:06:48.246423] Epoch: [96]  [194/195]  eta: 0:00:00  lr: 0.002367  loss: 0.4696 (0.4697)  time: 0.5104  data: 0.0001  max mem: 9341
[16:06:48.412227] Epoch: [96] Total time: 0:01:40 (0.5179 s / it)
[16:06:48.420936] Averaged stats: lr: 0.002367  loss: 0.4696 (0.4710)
[16:06:53.266422] {"train_lr": 0.002367466066074045, "train_loss": 0.47101609878815137, "epoch": 96}
[16:06:53.266786] [16:06:53.266881] Training epoch 96 for 0:01:45
[16:06:53.266998] [16:06:53.272138] log_dir: ./exp/debug/cifar100-LT/debug
[16:06:54.784039] Epoch: [97]  [  0/195]  eta: 0:04:54  lr: 0.002367  loss: 0.4130 (0.4130)  time: 1.5104  data: 0.9832  max mem: 9341
[16:07:05.007253] Epoch: [97]  [ 20/195]  eta: 0:01:37  lr: 0.002367  loss: 0.4678 (0.4622)  time: 0.5111  data: 0.0002  max mem: 9341
[16:07:15.223273] Epoch: [97]  [ 40/195]  eta: 0:01:22  lr: 0.002367  loss: 0.4713 (0.4641)  time: 0.5107  data: 0.0002  max mem: 9341
[16:07:25.432296] Epoch: [97]  [ 60/195]  eta: 0:01:11  lr: 0.002367  loss: 0.4540 (0.4642)  time: 0.5104  data: 0.0002  max mem: 9341
[16:07:35.692754] Epoch: [97]  [ 80/195]  eta: 0:01:00  lr: 0.002366  loss: 0.4799 (0.4670)  time: 0.5130  data: 0.0002  max mem: 9341
[16:07:45.898618] Epoch: [97]  [100/195]  eta: 0:00:49  lr: 0.002366  loss: 0.4702 (0.4673)  time: 0.5102  data: 0.0002  max mem: 9341
[16:07:56.112635] Epoch: [97]  [120/195]  eta: 0:00:38  lr: 0.002366  loss: 0.4697 (0.4670)  time: 0.5106  data: 0.0002  max mem: 9341
[16:08:06.321610] Epoch: [97]  [140/195]  eta: 0:00:28  lr: 0.002366  loss: 0.4677 (0.4658)  time: 0.5104  data: 0.0002  max mem: 9341
[16:08:16.573864] Epoch: [97]  [160/195]  eta: 0:00:18  lr: 0.002366  loss: 0.4847 (0.4675)  time: 0.5126  data: 0.0002  max mem: 9341
[16:08:26.742224] Epoch: [97]  [180/195]  eta: 0:00:07  lr: 0.002366  loss: 0.4702 (0.4681)  time: 0.5084  data: 0.0001  max mem: 9341
[16:08:33.874961] Epoch: [97]  [194/195]  eta: 0:00:00  lr: 0.002366  loss: 0.4559 (0.4667)  time: 0.5105  data: 0.0001  max mem: 9341
[16:08:34.054388] Epoch: [97] Total time: 0:01:40 (0.5168 s / it)
[16:08:34.060846] Averaged stats: lr: 0.002366  loss: 0.4559 (0.4697)
[16:08:38.713161] {"train_lr": 0.002366308890899656, "train_loss": 0.46970136329913753, "epoch": 97}
[16:08:38.713427] [16:08:38.713509] Training epoch 97 for 0:01:45
[16:08:38.713562] [16:08:38.718100] log_dir: ./exp/debug/cifar100-LT/debug
[16:08:40.286306] Epoch: [98]  [  0/195]  eta: 0:05:05  lr: 0.002366  loss: 0.4895 (0.4895)  time: 1.5673  data: 1.0620  max mem: 9341
[16:08:50.515163] Epoch: [98]  [ 20/195]  eta: 0:01:38  lr: 0.002366  loss: 0.4580 (0.4628)  time: 0.5114  data: 0.0002  max mem: 9341
[16:09:00.733711] Epoch: [98]  [ 40/195]  eta: 0:01:23  lr: 0.002365  loss: 0.4534 (0.4656)  time: 0.5109  data: 0.0002  max mem: 9341
[16:09:10.956695] Epoch: [98]  [ 60/195]  eta: 0:01:11  lr: 0.002365  loss: 0.4755 (0.4684)  time: 0.5111  data: 0.0002  max mem: 9341
[16:09:21.210145] Epoch: [98]  [ 80/195]  eta: 0:01:00  lr: 0.002365  loss: 0.4675 (0.4692)  time: 0.5126  data: 0.0002  max mem: 9341
[16:09:31.421680] Epoch: [98]  [100/195]  eta: 0:00:49  lr: 0.002365  loss: 0.4638 (0.4686)  time: 0.5105  data: 0.0003  max mem: 9341
[16:09:41.636381] Epoch: [98]  [120/195]  eta: 0:00:38  lr: 0.002365  loss: 0.4609 (0.4661)  time: 0.5107  data: 0.0002  max mem: 9341
[16:09:51.845582] Epoch: [98]  [140/195]  eta: 0:00:28  lr: 0.002365  loss: 0.4565 (0.4660)  time: 0.5104  data: 0.0002  max mem: 9341
[16:10:02.102077] Epoch: [98]  [160/195]  eta: 0:00:18  lr: 0.002365  loss: 0.4829 (0.4681)  time: 0.5128  data: 0.0002  max mem: 9341
[16:10:12.280618] Epoch: [98]  [180/195]  eta: 0:00:07  lr: 0.002365  loss: 0.4659 (0.4675)  time: 0.5089  data: 0.0001  max mem: 9341
[16:10:19.413210] Epoch: [98]  [194/195]  eta: 0:00:00  lr: 0.002365  loss: 0.4622 (0.4677)  time: 0.5108  data: 0.0001  max mem: 9341
[16:10:19.596916] Epoch: [98] Total time: 0:01:40 (0.5173 s / it)
[16:10:19.612509] Averaged stats: lr: 0.002365  loss: 0.4622 (0.4677)
[16:10:24.300318] {"train_lr": 0.0023651317867242694, "train_loss": 0.46769053072501454, "epoch": 98}
[16:10:24.300665] [16:10:24.300750] Training epoch 98 for 0:01:45
[16:10:24.300805] [16:10:24.305524] log_dir: ./exp/debug/cifar100-LT/debug
[16:10:25.918570] Epoch: [99]  [  0/195]  eta: 0:05:14  lr: 0.002364  loss: 0.4606 (0.4606)  time: 1.6117  data: 1.1029  max mem: 9341
[16:10:36.157215] Epoch: [99]  [ 20/195]  eta: 0:01:38  lr: 0.002364  loss: 0.4567 (0.4541)  time: 0.5119  data: 0.0002  max mem: 9341
[16:10:46.394927] Epoch: [99]  [ 40/195]  eta: 0:01:23  lr: 0.002364  loss: 0.4740 (0.4643)  time: 0.5118  data: 0.0002  max mem: 9341
[16:10:56.608688] Epoch: [99]  [ 60/195]  eta: 0:01:11  lr: 0.002364  loss: 0.4563 (0.4649)  time: 0.5106  data: 0.0002  max mem: 9341
[16:11:06.871376] Epoch: [99]  [ 80/195]  eta: 0:01:00  lr: 0.002364  loss: 0.4662 (0.4662)  time: 0.5131  data: 0.0002  max mem: 9341
[16:11:17.083950] Epoch: [99]  [100/195]  eta: 0:00:49  lr: 0.002364  loss: 0.4867 (0.4697)  time: 0.5106  data: 0.0002  max mem: 9341
[16:11:27.297875] Epoch: [99]  [120/195]  eta: 0:00:39  lr: 0.002364  loss: 0.4721 (0.4711)  time: 0.5106  data: 0.0002  max mem: 9341
[16:11:37.508017] Epoch: [99]  [140/195]  eta: 0:00:28  lr: 0.002364  loss: 0.4744 (0.4720)  time: 0.5104  data: 0.0002  max mem: 9341
[16:11:47.761999] Epoch: [99]  [160/195]  eta: 0:00:18  lr: 0.002363  loss: 0.4597 (0.4709)  time: 0.5126  data: 0.0002  max mem: 9341
[16:11:57.952921] Epoch: [99]  [180/195]  eta: 0:00:07  lr: 0.002363  loss: 0.4671 (0.4708)  time: 0.5095  data: 0.0002  max mem: 9341
[16:12:05.105698] Epoch: [99]  [194/195]  eta: 0:00:00  lr: 0.002363  loss: 0.4662 (0.4706)  time: 0.5126  data: 0.0001  max mem: 9341
[16:12:05.276210] Epoch: [99] Total time: 0:01:40 (0.5178 s / it)
[16:12:05.292949] Averaged stats: lr: 0.002363  loss: 0.4662 (0.4713)
[16:12:09.978739] {"train_lr": 0.002363934773661366, "train_loss": 0.47134193487656423, "epoch": 99}
[16:12:09.979028] [16:12:09.979110] Training epoch 99 for 0:01:45
[16:12:09.979222] [16:12:09.983644] log_dir: ./exp/debug/cifar100-LT/debug
[16:12:11.792057] Epoch: [100]  [  0/195]  eta: 0:05:52  lr: 0.002363  loss: 0.4524 (0.4524)  time: 1.8070  data: 1.3198  max mem: 9341
[16:12:22.050452] Epoch: [100]  [ 20/195]  eta: 0:01:40  lr: 0.002363  loss: 0.4685 (0.4683)  time: 0.5129  data: 0.0002  max mem: 9341
[16:12:32.293479] Epoch: [100]  [ 40/195]  eta: 0:01:24  lr: 0.002363  loss: 0.4637 (0.4677)  time: 0.5121  data: 0.0002  max mem: 9341
[16:12:42.525628] Epoch: [100]  [ 60/195]  eta: 0:01:12  lr: 0.002363  loss: 0.4698 (0.4691)  time: 0.5115  data: 0.0002  max mem: 9341
[16:12:52.829582] Epoch: [100]  [ 80/195]  eta: 0:01:00  lr: 0.002363  loss: 0.4641 (0.4685)  time: 0.5151  data: 0.0002  max mem: 9341
[16:13:03.075228] Epoch: [100]  [100/195]  eta: 0:00:49  lr: 0.002363  loss: 0.4657 (0.4672)  time: 0.5122  data: 0.0002  max mem: 9341
[16:13:13.316925] Epoch: [100]  [120/195]  eta: 0:00:39  lr: 0.002363  loss: 0.4613 (0.4664)  time: 0.5120  data: 0.0002  max mem: 9341
[16:13:23.553682] Epoch: [100]  [140/195]  eta: 0:00:28  lr: 0.002362  loss: 0.4718 (0.4671)  time: 0.5118  data: 0.0002  max mem: 9341
[16:13:33.858804] Epoch: [100]  [160/195]  eta: 0:00:18  lr: 0.002362  loss: 0.4607 (0.4664)  time: 0.5152  data: 0.0002  max mem: 9341
[16:13:44.056278] Epoch: [100]  [180/195]  eta: 0:00:07  lr: 0.002362  loss: 0.4624 (0.4665)  time: 0.5098  data: 0.0001  max mem: 9341
[16:13:51.213274] Epoch: [100]  [194/195]  eta: 0:00:00  lr: 0.002362  loss: 0.4855 (0.4679)  time: 0.5129  data: 0.0001  max mem: 9341
[16:13:51.389362] Epoch: [100] Total time: 0:01:41 (0.5200 s / it)
[16:13:51.395667] Averaged stats: lr: 0.002362  loss: 0.4855 (0.4687)
[16:13:56.141893] {"train_lr": 0.002362717872164575, "train_loss": 0.46866955440013836, "epoch": 100}
[16:13:56.142201] [16:13:56.142289] Training epoch 100 for 0:01:46
[16:13:56.142342] [16:13:56.146831] log_dir: ./exp/debug/cifar100-LT/debug
[16:13:58.031492] Epoch: [101]  [  0/195]  eta: 0:06:07  lr: 0.002362  loss: 0.4628 (0.4628)  time: 1.8834  data: 1.3850  max mem: 9341
[16:14:08.255308] Epoch: [101]  [ 20/195]  eta: 0:01:40  lr: 0.002362  loss: 0.4746 (0.4734)  time: 0.5111  data: 0.0002  max mem: 9341
[16:14:18.473634] Epoch: [101]  [ 40/195]  eta: 0:01:24  lr: 0.002362  loss: 0.4747 (0.4731)  time: 0.5109  data: 0.0002  max mem: 9341
[16:14:28.685220] Epoch: [101]  [ 60/195]  eta: 0:01:12  lr: 0.002362  loss: 0.4612 (0.4706)  time: 0.5105  data: 0.0002  max mem: 9341
[16:14:38.962173] Epoch: [101]  [ 80/195]  eta: 0:01:00  lr: 0.002362  loss: 0.4681 (0.4706)  time: 0.5138  data: 0.0002  max mem: 9341
[16:14:49.170978] Epoch: [101]  [100/195]  eta: 0:00:49  lr: 0.002361  loss: 0.4631 (0.4695)  time: 0.5104  data: 0.0002  max mem: 9341
[16:14:59.382163] Epoch: [101]  [120/195]  eta: 0:00:39  lr: 0.002361  loss: 0.4806 (0.4709)  time: 0.5105  data: 0.0002  max mem: 9341
[16:15:09.595748] Epoch: [101]  [140/195]  eta: 0:00:28  lr: 0.002361  loss: 0.4623 (0.4700)  time: 0.5106  data: 0.0002  max mem: 9341
[16:15:19.850973] Epoch: [101]  [160/195]  eta: 0:00:18  lr: 0.002361  loss: 0.4643 (0.4698)  time: 0.5127  data: 0.0002  max mem: 9341
[16:15:30.020997] Epoch: [101]  [180/195]  eta: 0:00:07  lr: 0.002361  loss: 0.4752 (0.4707)  time: 0.5084  data: 0.0001  max mem: 9341
[16:15:37.153352] Epoch: [101]  [194/195]  eta: 0:00:00  lr: 0.002361  loss: 0.4816 (0.4708)  time: 0.5104  data: 0.0001  max mem: 9341
[16:15:37.318472] Epoch: [101] Total time: 0:01:41 (0.5188 s / it)
[16:15:37.334953] Averaged stats: lr: 0.002361  loss: 0.4816 (0.4693)
[16:15:42.002307] {"train_lr": 0.0023614811030274055, "train_loss": 0.46928231975971124, "epoch": 101}
[16:15:42.002636] [16:15:42.002719] Training epoch 101 for 0:01:45
[16:15:42.002773] [16:15:42.007491] log_dir: ./exp/debug/cifar100-LT/debug
[16:15:43.713114] Epoch: [102]  [  0/195]  eta: 0:05:32  lr: 0.002361  loss: 0.4939 (0.4939)  time: 1.7043  data: 1.2008  max mem: 9341
[16:15:53.929419] Epoch: [102]  [ 20/195]  eta: 0:01:39  lr: 0.002361  loss: 0.4777 (0.4732)  time: 0.5108  data: 0.0002  max mem: 9341
[16:16:04.148395] Epoch: [102]  [ 40/195]  eta: 0:01:23  lr: 0.002361  loss: 0.4739 (0.4732)  time: 0.5109  data: 0.0002  max mem: 9341
[16:16:14.384962] Epoch: [102]  [ 60/195]  eta: 0:01:11  lr: 0.002360  loss: 0.4539 (0.4677)  time: 0.5117  data: 0.0002  max mem: 9341
[16:16:24.684969] Epoch: [102]  [ 80/195]  eta: 0:01:00  lr: 0.002360  loss: 0.4662 (0.4663)  time: 0.5149  data: 0.0002  max mem: 9341
[16:16:34.947241] Epoch: [102]  [100/195]  eta: 0:00:49  lr: 0.002360  loss: 0.4613 (0.4655)  time: 0.5130  data: 0.0002  max mem: 9341
[16:16:45.178622] Epoch: [102]  [120/195]  eta: 0:00:39  lr: 0.002360  loss: 0.4668 (0.4656)  time: 0.5115  data: 0.0002  max mem: 9341
[16:16:55.414314] Epoch: [102]  [140/195]  eta: 0:00:28  lr: 0.002360  loss: 0.4692 (0.4659)  time: 0.5117  data: 0.0002  max mem: 9341
[16:17:05.709986] Epoch: [102]  [160/195]  eta: 0:00:18  lr: 0.002360  loss: 0.4591 (0.4651)  time: 0.5147  data: 0.0002  max mem: 9341
[16:17:15.908482] Epoch: [102]  [180/195]  eta: 0:00:07  lr: 0.002360  loss: 0.4680 (0.4656)  time: 0.5099  data: 0.0001  max mem: 9341
[16:17:23.059145] Epoch: [102]  [194/195]  eta: 0:00:00  lr: 0.002360  loss: 0.4647 (0.4641)  time: 0.5126  data: 0.0001  max mem: 9341
[16:17:23.231935] Epoch: [102] Total time: 0:01:41 (0.5191 s / it)
[16:17:23.240045] Averaged stats: lr: 0.002360  loss: 0.4647 (0.4648)
[16:17:27.903310] {"train_lr": 0.002360224487382819, "train_loss": 0.464754511263126, "epoch": 102}
[16:17:27.903581] [16:17:27.903671] Training epoch 102 for 0:01:45
[16:17:27.903724] [16:17:27.908723] log_dir: ./exp/debug/cifar100-LT/debug
[16:17:29.573483] Epoch: [103]  [  0/195]  eta: 0:05:24  lr: 0.002360  loss: 0.4849 (0.4849)  time: 1.6633  data: 1.1483  max mem: 9341
[16:17:39.791448] Epoch: [103]  [ 20/195]  eta: 0:01:38  lr: 0.002359  loss: 0.4539 (0.4538)  time: 0.5108  data: 0.0002  max mem: 9341
[16:17:50.010698] Epoch: [103]  [ 40/195]  eta: 0:01:23  lr: 0.002359  loss: 0.4627 (0.4584)  time: 0.5109  data: 0.0002  max mem: 9341
[16:18:00.255365] Epoch: [103]  [ 60/195]  eta: 0:01:11  lr: 0.002359  loss: 0.4699 (0.4624)  time: 0.5122  data: 0.0002  max mem: 9341
[16:18:10.560956] Epoch: [103]  [ 80/195]  eta: 0:01:00  lr: 0.002359  loss: 0.4564 (0.4610)  time: 0.5152  data: 0.0002  max mem: 9341
[16:18:20.799210] Epoch: [103]  [100/195]  eta: 0:00:49  lr: 0.002359  loss: 0.4600 (0.4619)  time: 0.5118  data: 0.0002  max mem: 9341
[16:18:31.037678] Epoch: [103]  [120/195]  eta: 0:00:39  lr: 0.002359  loss: 0.4632 (0.4624)  time: 0.5118  data: 0.0002  max mem: 9341
[16:18:41.279071] Epoch: [103]  [140/195]  eta: 0:00:28  lr: 0.002359  loss: 0.4821 (0.4638)  time: 0.5120  data: 0.0002  max mem: 9341
[16:18:51.577316] Epoch: [103]  [160/195]  eta: 0:00:18  lr: 0.002358  loss: 0.4610 (0.4634)  time: 0.5148  data: 0.0002  max mem: 9341
[16:19:01.768772] Epoch: [103]  [180/195]  eta: 0:00:07  lr: 0.002358  loss: 0.4743 (0.4639)  time: 0.5095  data: 0.0002  max mem: 9341
[16:19:08.925254] Epoch: [103]  [194/195]  eta: 0:00:00  lr: 0.002358  loss: 0.4616 (0.4638)  time: 0.5127  data: 0.0001  max mem: 9341
[16:19:09.090011] Epoch: [103] Total time: 0:01:41 (0.5189 s / it)
[16:19:09.104658] Averaged stats: lr: 0.002358  loss: 0.4616 (0.4634)
[16:19:13.807900] {"train_lr": 0.0023589480467029102, "train_loss": 0.4634216444996687, "epoch": 103}
[16:19:13.808276] [16:19:13.808366] Training epoch 103 for 0:01:45
[16:19:13.808420] [16:19:13.812899] log_dir: ./exp/debug/cifar100-LT/debug
[16:19:15.644094] Epoch: [104]  [  0/195]  eta: 0:05:56  lr: 0.002358  loss: 0.4718 (0.4718)  time: 1.8298  data: 1.3434  max mem: 9341
[16:19:25.879790] Epoch: [104]  [ 20/195]  eta: 0:01:40  lr: 0.002358  loss: 0.4723 (0.4688)  time: 0.5117  data: 0.0002  max mem: 9341
[16:19:36.095924] Epoch: [104]  [ 40/195]  eta: 0:01:24  lr: 0.002358  loss: 0.4799 (0.4732)  time: 0.5108  data: 0.0002  max mem: 9341
[16:19:46.315862] Epoch: [104]  [ 60/195]  eta: 0:01:11  lr: 0.002358  loss: 0.4722 (0.4732)  time: 0.5109  data: 0.0002  max mem: 9341
[16:19:56.575447] Epoch: [104]  [ 80/195]  eta: 0:01:00  lr: 0.002358  loss: 0.4778 (0.4729)  time: 0.5129  data: 0.0002  max mem: 9341
[16:20:06.788299] Epoch: [104]  [100/195]  eta: 0:00:49  lr: 0.002358  loss: 0.4699 (0.4722)  time: 0.5106  data: 0.0002  max mem: 9341
[16:20:16.997841] Epoch: [104]  [120/195]  eta: 0:00:39  lr: 0.002358  loss: 0.4600 (0.4710)  time: 0.5104  data: 0.0002  max mem: 9341
[16:20:27.213369] Epoch: [104]  [140/195]  eta: 0:00:28  lr: 0.002357  loss: 0.4676 (0.4712)  time: 0.5107  data: 0.0002  max mem: 9341
[16:20:37.467389] Epoch: [104]  [160/195]  eta: 0:00:18  lr: 0.002357  loss: 0.4616 (0.4702)  time: 0.5126  data: 0.0002  max mem: 9341
[16:20:47.644589] Epoch: [104]  [180/195]  eta: 0:00:07  lr: 0.002357  loss: 0.4746 (0.4707)  time: 0.5088  data: 0.0002  max mem: 9341
[16:20:54.781874] Epoch: [104]  [194/195]  eta: 0:00:00  lr: 0.002357  loss: 0.4660 (0.4700)  time: 0.5110  data: 0.0001  max mem: 9341
[16:20:54.942563] Epoch: [104] Total time: 0:01:41 (0.5186 s / it)
[16:20:54.958354] Averaged stats: lr: 0.002357  loss: 0.4660 (0.4676)
[16:20:59.691809] {"train_lr": 0.00235765180279853, "train_loss": 0.467627250116605, "epoch": 104}
[16:20:59.692171] [16:20:59.692258] Training epoch 104 for 0:01:45
[16:20:59.692312] [16:20:59.696930] log_dir: ./exp/debug/cifar100-LT/debug
[16:21:01.199304] Epoch: [105]  [  0/195]  eta: 0:04:52  lr: 0.002357  loss: 0.4409 (0.4409)  time: 1.5005  data: 0.9852  max mem: 9341
[16:21:11.416645] Epoch: [105]  [ 20/195]  eta: 0:01:37  lr: 0.002357  loss: 0.4595 (0.4641)  time: 0.5108  data: 0.0002  max mem: 9341
[16:21:21.639315] Epoch: [105]  [ 40/195]  eta: 0:01:22  lr: 0.002357  loss: 0.4788 (0.4704)  time: 0.5111  data: 0.0002  max mem: 9341
[16:21:31.856560] Epoch: [105]  [ 60/195]  eta: 0:01:11  lr: 0.002357  loss: 0.4618 (0.4694)  time: 0.5108  data: 0.0002  max mem: 9341
[16:21:42.113921] Epoch: [105]  [ 80/195]  eta: 0:01:00  lr: 0.002356  loss: 0.4597 (0.4671)  time: 0.5128  data: 0.0002  max mem: 9341
[16:21:52.332214] Epoch: [105]  [100/195]  eta: 0:00:49  lr: 0.002356  loss: 0.4611 (0.4674)  time: 0.5109  data: 0.0002  max mem: 9341
[16:22:02.558255] Epoch: [105]  [120/195]  eta: 0:00:38  lr: 0.002356  loss: 0.4529 (0.4659)  time: 0.5112  data: 0.0002  max mem: 9341
[16:22:12.796334] Epoch: [105]  [140/195]  eta: 0:00:28  lr: 0.002356  loss: 0.4677 (0.4664)  time: 0.5118  data: 0.0002  max mem: 9341
[16:22:23.092450] Epoch: [105]  [160/195]  eta: 0:00:18  lr: 0.002356  loss: 0.4703 (0.4665)  time: 0.5147  data: 0.0002  max mem: 9341
[16:22:33.298321] Epoch: [105]  [180/195]  eta: 0:00:07  lr: 0.002356  loss: 0.4580 (0.4657)  time: 0.5102  data: 0.0001  max mem: 9341
[16:22:40.454954] Epoch: [105]  [194/195]  eta: 0:00:00  lr: 0.002356  loss: 0.4619 (0.4654)  time: 0.5130  data: 0.0001  max mem: 9341
[16:22:40.623628] Epoch: [105] Total time: 0:01:40 (0.5176 s / it)
[16:22:40.631984] Averaged stats: lr: 0.002356  loss: 0.4619 (0.4629)
[16:22:45.323900] {"train_lr": 0.00235633577781891, "train_loss": 0.4628816354351166, "epoch": 105}
[16:22:45.324359] [16:22:45.324461] Training epoch 105 for 0:01:45
[16:22:45.324516] [16:22:45.329782] log_dir: ./exp/debug/cifar100-LT/debug
[16:22:47.224399] Epoch: [106]  [  0/195]  eta: 0:06:09  lr: 0.002356  loss: 0.4666 (0.4666)  time: 1.8933  data: 1.3859  max mem: 9341
[16:22:57.454013] Epoch: [106]  [ 20/195]  eta: 0:01:41  lr: 0.002356  loss: 0.4509 (0.4516)  time: 0.5114  data: 0.0002  max mem: 9341
[16:23:07.673647] Epoch: [106]  [ 40/195]  eta: 0:01:24  lr: 0.002355  loss: 0.4527 (0.4538)  time: 0.5109  data: 0.0002  max mem: 9341
[16:23:17.894788] Epoch: [106]  [ 60/195]  eta: 0:01:12  lr: 0.002355  loss: 0.4550 (0.4552)  time: 0.5110  data: 0.0002  max mem: 9341
[16:23:28.158042] Epoch: [106]  [ 80/195]  eta: 0:01:00  lr: 0.002355  loss: 0.4523 (0.4560)  time: 0.5131  data: 0.0002  max mem: 9341
[16:23:38.380339] Epoch: [106]  [100/195]  eta: 0:00:49  lr: 0.002355  loss: 0.4524 (0.4564)  time: 0.5111  data: 0.0002  max mem: 9341
[16:23:48.599884] Epoch: [106]  [120/195]  eta: 0:00:39  lr: 0.002355  loss: 0.4431 (0.4562)  time: 0.5109  data: 0.0002  max mem: 9341
[16:23:58.841734] Epoch: [106]  [140/195]  eta: 0:00:28  lr: 0.002355  loss: 0.4478 (0.4553)  time: 0.5120  data: 0.0002  max mem: 9341
[16:24:09.112379] Epoch: [106]  [160/195]  eta: 0:00:18  lr: 0.002355  loss: 0.4628 (0.4565)  time: 0.5135  data: 0.0002  max mem: 9341
[16:24:19.298773] Epoch: [106]  [180/195]  eta: 0:00:07  lr: 0.002354  loss: 0.4476 (0.4565)  time: 0.5093  data: 0.0001  max mem: 9341
[16:24:26.435792] Epoch: [106]  [194/195]  eta: 0:00:00  lr: 0.002354  loss: 0.4558 (0.4568)  time: 0.5110  data: 0.0001  max mem: 9341
[16:24:26.607859] Epoch: [106] Total time: 0:01:41 (0.5194 s / it)
[16:24:26.623265] Averaged stats: lr: 0.002354  loss: 0.4558 (0.4580)
[16:24:31.291383] {"train_lr": 0.002354999994251281, "train_loss": 0.4580445986527663, "epoch": 106}
[16:24:31.291649] [16:24:31.291737] Training epoch 106 for 0:01:45
[16:24:31.291803] [16:24:31.296747] log_dir: ./exp/debug/cifar100-LT/debug
[16:24:33.099347] Epoch: [107]  [  0/195]  eta: 0:05:51  lr: 0.002354  loss: 0.4422 (0.4422)  time: 1.8016  data: 1.3070  max mem: 9341
[16:24:43.322495] Epoch: [107]  [ 20/195]  eta: 0:01:40  lr: 0.002354  loss: 0.4589 (0.4596)  time: 0.5111  data: 0.0002  max mem: 9341
[16:24:53.544637] Epoch: [107]  [ 40/195]  eta: 0:01:24  lr: 0.002354  loss: 0.4667 (0.4638)  time: 0.5110  data: 0.0002  max mem: 9341
[16:25:03.761930] Epoch: [107]  [ 60/195]  eta: 0:01:11  lr: 0.002354  loss: 0.4568 (0.4625)  time: 0.5108  data: 0.0002  max mem: 9341
[16:25:14.046320] Epoch: [107]  [ 80/195]  eta: 0:01:00  lr: 0.002354  loss: 0.4493 (0.4607)  time: 0.5142  data: 0.0002  max mem: 9341
[16:25:24.261792] Epoch: [107]  [100/195]  eta: 0:00:49  lr: 0.002354  loss: 0.4678 (0.4621)  time: 0.5107  data: 0.0002  max mem: 9341
[16:25:34.480633] Epoch: [107]  [120/195]  eta: 0:00:39  lr: 0.002353  loss: 0.4640 (0.4625)  time: 0.5109  data: 0.0002  max mem: 9341
[16:25:44.695989] Epoch: [107]  [140/195]  eta: 0:00:28  lr: 0.002353  loss: 0.4699 (0.4630)  time: 0.5107  data: 0.0002  max mem: 9341
[16:25:54.956810] Epoch: [107]  [160/195]  eta: 0:00:18  lr: 0.002353  loss: 0.4507 (0.4622)  time: 0.5130  data: 0.0002  max mem: 9341
[16:26:05.124947] Epoch: [107]  [180/195]  eta: 0:00:07  lr: 0.002353  loss: 0.4540 (0.4612)  time: 0.5084  data: 0.0001  max mem: 9341
[16:26:12.263047] Epoch: [107]  [194/195]  eta: 0:00:00  lr: 0.002353  loss: 0.4448 (0.4613)  time: 0.5108  data: 0.0001  max mem: 9341
[16:26:12.427634] Epoch: [107] Total time: 0:01:41 (0.5186 s / it)
[16:26:12.437780] Averaged stats: lr: 0.002353  loss: 0.4448 (0.4584)
[16:26:17.149912] {"train_lr": 0.002353644474920518, "train_loss": 0.4583579149765846, "epoch": 107}
[16:26:17.150235] [16:26:17.150320] Training epoch 107 for 0:01:45
[16:26:17.150374] [16:26:17.154856] log_dir: ./exp/debug/cifar100-LT/debug
[16:26:18.866451] Epoch: [108]  [  0/195]  eta: 0:05:33  lr: 0.002353  loss: 0.4877 (0.4877)  time: 1.7102  data: 1.2039  max mem: 9341
[16:26:29.089418] Epoch: [108]  [ 20/195]  eta: 0:01:39  lr: 0.002353  loss: 0.4582 (0.4631)  time: 0.5111  data: 0.0002  max mem: 9341
[16:26:39.307428] Epoch: [108]  [ 40/195]  eta: 0:01:23  lr: 0.002353  loss: 0.4622 (0.4641)  time: 0.5108  data: 0.0002  max mem: 9341
[16:26:49.525903] Epoch: [108]  [ 60/195]  eta: 0:01:11  lr: 0.002353  loss: 0.4613 (0.4627)  time: 0.5109  data: 0.0002  max mem: 9341
[16:26:59.801142] Epoch: [108]  [ 80/195]  eta: 0:01:00  lr: 0.002352  loss: 0.4679 (0.4637)  time: 0.5137  data: 0.0002  max mem: 9341
[16:27:10.016892] Epoch: [108]  [100/195]  eta: 0:00:49  lr: 0.002352  loss: 0.4573 (0.4630)  time: 0.5107  data: 0.0002  max mem: 9341
[16:27:20.228699] Epoch: [108]  [120/195]  eta: 0:00:39  lr: 0.002352  loss: 0.4838 (0.4658)  time: 0.5105  data: 0.0002  max mem: 9341
[16:27:30.434354] Epoch: [108]  [140/195]  eta: 0:00:28  lr: 0.002352  loss: 0.4787 (0.4672)  time: 0.5102  data: 0.0002  max mem: 9341
[16:27:40.689944] Epoch: [108]  [160/195]  eta: 0:00:18  lr: 0.002352  loss: 0.4694 (0.4681)  time: 0.5127  data: 0.0002  max mem: 9341
[16:27:50.857107] Epoch: [108]  [180/195]  eta: 0:00:07  lr: 0.002352  loss: 0.4756 (0.4686)  time: 0.5083  data: 0.0001  max mem: 9341
[16:27:57.993575] Epoch: [108]  [194/195]  eta: 0:00:00  lr: 0.002352  loss: 0.4681 (0.4688)  time: 0.5106  data: 0.0001  max mem: 9341
[16:27:58.161524] Epoch: [108] Total time: 0:01:41 (0.5180 s / it)
[16:27:58.183751] Averaged stats: lr: 0.002352  loss: 0.4681 (0.4684)
[16:28:02.888433] {"train_lr": 0.002352269242988679, "train_loss": 0.4684490782710222, "epoch": 108}
[16:28:02.888741] [16:28:02.888831] Training epoch 108 for 0:01:45
[16:28:02.888885] [16:28:02.893579] log_dir: ./exp/debug/cifar100-LT/debug
[16:28:04.594162] Epoch: [109]  [  0/195]  eta: 0:05:31  lr: 0.002352  loss: 0.4624 (0.4624)  time: 1.6996  data: 1.1866  max mem: 9341
[16:28:14.829271] Epoch: [109]  [ 20/195]  eta: 0:01:39  lr: 0.002351  loss: 0.4661 (0.4688)  time: 0.5117  data: 0.0002  max mem: 9341
[16:28:25.045902] Epoch: [109]  [ 40/195]  eta: 0:01:23  lr: 0.002351  loss: 0.4704 (0.4674)  time: 0.5108  data: 0.0002  max mem: 9341
[16:28:35.260242] Epoch: [109]  [ 60/195]  eta: 0:01:11  lr: 0.002351  loss: 0.4900 (0.4758)  time: 0.5107  data: 0.0002  max mem: 9341
[16:28:45.521671] Epoch: [109]  [ 80/195]  eta: 0:01:00  lr: 0.002351  loss: 0.4712 (0.4745)  time: 0.5130  data: 0.0002  max mem: 9341
[16:28:55.736599] Epoch: [109]  [100/195]  eta: 0:00:49  lr: 0.002351  loss: 0.4657 (0.4727)  time: 0.5107  data: 0.0002  max mem: 9341
[16:29:05.963687] Epoch: [109]  [120/195]  eta: 0:00:39  lr: 0.002351  loss: 0.4697 (0.4719)  time: 0.5113  data: 0.0002  max mem: 9341
[16:29:16.194770] Epoch: [109]  [140/195]  eta: 0:00:28  lr: 0.002351  loss: 0.4636 (0.4711)  time: 0.5115  data: 0.0002  max mem: 9341
[16:29:26.496293] Epoch: [109]  [160/195]  eta: 0:00:18  lr: 0.002350  loss: 0.4755 (0.4710)  time: 0.5150  data: 0.0002  max mem: 9341
[16:29:36.698513] Epoch: [109]  [180/195]  eta: 0:00:07  lr: 0.002350  loss: 0.4584 (0.4702)  time: 0.5101  data: 0.0001  max mem: 9341
[16:29:43.856559] Epoch: [109]  [194/195]  eta: 0:00:00  lr: 0.002350  loss: 0.4569 (0.4692)  time: 0.5129  data: 0.0001  max mem: 9341
[16:29:44.025151] Epoch: [109] Total time: 0:01:41 (0.5186 s / it)
[16:29:44.033544] Averaged stats: lr: 0.002350  loss: 0.4569 (0.4712)
[16:29:48.713571] {"train_lr": 0.0023508743219546953, "train_loss": 0.47118039654615596, "epoch": 109}
[16:29:48.713831] [16:29:48.713917] Training epoch 109 for 0:01:45
[16:29:48.713970] [16:29:48.718408] log_dir: ./exp/debug/cifar100-LT/debug
[16:29:50.253404] Epoch: [110]  [  0/195]  eta: 0:04:59  lr: 0.002350  loss: 0.5191 (0.5191)  time: 1.5341  data: 1.0289  max mem: 9341
[16:30:00.493261] Epoch: [110]  [ 20/195]  eta: 0:01:38  lr: 0.002350  loss: 0.4659 (0.4691)  time: 0.5119  data: 0.0002  max mem: 9341
[16:30:10.724592] Epoch: [110]  [ 40/195]  eta: 0:01:23  lr: 0.002350  loss: 0.4621 (0.4667)  time: 0.5115  data: 0.0002  max mem: 9341
[16:30:20.959281] Epoch: [110]  [ 60/195]  eta: 0:01:11  lr: 0.002350  loss: 0.4643 (0.4655)  time: 0.5117  data: 0.0002  max mem: 9341
[16:30:31.244484] Epoch: [110]  [ 80/195]  eta: 0:01:00  lr: 0.002350  loss: 0.4683 (0.4647)  time: 0.5142  data: 0.0002  max mem: 9341
[16:30:41.451346] Epoch: [110]  [100/195]  eta: 0:00:49  lr: 0.002349  loss: 0.4461 (0.4623)  time: 0.5103  data: 0.0002  max mem: 9341
[16:30:51.667481] Epoch: [110]  [120/195]  eta: 0:00:39  lr: 0.002349  loss: 0.4574 (0.4617)  time: 0.5107  data: 0.0002  max mem: 9341
[16:31:01.884490] Epoch: [110]  [140/195]  eta: 0:00:28  lr: 0.002349  loss: 0.4590 (0.4616)  time: 0.5108  data: 0.0002  max mem: 9341
[16:31:12.140254] Epoch: [110]  [160/195]  eta: 0:00:18  lr: 0.002349  loss: 0.4647 (0.4620)  time: 0.5127  data: 0.0002  max mem: 9341
[16:31:22.313667] Epoch: [110]  [180/195]  eta: 0:00:07  lr: 0.002349  loss: 0.4546 (0.4611)  time: 0.5086  data: 0.0001  max mem: 9341
[16:31:29.443257] Epoch: [110]  [194/195]  eta: 0:00:00  lr: 0.002349  loss: 0.4609 (0.4609)  time: 0.5104  data: 0.0001  max mem: 9341
[16:31:29.620336] Epoch: [110] Total time: 0:01:40 (0.5174 s / it)
[16:31:29.625222] Averaged stats: lr: 0.002349  loss: 0.4609 (0.4602)
[16:31:34.331664] {"train_lr": 0.0023494597356539188, "train_loss": 0.46018159500299355, "epoch": 110}
[16:31:34.331934] [16:31:34.332024] Training epoch 110 for 0:01:45
[16:31:34.332103] [16:31:34.337006] log_dir: ./exp/debug/cifar100-LT/debug
[16:31:35.910538] Epoch: [111]  [  0/195]  eta: 0:05:06  lr: 0.002349  loss: 0.4409 (0.4409)  time: 1.5725  data: 1.0684  max mem: 9341
[16:31:46.138470] Epoch: [111]  [ 20/195]  eta: 0:01:38  lr: 0.002349  loss: 0.4540 (0.4571)  time: 0.5113  data: 0.0002  max mem: 9341
[16:31:56.381674] Epoch: [111]  [ 40/195]  eta: 0:01:23  lr: 0.002348  loss: 0.4533 (0.4589)  time: 0.5121  data: 0.0002  max mem: 9341
[16:32:06.605245] Epoch: [111]  [ 60/195]  eta: 0:01:11  lr: 0.002348  loss: 0.4511 (0.4572)  time: 0.5111  data: 0.0002  max mem: 9341
[16:32:16.891638] Epoch: [111]  [ 80/195]  eta: 0:01:00  lr: 0.002348  loss: 0.4560 (0.4564)  time: 0.5142  data: 0.0002  max mem: 9341
[16:32:27.122071] Epoch: [111]  [100/195]  eta: 0:00:49  lr: 0.002348  loss: 0.4468 (0.4547)  time: 0.5115  data: 0.0002  max mem: 9341
[16:32:37.353593] Epoch: [111]  [120/195]  eta: 0:00:39  lr: 0.002348  loss: 0.4467 (0.4546)  time: 0.5115  data: 0.0002  max mem: 9341
[16:32:47.567052] Epoch: [111]  [140/195]  eta: 0:00:28  lr: 0.002348  loss: 0.4553 (0.4555)  time: 0.5106  data: 0.0002  max mem: 9341
[16:32:57.868882] Epoch: [111]  [160/195]  eta: 0:00:18  lr: 0.002348  loss: 0.4489 (0.4555)  time: 0.5150  data: 0.0002  max mem: 9341
[16:33:08.060376] Epoch: [111]  [180/195]  eta: 0:00:07  lr: 0.002347  loss: 0.4474 (0.4554)  time: 0.5095  data: 0.0001  max mem: 9341
[16:33:15.213403] Epoch: [111]  [194/195]  eta: 0:00:00  lr: 0.002347  loss: 0.4497 (0.4557)  time: 0.5126  data: 0.0001  max mem: 9341
[16:33:15.384169] Epoch: [111] Total time: 0:01:41 (0.5182 s / it)
[16:33:15.408422] Averaged stats: lr: 0.002347  loss: 0.4497 (0.4559)
[16:33:20.038594] {"train_lr": 0.00234802550825774, "train_loss": 0.4559115455700801, "epoch": 111}
[16:33:20.038856] [16:33:20.038940] Training epoch 111 for 0:01:45
[16:33:20.038991] [16:33:20.043455] log_dir: ./exp/debug/cifar100-LT/debug
[16:33:21.814649] Epoch: [112]  [  0/195]  eta: 0:05:45  lr: 0.002347  loss: 0.4150 (0.4150)  time: 1.7702  data: 1.2731  max mem: 9341
[16:33:32.027564] Epoch: [112]  [ 20/195]  eta: 0:01:39  lr: 0.002347  loss: 0.4551 (0.4529)  time: 0.5106  data: 0.0002  max mem: 9341
[16:33:42.242746] Epoch: [112]  [ 40/195]  eta: 0:01:23  lr: 0.002347  loss: 0.4559 (0.4558)  time: 0.5107  data: 0.0002  max mem: 9341
[16:33:52.458286] Epoch: [112]  [ 60/195]  eta: 0:01:11  lr: 0.002347  loss: 0.4586 (0.4569)  time: 0.5107  data: 0.0002  max mem: 9341
[16:34:02.719765] Epoch: [112]  [ 80/195]  eta: 0:01:00  lr: 0.002347  loss: 0.4531 (0.4573)  time: 0.5130  data: 0.0002  max mem: 9341
[16:34:12.930831] Epoch: [112]  [100/195]  eta: 0:00:49  lr: 0.002347  loss: 0.4578 (0.4570)  time: 0.5105  data: 0.0002  max mem: 9341
[16:34:23.143760] Epoch: [112]  [120/195]  eta: 0:00:39  lr: 0.002346  loss: 0.4552 (0.4559)  time: 0.5106  data: 0.0002  max mem: 9341
[16:34:33.358226] Epoch: [112]  [140/195]  eta: 0:00:28  lr: 0.002346  loss: 0.4553 (0.4555)  time: 0.5107  data: 0.0002  max mem: 9341
[16:34:43.612652] Epoch: [112]  [160/195]  eta: 0:00:18  lr: 0.002346  loss: 0.4372 (0.4539)  time: 0.5127  data: 0.0002  max mem: 9341
[16:34:53.782430] Epoch: [112]  [180/195]  eta: 0:00:07  lr: 0.002346  loss: 0.4505 (0.4543)  time: 0.5084  data: 0.0002  max mem: 9341
[16:35:00.914276] Epoch: [112]  [194/195]  eta: 0:00:00  lr: 0.002346  loss: 0.4462 (0.4543)  time: 0.5105  data: 0.0001  max mem: 9341
[16:35:01.088598] Epoch: [112] Total time: 0:01:41 (0.5182 s / it)
[16:35:01.089335] Averaged stats: lr: 0.002346  loss: 0.4462 (0.4548)
[16:35:05.794993] {"train_lr": 0.0023465716642731248, "train_loss": 0.45476250193822076, "epoch": 112}
[16:35:05.795268] [16:35:05.795353] Training epoch 112 for 0:01:45
[16:35:05.795406] [16:35:05.799944] log_dir: ./exp/debug/cifar100-LT/debug
[16:35:07.586831] Epoch: [113]  [  0/195]  eta: 0:05:48  lr: 0.002346  loss: 0.3755 (0.3755)  time: 1.7861  data: 1.2825  max mem: 9341
[16:35:17.807321] Epoch: [113]  [ 20/195]  eta: 0:01:40  lr: 0.002346  loss: 0.4483 (0.4530)  time: 0.5110  data: 0.0002  max mem: 9341
[16:35:28.025305] Epoch: [113]  [ 40/195]  eta: 0:01:24  lr: 0.002346  loss: 0.4524 (0.4563)  time: 0.5108  data: 0.0002  max mem: 9341
[16:35:38.245842] Epoch: [113]  [ 60/195]  eta: 0:01:11  lr: 0.002345  loss: 0.4513 (0.4553)  time: 0.5110  data: 0.0002  max mem: 9341
[16:35:48.527877] Epoch: [113]  [ 80/195]  eta: 0:01:00  lr: 0.002345  loss: 0.4661 (0.4566)  time: 0.5140  data: 0.0002  max mem: 9341
[16:35:58.745769] Epoch: [113]  [100/195]  eta: 0:00:49  lr: 0.002345  loss: 0.4538 (0.4574)  time: 0.5108  data: 0.0002  max mem: 9341
[16:36:08.960632] Epoch: [113]  [120/195]  eta: 0:00:39  lr: 0.002345  loss: 0.4568 (0.4582)  time: 0.5107  data: 0.0002  max mem: 9341
[16:36:19.169453] Epoch: [113]  [140/195]  eta: 0:00:28  lr: 0.002345  loss: 0.4453 (0.4573)  time: 0.5104  data: 0.0002  max mem: 9341
[16:36:29.419532] Epoch: [113]  [160/195]  eta: 0:00:18  lr: 0.002345  loss: 0.4460 (0.4564)  time: 0.5124  data: 0.0002  max mem: 9341
[16:36:39.597369] Epoch: [113]  [180/195]  eta: 0:00:07  lr: 0.002344  loss: 0.4541 (0.4562)  time: 0.5088  data: 0.0001  max mem: 9341
[16:36:46.727731] Epoch: [113]  [194/195]  eta: 0:00:00  lr: 0.002344  loss: 0.4633 (0.4566)  time: 0.5105  data: 0.0001  max mem: 9341
[16:36:46.892721] Epoch: [113] Total time: 0:01:41 (0.5184 s / it)
[16:36:46.903799] Averaged stats: lr: 0.002344  loss: 0.4633 (0.4548)
[16:36:51.680969] {"train_lr": 0.0023450982285422675, "train_loss": 0.4547743761386627, "epoch": 113}
[16:36:51.681220] [16:36:51.681304] Training epoch 113 for 0:01:45
[16:36:51.681356] [16:36:51.685766] log_dir: ./exp/debug/cifar100-LT/debug
[16:36:53.603448] Epoch: [114]  [  0/195]  eta: 0:06:13  lr: 0.002344  loss: 0.4691 (0.4691)  time: 1.9169  data: 1.4188  max mem: 9341
[16:37:03.822975] Epoch: [114]  [ 20/195]  eta: 0:01:41  lr: 0.002344  loss: 0.4569 (0.4514)  time: 0.5109  data: 0.0002  max mem: 9341
[16:37:14.044452] Epoch: [114]  [ 40/195]  eta: 0:01:24  lr: 0.002344  loss: 0.4607 (0.4571)  time: 0.5110  data: 0.0002  max mem: 9341
[16:37:24.269199] Epoch: [114]  [ 60/195]  eta: 0:01:12  lr: 0.002344  loss: 0.4604 (0.4598)  time: 0.5112  data: 0.0002  max mem: 9341
[16:37:34.531741] Epoch: [114]  [ 80/195]  eta: 0:01:00  lr: 0.002344  loss: 0.4682 (0.4608)  time: 0.5131  data: 0.0002  max mem: 9341
[16:37:44.762576] Epoch: [114]  [100/195]  eta: 0:00:49  lr: 0.002344  loss: 0.4490 (0.4582)  time: 0.5115  data: 0.0002  max mem: 9341
[16:37:54.978181] Epoch: [114]  [120/195]  eta: 0:00:39  lr: 0.002343  loss: 0.4635 (0.4594)  time: 0.5107  data: 0.0002  max mem: 9341
[16:38:05.196654] Epoch: [114]  [140/195]  eta: 0:00:28  lr: 0.002343  loss: 0.4490 (0.4582)  time: 0.5109  data: 0.0002  max mem: 9341
[16:38:15.457530] Epoch: [114]  [160/195]  eta: 0:00:18  lr: 0.002343  loss: 0.4706 (0.4589)  time: 0.5130  data: 0.0002  max mem: 9341
[16:38:25.638118] Epoch: [114]  [180/195]  eta: 0:00:07  lr: 0.002343  loss: 0.4481 (0.4582)  time: 0.5090  data: 0.0001  max mem: 9341
[16:38:32.774243] Epoch: [114]  [194/195]  eta: 0:00:00  lr: 0.002343  loss: 0.4555 (0.4580)  time: 0.5109  data: 0.0001  max mem: 9341
[16:38:32.957476] Epoch: [114] Total time: 0:01:41 (0.5193 s / it)
[16:38:32.958283] Averaged stats: lr: 0.002343  loss: 0.4555 (0.4567)
[16:38:37.629184] {"train_lr": 0.0023436052262421204, "train_loss": 0.45666420990839984, "epoch": 114}
[16:38:37.629467] [16:38:37.629554] Training epoch 114 for 0:01:45
[16:38:37.629630] [16:38:37.634610] log_dir: ./exp/debug/cifar100-LT/debug
[16:38:39.442590] Epoch: [115]  [  0/195]  eta: 0:05:52  lr: 0.002343  loss: 0.4551 (0.4551)  time: 1.8060  data: 1.2962  max mem: 9341
[16:38:49.667660] Epoch: [115]  [ 20/195]  eta: 0:01:40  lr: 0.002343  loss: 0.4461 (0.4469)  time: 0.5112  data: 0.0002  max mem: 9341
[16:38:59.891724] Epoch: [115]  [ 40/195]  eta: 0:01:24  lr: 0.002343  loss: 0.4485 (0.4491)  time: 0.5111  data: 0.0002  max mem: 9341
[16:39:10.114601] Epoch: [115]  [ 60/195]  eta: 0:01:11  lr: 0.002342  loss: 0.4564 (0.4544)  time: 0.5111  data: 0.0002  max mem: 9341
[16:39:20.377522] Epoch: [115]  [ 80/195]  eta: 0:01:00  lr: 0.002342  loss: 0.4561 (0.4546)  time: 0.5131  data: 0.0002  max mem: 9341
[16:39:30.590638] Epoch: [115]  [100/195]  eta: 0:00:49  lr: 0.002342  loss: 0.4574 (0.4563)  time: 0.5106  data: 0.0002  max mem: 9341
[16:39:40.801527] Epoch: [115]  [120/195]  eta: 0:00:39  lr: 0.002342  loss: 0.4448 (0.4554)  time: 0.5105  data: 0.0002  max mem: 9341
[16:39:51.020867] Epoch: [115]  [140/195]  eta: 0:00:28  lr: 0.002342  loss: 0.4561 (0.4555)  time: 0.5109  data: 0.0002  max mem: 9341
[16:40:01.283650] Epoch: [115]  [160/195]  eta: 0:00:18  lr: 0.002342  loss: 0.4495 (0.4554)  time: 0.5131  data: 0.0002  max mem: 9341
[16:40:11.456649] Epoch: [115]  [180/195]  eta: 0:00:07  lr: 0.002341  loss: 0.4536 (0.4556)  time: 0.5086  data: 0.0002  max mem: 9341
[16:40:18.583352] Epoch: [115]  [194/195]  eta: 0:00:00  lr: 0.002341  loss: 0.4531 (0.4557)  time: 0.5103  data: 0.0001  max mem: 9341
[16:40:18.772803] Epoch: [115] Total time: 0:01:41 (0.5187 s / it)
[16:40:18.786400] Averaged stats: lr: 0.002341  loss: 0.4531 (0.4549)
[16:40:23.541599] {"train_lr": 0.002342092682883972, "train_loss": 0.4549107574117489, "epoch": 115}
[16:40:23.541904] [16:40:23.541985] Training epoch 115 for 0:01:45
[16:40:23.542037] [16:40:23.546463] log_dir: ./exp/debug/cifar100-LT/debug
[16:40:25.254656] Epoch: [116]  [  0/195]  eta: 0:05:32  lr: 0.002341  loss: 0.4652 (0.4652)  time: 1.7069  data: 1.2056  max mem: 9341
[16:40:35.465098] Epoch: [116]  [ 20/195]  eta: 0:01:39  lr: 0.002341  loss: 0.4453 (0.4510)  time: 0.5105  data: 0.0002  max mem: 9341
[16:40:45.679892] Epoch: [116]  [ 40/195]  eta: 0:01:23  lr: 0.002341  loss: 0.4556 (0.4559)  time: 0.5107  data: 0.0002  max mem: 9341
[16:40:55.895664] Epoch: [116]  [ 60/195]  eta: 0:01:11  lr: 0.002341  loss: 0.4464 (0.4536)  time: 0.5107  data: 0.0002  max mem: 9341
[16:41:06.158417] Epoch: [116]  [ 80/195]  eta: 0:01:00  lr: 0.002341  loss: 0.4532 (0.4539)  time: 0.5131  data: 0.0002  max mem: 9341
[16:41:16.368212] Epoch: [116]  [100/195]  eta: 0:00:49  lr: 0.002341  loss: 0.4483 (0.4538)  time: 0.5104  data: 0.0002  max mem: 9341
[16:41:26.584278] Epoch: [116]  [120/195]  eta: 0:00:39  lr: 0.002340  loss: 0.4533 (0.4543)  time: 0.5107  data: 0.0002  max mem: 9341
[16:41:36.796845] Epoch: [116]  [140/195]  eta: 0:00:28  lr: 0.002340  loss: 0.4647 (0.4557)  time: 0.5106  data: 0.0002  max mem: 9341
[16:41:47.056859] Epoch: [116]  [160/195]  eta: 0:00:18  lr: 0.002340  loss: 0.4539 (0.4558)  time: 0.5129  data: 0.0002  max mem: 9341
[16:41:57.232999] Epoch: [116]  [180/195]  eta: 0:00:07  lr: 0.002340  loss: 0.4652 (0.4566)  time: 0.5087  data: 0.0001  max mem: 9341
[16:42:04.362966] Epoch: [116]  [194/195]  eta: 0:00:00  lr: 0.002340  loss: 0.4630 (0.4573)  time: 0.5105  data: 0.0001  max mem: 9341
[16:42:04.549081] Epoch: [116] Total time: 0:01:41 (0.5180 s / it)
[16:42:04.549944] Averaged stats: lr: 0.002340  loss: 0.4630 (0.4562)
[16:42:09.264785] {"train_lr": 0.002340560624313014, "train_loss": 0.45620330373446144, "epoch": 116}
[16:42:09.265152] [16:42:09.265240] Training epoch 116 for 0:01:45
[16:42:09.265294] [16:42:09.270242] log_dir: ./exp/debug/cifar100-LT/debug
[16:42:11.047239] Epoch: [117]  [  0/195]  eta: 0:05:46  lr: 0.002340  loss: 0.4537 (0.4537)  time: 1.7757  data: 1.2840  max mem: 9341
[16:42:21.288127] Epoch: [117]  [ 20/195]  eta: 0:01:40  lr: 0.002340  loss: 0.4599 (0.4633)  time: 0.5120  data: 0.0002  max mem: 9341
[16:42:31.506422] Epoch: [117]  [ 40/195]  eta: 0:01:24  lr: 0.002339  loss: 0.4531 (0.4588)  time: 0.5109  data: 0.0002  max mem: 9341
[16:42:41.725786] Epoch: [117]  [ 60/195]  eta: 0:01:11  lr: 0.002339  loss: 0.4716 (0.4618)  time: 0.5109  data: 0.0002  max mem: 9341
[16:42:52.005302] Epoch: [117]  [ 80/195]  eta: 0:01:00  lr: 0.002339  loss: 0.4671 (0.4624)  time: 0.5139  data: 0.0002  max mem: 9341
[16:43:02.240920] Epoch: [117]  [100/195]  eta: 0:00:49  lr: 0.002339  loss: 0.4621 (0.4620)  time: 0.5117  data: 0.0002  max mem: 9341
[16:43:12.480148] Epoch: [117]  [120/195]  eta: 0:00:39  lr: 0.002339  loss: 0.4524 (0.4607)  time: 0.5119  data: 0.0002  max mem: 9341
[16:43:22.716883] Epoch: [117]  [140/195]  eta: 0:00:28  lr: 0.002339  loss: 0.4535 (0.4596)  time: 0.5118  data: 0.0002  max mem: 9341
[16:43:33.014608] Epoch: [117]  [160/195]  eta: 0:00:18  lr: 0.002338  loss: 0.4592 (0.4591)  time: 0.5148  data: 0.0002  max mem: 9341
[16:43:43.195801] Epoch: [117]  [180/195]  eta: 0:00:07  lr: 0.002338  loss: 0.4532 (0.4585)  time: 0.5090  data: 0.0001  max mem: 9341
[16:43:50.329621] Epoch: [117]  [194/195]  eta: 0:00:00  lr: 0.002338  loss: 0.4532 (0.4587)  time: 0.5108  data: 0.0001  max mem: 9341
[16:43:50.490706] Epoch: [117] Total time: 0:01:41 (0.5191 s / it)
[16:43:50.509827] Averaged stats: lr: 0.002338  loss: 0.4532 (0.4541)
[16:43:55.203043] {"train_lr": 0.002339009076707892, "train_loss": 0.45408173352479936, "epoch": 117}
[16:43:55.203312] [16:43:55.203400] Training epoch 117 for 0:01:45
[16:43:55.203454] [16:43:55.207970] log_dir: ./exp/debug/cifar100-LT/debug
[16:43:56.950761] Epoch: [118]  [  0/195]  eta: 0:05:39  lr: 0.002338  loss: 0.3934 (0.3934)  time: 1.7415  data: 1.2281  max mem: 9341
[16:44:07.178428] Epoch: [118]  [ 20/195]  eta: 0:01:39  lr: 0.002338  loss: 0.4501 (0.4503)  time: 0.5113  data: 0.0002  max mem: 9341
[16:44:17.390831] Epoch: [118]  [ 40/195]  eta: 0:01:23  lr: 0.002338  loss: 0.4519 (0.4515)  time: 0.5106  data: 0.0002  max mem: 9341
[16:44:27.607727] Epoch: [118]  [ 60/195]  eta: 0:01:11  lr: 0.002338  loss: 0.4345 (0.4477)  time: 0.5108  data: 0.0002  max mem: 9341
[16:44:37.861728] Epoch: [118]  [ 80/195]  eta: 0:01:00  lr: 0.002338  loss: 0.4675 (0.4524)  time: 0.5126  data: 0.0002  max mem: 9341
[16:44:48.078089] Epoch: [118]  [100/195]  eta: 0:00:49  lr: 0.002337  loss: 0.4443 (0.4521)  time: 0.5108  data: 0.0002  max mem: 9341
[16:44:58.292460] Epoch: [118]  [120/195]  eta: 0:00:39  lr: 0.002337  loss: 0.4592 (0.4536)  time: 0.5107  data: 0.0002  max mem: 9341
[16:45:08.503551] Epoch: [118]  [140/195]  eta: 0:00:28  lr: 0.002337  loss: 0.4443 (0.4527)  time: 0.5105  data: 0.0002  max mem: 9341
[16:45:18.766939] Epoch: [118]  [160/195]  eta: 0:00:18  lr: 0.002337  loss: 0.4510 (0.4534)  time: 0.5131  data: 0.0002  max mem: 9341
[16:45:28.947287] Epoch: [118]  [180/195]  eta: 0:00:07  lr: 0.002337  loss: 0.4569 (0.4538)  time: 0.5090  data: 0.0001  max mem: 9341
[16:45:36.089894] Epoch: [118]  [194/195]  eta: 0:00:00  lr: 0.002337  loss: 0.4515 (0.4542)  time: 0.5115  data: 0.0001  max mem: 9341
[16:45:36.256593] Epoch: [118] Total time: 0:01:41 (0.5182 s / it)
[16:45:36.272820] Averaged stats: lr: 0.002337  loss: 0.4515 (0.4574)
[16:45:40.967445] {"train_lr": 0.002337438066580292, "train_loss": 0.457396567823031, "epoch": 118}
[16:45:40.967709] [16:45:40.967800] Training epoch 118 for 0:01:45
[16:45:40.967854] [16:45:40.972397] log_dir: ./exp/debug/cifar100-LT/debug
[16:45:42.671876] Epoch: [119]  [  0/195]  eta: 0:05:31  lr: 0.002337  loss: 0.4507 (0.4507)  time: 1.6984  data: 1.1960  max mem: 9341
[16:45:52.907512] Epoch: [119]  [ 20/195]  eta: 0:01:39  lr: 0.002336  loss: 0.4423 (0.4467)  time: 0.5117  data: 0.0002  max mem: 9341
[16:46:03.121791] Epoch: [119]  [ 40/195]  eta: 0:01:23  lr: 0.002336  loss: 0.4441 (0.4482)  time: 0.5107  data: 0.0002  max mem: 9341
[16:46:13.339053] Epoch: [119]  [ 60/195]  eta: 0:01:11  lr: 0.002336  loss: 0.4410 (0.4471)  time: 0.5108  data: 0.0002  max mem: 9341
[16:46:23.620159] Epoch: [119]  [ 80/195]  eta: 0:01:00  lr: 0.002336  loss: 0.4438 (0.4466)  time: 0.5140  data: 0.0002  max mem: 9341
[16:46:33.855338] Epoch: [119]  [100/195]  eta: 0:00:49  lr: 0.002336  loss: 0.4523 (0.4489)  time: 0.5117  data: 0.0002  max mem: 9341
[16:46:44.091654] Epoch: [119]  [120/195]  eta: 0:00:39  lr: 0.002336  loss: 0.4587 (0.4502)  time: 0.5118  data: 0.0002  max mem: 9341
[16:46:54.328653] Epoch: [119]  [140/195]  eta: 0:00:28  lr: 0.002336  loss: 0.4557 (0.4508)  time: 0.5118  data: 0.0002  max mem: 9341
[16:47:04.626846] Epoch: [119]  [160/195]  eta: 0:00:18  lr: 0.002335  loss: 0.4618 (0.4517)  time: 0.5149  data: 0.0002  max mem: 9341
[16:47:14.813968] Epoch: [119]  [180/195]  eta: 0:00:07  lr: 0.002335  loss: 0.4566 (0.4520)  time: 0.5093  data: 0.0001  max mem: 9341
[16:47:21.962456] Epoch: [119]  [194/195]  eta: 0:00:00  lr: 0.002335  loss: 0.4464 (0.4519)  time: 0.5124  data: 0.0001  max mem: 9341
[16:47:22.129235] Epoch: [119] Total time: 0:01:41 (0.5188 s / it)
[16:47:22.138842] Averaged stats: lr: 0.002335  loss: 0.4464 (0.4538)
[16:47:26.833906] {"train_lr": 0.002335847620774425, "train_loss": 0.4537874018152555, "epoch": 119}
[16:47:26.834231] [16:47:26.834327] Training epoch 119 for 0:01:45
[16:47:26.834379] [16:47:26.838833] log_dir: ./exp/debug/cifar100-LT/debug
[16:47:28.657314] Epoch: [120]  [  0/195]  eta: 0:05:54  lr: 0.002335  loss: 0.4695 (0.4695)  time: 1.8171  data: 1.3285  max mem: 9341
[16:47:38.985131] Epoch: [120]  [ 20/195]  eta: 0:01:41  lr: 0.002335  loss: 0.4542 (0.4499)  time: 0.5163  data: 0.0002  max mem: 9341
[16:47:49.224503] Epoch: [120]  [ 40/195]  eta: 0:01:24  lr: 0.002335  loss: 0.4478 (0.4491)  time: 0.5119  data: 0.0002  max mem: 9341
[16:47:59.438981] Epoch: [120]  [ 60/195]  eta: 0:01:12  lr: 0.002335  loss: 0.4553 (0.4505)  time: 0.5107  data: 0.0002  max mem: 9341
[16:48:09.703660] Epoch: [120]  [ 80/195]  eta: 0:01:00  lr: 0.002334  loss: 0.4502 (0.4507)  time: 0.5132  data: 0.0002  max mem: 9341
[16:48:19.924157] Epoch: [120]  [100/195]  eta: 0:00:49  lr: 0.002334  loss: 0.4408 (0.4493)  time: 0.5110  data: 0.0002  max mem: 9341
[16:48:30.144918] Epoch: [120]  [120/195]  eta: 0:00:39  lr: 0.002334  loss: 0.4354 (0.4489)  time: 0.5110  data: 0.0002  max mem: 9341
[16:48:40.360628] Epoch: [120]  [140/195]  eta: 0:00:28  lr: 0.002334  loss: 0.4459 (0.4485)  time: 0.5107  data: 0.0002  max mem: 9341
[16:48:50.618381] Epoch: [120]  [160/195]  eta: 0:00:18  lr: 0.002334  loss: 0.4357 (0.4478)  time: 0.5128  data: 0.0002  max mem: 9341
[16:49:00.792345] Epoch: [120]  [180/195]  eta: 0:00:07  lr: 0.002334  loss: 0.4512 (0.4479)  time: 0.5086  data: 0.0001  max mem: 9341
[16:49:07.925229] Epoch: [120]  [194/195]  eta: 0:00:00  lr: 0.002333  loss: 0.4505 (0.4479)  time: 0.5107  data: 0.0001  max mem: 9341
[16:49:08.108007] Epoch: [120] Total time: 0:01:41 (0.5193 s / it)
[16:49:08.126022] Averaged stats: lr: 0.002333  loss: 0.4505 (0.4483)
[16:49:12.695093] {"train_lr": 0.002334237766466635, "train_loss": 0.44828112178888074, "epoch": 120}
[16:49:12.695448] [16:49:12.695539] Training epoch 120 for 0:01:45
[16:49:12.695594] [16:49:12.700633] log_dir: ./exp/debug/cifar100-LT/debug
[16:49:14.419078] Epoch: [121]  [  0/195]  eta: 0:05:34  lr: 0.002333  loss: 0.4836 (0.4836)  time: 1.7171  data: 1.2119  max mem: 9341
[16:49:24.638578] Epoch: [121]  [ 20/195]  eta: 0:01:39  lr: 0.002333  loss: 0.4509 (0.4559)  time: 0.5109  data: 0.0002  max mem: 9341
[16:49:34.852118] Epoch: [121]  [ 40/195]  eta: 0:01:23  lr: 0.002333  loss: 0.4582 (0.4558)  time: 0.5106  data: 0.0002  max mem: 9341
[16:49:45.066296] Epoch: [121]  [ 60/195]  eta: 0:01:11  lr: 0.002333  loss: 0.4395 (0.4507)  time: 0.5106  data: 0.0002  max mem: 9341
[16:49:55.320518] Epoch: [121]  [ 80/195]  eta: 0:01:00  lr: 0.002333  loss: 0.4494 (0.4516)  time: 0.5127  data: 0.0002  max mem: 9341
[16:50:05.534399] Epoch: [121]  [100/195]  eta: 0:00:49  lr: 0.002333  loss: 0.4459 (0.4523)  time: 0.5106  data: 0.0002  max mem: 9341
[16:50:15.742982] Epoch: [121]  [120/195]  eta: 0:00:39  lr: 0.002332  loss: 0.4373 (0.4510)  time: 0.5104  data: 0.0002  max mem: 9341
[16:50:25.953651] Epoch: [121]  [140/195]  eta: 0:00:28  lr: 0.002332  loss: 0.4436 (0.4497)  time: 0.5105  data: 0.0002  max mem: 9341
[16:50:36.214822] Epoch: [121]  [160/195]  eta: 0:00:18  lr: 0.002332  loss: 0.4441 (0.4499)  time: 0.5130  data: 0.0002  max mem: 9341
[16:50:46.390092] Epoch: [121]  [180/195]  eta: 0:00:07  lr: 0.002332  loss: 0.4378 (0.4489)  time: 0.5087  data: 0.0001  max mem: 9341
[16:50:53.521792] Epoch: [121]  [194/195]  eta: 0:00:00  lr: 0.002332  loss: 0.4347 (0.4485)  time: 0.5107  data: 0.0001  max mem: 9341
[16:50:53.697854] Epoch: [121] Total time: 0:01:40 (0.5179 s / it)
[16:50:53.700573] Averaged stats: lr: 0.002332  loss: 0.4347 (0.4482)
[16:50:58.442346] {"train_lr": 0.002332608531164884, "train_loss": 0.4482156901787489, "epoch": 121}
[16:50:58.442620] [16:50:58.442707] Training epoch 121 for 0:01:45
[16:50:58.442761] [16:50:58.447338] log_dir: ./exp/debug/cifar100-LT/debug
[16:51:00.122530] Epoch: [122]  [  0/195]  eta: 0:05:26  lr: 0.002332  loss: 0.4658 (0.4658)  time: 1.6742  data: 1.1681  max mem: 9341
[16:51:10.339157] Epoch: [122]  [ 20/195]  eta: 0:01:39  lr: 0.002332  loss: 0.4362 (0.4402)  time: 0.5108  data: 0.0002  max mem: 9341
[16:51:20.556931] Epoch: [122]  [ 40/195]  eta: 0:01:23  lr: 0.002331  loss: 0.4382 (0.4364)  time: 0.5108  data: 0.0002  max mem: 9341
[16:51:30.782249] Epoch: [122]  [ 60/195]  eta: 0:01:11  lr: 0.002331  loss: 0.4424 (0.4396)  time: 0.5112  data: 0.0002  max mem: 9341
[16:51:41.043350] Epoch: [122]  [ 80/195]  eta: 0:01:00  lr: 0.002331  loss: 0.4517 (0.4429)  time: 0.5130  data: 0.0002  max mem: 9341
[16:51:51.265397] Epoch: [122]  [100/195]  eta: 0:00:49  lr: 0.002331  loss: 0.4341 (0.4422)  time: 0.5110  data: 0.0002  max mem: 9341
[16:52:01.506097] Epoch: [122]  [120/195]  eta: 0:00:39  lr: 0.002331  loss: 0.4483 (0.4436)  time: 0.5120  data: 0.0002  max mem: 9341
[16:52:11.743221] Epoch: [122]  [140/195]  eta: 0:00:28  lr: 0.002331  loss: 0.4341 (0.4435)  time: 0.5118  data: 0.0002  max mem: 9341
[16:52:22.046141] Epoch: [122]  [160/195]  eta: 0:00:18  lr: 0.002330  loss: 0.4444 (0.4438)  time: 0.5151  data: 0.0002  max mem: 9341
[16:52:32.247778] Epoch: [122]  [180/195]  eta: 0:00:07  lr: 0.002330  loss: 0.4455 (0.4439)  time: 0.5100  data: 0.0001  max mem: 9341
[16:52:39.402338] Epoch: [122]  [194/195]  eta: 0:00:00  lr: 0.002330  loss: 0.4476 (0.4441)  time: 0.5129  data: 0.0001  max mem: 9341
[16:52:39.559437] Epoch: [122] Total time: 0:01:41 (0.5185 s / it)
[16:52:39.589712] Averaged stats: lr: 0.002330  loss: 0.4476 (0.4475)
[16:52:44.305036] {"train_lr": 0.0023309599427083004, "train_loss": 0.44748588578823284, "epoch": 122}
[16:52:44.305295] [16:52:44.305380] Training epoch 122 for 0:01:45
[16:52:44.305433] [16:52:44.309923] log_dir: ./exp/debug/cifar100-LT/debug
[16:52:46.032312] Epoch: [123]  [  0/195]  eta: 0:05:35  lr: 0.002330  loss: 0.4720 (0.4720)  time: 1.7216  data: 1.2036  max mem: 9341
[16:52:56.250238] Epoch: [123]  [ 20/195]  eta: 0:01:39  lr: 0.002330  loss: 0.4379 (0.4474)  time: 0.5108  data: 0.0002  max mem: 9341
[16:53:06.468667] Epoch: [123]  [ 40/195]  eta: 0:01:23  lr: 0.002330  loss: 0.4433 (0.4476)  time: 0.5108  data: 0.0002  max mem: 9341
[16:53:16.685571] Epoch: [123]  [ 60/195]  eta: 0:01:11  lr: 0.002330  loss: 0.4494 (0.4485)  time: 0.5108  data: 0.0002  max mem: 9341
[16:53:26.948516] Epoch: [123]  [ 80/195]  eta: 0:01:00  lr: 0.002329  loss: 0.4572 (0.4509)  time: 0.5131  data: 0.0002  max mem: 9341
[16:53:37.157049] Epoch: [123]  [100/195]  eta: 0:00:49  lr: 0.002329  loss: 0.4566 (0.4514)  time: 0.5104  data: 0.0002  max mem: 9341
[16:53:47.368216] Epoch: [123]  [120/195]  eta: 0:00:39  lr: 0.002329  loss: 0.4474 (0.4515)  time: 0.5105  data: 0.0002  max mem: 9341
[16:53:57.582780] Epoch: [123]  [140/195]  eta: 0:00:28  lr: 0.002329  loss: 0.4565 (0.4528)  time: 0.5107  data: 0.0002  max mem: 9341
[16:54:07.842110] Epoch: [123]  [160/195]  eta: 0:00:18  lr: 0.002329  loss: 0.4452 (0.4521)  time: 0.5129  data: 0.0002  max mem: 9341
[16:54:18.021850] Epoch: [123]  [180/195]  eta: 0:00:07  lr: 0.002329  loss: 0.4527 (0.4517)  time: 0.5089  data: 0.0001  max mem: 9341
[16:54:25.154479] Epoch: [123]  [194/195]  eta: 0:00:00  lr: 0.002328  loss: 0.4547 (0.4513)  time: 0.5106  data: 0.0001  max mem: 9341
[16:54:25.331217] Epoch: [123] Total time: 0:01:41 (0.5181 s / it)
[16:54:25.342234] Averaged stats: lr: 0.002328  loss: 0.4547 (0.4507)
[16:54:30.074214] {"train_lr": 0.0023292920292667363, "train_loss": 0.4507043956946104, "epoch": 123}
[16:54:30.074513] [16:54:30.074604] Training epoch 123 for 0:01:45
[16:54:30.074659] [16:54:30.079341] log_dir: ./exp/debug/cifar100-LT/debug
[16:54:31.882859] Epoch: [124]  [  0/195]  eta: 0:05:51  lr: 0.002328  loss: 0.4536 (0.4536)  time: 1.8028  data: 1.3113  max mem: 9341
[16:54:42.111027] Epoch: [124]  [ 20/195]  eta: 0:01:40  lr: 0.002328  loss: 0.4483 (0.4488)  time: 0.5114  data: 0.0002  max mem: 9341
[16:54:52.328610] Epoch: [124]  [ 40/195]  eta: 0:01:24  lr: 0.002328  loss: 0.4546 (0.4528)  time: 0.5108  data: 0.0002  max mem: 9341
[16:55:02.563377] Epoch: [124]  [ 60/195]  eta: 0:01:11  lr: 0.002328  loss: 0.4629 (0.4569)  time: 0.5117  data: 0.0002  max mem: 9341
[16:55:12.863953] Epoch: [124]  [ 80/195]  eta: 0:01:00  lr: 0.002328  loss: 0.4418 (0.4542)  time: 0.5150  data: 0.0002  max mem: 9341
[16:55:23.095086] Epoch: [124]  [100/195]  eta: 0:00:49  lr: 0.002328  loss: 0.4399 (0.4521)  time: 0.5115  data: 0.0002  max mem: 9341
[16:55:33.332036] Epoch: [124]  [120/195]  eta: 0:00:39  lr: 0.002327  loss: 0.4469 (0.4524)  time: 0.5118  data: 0.0002  max mem: 9341
[16:55:43.565788] Epoch: [124]  [140/195]  eta: 0:00:28  lr: 0.002327  loss: 0.4443 (0.4519)  time: 0.5116  data: 0.0002  max mem: 9341
[16:55:53.862237] Epoch: [124]  [160/195]  eta: 0:00:18  lr: 0.002327  loss: 0.4527 (0.4516)  time: 0.5148  data: 0.0002  max mem: 9341
[16:56:04.055892] Epoch: [124]  [180/195]  eta: 0:00:07  lr: 0.002327  loss: 0.4645 (0.4526)  time: 0.5096  data: 0.0001  max mem: 9341
[16:56:11.210734] Epoch: [124]  [194/195]  eta: 0:00:00  lr: 0.002327  loss: 0.4457 (0.4513)  time: 0.5127  data: 0.0001  max mem: 9341
[16:56:11.388268] Epoch: [124] Total time: 0:01:41 (0.5195 s / it)
[16:56:11.392382] Averaged stats: lr: 0.002327  loss: 0.4457 (0.4482)
[16:56:16.096771] {"train_lr": 0.0023276048193401917, "train_loss": 0.4481927108306151, "epoch": 124}
[16:56:16.097110] [16:56:16.097206] Training epoch 124 for 0:01:46
[16:56:16.097260] [16:56:16.102468] log_dir: ./exp/debug/cifar100-LT/debug
[16:56:17.810544] Epoch: [125]  [  0/195]  eta: 0:05:32  lr: 0.002327  loss: 0.4412 (0.4412)  time: 1.7067  data: 1.1930  max mem: 9341
[16:56:28.029656] Epoch: [125]  [ 20/195]  eta: 0:01:39  lr: 0.002327  loss: 0.4379 (0.4461)  time: 0.5109  data: 0.0002  max mem: 9341
[16:56:38.246573] Epoch: [125]  [ 40/195]  eta: 0:01:23  lr: 0.002326  loss: 0.4550 (0.4516)  time: 0.5108  data: 0.0002  max mem: 9341
[16:56:48.457235] Epoch: [125]  [ 60/195]  eta: 0:01:11  lr: 0.002326  loss: 0.4448 (0.4500)  time: 0.5105  data: 0.0002  max mem: 9341
[16:56:58.734563] Epoch: [125]  [ 80/195]  eta: 0:01:00  lr: 0.002326  loss: 0.4350 (0.4473)  time: 0.5138  data: 0.0002  max mem: 9341
[16:57:08.970142] Epoch: [125]  [100/195]  eta: 0:00:49  lr: 0.002326  loss: 0.4475 (0.4481)  time: 0.5117  data: 0.0002  max mem: 9341
[16:57:19.199892] Epoch: [125]  [120/195]  eta: 0:00:39  lr: 0.002326  loss: 0.4518 (0.4490)  time: 0.5114  data: 0.0002  max mem: 9341
[16:57:29.432045] Epoch: [125]  [140/195]  eta: 0:00:28  lr: 0.002326  loss: 0.4545 (0.4491)  time: 0.5116  data: 0.0002  max mem: 9341
[16:57:39.737378] Epoch: [125]  [160/195]  eta: 0:00:18  lr: 0.002325  loss: 0.4458 (0.4487)  time: 0.5152  data: 0.0002  max mem: 9341
[16:57:49.910442] Epoch: [125]  [180/195]  eta: 0:00:07  lr: 0.002325  loss: 0.4562 (0.4495)  time: 0.5086  data: 0.0001  max mem: 9341
[16:57:57.043093] Epoch: [125]  [194/195]  eta: 0:00:00  lr: 0.002325  loss: 0.4487 (0.4494)  time: 0.5107  data: 0.0001  max mem: 9341
[16:57:57.217196] Epoch: [125] Total time: 0:01:41 (0.5185 s / it)
[16:57:57.238767] Averaged stats: lr: 0.002325  loss: 0.4487 (0.4483)
[16:58:01.967288] {"train_lr": 0.0023258983417584755, "train_loss": 0.4482953301224953, "epoch": 125}
[16:58:01.967546] [16:58:01.967628] Training epoch 125 for 0:01:45
[16:58:01.967681] [16:58:01.972226] log_dir: ./exp/debug/cifar100-LT/debug
[16:58:03.525765] Epoch: [126]  [  0/195]  eta: 0:05:02  lr: 0.002325  loss: 0.4677 (0.4677)  time: 1.5525  data: 1.0507  max mem: 9341
[16:58:13.747118] Epoch: [126]  [ 20/195]  eta: 0:01:38  lr: 0.002325  loss: 0.4373 (0.4471)  time: 0.5110  data: 0.0002  max mem: 9341
[16:58:23.964484] Epoch: [126]  [ 40/195]  eta: 0:01:23  lr: 0.002325  loss: 0.4331 (0.4460)  time: 0.5108  data: 0.0002  max mem: 9341
[16:58:34.188084] Epoch: [126]  [ 60/195]  eta: 0:01:11  lr: 0.002325  loss: 0.4457 (0.4473)  time: 0.5111  data: 0.0002  max mem: 9341
[16:58:44.475874] Epoch: [126]  [ 80/195]  eta: 0:01:00  lr: 0.002324  loss: 0.4489 (0.4479)  time: 0.5143  data: 0.0002  max mem: 9341
[16:58:54.692108] Epoch: [126]  [100/195]  eta: 0:00:49  lr: 0.002324  loss: 0.4417 (0.4468)  time: 0.5108  data: 0.0002  max mem: 9341
[16:59:04.913679] Epoch: [126]  [120/195]  eta: 0:00:39  lr: 0.002324  loss: 0.4478 (0.4480)  time: 0.5110  data: 0.0002  max mem: 9341
[16:59:15.132545] Epoch: [126]  [140/195]  eta: 0:00:28  lr: 0.002324  loss: 0.4398 (0.4472)  time: 0.5109  data: 0.0002  max mem: 9341
[16:59:25.394492] Epoch: [126]  [160/195]  eta: 0:00:18  lr: 0.002324  loss: 0.4449 (0.4470)  time: 0.5130  data: 0.0002  max mem: 9341
[16:59:35.573975] Epoch: [126]  [180/195]  eta: 0:00:07  lr: 0.002323  loss: 0.4436 (0.4469)  time: 0.5089  data: 0.0001  max mem: 9341
[16:59:42.713965] Epoch: [126]  [194/195]  eta: 0:00:00  lr: 0.002323  loss: 0.4534 (0.4472)  time: 0.5110  data: 0.0001  max mem: 9341
[16:59:42.877984] Epoch: [126] Total time: 0:01:40 (0.5175 s / it)
[16:59:42.899721] Averaged stats: lr: 0.002323  loss: 0.4534 (0.4457)
[16:59:47.612417] {"train_lr": 0.0023241726256805484, "train_loss": 0.4457007319117204, "epoch": 126}
[16:59:47.612750] [16:59:47.612854] Training epoch 126 for 0:01:45
[16:59:47.612907] [16:59:47.617443] log_dir: ./exp/debug/cifar100-LT/debug
[16:59:49.257745] Epoch: [127]  [  0/195]  eta: 0:05:19  lr: 0.002323  loss: 0.4446 (0.4446)  time: 1.6389  data: 1.1327  max mem: 9341
[16:59:59.474798] Epoch: [127]  [ 20/195]  eta: 0:01:38  lr: 0.002323  loss: 0.4487 (0.4506)  time: 0.5108  data: 0.0002  max mem: 9341
[17:00:09.701130] Epoch: [127]  [ 40/195]  eta: 0:01:23  lr: 0.002323  loss: 0.4368 (0.4434)  time: 0.5113  data: 0.0002  max mem: 9341
[17:00:19.922970] Epoch: [127]  [ 60/195]  eta: 0:01:11  lr: 0.002323  loss: 0.4453 (0.4451)  time: 0.5110  data: 0.0002  max mem: 9341
[17:00:30.191102] Epoch: [127]  [ 80/195]  eta: 0:01:00  lr: 0.002323  loss: 0.4390 (0.4442)  time: 0.5134  data: 0.0002  max mem: 9341
[17:00:40.435352] Epoch: [127]  [100/195]  eta: 0:00:49  lr: 0.002322  loss: 0.4359 (0.4442)  time: 0.5122  data: 0.0002  max mem: 9341
[17:00:50.677381] Epoch: [127]  [120/195]  eta: 0:00:39  lr: 0.002322  loss: 0.4521 (0.4449)  time: 0.5120  data: 0.0002  max mem: 9341
[17:01:00.924137] Epoch: [127]  [140/195]  eta: 0:00:28  lr: 0.002322  loss: 0.4372 (0.4440)  time: 0.5123  data: 0.0002  max mem: 9341
[17:01:11.227668] Epoch: [127]  [160/195]  eta: 0:00:18  lr: 0.002322  loss: 0.4394 (0.4438)  time: 0.5151  data: 0.0002  max mem: 9341
[17:01:21.423214] Epoch: [127]  [180/195]  eta: 0:00:07  lr: 0.002322  loss: 0.4361 (0.4436)  time: 0.5097  data: 0.0001  max mem: 9341
[17:01:28.576028] Epoch: [127]  [194/195]  eta: 0:00:00  lr: 0.002322  loss: 0.4388 (0.4445)  time: 0.5126  data: 0.0001  max mem: 9341
[17:01:28.750914] Epoch: [127] Total time: 0:01:41 (0.5186 s / it)
[17:01:28.753468] Averaged stats: lr: 0.002322  loss: 0.4388 (0.4442)
[17:01:33.475666] {"train_lr": 0.0023224277005941445, "train_loss": 0.4442131317578829, "epoch": 127}
[17:01:33.475926] [17:01:33.476010] Training epoch 127 for 0:01:45
[17:01:33.476063] [17:01:33.480553] log_dir: ./exp/debug/cifar100-LT/debug
[17:01:35.101370] Epoch: [128]  [  0/195]  eta: 0:05:15  lr: 0.002321  loss: 0.4179 (0.4179)  time: 1.6194  data: 1.0977  max mem: 9341
[17:01:45.332511] Epoch: [128]  [ 20/195]  eta: 0:01:38  lr: 0.002321  loss: 0.4381 (0.4455)  time: 0.5115  data: 0.0002  max mem: 9341
[17:01:55.567557] Epoch: [128]  [ 40/195]  eta: 0:01:23  lr: 0.002321  loss: 0.4520 (0.4473)  time: 0.5117  data: 0.0002  max mem: 9341
[17:02:05.788452] Epoch: [128]  [ 60/195]  eta: 0:01:11  lr: 0.002321  loss: 0.4532 (0.4474)  time: 0.5110  data: 0.0002  max mem: 9341
[17:02:16.055615] Epoch: [128]  [ 80/195]  eta: 0:01:00  lr: 0.002321  loss: 0.4610 (0.4504)  time: 0.5133  data: 0.0002  max mem: 9341
[17:02:26.278369] Epoch: [128]  [100/195]  eta: 0:00:49  lr: 0.002321  loss: 0.4574 (0.4515)  time: 0.5111  data: 0.0002  max mem: 9341
[17:02:36.496841] Epoch: [128]  [120/195]  eta: 0:00:39  lr: 0.002320  loss: 0.4460 (0.4504)  time: 0.5109  data: 0.0002  max mem: 9341
[17:02:46.724651] Epoch: [128]  [140/195]  eta: 0:00:28  lr: 0.002320  loss: 0.4702 (0.4526)  time: 0.5113  data: 0.0002  max mem: 9341
[17:02:56.995378] Epoch: [128]  [160/195]  eta: 0:00:18  lr: 0.002320  loss: 0.4717 (0.4545)  time: 0.5135  data: 0.0002  max mem: 9341
[17:03:07.170717] Epoch: [128]  [180/195]  eta: 0:00:07  lr: 0.002320  loss: 0.4637 (0.4560)  time: 0.5087  data: 0.0001  max mem: 9341
[17:03:14.305805] Epoch: [128]  [194/195]  eta: 0:00:00  lr: 0.002320  loss: 0.4630 (0.4557)  time: 0.5109  data: 0.0001  max mem: 9341
[17:03:14.470670] Epoch: [128] Total time: 0:01:40 (0.5179 s / it)
[17:03:14.491121] Averaged stats: lr: 0.002320  loss: 0.4630 (0.4544)
[17:03:19.355351] {"train_lr": 0.0023206635963152217, "train_loss": 0.4543967441870616, "epoch": 128}
[17:03:19.355674] [17:03:19.355759] Training epoch 128 for 0:01:45
[17:03:19.355812] [17:03:19.360289] log_dir: ./exp/debug/cifar100-LT/debug
[17:03:21.072630] Epoch: [129]  [  0/195]  eta: 0:05:33  lr: 0.002320  loss: 0.4646 (0.4646)  time: 1.7096  data: 1.1913  max mem: 9341
[17:03:31.289187] Epoch: [129]  [ 20/195]  eta: 0:01:39  lr: 0.002320  loss: 0.4561 (0.4601)  time: 0.5108  data: 0.0002  max mem: 9341
[17:03:41.505844] Epoch: [129]  [ 40/195]  eta: 0:01:23  lr: 0.002319  loss: 0.4494 (0.4565)  time: 0.5108  data: 0.0002  max mem: 9341
[17:03:51.722124] Epoch: [129]  [ 60/195]  eta: 0:01:11  lr: 0.002319  loss: 0.4433 (0.4538)  time: 0.5108  data: 0.0002  max mem: 9341
[17:04:01.986547] Epoch: [129]  [ 80/195]  eta: 0:01:00  lr: 0.002319  loss: 0.4505 (0.4515)  time: 0.5132  data: 0.0002  max mem: 9341
[17:04:12.204618] Epoch: [129]  [100/195]  eta: 0:00:49  lr: 0.002319  loss: 0.4577 (0.4521)  time: 0.5108  data: 0.0002  max mem: 9341
[17:04:22.415406] Epoch: [129]  [120/195]  eta: 0:00:39  lr: 0.002319  loss: 0.4445 (0.4512)  time: 0.5105  data: 0.0002  max mem: 9341
[17:04:32.624999] Epoch: [129]  [140/195]  eta: 0:00:28  lr: 0.002319  loss: 0.4616 (0.4511)  time: 0.5104  data: 0.0002  max mem: 9341
[17:04:42.883679] Epoch: [129]  [160/195]  eta: 0:00:18  lr: 0.002318  loss: 0.4257 (0.4488)  time: 0.5129  data: 0.0002  max mem: 9341
[17:04:53.049274] Epoch: [129]  [180/195]  eta: 0:00:07  lr: 0.002318  loss: 0.4515 (0.4496)  time: 0.5082  data: 0.0001  max mem: 9341
[17:05:00.183125] Epoch: [129]  [194/195]  eta: 0:00:00  lr: 0.002318  loss: 0.4453 (0.4486)  time: 0.5105  data: 0.0001  max mem: 9341
[17:05:00.357519] Epoch: [129] Total time: 0:01:40 (0.5179 s / it)
[17:05:00.381429] Averaged stats: lr: 0.002318  loss: 0.4453 (0.4497)
[17:05:05.132521] {"train_lr": 0.0023188803429874576, "train_loss": 0.44972565915340035, "epoch": 129}
[17:05:05.132800] [17:05:05.132889] Training epoch 129 for 0:01:45
[17:05:05.132943] [17:05:05.137420] log_dir: ./exp/debug/cifar100-LT/debug
[17:05:06.859848] Epoch: [130]  [  0/195]  eta: 0:05:35  lr: 0.002318  loss: 0.4594 (0.4594)  time: 1.7214  data: 1.2156  max mem: 9341
[17:05:17.081545] Epoch: [130]  [ 20/195]  eta: 0:01:39  lr: 0.002318  loss: 0.4412 (0.4473)  time: 0.5110  data: 0.0002  max mem: 9341
[17:05:27.301609] Epoch: [130]  [ 40/195]  eta: 0:01:23  lr: 0.002318  loss: 0.4478 (0.4498)  time: 0.5109  data: 0.0002  max mem: 9341
[17:05:37.520326] Epoch: [130]  [ 60/195]  eta: 0:01:11  lr: 0.002317  loss: 0.4405 (0.4482)  time: 0.5109  data: 0.0002  max mem: 9341
[17:05:47.777735] Epoch: [130]  [ 80/195]  eta: 0:01:00  lr: 0.002317  loss: 0.4485 (0.4489)  time: 0.5128  data: 0.0002  max mem: 9341
[17:05:57.984508] Epoch: [130]  [100/195]  eta: 0:00:49  lr: 0.002317  loss: 0.4430 (0.4484)  time: 0.5103  data: 0.0002  max mem: 9341
[17:06:08.197390] Epoch: [130]  [120/195]  eta: 0:00:39  lr: 0.002317  loss: 0.4449 (0.4480)  time: 0.5106  data: 0.0002  max mem: 9341
[17:06:18.410248] Epoch: [130]  [140/195]  eta: 0:00:28  lr: 0.002317  loss: 0.4520 (0.4480)  time: 0.5106  data: 0.0002  max mem: 9341
[17:06:28.668519] Epoch: [130]  [160/195]  eta: 0:00:18  lr: 0.002316  loss: 0.4297 (0.4474)  time: 0.5129  data: 0.0002  max mem: 9341
[17:06:38.850723] Epoch: [130]  [180/195]  eta: 0:00:07  lr: 0.002316  loss: 0.4473 (0.4479)  time: 0.5091  data: 0.0001  max mem: 9341
[17:06:45.984422] Epoch: [130]  [194/195]  eta: 0:00:00  lr: 0.002316  loss: 0.4407 (0.4476)  time: 0.5109  data: 0.0001  max mem: 9341
[17:06:46.149885] Epoch: [130] Total time: 0:01:41 (0.5180 s / it)
[17:06:46.161661] Averaged stats: lr: 0.002316  loss: 0.4407 (0.4460)
[17:06:51.001823] {"train_lr": 0.002317077971081725, "train_loss": 0.4459649710318981, "epoch": 130}
[17:06:51.002153] [17:06:51.002240] Training epoch 130 for 0:01:45
[17:06:51.002293] [17:06:51.006821] log_dir: ./exp/debug/cifar100-LT/debug
[17:06:52.814118] Epoch: [131]  [  0/195]  eta: 0:05:52  lr: 0.002316  loss: 0.4047 (0.4047)  time: 1.8061  data: 1.3027  max mem: 9341
[17:07:03.033682] Epoch: [131]  [ 20/195]  eta: 0:01:40  lr: 0.002316  loss: 0.4518 (0.4532)  time: 0.5109  data: 0.0002  max mem: 9341
[17:07:13.250356] Epoch: [131]  [ 40/195]  eta: 0:01:24  lr: 0.002316  loss: 0.4466 (0.4521)  time: 0.5108  data: 0.0002  max mem: 9341
[17:07:23.463010] Epoch: [131]  [ 60/195]  eta: 0:01:11  lr: 0.002316  loss: 0.4431 (0.4519)  time: 0.5106  data: 0.0002  max mem: 9341
[17:07:33.720716] Epoch: [131]  [ 80/195]  eta: 0:01:00  lr: 0.002315  loss: 0.4387 (0.4488)  time: 0.5128  data: 0.0002  max mem: 9341
[17:07:43.933647] Epoch: [131]  [100/195]  eta: 0:00:49  lr: 0.002315  loss: 0.4499 (0.4501)  time: 0.5106  data: 0.0002  max mem: 9341
[17:07:54.141808] Epoch: [131]  [120/195]  eta: 0:00:39  lr: 0.002315  loss: 0.4452 (0.4489)  time: 0.5103  data: 0.0002  max mem: 9341
[17:08:04.351454] Epoch: [131]  [140/195]  eta: 0:00:28  lr: 0.002315  loss: 0.4374 (0.4475)  time: 0.5104  data: 0.0002  max mem: 9341
[17:08:14.610349] Epoch: [131]  [160/195]  eta: 0:00:18  lr: 0.002315  loss: 0.4458 (0.4472)  time: 0.5129  data: 0.0002  max mem: 9341
[17:08:24.778385] Epoch: [131]  [180/195]  eta: 0:00:07  lr: 0.002314  loss: 0.4441 (0.4468)  time: 0.5083  data: 0.0001  max mem: 9341
[17:08:31.915704] Epoch: [131]  [194/195]  eta: 0:00:00  lr: 0.002314  loss: 0.4435 (0.4465)  time: 0.5108  data: 0.0001  max mem: 9341
[17:08:32.082762] Epoch: [131] Total time: 0:01:41 (0.5183 s / it)
[17:08:32.094037] Averaged stats: lr: 0.002314  loss: 0.4435 (0.4438)
[17:08:36.840199] {"train_lr": 0.002315256511395592, "train_loss": 0.4438410014296189, "epoch": 131}
[17:08:36.840466] [17:08:36.840551] Training epoch 131 for 0:01:45
[17:08:36.840605] [17:08:36.845058] log_dir: ./exp/debug/cifar100-LT/debug
[17:08:38.389316] Epoch: [132]  [  0/195]  eta: 0:05:00  lr: 0.002314  loss: 0.4233 (0.4233)  time: 1.5431  data: 1.0486  max mem: 9341
[17:08:48.612555] Epoch: [132]  [ 20/195]  eta: 0:01:38  lr: 0.002314  loss: 0.4425 (0.4413)  time: 0.5111  data: 0.0002  max mem: 9341
[17:08:58.833867] Epoch: [132]  [ 40/195]  eta: 0:01:23  lr: 0.002314  loss: 0.4335 (0.4392)  time: 0.5110  data: 0.0002  max mem: 9341
[17:09:09.073754] Epoch: [132]  [ 60/195]  eta: 0:01:11  lr: 0.002314  loss: 0.4192 (0.4361)  time: 0.5119  data: 0.0002  max mem: 9341
[17:09:19.328469] Epoch: [132]  [ 80/195]  eta: 0:01:00  lr: 0.002314  loss: 0.4359 (0.4375)  time: 0.5127  data: 0.0002  max mem: 9341
[17:09:29.550810] Epoch: [132]  [100/195]  eta: 0:00:49  lr: 0.002313  loss: 0.4490 (0.4400)  time: 0.5111  data: 0.0002  max mem: 9341
[17:09:39.762292] Epoch: [132]  [120/195]  eta: 0:00:38  lr: 0.002313  loss: 0.4420 (0.4400)  time: 0.5105  data: 0.0002  max mem: 9341
[17:09:49.972476] Epoch: [132]  [140/195]  eta: 0:00:28  lr: 0.002313  loss: 0.4396 (0.4404)  time: 0.5105  data: 0.0004  max mem: 9341
[17:10:00.232445] Epoch: [132]  [160/195]  eta: 0:00:18  lr: 0.002313  loss: 0.4401 (0.4407)  time: 0.5129  data: 0.0002  max mem: 9341
[17:10:10.405577] Epoch: [132]  [180/195]  eta: 0:00:07  lr: 0.002313  loss: 0.4329 (0.4405)  time: 0.5086  data: 0.0001  max mem: 9341
[17:10:17.538956] Epoch: [132]  [194/195]  eta: 0:00:00  lr: 0.002312  loss: 0.4329 (0.4409)  time: 0.5107  data: 0.0001  max mem: 9341
[17:10:17.710348] Epoch: [132] Total time: 0:01:40 (0.5173 s / it)
[17:10:17.733846] Averaged stats: lr: 0.002312  loss: 0.4329 (0.4429)
[17:10:22.288574] {"train_lr": 0.002313415995052764, "train_loss": 0.442889458934466, "epoch": 132}
[17:10:22.288846] [17:10:22.288932] Training epoch 132 for 0:01:45
[17:10:22.288984] [17:10:22.293942] log_dir: ./exp/debug/cifar100-LT/debug
[17:10:23.952620] Epoch: [133]  [  0/195]  eta: 0:05:23  lr: 0.002312  loss: 0.4359 (0.4359)  time: 1.6576  data: 1.1429  max mem: 9341
[17:10:34.212223] Epoch: [133]  [ 20/195]  eta: 0:01:39  lr: 0.002312  loss: 0.4363 (0.4398)  time: 0.5129  data: 0.0002  max mem: 9341
[17:10:44.424556] Epoch: [133]  [ 40/195]  eta: 0:01:23  lr: 0.002312  loss: 0.4493 (0.4438)  time: 0.5106  data: 0.0002  max mem: 9341
[17:10:54.646977] Epoch: [133]  [ 60/195]  eta: 0:01:11  lr: 0.002312  loss: 0.4396 (0.4446)  time: 0.5110  data: 0.0002  max mem: 9341
[17:11:04.908848] Epoch: [133]  [ 80/195]  eta: 0:01:00  lr: 0.002312  loss: 0.4452 (0.4468)  time: 0.5130  data: 0.0002  max mem: 9341
[17:11:15.125679] Epoch: [133]  [100/195]  eta: 0:00:49  lr: 0.002311  loss: 0.4423 (0.4458)  time: 0.5108  data: 0.0002  max mem: 9341
[17:11:25.336005] Epoch: [133]  [120/195]  eta: 0:00:39  lr: 0.002311  loss: 0.4354 (0.4446)  time: 0.5105  data: 0.0002  max mem: 9341
[17:11:35.548203] Epoch: [133]  [140/195]  eta: 0:00:28  lr: 0.002311  loss: 0.4355 (0.4443)  time: 0.5105  data: 0.0002  max mem: 9341
[17:11:45.799400] Epoch: [133]  [160/195]  eta: 0:00:18  lr: 0.002311  loss: 0.4406 (0.4438)  time: 0.5125  data: 0.0002  max mem: 9341
[17:11:55.964953] Epoch: [133]  [180/195]  eta: 0:00:07  lr: 0.002311  loss: 0.4340 (0.4424)  time: 0.5082  data: 0.0001  max mem: 9341
[17:12:03.096962] Epoch: [133]  [194/195]  eta: 0:00:00  lr: 0.002311  loss: 0.4287 (0.4422)  time: 0.5104  data: 0.0001  max mem: 9341
[17:12:03.267061] Epoch: [133] Total time: 0:01:40 (0.5178 s / it)
[17:12:03.267746] Averaged stats: lr: 0.002311  loss: 0.4287 (0.4418)
[17:12:07.963231] {"train_lr": 0.0023115564535026124, "train_loss": 0.4418256442134197, "epoch": 133}
[17:12:07.963619] [17:12:07.963708] Training epoch 133 for 0:01:45
[17:12:07.963762] [17:12:07.968396] log_dir: ./exp/debug/cifar100-LT/debug
[17:12:09.572836] Epoch: [134]  [  0/195]  eta: 0:05:12  lr: 0.002311  loss: 0.4140 (0.4140)  time: 1.6031  data: 1.1024  max mem: 9341
[17:12:19.790148] Epoch: [134]  [ 20/195]  eta: 0:01:38  lr: 0.002310  loss: 0.4294 (0.4323)  time: 0.5108  data: 0.0002  max mem: 9341
[17:12:30.002579] Epoch: [134]  [ 40/195]  eta: 0:01:23  lr: 0.002310  loss: 0.4364 (0.4342)  time: 0.5106  data: 0.0002  max mem: 9341
[17:12:40.219105] Epoch: [134]  [ 60/195]  eta: 0:01:11  lr: 0.002310  loss: 0.4341 (0.4362)  time: 0.5108  data: 0.0002  max mem: 9341
[17:12:50.479113] Epoch: [134]  [ 80/195]  eta: 0:01:00  lr: 0.002310  loss: 0.4479 (0.4387)  time: 0.5129  data: 0.0002  max mem: 9341
[17:13:00.698550] Epoch: [134]  [100/195]  eta: 0:00:49  lr: 0.002310  loss: 0.4472 (0.4403)  time: 0.5109  data: 0.0002  max mem: 9341
[17:13:10.913802] Epoch: [134]  [120/195]  eta: 0:00:39  lr: 0.002309  loss: 0.4318 (0.4407)  time: 0.5107  data: 0.0002  max mem: 9341
[17:13:21.125011] Epoch: [134]  [140/195]  eta: 0:00:28  lr: 0.002309  loss: 0.4366 (0.4400)  time: 0.5105  data: 0.0002  max mem: 9341
[17:13:31.381130] Epoch: [134]  [160/195]  eta: 0:00:18  lr: 0.002309  loss: 0.4487 (0.4407)  time: 0.5128  data: 0.0002  max mem: 9341
[17:13:41.552475] Epoch: [134]  [180/195]  eta: 0:00:07  lr: 0.002309  loss: 0.4456 (0.4411)  time: 0.5085  data: 0.0001  max mem: 9341
[17:13:48.677657] Epoch: [134]  [194/195]  eta: 0:00:00  lr: 0.002309  loss: 0.4456 (0.4415)  time: 0.5103  data: 0.0001  max mem: 9341
[17:13:48.845019] Epoch: [134] Total time: 0:01:40 (0.5173 s / it)
[17:13:48.861856] Averaged stats: lr: 0.002309  loss: 0.4456 (0.4414)
[17:13:53.547855] {"train_lr": 0.0023096779185195595, "train_loss": 0.44137948961594164, "epoch": 134}
[17:13:53.548216] [17:13:53.548314] Training epoch 134 for 0:01:45
[17:13:53.548376] [17:13:53.553420] log_dir: ./exp/debug/cifar100-LT/debug
[17:13:55.422358] Epoch: [135]  [  0/195]  eta: 0:06:04  lr: 0.002309  loss: 0.4123 (0.4123)  time: 1.8677  data: 1.3695  max mem: 9341
[17:14:05.636488] Epoch: [135]  [ 20/195]  eta: 0:01:40  lr: 0.002308  loss: 0.4375 (0.4403)  time: 0.5106  data: 0.0002  max mem: 9341
[17:14:15.861472] Epoch: [135]  [ 40/195]  eta: 0:01:24  lr: 0.002308  loss: 0.4323 (0.4421)  time: 0.5111  data: 0.0002  max mem: 9341
[17:14:26.074251] Epoch: [135]  [ 60/195]  eta: 0:01:11  lr: 0.002308  loss: 0.4550 (0.4451)  time: 0.5106  data: 0.0002  max mem: 9341
[17:14:36.327941] Epoch: [135]  [ 80/195]  eta: 0:01:00  lr: 0.002308  loss: 0.4413 (0.4454)  time: 0.5126  data: 0.0002  max mem: 9341
[17:14:46.546760] Epoch: [135]  [100/195]  eta: 0:00:49  lr: 0.002308  loss: 0.4396 (0.4461)  time: 0.5109  data: 0.0002  max mem: 9341
[17:14:56.760187] Epoch: [135]  [120/195]  eta: 0:00:39  lr: 0.002308  loss: 0.4439 (0.4455)  time: 0.5106  data: 0.0002  max mem: 9341
[17:15:07.010841] Epoch: [135]  [140/195]  eta: 0:00:28  lr: 0.002307  loss: 0.4415 (0.4454)  time: 0.5125  data: 0.0002  max mem: 9341
[17:15:17.269922] Epoch: [135]  [160/195]  eta: 0:00:18  lr: 0.002307  loss: 0.4396 (0.4446)  time: 0.5129  data: 0.0002  max mem: 9341
[17:15:27.442467] Epoch: [135]  [180/195]  eta: 0:00:07  lr: 0.002307  loss: 0.4377 (0.4438)  time: 0.5086  data: 0.0002  max mem: 9341
[17:15:34.568884] Epoch: [135]  [194/195]  eta: 0:00:00  lr: 0.002307  loss: 0.4515 (0.4442)  time: 0.5102  data: 0.0001  max mem: 9341
[17:15:34.739860] Epoch: [135] Total time: 0:01:41 (0.5189 s / it)
[17:15:34.749282] Averaged stats: lr: 0.002307  loss: 0.4515 (0.4441)
[17:15:39.456526] {"train_lr": 0.0023077804222025817, "train_loss": 0.4440928835517321, "epoch": 135}
[17:15:39.456806] [17:15:39.456899] Training epoch 135 for 0:01:45
[17:15:39.456951] [17:15:39.461411] log_dir: ./exp/debug/cifar100-LT/debug
[17:15:41.148053] Epoch: [136]  [  0/195]  eta: 0:05:28  lr: 0.002307  loss: 0.4588 (0.4588)  time: 1.6853  data: 1.1788  max mem: 9341
[17:15:51.367721] Epoch: [136]  [ 20/195]  eta: 0:01:39  lr: 0.002307  loss: 0.4368 (0.4400)  time: 0.5109  data: 0.0002  max mem: 9341
[17:16:01.582811] Epoch: [136]  [ 40/195]  eta: 0:01:23  lr: 0.002306  loss: 0.4307 (0.4387)  time: 0.5107  data: 0.0002  max mem: 9341
[17:16:11.791532] Epoch: [136]  [ 60/195]  eta: 0:01:11  lr: 0.002306  loss: 0.4413 (0.4404)  time: 0.5104  data: 0.0002  max mem: 9341
[17:16:22.057937] Epoch: [136]  [ 80/195]  eta: 0:01:00  lr: 0.002306  loss: 0.4440 (0.4411)  time: 0.5133  data: 0.0002  max mem: 9341
[17:16:32.277197] Epoch: [136]  [100/195]  eta: 0:00:49  lr: 0.002306  loss: 0.4343 (0.4413)  time: 0.5109  data: 0.0002  max mem: 9341
[17:16:42.493108] Epoch: [136]  [120/195]  eta: 0:00:39  lr: 0.002306  loss: 0.4327 (0.4407)  time: 0.5107  data: 0.0002  max mem: 9341
[17:16:52.714196] Epoch: [136]  [140/195]  eta: 0:00:28  lr: 0.002305  loss: 0.4373 (0.4402)  time: 0.5110  data: 0.0002  max mem: 9341
[17:17:02.975331] Epoch: [136]  [160/195]  eta: 0:00:18  lr: 0.002305  loss: 0.4309 (0.4394)  time: 0.5130  data: 0.0002  max mem: 9341
[17:17:13.145804] Epoch: [136]  [180/195]  eta: 0:00:07  lr: 0.002305  loss: 0.4406 (0.4399)  time: 0.5085  data: 0.0001  max mem: 9341
[17:17:20.283691] Epoch: [136]  [194/195]  eta: 0:00:00  lr: 0.002305  loss: 0.4331 (0.4402)  time: 0.5108  data: 0.0001  max mem: 9341
[17:17:20.456966] Epoch: [136] Total time: 0:01:40 (0.5179 s / it)
[17:17:20.497390] Averaged stats: lr: 0.002305  loss: 0.4331 (0.4400)
[17:17:25.277344] {"train_lr": 0.002305863996974657, "train_loss": 0.44004246080533055, "epoch": 136}
[17:17:25.277649] [17:17:25.277739] Training epoch 136 for 0:01:45
[17:17:25.277793] [17:17:25.282546] log_dir: ./exp/debug/cifar100-LT/debug
[17:17:26.846136] Epoch: [137]  [  0/195]  eta: 0:05:04  lr: 0.002305  loss: 0.4412 (0.4412)  time: 1.5627  data: 1.0661  max mem: 9341
[17:17:37.067854] Epoch: [137]  [ 20/195]  eta: 0:01:38  lr: 0.002305  loss: 0.4174 (0.4266)  time: 0.5110  data: 0.0002  max mem: 9341
[17:17:47.312952] Epoch: [137]  [ 40/195]  eta: 0:01:23  lr: 0.002305  loss: 0.4422 (0.4356)  time: 0.5122  data: 0.0003  max mem: 9341
[17:17:57.531799] Epoch: [137]  [ 60/195]  eta: 0:01:11  lr: 0.002304  loss: 0.4252 (0.4328)  time: 0.5109  data: 0.0002  max mem: 9341
[17:18:07.789239] Epoch: [137]  [ 80/195]  eta: 0:01:00  lr: 0.002304  loss: 0.4315 (0.4331)  time: 0.5128  data: 0.0002  max mem: 9341
[17:18:17.999105] Epoch: [137]  [100/195]  eta: 0:00:49  lr: 0.002304  loss: 0.4349 (0.4337)  time: 0.5104  data: 0.0002  max mem: 9341
[17:18:28.209685] Epoch: [137]  [120/195]  eta: 0:00:38  lr: 0.002304  loss: 0.4375 (0.4337)  time: 0.5105  data: 0.0002  max mem: 9341
[17:18:38.424482] Epoch: [137]  [140/195]  eta: 0:00:28  lr: 0.002304  loss: 0.4369 (0.4353)  time: 0.5107  data: 0.0002  max mem: 9341
[17:18:48.682936] Epoch: [137]  [160/195]  eta: 0:00:18  lr: 0.002303  loss: 0.4342 (0.4358)  time: 0.5129  data: 0.0002  max mem: 9341
[17:18:58.850313] Epoch: [137]  [180/195]  eta: 0:00:07  lr: 0.002303  loss: 0.4322 (0.4362)  time: 0.5083  data: 0.0001  max mem: 9341
[17:19:05.977504] Epoch: [137]  [194/195]  eta: 0:00:00  lr: 0.002303  loss: 0.4303 (0.4352)  time: 0.5103  data: 0.0001  max mem: 9341
[17:19:06.148547] Epoch: [137] Total time: 0:01:40 (0.5173 s / it)
[17:19:06.172057] Averaged stats: lr: 0.002303  loss: 0.4303 (0.4367)
[17:19:10.867624] {"train_lr": 0.0023039286755822106, "train_loss": 0.4367442618959989, "epoch": 137}
[17:19:10.867885] [17:19:10.867969] Training epoch 137 for 0:01:45
[17:19:10.868022] [17:19:10.872482] log_dir: ./exp/debug/cifar100-LT/debug
[17:19:12.610973] Epoch: [138]  [  0/195]  eta: 0:05:38  lr: 0.002303  loss: 0.4426 (0.4426)  time: 1.7374  data: 1.2382  max mem: 9341
[17:19:22.825829] Epoch: [138]  [ 20/195]  eta: 0:01:39  lr: 0.002303  loss: 0.4323 (0.4335)  time: 0.5107  data: 0.0002  max mem: 9341
[17:19:33.065417] Epoch: [138]  [ 40/195]  eta: 0:01:23  lr: 0.002303  loss: 0.4285 (0.4323)  time: 0.5119  data: 0.0002  max mem: 9341
[17:19:43.279988] Epoch: [138]  [ 60/195]  eta: 0:01:11  lr: 0.002302  loss: 0.4348 (0.4341)  time: 0.5107  data: 0.0002  max mem: 9341
[17:19:53.535894] Epoch: [138]  [ 80/195]  eta: 0:01:00  lr: 0.002302  loss: 0.4384 (0.4355)  time: 0.5127  data: 0.0002  max mem: 9341
[17:20:03.754316] Epoch: [138]  [100/195]  eta: 0:00:49  lr: 0.002302  loss: 0.4259 (0.4346)  time: 0.5109  data: 0.0002  max mem: 9341
[17:20:13.970374] Epoch: [138]  [120/195]  eta: 0:00:39  lr: 0.002302  loss: 0.4299 (0.4347)  time: 0.5107  data: 0.0002  max mem: 9341
[17:20:24.209863] Epoch: [138]  [140/195]  eta: 0:00:28  lr: 0.002302  loss: 0.4327 (0.4350)  time: 0.5119  data: 0.0002  max mem: 9341
[17:20:34.477555] Epoch: [138]  [160/195]  eta: 0:00:18  lr: 0.002301  loss: 0.4301 (0.4346)  time: 0.5133  data: 0.0002  max mem: 9341
[17:20:44.649418] Epoch: [138]  [180/195]  eta: 0:00:07  lr: 0.002301  loss: 0.4540 (0.4361)  time: 0.5085  data: 0.0001  max mem: 9341
[17:20:51.783137] Epoch: [138]  [194/195]  eta: 0:00:00  lr: 0.002301  loss: 0.4218 (0.4351)  time: 0.5107  data: 0.0001  max mem: 9341
[17:20:51.963911] Epoch: [138] Total time: 0:01:41 (0.5184 s / it)
[17:20:51.964606] Averaged stats: lr: 0.002301  loss: 0.4218 (0.4358)
[17:20:56.564184] {"train_lr": 0.002301974491094543, "train_loss": 0.435836804104157, "epoch": 138}
[17:20:56.564457] [17:20:56.564542] Training epoch 138 for 0:01:45
[17:20:56.564596] [17:20:56.569054] log_dir: ./exp/debug/cifar100-LT/debug
[17:20:58.359834] Epoch: [139]  [  0/195]  eta: 0:05:49  lr: 0.002301  loss: 0.4428 (0.4428)  time: 1.7898  data: 1.2842  max mem: 9341
[17:21:08.591421] Epoch: [139]  [ 20/195]  eta: 0:01:40  lr: 0.002301  loss: 0.4417 (0.4381)  time: 0.5115  data: 0.0002  max mem: 9341
[17:21:18.806994] Epoch: [139]  [ 40/195]  eta: 0:01:24  lr: 0.002301  loss: 0.4470 (0.4422)  time: 0.5107  data: 0.0002  max mem: 9341
[17:21:29.025086] Epoch: [139]  [ 60/195]  eta: 0:01:11  lr: 0.002300  loss: 0.4398 (0.4421)  time: 0.5108  data: 0.0002  max mem: 9341
[17:21:39.284857] Epoch: [139]  [ 80/195]  eta: 0:01:00  lr: 0.002300  loss: 0.4458 (0.4445)  time: 0.5129  data: 0.0002  max mem: 9341
[17:21:49.499086] Epoch: [139]  [100/195]  eta: 0:00:49  lr: 0.002300  loss: 0.4387 (0.4429)  time: 0.5107  data: 0.0002  max mem: 9341
[17:21:59.713014] Epoch: [139]  [120/195]  eta: 0:00:39  lr: 0.002300  loss: 0.4389 (0.4430)  time: 0.5106  data: 0.0002  max mem: 9341
[17:22:09.935488] Epoch: [139]  [140/195]  eta: 0:00:28  lr: 0.002300  loss: 0.4368 (0.4430)  time: 0.5111  data: 0.0002  max mem: 9341
[17:22:20.239133] Epoch: [139]  [160/195]  eta: 0:00:18  lr: 0.002299  loss: 0.4426 (0.4427)  time: 0.5151  data: 0.0002  max mem: 9341
[17:22:30.434533] Epoch: [139]  [180/195]  eta: 0:00:07  lr: 0.002299  loss: 0.4407 (0.4427)  time: 0.5097  data: 0.0001  max mem: 9341
[17:22:37.587756] Epoch: [139]  [194/195]  eta: 0:00:00  lr: 0.002299  loss: 0.4499 (0.4428)  time: 0.5129  data: 0.0001  max mem: 9341
[17:22:37.773720] Epoch: [139] Total time: 0:01:41 (0.5190 s / it)
[17:22:37.778640] Averaged stats: lr: 0.002299  loss: 0.4499 (0.4415)
[17:22:42.451436] {"train_lr": 0.00230000147690327, "train_loss": 0.44152859785617926, "epoch": 139}
[17:22:42.451853] [17:22:42.451946] Training epoch 139 for 0:01:45
[17:22:42.452000] [17:22:42.457153] log_dir: ./exp/debug/cifar100-LT/debug
[17:22:44.258641] Epoch: [140]  [  0/195]  eta: 0:05:51  lr: 0.002299  loss: 0.4870 (0.4870)  time: 1.8001  data: 1.2921  max mem: 9341
[17:22:54.488332] Epoch: [140]  [ 20/195]  eta: 0:01:40  lr: 0.002299  loss: 0.4383 (0.4452)  time: 0.5114  data: 0.0002  max mem: 9341
[17:23:04.703412] Epoch: [140]  [ 40/195]  eta: 0:01:24  lr: 0.002299  loss: 0.4393 (0.4433)  time: 0.5107  data: 0.0002  max mem: 9341
[17:23:14.928124] Epoch: [140]  [ 60/195]  eta: 0:01:11  lr: 0.002298  loss: 0.4424 (0.4427)  time: 0.5112  data: 0.0002  max mem: 9341
[17:23:25.224616] Epoch: [140]  [ 80/195]  eta: 0:01:00  lr: 0.002298  loss: 0.4341 (0.4399)  time: 0.5148  data: 0.0002  max mem: 9341
[17:23:35.461550] Epoch: [140]  [100/195]  eta: 0:00:49  lr: 0.002298  loss: 0.4211 (0.4371)  time: 0.5118  data: 0.0002  max mem: 9341
[17:23:45.674338] Epoch: [140]  [120/195]  eta: 0:00:39  lr: 0.002298  loss: 0.4383 (0.4380)  time: 0.5106  data: 0.0002  max mem: 9341
[17:23:55.887871] Epoch: [140]  [140/195]  eta: 0:00:28  lr: 0.002298  loss: 0.4369 (0.4378)  time: 0.5106  data: 0.0002  max mem: 9341
[17:24:06.171018] Epoch: [140]  [160/195]  eta: 0:00:18  lr: 0.002297  loss: 0.4437 (0.4383)  time: 0.5141  data: 0.0002  max mem: 9341
[17:24:16.335574] Epoch: [140]  [180/195]  eta: 0:00:07  lr: 0.002297  loss: 0.4432 (0.4386)  time: 0.5082  data: 0.0001  max mem: 9341
[17:24:23.465021] Epoch: [140]  [194/195]  eta: 0:00:00  lr: 0.002297  loss: 0.4303 (0.4390)  time: 0.5105  data: 0.0001  max mem: 9341
[17:24:23.632617] Epoch: [140] Total time: 0:01:41 (0.5188 s / it)
[17:24:23.656542] Averaged stats: lr: 0.002297  loss: 0.4303 (0.4386)
[17:24:28.360147] {"train_lr": 0.002298009666721758, "train_loss": 0.43860088770206157, "epoch": 140}
[17:24:28.360468] [17:24:28.360552] Training epoch 140 for 0:01:45
[17:24:28.360605] [17:24:28.365084] log_dir: ./exp/debug/cifar100-LT/debug
[17:24:30.053966] Epoch: [141]  [  0/195]  eta: 0:05:29  lr: 0.002297  loss: 0.5228 (0.5228)  time: 1.6876  data: 1.1787  max mem: 9341
[17:24:40.270748] Epoch: [141]  [ 20/195]  eta: 0:01:39  lr: 0.002297  loss: 0.4401 (0.4421)  time: 0.5108  data: 0.0002  max mem: 9341
[17:24:50.498415] Epoch: [141]  [ 40/195]  eta: 0:01:23  lr: 0.002297  loss: 0.4309 (0.4398)  time: 0.5113  data: 0.0002  max mem: 9341
[17:25:00.714546] Epoch: [141]  [ 60/195]  eta: 0:01:11  lr: 0.002296  loss: 0.4254 (0.4372)  time: 0.5108  data: 0.0002  max mem: 9341
[17:25:10.969558] Epoch: [141]  [ 80/195]  eta: 0:01:00  lr: 0.002296  loss: 0.4363 (0.4354)  time: 0.5127  data: 0.0002  max mem: 9341
[17:25:21.181794] Epoch: [141]  [100/195]  eta: 0:00:49  lr: 0.002296  loss: 0.4337 (0.4351)  time: 0.5106  data: 0.0002  max mem: 9341
[17:25:31.395643] Epoch: [141]  [120/195]  eta: 0:00:39  lr: 0.002296  loss: 0.4427 (0.4369)  time: 0.5106  data: 0.0002  max mem: 9341
[17:25:41.603977] Epoch: [141]  [140/195]  eta: 0:00:28  lr: 0.002296  loss: 0.4540 (0.4395)  time: 0.5104  data: 0.0002  max mem: 9341
[17:25:51.858844] Epoch: [141]  [160/195]  eta: 0:00:18  lr: 0.002295  loss: 0.4527 (0.4411)  time: 0.5127  data: 0.0002  max mem: 9341
[17:26:02.023646] Epoch: [141]  [180/195]  eta: 0:00:07  lr: 0.002295  loss: 0.4353 (0.4402)  time: 0.5082  data: 0.0001  max mem: 9341
[17:26:09.151897] Epoch: [141]  [194/195]  eta: 0:00:00  lr: 0.002295  loss: 0.4311 (0.4396)  time: 0.5103  data: 0.0001  max mem: 9341
[17:26:09.326312] Epoch: [141] Total time: 0:01:40 (0.5177 s / it)
[17:26:09.331252] Averaged stats: lr: 0.002295  loss: 0.4311 (0.4378)
[17:26:14.032884] {"train_lr": 0.0022959990945845615, "train_loss": 0.4378121692018631, "epoch": 141}
[17:26:14.033219] [17:26:14.033304] Training epoch 141 for 0:01:45
[17:26:14.033358] [17:26:14.037820] log_dir: ./exp/debug/cifar100-LT/debug
[17:26:15.703602] Epoch: [142]  [  0/195]  eta: 0:05:24  lr: 0.002295  loss: 0.4678 (0.4678)  time: 1.6644  data: 1.1603  max mem: 9341
[17:26:25.917778] Epoch: [142]  [ 20/195]  eta: 0:01:38  lr: 0.002295  loss: 0.4458 (0.4443)  time: 0.5106  data: 0.0002  max mem: 9341
[17:26:36.131939] Epoch: [142]  [ 40/195]  eta: 0:01:23  lr: 0.002295  loss: 0.4395 (0.4427)  time: 0.5106  data: 0.0002  max mem: 9341
[17:26:46.347707] Epoch: [142]  [ 60/195]  eta: 0:01:11  lr: 0.002294  loss: 0.4411 (0.4414)  time: 0.5107  data: 0.0002  max mem: 9341
[17:26:56.607851] Epoch: [142]  [ 80/195]  eta: 0:01:00  lr: 0.002294  loss: 0.4259 (0.4374)  time: 0.5130  data: 0.0002  max mem: 9341
[17:27:06.852026] Epoch: [142]  [100/195]  eta: 0:00:49  lr: 0.002294  loss: 0.4276 (0.4360)  time: 0.5122  data: 0.0002  max mem: 9341
[17:27:17.065364] Epoch: [142]  [120/195]  eta: 0:00:39  lr: 0.002294  loss: 0.4226 (0.4337)  time: 0.5106  data: 0.0002  max mem: 9341
[17:27:27.277501] Epoch: [142]  [140/195]  eta: 0:00:28  lr: 0.002294  loss: 0.4368 (0.4345)  time: 0.5105  data: 0.0002  max mem: 9341
[17:27:37.530236] Epoch: [142]  [160/195]  eta: 0:00:18  lr: 0.002293  loss: 0.4271 (0.4336)  time: 0.5126  data: 0.0002  max mem: 9341
[17:27:47.702174] Epoch: [142]  [180/195]  eta: 0:00:07  lr: 0.002293  loss: 0.4383 (0.4332)  time: 0.5085  data: 0.0001  max mem: 9341
[17:27:54.834484] Epoch: [142]  [194/195]  eta: 0:00:00  lr: 0.002293  loss: 0.4244 (0.4327)  time: 0.5107  data: 0.0001  max mem: 9341
[17:27:55.000931] Epoch: [142] Total time: 0:01:40 (0.5178 s / it)
[17:27:55.016712] Averaged stats: lr: 0.002293  loss: 0.4244 (0.4352)
[17:27:59.719121] {"train_lr": 0.0022939697948467937, "train_loss": 0.4352447292361504, "epoch": 142}
[17:27:59.719384] [17:27:59.719474] Training epoch 142 for 0:01:45
[17:27:59.719527] [17:27:59.724031] log_dir: ./exp/debug/cifar100-LT/debug
[17:28:01.368256] Epoch: [143]  [  0/195]  eta: 0:05:20  lr: 0.002293  loss: 0.4986 (0.4986)  time: 1.6429  data: 1.1467  max mem: 9341
[17:28:11.609108] Epoch: [143]  [ 20/195]  eta: 0:01:39  lr: 0.002293  loss: 0.4170 (0.4318)  time: 0.5120  data: 0.0002  max mem: 9341
[17:28:21.823950] Epoch: [143]  [ 40/195]  eta: 0:01:23  lr: 0.002293  loss: 0.4440 (0.4362)  time: 0.5107  data: 0.0002  max mem: 9341
[17:28:32.035620] Epoch: [143]  [ 60/195]  eta: 0:01:11  lr: 0.002292  loss: 0.4261 (0.4353)  time: 0.5105  data: 0.0002  max mem: 9341
[17:28:42.286296] Epoch: [143]  [ 80/195]  eta: 0:01:00  lr: 0.002292  loss: 0.4368 (0.4378)  time: 0.5125  data: 0.0002  max mem: 9341
[17:28:52.513160] Epoch: [143]  [100/195]  eta: 0:00:49  lr: 0.002292  loss: 0.4309 (0.4371)  time: 0.5113  data: 0.0002  max mem: 9341
[17:29:02.745864] Epoch: [143]  [120/195]  eta: 0:00:39  lr: 0.002292  loss: 0.4230 (0.4357)  time: 0.5116  data: 0.0002  max mem: 9341
[17:29:12.974781] Epoch: [143]  [140/195]  eta: 0:00:28  lr: 0.002292  loss: 0.4426 (0.4363)  time: 0.5114  data: 0.0002  max mem: 9341
[17:29:23.272195] Epoch: [143]  [160/195]  eta: 0:00:18  lr: 0.002291  loss: 0.4244 (0.4354)  time: 0.5148  data: 0.0002  max mem: 9341
[17:29:33.461176] Epoch: [143]  [180/195]  eta: 0:00:07  lr: 0.002291  loss: 0.4465 (0.4366)  time: 0.5094  data: 0.0001  max mem: 9341
[17:29:40.607467] Epoch: [143]  [194/195]  eta: 0:00:00  lr: 0.002291  loss: 0.4515 (0.4371)  time: 0.5122  data: 0.0001  max mem: 9341
[17:29:40.775492] Epoch: [143] Total time: 0:01:41 (0.5182 s / it)
[17:29:40.812224] Averaged stats: lr: 0.002291  loss: 0.4515 (0.4364)
[17:29:45.502744] {"train_lr": 0.0022919218021836096, "train_loss": 0.43643709531961344, "epoch": 143}
[17:29:45.503045] [17:29:45.503135] Training epoch 143 for 0:01:45
[17:29:45.503189] [17:29:45.507827] log_dir: ./exp/debug/cifar100-LT/debug
[17:29:47.339358] Epoch: [144]  [  0/195]  eta: 0:05:56  lr: 0.002291  loss: 0.4276 (0.4276)  time: 1.8298  data: 1.3335  max mem: 9341
[17:29:57.577981] Epoch: [144]  [ 20/195]  eta: 0:01:40  lr: 0.002291  loss: 0.4449 (0.4369)  time: 0.5119  data: 0.0002  max mem: 9341
[17:30:07.819627] Epoch: [144]  [ 40/195]  eta: 0:01:24  lr: 0.002290  loss: 0.4418 (0.4384)  time: 0.5120  data: 0.0002  max mem: 9341
[17:30:18.053437] Epoch: [144]  [ 60/195]  eta: 0:01:12  lr: 0.002290  loss: 0.4353 (0.4373)  time: 0.5116  data: 0.0003  max mem: 9341
[17:30:28.344880] Epoch: [144]  [ 80/195]  eta: 0:01:00  lr: 0.002290  loss: 0.4325 (0.4370)  time: 0.5145  data: 0.0003  max mem: 9341
[17:30:38.574718] Epoch: [144]  [100/195]  eta: 0:00:49  lr: 0.002290  loss: 0.4327 (0.4361)  time: 0.5114  data: 0.0002  max mem: 9341
[17:30:48.815798] Epoch: [144]  [120/195]  eta: 0:00:39  lr: 0.002290  loss: 0.4393 (0.4365)  time: 0.5120  data: 0.0002  max mem: 9341
[17:30:59.050397] Epoch: [144]  [140/195]  eta: 0:00:28  lr: 0.002289  loss: 0.4459 (0.4375)  time: 0.5117  data: 0.0002  max mem: 9341
[17:31:09.341006] Epoch: [144]  [160/195]  eta: 0:00:18  lr: 0.002289  loss: 0.4275 (0.4363)  time: 0.5145  data: 0.0002  max mem: 9341
[17:31:19.526891] Epoch: [144]  [180/195]  eta: 0:00:07  lr: 0.002289  loss: 0.4316 (0.4363)  time: 0.5092  data: 0.0001  max mem: 9341
[17:31:26.677780] Epoch: [144]  [194/195]  eta: 0:00:00  lr: 0.002289  loss: 0.4303 (0.4363)  time: 0.5123  data: 0.0001  max mem: 9341
[17:31:26.840818] Epoch: [144] Total time: 0:01:41 (0.5197 s / it)
[17:31:26.858836] Averaged stats: lr: 0.002289  loss: 0.4303 (0.4390)
[17:31:31.620406] {"train_lr": 0.002289855151589537, "train_loss": 0.4390028389982688, "epoch": 144}
[17:31:31.620737] [17:31:31.620822] Training epoch 144 for 0:01:46
[17:31:31.620876] [17:31:31.625346] log_dir: ./exp/debug/cifar100-LT/debug
[17:31:33.403992] Epoch: [145]  [  0/195]  eta: 0:05:46  lr: 0.002289  loss: 0.4476 (0.4476)  time: 1.7769  data: 1.2620  max mem: 9341
[17:31:43.610220] Epoch: [145]  [ 20/195]  eta: 0:01:39  lr: 0.002289  loss: 0.4197 (0.4268)  time: 0.5103  data: 0.0002  max mem: 9341
[17:31:53.823261] Epoch: [145]  [ 40/195]  eta: 0:01:23  lr: 0.002288  loss: 0.4411 (0.4344)  time: 0.5106  data: 0.0002  max mem: 9341
[17:32:04.039020] Epoch: [145]  [ 60/195]  eta: 0:01:11  lr: 0.002288  loss: 0.4380 (0.4365)  time: 0.5107  data: 0.0002  max mem: 9341
[17:32:14.290531] Epoch: [145]  [ 80/195]  eta: 0:01:00  lr: 0.002288  loss: 0.4356 (0.4357)  time: 0.5125  data: 0.0002  max mem: 9341
[17:32:24.499799] Epoch: [145]  [100/195]  eta: 0:00:49  lr: 0.002288  loss: 0.4318 (0.4348)  time: 0.5104  data: 0.0002  max mem: 9341
[17:32:34.730924] Epoch: [145]  [120/195]  eta: 0:00:39  lr: 0.002288  loss: 0.4439 (0.4356)  time: 0.5115  data: 0.0002  max mem: 9341
[17:32:44.950998] Epoch: [145]  [140/195]  eta: 0:00:28  lr: 0.002287  loss: 0.4441 (0.4370)  time: 0.5109  data: 0.0002  max mem: 9341
[17:32:55.206742] Epoch: [145]  [160/195]  eta: 0:00:18  lr: 0.002287  loss: 0.4526 (0.4385)  time: 0.5127  data: 0.0002  max mem: 9341
[17:33:05.386146] Epoch: [145]  [180/195]  eta: 0:00:07  lr: 0.002287  loss: 0.4355 (0.4383)  time: 0.5089  data: 0.0001  max mem: 9341
[17:33:12.524561] Epoch: [145]  [194/195]  eta: 0:00:00  lr: 0.002287  loss: 0.4375 (0.4381)  time: 0.5110  data: 0.0001  max mem: 9341
[17:33:12.705079] Epoch: [145] Total time: 0:01:41 (0.5184 s / it)
[17:33:12.707717] Averaged stats: lr: 0.002287  loss: 0.4375 (0.4408)
[17:33:17.429042] {"train_lr": 0.002287769878377929, "train_loss": 0.44081589101980895, "epoch": 145}
[17:33:17.429373] [17:33:17.429458] Training epoch 145 for 0:01:45
[17:33:17.429512] [17:33:17.434292] log_dir: ./exp/debug/cifar100-LT/debug
[17:33:19.063990] Epoch: [146]  [  0/195]  eta: 0:05:17  lr: 0.002287  loss: 0.4948 (0.4948)  time: 1.6285  data: 1.1255  max mem: 9341
[17:33:29.284172] Epoch: [146]  [ 20/195]  eta: 0:01:38  lr: 0.002286  loss: 0.4481 (0.4522)  time: 0.5110  data: 0.0002  max mem: 9341
[17:33:39.496784] Epoch: [146]  [ 40/195]  eta: 0:01:23  lr: 0.002286  loss: 0.4381 (0.4463)  time: 0.5106  data: 0.0002  max mem: 9341
[17:33:49.712739] Epoch: [146]  [ 60/195]  eta: 0:01:11  lr: 0.002286  loss: 0.4344 (0.4459)  time: 0.5107  data: 0.0002  max mem: 9341
[17:33:59.980987] Epoch: [146]  [ 80/195]  eta: 0:01:00  lr: 0.002286  loss: 0.4479 (0.4457)  time: 0.5133  data: 0.0002  max mem: 9341
[17:34:10.195725] Epoch: [146]  [100/195]  eta: 0:00:49  lr: 0.002286  loss: 0.4402 (0.4449)  time: 0.5107  data: 0.0002  max mem: 9341
[17:34:20.415483] Epoch: [146]  [120/195]  eta: 0:00:39  lr: 0.002285  loss: 0.4350 (0.4441)  time: 0.5109  data: 0.0002  max mem: 9341
[17:34:30.631409] Epoch: [146]  [140/195]  eta: 0:00:28  lr: 0.002285  loss: 0.4330 (0.4425)  time: 0.5107  data: 0.0002  max mem: 9341
[17:34:40.896414] Epoch: [146]  [160/195]  eta: 0:00:18  lr: 0.002285  loss: 0.4466 (0.4429)  time: 0.5132  data: 0.0002  max mem: 9341
[17:34:51.070466] Epoch: [146]  [180/195]  eta: 0:00:07  lr: 0.002285  loss: 0.4453 (0.4423)  time: 0.5087  data: 0.0002  max mem: 9341
[17:34:58.202561] Epoch: [146]  [194/195]  eta: 0:00:00  lr: 0.002285  loss: 0.4318 (0.4420)  time: 0.5106  data: 0.0001  max mem: 9341
[17:34:58.373734] Epoch: [146] Total time: 0:01:40 (0.5176 s / it)
[17:34:58.398334] Averaged stats: lr: 0.002285  loss: 0.4318 (0.4387)
[17:35:03.093609] {"train_lr": 0.002285666018180369, "train_loss": 0.4387169091365276, "epoch": 146}
[17:35:03.093874] [17:35:03.093962] Training epoch 146 for 0:01:45
[17:35:03.094016] [17:35:03.098492] log_dir: ./exp/debug/cifar100-LT/debug
[17:35:04.799398] Epoch: [147]  [  0/195]  eta: 0:05:31  lr: 0.002285  loss: 0.4432 (0.4432)  time: 1.6999  data: 1.1870  max mem: 9341
[17:35:15.036760] Epoch: [147]  [ 20/195]  eta: 0:01:39  lr: 0.002284  loss: 0.4156 (0.4200)  time: 0.5118  data: 0.0002  max mem: 9341
[17:35:25.275311] Epoch: [147]  [ 40/195]  eta: 0:01:23  lr: 0.002284  loss: 0.4209 (0.4212)  time: 0.5119  data: 0.0002  max mem: 9341
[17:35:35.512060] Epoch: [147]  [ 60/195]  eta: 0:01:11  lr: 0.002284  loss: 0.4310 (0.4242)  time: 0.5118  data: 0.0002  max mem: 9341
[17:35:45.816862] Epoch: [147]  [ 80/195]  eta: 0:01:00  lr: 0.002284  loss: 0.4243 (0.4244)  time: 0.5152  data: 0.0002  max mem: 9341
[17:35:56.061481] Epoch: [147]  [100/195]  eta: 0:00:49  lr: 0.002283  loss: 0.4309 (0.4262)  time: 0.5122  data: 0.0002  max mem: 9341
[17:36:06.298899] Epoch: [147]  [120/195]  eta: 0:00:39  lr: 0.002283  loss: 0.4382 (0.4283)  time: 0.5118  data: 0.0002  max mem: 9341
[17:36:16.536476] Epoch: [147]  [140/195]  eta: 0:00:28  lr: 0.002283  loss: 0.4193 (0.4277)  time: 0.5118  data: 0.0002  max mem: 9341
[17:36:26.836313] Epoch: [147]  [160/195]  eta: 0:00:18  lr: 0.002283  loss: 0.4315 (0.4277)  time: 0.5149  data: 0.0002  max mem: 9341
[17:36:37.039835] Epoch: [147]  [180/195]  eta: 0:00:07  lr: 0.002283  loss: 0.4230 (0.4280)  time: 0.5101  data: 0.0001  max mem: 9341
[17:36:44.196715] Epoch: [147]  [194/195]  eta: 0:00:00  lr: 0.002282  loss: 0.4330 (0.4287)  time: 0.5128  data: 0.0001  max mem: 9341
[17:36:44.371416] Epoch: [147] Total time: 0:01:41 (0.5193 s / it)
[17:36:44.378833] Averaged stats: lr: 0.002282  loss: 0.4330 (0.4308)
[17:36:49.083489] {"train_lr": 0.002283543606946009, "train_loss": 0.43078821656795646, "epoch": 147}
[17:36:49.083837] [17:36:49.083926] Training epoch 147 for 0:01:45
[17:36:49.083980] [17:36:49.088481] log_dir: ./exp/debug/cifar100-LT/debug
[17:36:50.648993] Epoch: [148]  [  0/195]  eta: 0:05:04  lr: 0.002282  loss: 0.4687 (0.4687)  time: 1.5591  data: 1.0531  max mem: 9341
[17:37:00.872174] Epoch: [148]  [ 20/195]  eta: 0:01:38  lr: 0.002282  loss: 0.4331 (0.4330)  time: 0.5111  data: 0.0002  max mem: 9341
[17:37:11.091074] Epoch: [148]  [ 40/195]  eta: 0:01:23  lr: 0.002282  loss: 0.4249 (0.4318)  time: 0.5109  data: 0.0002  max mem: 9341
[17:37:21.310677] Epoch: [148]  [ 60/195]  eta: 0:01:11  lr: 0.002282  loss: 0.4334 (0.4333)  time: 0.5109  data: 0.0002  max mem: 9341
[17:37:31.572538] Epoch: [148]  [ 80/195]  eta: 0:01:00  lr: 0.002282  loss: 0.4304 (0.4325)  time: 0.5130  data: 0.0002  max mem: 9341
[17:37:41.784651] Epoch: [148]  [100/195]  eta: 0:00:49  lr: 0.002281  loss: 0.4270 (0.4317)  time: 0.5105  data: 0.0002  max mem: 9341
[17:37:51.998118] Epoch: [148]  [120/195]  eta: 0:00:38  lr: 0.002281  loss: 0.4217 (0.4314)  time: 0.5106  data: 0.0002  max mem: 9341
[17:38:02.212889] Epoch: [148]  [140/195]  eta: 0:00:28  lr: 0.002281  loss: 0.4304 (0.4313)  time: 0.5107  data: 0.0002  max mem: 9341
[17:38:12.466936] Epoch: [148]  [160/195]  eta: 0:00:18  lr: 0.002281  loss: 0.4287 (0.4310)  time: 0.5126  data: 0.0002  max mem: 9341
[17:38:22.634186] Epoch: [148]  [180/195]  eta: 0:00:07  lr: 0.002280  loss: 0.4424 (0.4323)  time: 0.5083  data: 0.0001  max mem: 9341
[17:38:29.762831] Epoch: [148]  [194/195]  eta: 0:00:00  lr: 0.002280  loss: 0.4305 (0.4320)  time: 0.5103  data: 0.0001  max mem: 9341
[17:38:29.927811] Epoch: [148] Total time: 0:01:40 (0.5171 s / it)
[17:38:29.949438] Averaged stats: lr: 0.002280  loss: 0.4305 (0.4330)
[17:38:34.656505] {"train_lr": 0.002281402680941006, "train_loss": 0.4329946394532155, "epoch": 148}
[17:38:34.656802] [17:38:34.656890] Training epoch 148 for 0:01:45
[17:38:34.656943] [17:38:34.661492] log_dir: ./exp/debug/cifar100-LT/debug
[17:38:36.353118] Epoch: [149]  [  0/195]  eta: 0:05:29  lr: 0.002280  loss: 0.4373 (0.4373)  time: 1.6908  data: 1.1855  max mem: 9341
[17:38:46.592189] Epoch: [149]  [ 20/195]  eta: 0:01:39  lr: 0.002280  loss: 0.4345 (0.4357)  time: 0.5119  data: 0.0002  max mem: 9341
[17:38:56.840053] Epoch: [149]  [ 40/195]  eta: 0:01:23  lr: 0.002280  loss: 0.4283 (0.4333)  time: 0.5123  data: 0.0002  max mem: 9341
[17:39:07.077467] Epoch: [149]  [ 60/195]  eta: 0:01:11  lr: 0.002280  loss: 0.4340 (0.4335)  time: 0.5118  data: 0.0002  max mem: 9341
[17:39:17.393950] Epoch: [149]  [ 80/195]  eta: 0:01:00  lr: 0.002279  loss: 0.4305 (0.4345)  time: 0.5158  data: 0.0002  max mem: 9341
[17:39:27.633105] Epoch: [149]  [100/195]  eta: 0:00:49  lr: 0.002279  loss: 0.4430 (0.4367)  time: 0.5119  data: 0.0002  max mem: 9341
[17:39:37.876000] Epoch: [149]  [120/195]  eta: 0:00:39  lr: 0.002279  loss: 0.4483 (0.4385)  time: 0.5121  data: 0.0002  max mem: 9341
[17:39:48.128618] Epoch: [149]  [140/195]  eta: 0:00:28  lr: 0.002279  loss: 0.4299 (0.4376)  time: 0.5126  data: 0.0002  max mem: 9341
[17:39:58.438948] Epoch: [149]  [160/195]  eta: 0:00:18  lr: 0.002278  loss: 0.4348 (0.4371)  time: 0.5155  data: 0.0002  max mem: 9341
[17:40:08.635477] Epoch: [149]  [180/195]  eta: 0:00:07  lr: 0.002278  loss: 0.4331 (0.4367)  time: 0.5098  data: 0.0001  max mem: 9341
[17:40:15.792431] Epoch: [149]  [194/195]  eta: 0:00:00  lr: 0.002278  loss: 0.4313 (0.4370)  time: 0.5129  data: 0.0001  max mem: 9341
[17:40:15.976460] Epoch: [149] Total time: 0:01:41 (0.5196 s / it)
[17:40:15.977186] Averaged stats: lr: 0.002278  loss: 0.4313 (0.4357)
[17:40:20.670077] {"train_lr": 0.002279243276747875, "train_loss": 0.4357064058001225, "epoch": 149}
[17:40:20.670342] [17:40:20.670426] Training epoch 149 for 0:01:46
[17:40:20.670479] [17:40:20.675015] log_dir: ./exp/debug/cifar100-LT/debug
[17:40:22.522386] Epoch: [150]  [  0/195]  eta: 0:06:00  lr: 0.002278  loss: 0.4642 (0.4642)  time: 1.8465  data: 1.3354  max mem: 9341
[17:40:32.771970] Epoch: [150]  [ 20/195]  eta: 0:01:40  lr: 0.002278  loss: 0.4464 (0.4430)  time: 0.5124  data: 0.0002  max mem: 9341
[17:40:43.011118] Epoch: [150]  [ 40/195]  eta: 0:01:24  lr: 0.002278  loss: 0.4153 (0.4339)  time: 0.5119  data: 0.0002  max mem: 9341
[17:40:53.253635] Epoch: [150]  [ 60/195]  eta: 0:01:12  lr: 0.002278  loss: 0.4306 (0.4346)  time: 0.5121  data: 0.0002  max mem: 9341
[17:41:03.555192] Epoch: [150]  [ 80/195]  eta: 0:01:00  lr: 0.002277  loss: 0.4314 (0.4364)  time: 0.5150  data: 0.0002  max mem: 9341
[17:41:13.795594] Epoch: [150]  [100/195]  eta: 0:00:49  lr: 0.002277  loss: 0.4331 (0.4358)  time: 0.5120  data: 0.0002  max mem: 9341
[17:41:24.028407] Epoch: [150]  [120/195]  eta: 0:00:39  lr: 0.002277  loss: 0.4332 (0.4357)  time: 0.5116  data: 0.0002  max mem: 9341
[17:41:34.268933] Epoch: [150]  [140/195]  eta: 0:00:28  lr: 0.002277  loss: 0.4321 (0.4356)  time: 0.5120  data: 0.0002  max mem: 9341
[17:41:44.572643] Epoch: [150]  [160/195]  eta: 0:00:18  lr: 0.002276  loss: 0.4363 (0.4357)  time: 0.5151  data: 0.0002  max mem: 9341
[17:41:54.772948] Epoch: [150]  [180/195]  eta: 0:00:07  lr: 0.002276  loss: 0.4361 (0.4357)  time: 0.5100  data: 0.0001  max mem: 9341
[17:42:01.929897] Epoch: [150]  [194/195]  eta: 0:00:00  lr: 0.002276  loss: 0.4298 (0.4354)  time: 0.5129  data: 0.0001  max mem: 9341
[17:42:02.104693] Epoch: [150] Total time: 0:01:41 (0.5202 s / it)
[17:42:02.116213] Averaged stats: lr: 0.002276  loss: 0.4298 (0.4365)
[17:42:06.848971] {"train_lr": 0.0022770654312648745, "train_loss": 0.4365399484833082, "epoch": 150}
[17:42:06.849240] [17:42:06.849325] Training epoch 150 for 0:01:46
[17:42:06.849378] [17:42:06.853855] log_dir: ./exp/debug/cifar100-LT/debug
[17:42:08.663646] Epoch: [151]  [  0/195]  eta: 0:05:52  lr: 0.002276  loss: 0.4423 (0.4423)  time: 1.8085  data: 1.2940  max mem: 9341
[17:42:18.881312] Epoch: [151]  [ 20/195]  eta: 0:01:40  lr: 0.002276  loss: 0.4301 (0.4318)  time: 0.5108  data: 0.0002  max mem: 9341
[17:42:29.108059] Epoch: [151]  [ 40/195]  eta: 0:01:24  lr: 0.002276  loss: 0.4383 (0.4331)  time: 0.5113  data: 0.0002  max mem: 9341
[17:42:39.323817] Epoch: [151]  [ 60/195]  eta: 0:01:11  lr: 0.002275  loss: 0.4319 (0.4347)  time: 0.5107  data: 0.0002  max mem: 9341
[17:42:49.587965] Epoch: [151]  [ 80/195]  eta: 0:01:00  lr: 0.002275  loss: 0.4447 (0.4370)  time: 0.5132  data: 0.0002  max mem: 9341
[17:42:59.802413] Epoch: [151]  [100/195]  eta: 0:00:49  lr: 0.002275  loss: 0.4271 (0.4358)  time: 0.5107  data: 0.0002  max mem: 9341
[17:43:10.018332] Epoch: [151]  [120/195]  eta: 0:00:39  lr: 0.002275  loss: 0.4314 (0.4356)  time: 0.5107  data: 0.0002  max mem: 9341
[17:43:20.236979] Epoch: [151]  [140/195]  eta: 0:00:28  lr: 0.002274  loss: 0.4331 (0.4353)  time: 0.5109  data: 0.0002  max mem: 9341
[17:43:30.497996] Epoch: [151]  [160/195]  eta: 0:00:18  lr: 0.002274  loss: 0.4332 (0.4350)  time: 0.5130  data: 0.0002  max mem: 9341
[17:43:40.669667] Epoch: [151]  [180/195]  eta: 0:00:07  lr: 0.002274  loss: 0.4280 (0.4357)  time: 0.5085  data: 0.0002  max mem: 9341
[17:43:47.805148] Epoch: [151]  [194/195]  eta: 0:00:00  lr: 0.002274  loss: 0.4335 (0.4355)  time: 0.5108  data: 0.0001  max mem: 9341
[17:43:47.965297] Epoch: [151] Total time: 0:01:41 (0.5185 s / it)
[17:43:47.986696] Averaged stats: lr: 0.002274  loss: 0.4335 (0.4343)
[17:43:52.659941] {"train_lr": 0.0022748691817053835, "train_loss": 0.4342956160887694, "epoch": 151}
[17:43:52.660314] [17:43:52.660401] Training epoch 151 for 0:01:45
[17:43:52.660455] [17:43:52.664914] log_dir: ./exp/debug/cifar100-LT/debug
[17:43:54.461597] Epoch: [152]  [  0/195]  eta: 0:05:50  lr: 0.002274  loss: 0.4342 (0.4342)  time: 1.7951  data: 1.2987  max mem: 9341
[17:44:04.684163] Epoch: [152]  [ 20/195]  eta: 0:01:40  lr: 0.002273  loss: 0.4285 (0.4363)  time: 0.5110  data: 0.0003  max mem: 9341
[17:44:14.913547] Epoch: [152]  [ 40/195]  eta: 0:01:24  lr: 0.002273  loss: 0.4220 (0.4314)  time: 0.5114  data: 0.0003  max mem: 9341
[17:44:25.141758] Epoch: [152]  [ 60/195]  eta: 0:01:11  lr: 0.002273  loss: 0.4370 (0.4324)  time: 0.5113  data: 0.0002  max mem: 9341
[17:44:35.401984] Epoch: [152]  [ 80/195]  eta: 0:01:00  lr: 0.002273  loss: 0.4280 (0.4311)  time: 0.5130  data: 0.0002  max mem: 9341
[17:44:45.615354] Epoch: [152]  [100/195]  eta: 0:00:49  lr: 0.002273  loss: 0.4355 (0.4322)  time: 0.5106  data: 0.0002  max mem: 9341
[17:44:55.839344] Epoch: [152]  [120/195]  eta: 0:00:39  lr: 0.002272  loss: 0.4325 (0.4326)  time: 0.5111  data: 0.0002  max mem: 9341
[17:45:06.073136] Epoch: [152]  [140/195]  eta: 0:00:28  lr: 0.002272  loss: 0.4254 (0.4312)  time: 0.5116  data: 0.0002  max mem: 9341
[17:45:16.334889] Epoch: [152]  [160/195]  eta: 0:00:18  lr: 0.002272  loss: 0.4317 (0.4310)  time: 0.5130  data: 0.0002  max mem: 9341
[17:45:26.520110] Epoch: [152]  [180/195]  eta: 0:00:07  lr: 0.002272  loss: 0.4349 (0.4312)  time: 0.5092  data: 0.0002  max mem: 9341
[17:45:33.661413] Epoch: [152]  [194/195]  eta: 0:00:00  lr: 0.002271  loss: 0.4230 (0.4311)  time: 0.5113  data: 0.0001  max mem: 9341
[17:45:33.835599] Epoch: [152] Total time: 0:01:41 (0.5188 s / it)
[17:45:33.851250] Averaged stats: lr: 0.002271  loss: 0.4230 (0.4313)
[17:45:38.570982] {"train_lr": 0.002272654565597237, "train_loss": 0.4312861028772134, "epoch": 152}
[17:45:38.571263] [17:45:38.571360] Training epoch 152 for 0:01:45
[17:45:38.571415] [17:45:38.575899] log_dir: ./exp/debug/cifar100-LT/debug
[17:45:40.353940] Epoch: [153]  [  0/195]  eta: 0:05:46  lr: 0.002271  loss: 0.4259 (0.4259)  time: 1.7771  data: 1.2793  max mem: 9341
[17:45:50.598111] Epoch: [153]  [ 20/195]  eta: 0:01:40  lr: 0.002271  loss: 0.4339 (0.4304)  time: 0.5122  data: 0.0002  max mem: 9341
[17:46:00.842140] Epoch: [153]  [ 40/195]  eta: 0:01:24  lr: 0.002271  loss: 0.4294 (0.4300)  time: 0.5121  data: 0.0002  max mem: 9341
[17:46:11.086853] Epoch: [153]  [ 60/195]  eta: 0:01:11  lr: 0.002271  loss: 0.4218 (0.4297)  time: 0.5122  data: 0.0002  max mem: 9341
[17:46:21.393652] Epoch: [153]  [ 80/195]  eta: 0:01:00  lr: 0.002271  loss: 0.4295 (0.4308)  time: 0.5153  data: 0.0002  max mem: 9341
[17:46:31.639980] Epoch: [153]  [100/195]  eta: 0:00:49  lr: 0.002270  loss: 0.4358 (0.4321)  time: 0.5123  data: 0.0002  max mem: 9341
[17:46:41.854457] Epoch: [153]  [120/195]  eta: 0:00:39  lr: 0.002270  loss: 0.4253 (0.4319)  time: 0.5107  data: 0.0002  max mem: 9341
[17:46:52.066572] Epoch: [153]  [140/195]  eta: 0:00:28  lr: 0.002270  loss: 0.4317 (0.4308)  time: 0.5106  data: 0.0002  max mem: 9341
[17:47:02.327936] Epoch: [153]  [160/195]  eta: 0:00:18  lr: 0.002270  loss: 0.4339 (0.4311)  time: 0.5130  data: 0.0002  max mem: 9341
[17:47:12.503896] Epoch: [153]  [180/195]  eta: 0:00:07  lr: 0.002269  loss: 0.4163 (0.4298)  time: 0.5087  data: 0.0001  max mem: 9341
[17:47:19.637047] Epoch: [153]  [194/195]  eta: 0:00:00  lr: 0.002269  loss: 0.4387 (0.4307)  time: 0.5107  data: 0.0001  max mem: 9341
[17:47:19.819636] Epoch: [153] Total time: 0:01:41 (0.5192 s / it)
[17:47:19.820379] Averaged stats: lr: 0.002269  loss: 0.4387 (0.4283)
[17:47:24.542991] {"train_lr": 0.00227042162078212, "train_loss": 0.4283392954331178, "epoch": 153}
[17:47:24.543256] [17:47:24.543340] Training epoch 153 for 0:01:45
[17:47:24.543392] [17:47:24.547934] log_dir: ./exp/debug/cifar100-LT/debug
[17:47:26.229818] Epoch: [154]  [  0/195]  eta: 0:05:27  lr: 0.002269  loss: 0.4263 (0.4263)  time: 1.6804  data: 1.1722  max mem: 9341
[17:47:36.450006] Epoch: [154]  [ 20/195]  eta: 0:01:39  lr: 0.002269  loss: 0.4216 (0.4240)  time: 0.5110  data: 0.0002  max mem: 9341
[17:47:46.672286] Epoch: [154]  [ 40/195]  eta: 0:01:23  lr: 0.002269  loss: 0.4384 (0.4283)  time: 0.5110  data: 0.0002  max mem: 9341
[17:47:56.886207] Epoch: [154]  [ 60/195]  eta: 0:01:11  lr: 0.002269  loss: 0.4185 (0.4268)  time: 0.5106  data: 0.0002  max mem: 9341
[17:48:07.149709] Epoch: [154]  [ 80/195]  eta: 0:01:00  lr: 0.002268  loss: 0.4311 (0.4280)  time: 0.5131  data: 0.0002  max mem: 9341
[17:48:17.372315] Epoch: [154]  [100/195]  eta: 0:00:49  lr: 0.002268  loss: 0.4304 (0.4292)  time: 0.5111  data: 0.0002  max mem: 9341
[17:48:27.586665] Epoch: [154]  [120/195]  eta: 0:00:39  lr: 0.002268  loss: 0.4235 (0.4279)  time: 0.5107  data: 0.0002  max mem: 9341
[17:48:37.805209] Epoch: [154]  [140/195]  eta: 0:00:28  lr: 0.002268  loss: 0.4279 (0.4281)  time: 0.5109  data: 0.0002  max mem: 9341
[17:48:48.059512] Epoch: [154]  [160/195]  eta: 0:00:18  lr: 0.002267  loss: 0.4267 (0.4279)  time: 0.5127  data: 0.0002  max mem: 9341
[17:48:58.239002] Epoch: [154]  [180/195]  eta: 0:00:07  lr: 0.002267  loss: 0.4285 (0.4289)  time: 0.5089  data: 0.0001  max mem: 9341
[17:49:05.369197] Epoch: [154]  [194/195]  eta: 0:00:00  lr: 0.002267  loss: 0.4255 (0.4289)  time: 0.5106  data: 0.0001  max mem: 9341
[17:49:05.527204] Epoch: [154] Total time: 0:01:40 (0.5178 s / it)
[17:49:05.543335] Averaged stats: lr: 0.002267  loss: 0.4255 (0.4286)
[17:49:10.290302] {"train_lr": 0.002268170385414903, "train_loss": 0.4286387035861993, "epoch": 154}
[17:49:10.290580] [17:49:10.290664] Training epoch 154 for 0:01:45
[17:49:10.290718] [17:49:10.295295] log_dir: ./exp/debug/cifar100-LT/debug
[17:49:12.001073] Epoch: [155]  [  0/195]  eta: 0:05:32  lr: 0.002267  loss: 0.4461 (0.4461)  time: 1.7047  data: 1.2011  max mem: 9341
[17:49:22.221047] Epoch: [155]  [ 20/195]  eta: 0:01:39  lr: 0.002267  loss: 0.4292 (0.4278)  time: 0.5109  data: 0.0002  max mem: 9341
[17:49:32.440607] Epoch: [155]  [ 40/195]  eta: 0:01:23  lr: 0.002267  loss: 0.4280 (0.4295)  time: 0.5109  data: 0.0002  max mem: 9341
[17:49:42.658992] Epoch: [155]  [ 60/195]  eta: 0:01:11  lr: 0.002266  loss: 0.4236 (0.4286)  time: 0.5109  data: 0.0002  max mem: 9341
[17:49:52.915103] Epoch: [155]  [ 80/195]  eta: 0:01:00  lr: 0.002266  loss: 0.4231 (0.4277)  time: 0.5127  data: 0.0002  max mem: 9341
[17:50:03.127964] Epoch: [155]  [100/195]  eta: 0:00:49  lr: 0.002266  loss: 0.4319 (0.4283)  time: 0.5106  data: 0.0002  max mem: 9341
[17:50:13.346600] Epoch: [155]  [120/195]  eta: 0:00:39  lr: 0.002266  loss: 0.4284 (0.4280)  time: 0.5109  data: 0.0002  max mem: 9341
[17:50:23.561342] Epoch: [155]  [140/195]  eta: 0:00:28  lr: 0.002265  loss: 0.4222 (0.4271)  time: 0.5107  data: 0.0002  max mem: 9341
[17:50:33.823559] Epoch: [155]  [160/195]  eta: 0:00:18  lr: 0.002265  loss: 0.4230 (0.4276)  time: 0.5131  data: 0.0002  max mem: 9341
[17:50:44.004191] Epoch: [155]  [180/195]  eta: 0:00:07  lr: 0.002265  loss: 0.4217 (0.4263)  time: 0.5090  data: 0.0001  max mem: 9341
[17:50:51.134247] Epoch: [155]  [194/195]  eta: 0:00:00  lr: 0.002265  loss: 0.4269 (0.4268)  time: 0.5106  data: 0.0001  max mem: 9341
[17:50:51.315518] Epoch: [155] Total time: 0:01:41 (0.5181 s / it)
[17:50:51.324818] Averaged stats: lr: 0.002265  loss: 0.4269 (0.4278)
[17:50:56.023332] {"train_lr": 0.0022659008979629813, "train_loss": 0.4277602025331595, "epoch": 155}
[17:50:56.023694] [17:50:56.023779] Training epoch 155 for 0:01:45
[17:50:56.023833] [17:50:56.028438] log_dir: ./exp/debug/cifar100-LT/debug
[17:50:57.623758] Epoch: [156]  [  0/195]  eta: 0:05:10  lr: 0.002265  loss: 0.4351 (0.4351)  time: 1.5940  data: 1.0909  max mem: 9341
[17:51:07.857992] Epoch: [156]  [ 20/195]  eta: 0:01:38  lr: 0.002264  loss: 0.4337 (0.4330)  time: 0.5116  data: 0.0002  max mem: 9341
[17:51:18.078288] Epoch: [156]  [ 40/195]  eta: 0:01:23  lr: 0.002264  loss: 0.4366 (0.4328)  time: 0.5110  data: 0.0002  max mem: 9341
[17:51:28.297829] Epoch: [156]  [ 60/195]  eta: 0:01:11  lr: 0.002264  loss: 0.4417 (0.4350)  time: 0.5109  data: 0.0002  max mem: 9341
[17:51:38.554595] Epoch: [156]  [ 80/195]  eta: 0:01:00  lr: 0.002264  loss: 0.4267 (0.4356)  time: 0.5128  data: 0.0002  max mem: 9341
[17:51:48.770425] Epoch: [156]  [100/195]  eta: 0:00:49  lr: 0.002264  loss: 0.4399 (0.4371)  time: 0.5107  data: 0.0002  max mem: 9341
[17:51:58.988483] Epoch: [156]  [120/195]  eta: 0:00:39  lr: 0.002263  loss: 0.4411 (0.4384)  time: 0.5108  data: 0.0002  max mem: 9341
[17:52:09.203821] Epoch: [156]  [140/195]  eta: 0:00:28  lr: 0.002263  loss: 0.4515 (0.4410)  time: 0.5107  data: 0.0002  max mem: 9341
[17:52:19.462784] Epoch: [156]  [160/195]  eta: 0:00:18  lr: 0.002263  loss: 0.4286 (0.4413)  time: 0.5129  data: 0.0002  max mem: 9341
[17:52:29.631020] Epoch: [156]  [180/195]  eta: 0:00:07  lr: 0.002263  loss: 0.4283 (0.4403)  time: 0.5084  data: 0.0001  max mem: 9341
[17:52:36.759242] Epoch: [156]  [194/195]  eta: 0:00:00  lr: 0.002262  loss: 0.4377 (0.4406)  time: 0.5102  data: 0.0001  max mem: 9341
[17:52:36.931771] Epoch: [156] Total time: 0:01:40 (0.5175 s / it)
[17:52:36.946324] Averaged stats: lr: 0.002262  loss: 0.4377 (0.4377)
[17:52:41.681304] {"train_lr": 0.0022636131972056336, "train_loss": 0.43766763504499046, "epoch": 156}
[17:52:41.681531] [17:52:41.681623] Training epoch 156 for 0:01:45
[17:52:41.681677] [17:52:41.686086] log_dir: ./exp/debug/cifar100-LT/debug
[17:52:43.253885] Epoch: [157]  [  0/195]  eta: 0:05:05  lr: 0.002262  loss: 0.4651 (0.4651)  time: 1.5668  data: 1.0620  max mem: 9341
[17:52:53.487167] Epoch: [157]  [ 20/195]  eta: 0:01:38  lr: 0.002262  loss: 0.4533 (0.4488)  time: 0.5116  data: 0.0002  max mem: 9341
[17:53:03.724597] Epoch: [157]  [ 40/195]  eta: 0:01:23  lr: 0.002262  loss: 0.4410 (0.4495)  time: 0.5118  data: 0.0002  max mem: 9341
[17:53:13.965003] Epoch: [157]  [ 60/195]  eta: 0:01:11  lr: 0.002262  loss: 0.4239 (0.4459)  time: 0.5120  data: 0.0002  max mem: 9341
[17:53:24.228983] Epoch: [157]  [ 80/195]  eta: 0:01:00  lr: 0.002261  loss: 0.4318 (0.4428)  time: 0.5131  data: 0.0002  max mem: 9341
[17:53:34.439099] Epoch: [157]  [100/195]  eta: 0:00:49  lr: 0.002261  loss: 0.4297 (0.4423)  time: 0.5104  data: 0.0002  max mem: 9341
[17:53:44.650784] Epoch: [157]  [120/195]  eta: 0:00:39  lr: 0.002261  loss: 0.4468 (0.4426)  time: 0.5105  data: 0.0002  max mem: 9341
[17:53:54.862352] Epoch: [157]  [140/195]  eta: 0:00:28  lr: 0.002261  loss: 0.4676 (0.4454)  time: 0.5105  data: 0.0002  max mem: 9341
[17:54:05.124302] Epoch: [157]  [160/195]  eta: 0:00:18  lr: 0.002260  loss: 0.4414 (0.4455)  time: 0.5130  data: 0.0002  max mem: 9341
[17:54:15.300972] Epoch: [157]  [180/195]  eta: 0:00:07  lr: 0.002260  loss: 0.4404 (0.4449)  time: 0.5088  data: 0.0001  max mem: 9341
[17:54:22.433733] Epoch: [157]  [194/195]  eta: 0:00:00  lr: 0.002260  loss: 0.4295 (0.4438)  time: 0.5106  data: 0.0001  max mem: 9341
[17:54:22.601830] Epoch: [157] Total time: 0:01:40 (0.5175 s / it)
[17:54:22.618643] Averaged stats: lr: 0.002260  loss: 0.4295 (0.4418)
[17:54:27.316884] {"train_lr": 0.002261307322233349, "train_loss": 0.44178342505907403, "epoch": 157}
[17:54:27.317141] [17:54:27.317228] Training epoch 157 for 0:01:45
[17:54:27.317280] [17:54:27.321742] log_dir: ./exp/debug/cifar100-LT/debug
[17:54:29.004931] Epoch: [158]  [  0/195]  eta: 0:05:27  lr: 0.002260  loss: 0.4510 (0.4510)  time: 1.6819  data: 1.1817  max mem: 9341
[17:54:39.218301] Epoch: [158]  [ 20/195]  eta: 0:01:39  lr: 0.002260  loss: 0.4305 (0.4365)  time: 0.5106  data: 0.0004  max mem: 9341
[17:54:49.439171] Epoch: [158]  [ 40/195]  eta: 0:01:23  lr: 0.002260  loss: 0.4380 (0.4368)  time: 0.5110  data: 0.0002  max mem: 9341
[17:54:59.650327] Epoch: [158]  [ 60/195]  eta: 0:01:11  lr: 0.002259  loss: 0.4520 (0.4423)  time: 0.5105  data: 0.0002  max mem: 9341
[17:55:09.904152] Epoch: [158]  [ 80/195]  eta: 0:01:00  lr: 0.002259  loss: 0.4633 (0.4481)  time: 0.5126  data: 0.0002  max mem: 9341
[17:55:20.117053] Epoch: [158]  [100/195]  eta: 0:00:49  lr: 0.002259  loss: 0.4668 (0.4511)  time: 0.5106  data: 0.0002  max mem: 9341
[17:55:30.328301] Epoch: [158]  [120/195]  eta: 0:00:39  lr: 0.002259  loss: 0.4501 (0.4512)  time: 0.5105  data: 0.0002  max mem: 9341
[17:55:40.541010] Epoch: [158]  [140/195]  eta: 0:00:28  lr: 0.002259  loss: 0.4480 (0.4497)  time: 0.5106  data: 0.0002  max mem: 9341
[17:55:50.796291] Epoch: [158]  [160/195]  eta: 0:00:18  lr: 0.002258  loss: 0.4376 (0.4486)  time: 0.5127  data: 0.0002  max mem: 9341
[17:56:00.968616] Epoch: [158]  [180/195]  eta: 0:00:07  lr: 0.002258  loss: 0.4542 (0.4496)  time: 0.5086  data: 0.0001  max mem: 9341
[17:56:08.114074] Epoch: [158]  [194/195]  eta: 0:00:00  lr: 0.002258  loss: 0.4408 (0.4491)  time: 0.5113  data: 0.0001  max mem: 9341
[17:56:08.285156] Epoch: [158] Total time: 0:01:40 (0.5178 s / it)
[17:56:08.298472] Averaged stats: lr: 0.002258  loss: 0.4408 (0.4473)
[17:56:12.932429] {"train_lr": 0.0022589833124471797, "train_loss": 0.4472598475141403, "epoch": 158}
[17:56:12.932755] [17:56:12.932853] Training epoch 158 for 0:01:45
[17:56:12.932908] [17:56:12.937941] log_dir: ./exp/debug/cifar100-LT/debug
[17:56:14.697868] Epoch: [159]  [  0/195]  eta: 0:05:43  lr: 0.002258  loss: 0.4610 (0.4610)  time: 1.7591  data: 1.2415  max mem: 9341
[17:56:24.909865] Epoch: [159]  [ 20/195]  eta: 0:01:39  lr: 0.002258  loss: 0.4312 (0.4304)  time: 0.5105  data: 0.0002  max mem: 9341
[17:56:35.124712] Epoch: [159]  [ 40/195]  eta: 0:01:23  lr: 0.002257  loss: 0.4433 (0.4369)  time: 0.5107  data: 0.0002  max mem: 9341
[17:56:45.337303] Epoch: [159]  [ 60/195]  eta: 0:01:11  lr: 0.002257  loss: 0.4430 (0.4399)  time: 0.5106  data: 0.0002  max mem: 9341
[17:56:55.597345] Epoch: [159]  [ 80/195]  eta: 0:01:00  lr: 0.002257  loss: 0.4358 (0.4370)  time: 0.5129  data: 0.0002  max mem: 9341
[17:57:05.812432] Epoch: [159]  [100/195]  eta: 0:00:49  lr: 0.002257  loss: 0.4274 (0.4359)  time: 0.5107  data: 0.0002  max mem: 9341
[17:57:16.033528] Epoch: [159]  [120/195]  eta: 0:00:39  lr: 0.002256  loss: 0.4319 (0.4366)  time: 0.5110  data: 0.0002  max mem: 9341
[17:57:26.249459] Epoch: [159]  [140/195]  eta: 0:00:28  lr: 0.002256  loss: 0.4133 (0.4352)  time: 0.5107  data: 0.0002  max mem: 9341
[17:57:36.505849] Epoch: [159]  [160/195]  eta: 0:00:18  lr: 0.002256  loss: 0.4323 (0.4342)  time: 0.5128  data: 0.0002  max mem: 9341
[17:57:46.681763] Epoch: [159]  [180/195]  eta: 0:00:07  lr: 0.002256  loss: 0.4425 (0.4345)  time: 0.5087  data: 0.0001  max mem: 9341
[17:57:53.816466] Epoch: [159]  [194/195]  eta: 0:00:00  lr: 0.002255  loss: 0.4420 (0.4342)  time: 0.5107  data: 0.0001  max mem: 9341
[17:57:53.982993] Epoch: [159] Total time: 0:01:41 (0.5182 s / it)
[17:57:53.999313] Averaged stats: lr: 0.002255  loss: 0.4420 (0.4377)
[17:57:58.721755] {"train_lr": 0.002256641207558034, "train_loss": 0.4376559345385967, "epoch": 159}
[17:57:58.722013] [17:57:58.722100] Training epoch 159 for 0:01:45
[17:57:58.722153] [17:57:58.726704] log_dir: ./exp/debug/cifar100-LT/debug
[17:58:00.520413] Epoch: [160]  [  0/195]  eta: 0:05:49  lr: 0.002255  loss: 0.4361 (0.4361)  time: 1.7927  data: 1.2937  max mem: 9341
[17:58:10.730875] Epoch: [160]  [ 20/195]  eta: 0:01:40  lr: 0.002255  loss: 0.4254 (0.4269)  time: 0.5105  data: 0.0002  max mem: 9341
[17:58:20.952289] Epoch: [160]  [ 40/195]  eta: 0:01:24  lr: 0.002255  loss: 0.4263 (0.4296)  time: 0.5110  data: 0.0002  max mem: 9341
[17:58:31.164715] Epoch: [160]  [ 60/195]  eta: 0:01:11  lr: 0.002255  loss: 0.4356 (0.4312)  time: 0.5106  data: 0.0002  max mem: 9341
[17:58:41.423250] Epoch: [160]  [ 80/195]  eta: 0:01:00  lr: 0.002254  loss: 0.4362 (0.4307)  time: 0.5129  data: 0.0002  max mem: 9341
[17:58:51.630635] Epoch: [160]  [100/195]  eta: 0:00:49  lr: 0.002254  loss: 0.4270 (0.4297)  time: 0.5103  data: 0.0002  max mem: 9341
[17:59:01.854734] Epoch: [160]  [120/195]  eta: 0:00:39  lr: 0.002254  loss: 0.4303 (0.4298)  time: 0.5111  data: 0.0002  max mem: 9341
[17:59:12.073563] Epoch: [160]  [140/195]  eta: 0:00:28  lr: 0.002254  loss: 0.4316 (0.4308)  time: 0.5109  data: 0.0002  max mem: 9341
[17:59:22.338629] Epoch: [160]  [160/195]  eta: 0:00:18  lr: 0.002253  loss: 0.4238 (0.4310)  time: 0.5132  data: 0.0002  max mem: 9341
[17:59:32.526113] Epoch: [160]  [180/195]  eta: 0:00:07  lr: 0.002253  loss: 0.4202 (0.4300)  time: 0.5093  data: 0.0001  max mem: 9341
[17:59:39.665531] Epoch: [160]  [194/195]  eta: 0:00:00  lr: 0.002253  loss: 0.4236 (0.4297)  time: 0.5113  data: 0.0001  max mem: 9341
[17:59:39.834295] Epoch: [160] Total time: 0:01:41 (0.5185 s / it)
[17:59:39.851973] Averaged stats: lr: 0.002253  loss: 0.4236 (0.4296)
[17:59:44.476487] {"train_lr": 0.002254281047586022, "train_loss": 0.42955074165111934, "epoch": 160}
[17:59:44.476746] [17:59:44.476830] Training epoch 160 for 0:01:45
[17:59:44.476883] [17:59:44.481394] log_dir: ./exp/debug/cifar100-LT/debug
[17:59:46.039951] Epoch: [161]  [  0/195]  eta: 0:05:03  lr: 0.002253  loss: 0.4023 (0.4023)  time: 1.5572  data: 1.0587  max mem: 9341
[17:59:56.251476] Epoch: [161]  [ 20/195]  eta: 0:01:38  lr: 0.002253  loss: 0.4271 (0.4299)  time: 0.5105  data: 0.0002  max mem: 9341
[18:00:06.472761] Epoch: [161]  [ 40/195]  eta: 0:01:23  lr: 0.002253  loss: 0.4207 (0.4299)  time: 0.5110  data: 0.0002  max mem: 9341
[18:00:16.691117] Epoch: [161]  [ 60/195]  eta: 0:01:11  lr: 0.002252  loss: 0.4313 (0.4281)  time: 0.5109  data: 0.0002  max mem: 9341
[18:00:26.954734] Epoch: [161]  [ 80/195]  eta: 0:01:00  lr: 0.002252  loss: 0.4302 (0.4278)  time: 0.5131  data: 0.0002  max mem: 9341
[18:00:37.170567] Epoch: [161]  [100/195]  eta: 0:00:49  lr: 0.002252  loss: 0.4113 (0.4262)  time: 0.5107  data: 0.0002  max mem: 9341
[18:00:47.390207] Epoch: [161]  [120/195]  eta: 0:00:38  lr: 0.002252  loss: 0.4292 (0.4265)  time: 0.5109  data: 0.0002  max mem: 9341
[18:00:57.606981] Epoch: [161]  [140/195]  eta: 0:00:28  lr: 0.002251  loss: 0.4135 (0.4257)  time: 0.5108  data: 0.0002  max mem: 9341
[18:01:07.868561] Epoch: [161]  [160/195]  eta: 0:00:18  lr: 0.002251  loss: 0.4286 (0.4258)  time: 0.5130  data: 0.0002  max mem: 9341
[18:01:18.047777] Epoch: [161]  [180/195]  eta: 0:00:07  lr: 0.002251  loss: 0.4332 (0.4263)  time: 0.5089  data: 0.0001  max mem: 9341
[18:01:25.183156] Epoch: [161]  [194/195]  eta: 0:00:00  lr: 0.002251  loss: 0.4238 (0.4263)  time: 0.5108  data: 0.0001  max mem: 9341
[18:01:25.347178] Epoch: [161] Total time: 0:01:40 (0.5173 s / it)
[18:01:25.370616] Averaged stats: lr: 0.002251  loss: 0.4238 (0.4250)
[18:01:30.068404] {"train_lr": 0.0022519028728597693, "train_loss": 0.42497923783002756, "epoch": 161}
[18:01:30.068656] [18:01:30.068743] Training epoch 161 for 0:01:45
[18:01:30.068810] [18:01:30.073262] log_dir: ./exp/debug/cifar100-LT/debug
[18:01:31.672169] Epoch: [162]  [  0/195]  eta: 0:05:11  lr: 0.002251  loss: 0.3942 (0.3942)  time: 1.5979  data: 1.0799  max mem: 9341
[18:01:41.893864] Epoch: [162]  [ 20/195]  eta: 0:01:38  lr: 0.002250  loss: 0.4226 (0.4216)  time: 0.5110  data: 0.0002  max mem: 9341
[18:01:52.108611] Epoch: [162]  [ 40/195]  eta: 0:01:23  lr: 0.002250  loss: 0.4327 (0.4260)  time: 0.5107  data: 0.0002  max mem: 9341
[18:02:02.327541] Epoch: [162]  [ 60/195]  eta: 0:01:11  lr: 0.002250  loss: 0.4107 (0.4227)  time: 0.5109  data: 0.0002  max mem: 9341
[18:02:12.606343] Epoch: [162]  [ 80/195]  eta: 0:01:00  lr: 0.002250  loss: 0.4313 (0.4237)  time: 0.5139  data: 0.0002  max mem: 9341
[18:02:22.841787] Epoch: [162]  [100/195]  eta: 0:00:49  lr: 0.002249  loss: 0.4254 (0.4239)  time: 0.5117  data: 0.0002  max mem: 9341
[18:02:33.079650] Epoch: [162]  [120/195]  eta: 0:00:39  lr: 0.002249  loss: 0.4309 (0.4253)  time: 0.5118  data: 0.0002  max mem: 9341
[18:02:43.318295] Epoch: [162]  [140/195]  eta: 0:00:28  lr: 0.002249  loss: 0.4325 (0.4272)  time: 0.5119  data: 0.0002  max mem: 9341
[18:02:53.621978] Epoch: [162]  [160/195]  eta: 0:00:18  lr: 0.002249  loss: 0.4033 (0.4259)  time: 0.5151  data: 0.0002  max mem: 9341
[18:03:03.809951] Epoch: [162]  [180/195]  eta: 0:00:07  lr: 0.002248  loss: 0.4270 (0.4264)  time: 0.5093  data: 0.0001  max mem: 9341
[18:03:10.961482] Epoch: [162]  [194/195]  eta: 0:00:00  lr: 0.002248  loss: 0.4382 (0.4274)  time: 0.5127  data: 0.0001  max mem: 9341
[18:03:11.126891] Epoch: [162] Total time: 0:01:41 (0.5182 s / it)
[18:03:11.133744] Averaged stats: lr: 0.002248  loss: 0.4382 (0.4277)
[18:03:15.843754] {"train_lr": 0.0022495067240157116, "train_loss": 0.4277476336711492, "epoch": 162}
[18:03:15.844022] [18:03:15.844132] Training epoch 162 for 0:01:45
[18:03:15.844188] [18:03:15.848687] log_dir: ./exp/debug/cifar100-LT/debug
[18:03:17.601103] Epoch: [163]  [  0/195]  eta: 0:05:41  lr: 0.002248  loss: 0.4618 (0.4618)  time: 1.7500  data: 1.2602  max mem: 9341
[18:03:27.834364] Epoch: [163]  [ 20/195]  eta: 0:01:39  lr: 0.002248  loss: 0.4249 (0.4275)  time: 0.5116  data: 0.0002  max mem: 9341
[18:03:38.076242] Epoch: [163]  [ 40/195]  eta: 0:01:24  lr: 0.002248  loss: 0.4259 (0.4287)  time: 0.5120  data: 0.0002  max mem: 9341
[18:03:48.323187] Epoch: [163]  [ 60/195]  eta: 0:01:11  lr: 0.002248  loss: 0.4307 (0.4302)  time: 0.5123  data: 0.0002  max mem: 9341
[18:03:58.618460] Epoch: [163]  [ 80/195]  eta: 0:01:00  lr: 0.002247  loss: 0.4257 (0.4291)  time: 0.5147  data: 0.0002  max mem: 9341
[18:04:08.838329] Epoch: [163]  [100/195]  eta: 0:00:49  lr: 0.002247  loss: 0.4287 (0.4284)  time: 0.5109  data: 0.0002  max mem: 9341
[18:04:19.052239] Epoch: [163]  [120/195]  eta: 0:00:39  lr: 0.002247  loss: 0.4262 (0.4274)  time: 0.5106  data: 0.0002  max mem: 9341
[18:04:29.267370] Epoch: [163]  [140/195]  eta: 0:00:28  lr: 0.002247  loss: 0.4272 (0.4272)  time: 0.5107  data: 0.0002  max mem: 9341
[18:04:39.527785] Epoch: [163]  [160/195]  eta: 0:00:18  lr: 0.002246  loss: 0.4211 (0.4267)  time: 0.5130  data: 0.0002  max mem: 9341
[18:04:49.697197] Epoch: [163]  [180/195]  eta: 0:00:07  lr: 0.002246  loss: 0.4328 (0.4272)  time: 0.5084  data: 0.0002  max mem: 9341
[18:04:56.829579] Epoch: [163]  [194/195]  eta: 0:00:00  lr: 0.002246  loss: 0.4383 (0.4278)  time: 0.5105  data: 0.0001  max mem: 9341
[18:04:56.990064] Epoch: [163] Total time: 0:01:41 (0.5187 s / it)
[18:04:57.004124] Averaged stats: lr: 0.002246  loss: 0.4383 (0.4268)
[18:05:01.685683] {"train_lr": 0.0022470926419974253, "train_loss": 0.4268428412003395, "epoch": 163}
[18:05:01.686086] [18:05:01.686171] Training epoch 163 for 0:01:45
[18:05:01.686225] [18:05:01.690801] log_dir: ./exp/debug/cifar100-LT/debug
[18:05:03.166555] Epoch: [164]  [  0/195]  eta: 0:04:47  lr: 0.002246  loss: 0.3901 (0.3901)  time: 1.4744  data: 0.9815  max mem: 9341
[18:05:13.443529] Epoch: [164]  [ 20/195]  eta: 0:01:37  lr: 0.002246  loss: 0.4341 (0.4287)  time: 0.5138  data: 0.0002  max mem: 9341
[18:05:23.663195] Epoch: [164]  [ 40/195]  eta: 0:01:23  lr: 0.002245  loss: 0.4242 (0.4263)  time: 0.5109  data: 0.0002  max mem: 9341
[18:05:33.880778] Epoch: [164]  [ 60/195]  eta: 0:01:11  lr: 0.002245  loss: 0.4306 (0.4274)  time: 0.5108  data: 0.0002  max mem: 9341
[18:05:44.135985] Epoch: [164]  [ 80/195]  eta: 0:01:00  lr: 0.002245  loss: 0.4233 (0.4277)  time: 0.5127  data: 0.0002  max mem: 9341
[18:05:54.346555] Epoch: [164]  [100/195]  eta: 0:00:49  lr: 0.002245  loss: 0.4251 (0.4287)  time: 0.5105  data: 0.0002  max mem: 9341
[18:06:04.560397] Epoch: [164]  [120/195]  eta: 0:00:38  lr: 0.002244  loss: 0.4286 (0.4302)  time: 0.5106  data: 0.0002  max mem: 9341
[18:06:14.781653] Epoch: [164]  [140/195]  eta: 0:00:28  lr: 0.002244  loss: 0.4249 (0.4310)  time: 0.5110  data: 0.0002  max mem: 9341
[18:06:25.041747] Epoch: [164]  [160/195]  eta: 0:00:18  lr: 0.002244  loss: 0.4364 (0.4309)  time: 0.5129  data: 0.0002  max mem: 9341
[18:06:35.221005] Epoch: [164]  [180/195]  eta: 0:00:07  lr: 0.002244  loss: 0.4284 (0.4312)  time: 0.5089  data: 0.0001  max mem: 9341
[18:06:42.356434] Epoch: [164]  [194/195]  eta: 0:00:00  lr: 0.002243  loss: 0.4299 (0.4310)  time: 0.5108  data: 0.0001  max mem: 9341
[18:06:42.526277] Epoch: [164] Total time: 0:01:40 (0.5171 s / it)
[18:06:42.543636] Averaged stats: lr: 0.002243  loss: 0.4299 (0.4291)
[18:06:47.241180] {"train_lr": 0.002244660668054929, "train_loss": 0.42907746667281177, "epoch": 164}
[18:06:47.241445] [18:06:47.241529] Training epoch 164 for 0:01:45
[18:06:47.241582] [18:06:47.246374] log_dir: ./exp/debug/cifar100-LT/debug
[18:06:48.760458] Epoch: [165]  [  0/195]  eta: 0:04:55  lr: 0.002243  loss: 0.4437 (0.4437)  time: 1.5130  data: 1.0182  max mem: 9341
[18:06:58.982972] Epoch: [165]  [ 20/195]  eta: 0:01:37  lr: 0.002243  loss: 0.4227 (0.4245)  time: 0.5111  data: 0.0002  max mem: 9341
[18:07:09.201644] Epoch: [165]  [ 40/195]  eta: 0:01:22  lr: 0.002243  loss: 0.4142 (0.4201)  time: 0.5109  data: 0.0002  max mem: 9341
[18:07:19.430182] Epoch: [165]  [ 60/195]  eta: 0:01:11  lr: 0.002243  loss: 0.4192 (0.4210)  time: 0.5114  data: 0.0002  max mem: 9341
[18:07:29.714491] Epoch: [165]  [ 80/195]  eta: 0:01:00  lr: 0.002242  loss: 0.4237 (0.4223)  time: 0.5142  data: 0.0002  max mem: 9341
[18:07:39.934531] Epoch: [165]  [100/195]  eta: 0:00:49  lr: 0.002242  loss: 0.4246 (0.4228)  time: 0.5109  data: 0.0002  max mem: 9341
[18:07:50.156105] Epoch: [165]  [120/195]  eta: 0:00:38  lr: 0.002242  loss: 0.4125 (0.4215)  time: 0.5110  data: 0.0002  max mem: 9341
[18:08:00.369855] Epoch: [165]  [140/195]  eta: 0:00:28  lr: 0.002242  loss: 0.4242 (0.4215)  time: 0.5106  data: 0.0002  max mem: 9341
[18:08:10.629033] Epoch: [165]  [160/195]  eta: 0:00:18  lr: 0.002241  loss: 0.4227 (0.4224)  time: 0.5129  data: 0.0002  max mem: 9341
[18:08:20.801423] Epoch: [165]  [180/195]  eta: 0:00:07  lr: 0.002241  loss: 0.4210 (0.4235)  time: 0.5086  data: 0.0002  max mem: 9341
[18:08:27.933460] Epoch: [165]  [194/195]  eta: 0:00:00  lr: 0.002241  loss: 0.4266 (0.4247)  time: 0.5105  data: 0.0001  max mem: 9341
[18:08:28.108220] Epoch: [165] Total time: 0:01:40 (0.5172 s / it)
[18:08:28.123252] Averaged stats: lr: 0.002241  loss: 0.4266 (0.4265)
[18:08:32.853802] {"train_lr": 0.002242210843743921, "train_loss": 0.4264961324441127, "epoch": 165}
[18:08:32.854134] [18:08:32.854219] Training epoch 165 for 0:01:45
[18:08:32.854273] [18:08:32.858770] log_dir: ./exp/debug/cifar100-LT/debug
[18:08:34.587813] Epoch: [166]  [  0/195]  eta: 0:05:36  lr: 0.002241  loss: 0.4144 (0.4144)  time: 1.7278  data: 1.2398  max mem: 9341
[18:08:44.804752] Epoch: [166]  [ 20/195]  eta: 0:01:39  lr: 0.002241  loss: 0.4250 (0.4331)  time: 0.5108  data: 0.0002  max mem: 9341
[18:08:55.044619] Epoch: [166]  [ 40/195]  eta: 0:01:23  lr: 0.002240  loss: 0.4281 (0.4307)  time: 0.5119  data: 0.0002  max mem: 9341
[18:09:05.271279] Epoch: [166]  [ 60/195]  eta: 0:01:11  lr: 0.002240  loss: 0.4373 (0.4301)  time: 0.5113  data: 0.0002  max mem: 9341
[18:09:15.534250] Epoch: [166]  [ 80/195]  eta: 0:01:00  lr: 0.002240  loss: 0.4174 (0.4276)  time: 0.5131  data: 0.0002  max mem: 9341
[18:09:25.753724] Epoch: [166]  [100/195]  eta: 0:00:49  lr: 0.002240  loss: 0.4287 (0.4281)  time: 0.5109  data: 0.0002  max mem: 9341
[18:09:35.974102] Epoch: [166]  [120/195]  eta: 0:00:39  lr: 0.002239  loss: 0.4236 (0.4268)  time: 0.5110  data: 0.0002  max mem: 9341
[18:09:46.192359] Epoch: [166]  [140/195]  eta: 0:00:28  lr: 0.002239  loss: 0.4162 (0.4260)  time: 0.5109  data: 0.0002  max mem: 9341
[18:09:56.447963] Epoch: [166]  [160/195]  eta: 0:00:18  lr: 0.002239  loss: 0.4194 (0.4264)  time: 0.5127  data: 0.0002  max mem: 9341
[18:10:06.621967] Epoch: [166]  [180/195]  eta: 0:00:07  lr: 0.002239  loss: 0.4133 (0.4257)  time: 0.5086  data: 0.0001  max mem: 9341
[18:10:13.756747] Epoch: [166]  [194/195]  eta: 0:00:00  lr: 0.002238  loss: 0.4211 (0.4258)  time: 0.5107  data: 0.0001  max mem: 9341
[18:10:13.929372] Epoch: [166] Total time: 0:01:41 (0.5183 s / it)
[18:10:13.946584] Averaged stats: lr: 0.002238  loss: 0.4211 (0.4246)
[18:10:18.563816] {"train_lr": 0.0022397432109251625, "train_loss": 0.4245710549828334, "epoch": 166}
[18:10:18.564101] [18:10:18.564188] Training epoch 166 for 0:01:45
[18:10:18.564249] [18:10:18.568680] log_dir: ./exp/debug/cifar100-LT/debug
[18:10:20.192210] Epoch: [167]  [  0/195]  eta: 0:05:16  lr: 0.002238  loss: 0.4219 (0.4219)  time: 1.6226  data: 1.1282  max mem: 9341
[18:10:30.448188] Epoch: [167]  [ 20/195]  eta: 0:01:38  lr: 0.002238  loss: 0.4123 (0.4177)  time: 0.5127  data: 0.0002  max mem: 9341
[18:10:40.667273] Epoch: [167]  [ 40/195]  eta: 0:01:23  lr: 0.002238  loss: 0.4220 (0.4190)  time: 0.5109  data: 0.0002  max mem: 9341
[18:10:50.884859] Epoch: [167]  [ 60/195]  eta: 0:01:11  lr: 0.002238  loss: 0.4165 (0.4188)  time: 0.5108  data: 0.0002  max mem: 9341
[18:11:01.140430] Epoch: [167]  [ 80/195]  eta: 0:01:00  lr: 0.002237  loss: 0.4161 (0.4184)  time: 0.5127  data: 0.0002  max mem: 9341
[18:11:11.352390] Epoch: [167]  [100/195]  eta: 0:00:49  lr: 0.002237  loss: 0.4210 (0.4182)  time: 0.5105  data: 0.0002  max mem: 9341
[18:11:21.565510] Epoch: [167]  [120/195]  eta: 0:00:39  lr: 0.002237  loss: 0.4288 (0.4201)  time: 0.5106  data: 0.0002  max mem: 9341
[18:11:31.776553] Epoch: [167]  [140/195]  eta: 0:00:28  lr: 0.002237  loss: 0.4093 (0.4205)  time: 0.5105  data: 0.0002  max mem: 9341
[18:11:42.031469] Epoch: [167]  [160/195]  eta: 0:00:18  lr: 0.002236  loss: 0.4249 (0.4210)  time: 0.5127  data: 0.0002  max mem: 9341
[18:11:52.218527] Epoch: [167]  [180/195]  eta: 0:00:07  lr: 0.002236  loss: 0.4149 (0.4214)  time: 0.5093  data: 0.0001  max mem: 9341
[18:11:59.349059] Epoch: [167]  [194/195]  eta: 0:00:00  lr: 0.002236  loss: 0.4230 (0.4220)  time: 0.5106  data: 0.0001  max mem: 9341
[18:11:59.519968] Epoch: [167] Total time: 0:01:40 (0.5177 s / it)
[18:11:59.532440] Averaged stats: lr: 0.002236  loss: 0.4230 (0.4251)
[18:12:04.270217] {"train_lr": 0.0022372578117636635, "train_loss": 0.42506141788684404, "epoch": 167}
[18:12:04.270455] [18:12:04.270542] Training epoch 167 for 0:01:45
[18:12:04.270647] [18:12:04.275686] log_dir: ./exp/debug/cifar100-LT/debug
[18:12:05.994318] Epoch: [168]  [  0/195]  eta: 0:05:34  lr: 0.002236  loss: 0.4547 (0.4547)  time: 1.7173  data: 1.2103  max mem: 9341
[18:12:16.230355] Epoch: [168]  [ 20/195]  eta: 0:01:39  lr: 0.002236  loss: 0.4280 (0.4298)  time: 0.5117  data: 0.0002  max mem: 9341
[18:12:26.468488] Epoch: [168]  [ 40/195]  eta: 0:01:23  lr: 0.002235  loss: 0.4152 (0.4251)  time: 0.5118  data: 0.0002  max mem: 9341
[18:12:36.681987] Epoch: [168]  [ 60/195]  eta: 0:01:11  lr: 0.002235  loss: 0.4246 (0.4245)  time: 0.5106  data: 0.0002  max mem: 9341
[18:12:46.949163] Epoch: [168]  [ 80/195]  eta: 0:01:00  lr: 0.002235  loss: 0.4176 (0.4244)  time: 0.5133  data: 0.0002  max mem: 9341
[18:12:57.173297] Epoch: [168]  [100/195]  eta: 0:00:49  lr: 0.002235  loss: 0.4346 (0.4278)  time: 0.5111  data: 0.0002  max mem: 9341
[18:13:07.389672] Epoch: [168]  [120/195]  eta: 0:00:39  lr: 0.002234  loss: 0.4297 (0.4274)  time: 0.5108  data: 0.0002  max mem: 9341
[18:13:17.607343] Epoch: [168]  [140/195]  eta: 0:00:28  lr: 0.002234  loss: 0.4431 (0.4293)  time: 0.5108  data: 0.0002  max mem: 9341
[18:13:27.870432] Epoch: [168]  [160/195]  eta: 0:00:18  lr: 0.002234  loss: 0.4378 (0.4308)  time: 0.5131  data: 0.0002  max mem: 9341
[18:13:38.049554] Epoch: [168]  [180/195]  eta: 0:00:07  lr: 0.002234  loss: 0.4168 (0.4303)  time: 0.5089  data: 0.0001  max mem: 9341
[18:13:45.186237] Epoch: [168]  [194/195]  eta: 0:00:00  lr: 0.002233  loss: 0.4455 (0.4320)  time: 0.5110  data: 0.0001  max mem: 9341
[18:13:45.355896] Epoch: [168] Total time: 0:01:41 (0.5184 s / it)
[18:13:45.376938] Averaged stats: lr: 0.002233  loss: 0.4455 (0.4325)
[18:13:50.075213] {"train_lr": 0.0022347546887280657, "train_loss": 0.43250315017425095, "epoch": 168}
[18:13:50.075536] [18:13:50.075625] Training epoch 168 for 0:01:45
[18:13:50.075680] [18:13:50.080166] log_dir: ./exp/debug/cifar100-LT/debug
[18:13:51.753167] Epoch: [169]  [  0/195]  eta: 0:05:26  lr: 0.002233  loss: 0.4223 (0.4223)  time: 1.6720  data: 1.1634  max mem: 9341
[18:14:01.980890] Epoch: [169]  [ 20/195]  eta: 0:01:39  lr: 0.002233  loss: 0.4322 (0.4329)  time: 0.5113  data: 0.0002  max mem: 9341
[18:14:12.227715] Epoch: [169]  [ 40/195]  eta: 0:01:23  lr: 0.002233  loss: 0.4404 (0.4383)  time: 0.5123  data: 0.0002  max mem: 9341
[18:14:22.449774] Epoch: [169]  [ 60/195]  eta: 0:01:11  lr: 0.002233  loss: 0.4316 (0.4351)  time: 0.5110  data: 0.0002  max mem: 9341
[18:14:32.709726] Epoch: [169]  [ 80/195]  eta: 0:01:00  lr: 0.002232  loss: 0.4340 (0.4330)  time: 0.5129  data: 0.0002  max mem: 9341
[18:14:42.919630] Epoch: [169]  [100/195]  eta: 0:00:49  lr: 0.002232  loss: 0.4189 (0.4309)  time: 0.5104  data: 0.0002  max mem: 9341
[18:14:53.126599] Epoch: [169]  [120/195]  eta: 0:00:39  lr: 0.002232  loss: 0.4444 (0.4320)  time: 0.5103  data: 0.0002  max mem: 9341
[18:15:03.334438] Epoch: [169]  [140/195]  eta: 0:00:28  lr: 0.002232  loss: 0.4266 (0.4311)  time: 0.5103  data: 0.0002  max mem: 9341
[18:15:13.593667] Epoch: [169]  [160/195]  eta: 0:00:18  lr: 0.002231  loss: 0.4266 (0.4310)  time: 0.5129  data: 0.0002  max mem: 9341
[18:15:23.763884] Epoch: [169]  [180/195]  eta: 0:00:07  lr: 0.002231  loss: 0.4257 (0.4311)  time: 0.5085  data: 0.0002  max mem: 9341
[18:15:30.891458] Epoch: [169]  [194/195]  eta: 0:00:00  lr: 0.002231  loss: 0.4158 (0.4300)  time: 0.5103  data: 0.0001  max mem: 9341
[18:15:31.062126] Epoch: [169] Total time: 0:01:40 (0.5179 s / it)
[18:15:31.087435] Averaged stats: lr: 0.002231  loss: 0.4158 (0.4309)
[18:15:35.804711] {"train_lr": 0.0022322338845898126, "train_loss": 0.4309135046142798, "epoch": 169}
[18:15:35.804973] [18:15:35.805056] Training epoch 169 for 0:01:45
[18:15:35.805109] [18:15:35.809595] log_dir: ./exp/debug/cifar100-LT/debug
[18:15:37.425301] Epoch: [170]  [  0/195]  eta: 0:05:14  lr: 0.002231  loss: 0.4397 (0.4397)  time: 1.6146  data: 1.0994  max mem: 9341
[18:15:47.649841] Epoch: [170]  [ 20/195]  eta: 0:01:38  lr: 0.002231  loss: 0.4250 (0.4249)  time: 0.5112  data: 0.0002  max mem: 9341
[18:15:57.893548] Epoch: [170]  [ 40/195]  eta: 0:01:23  lr: 0.002230  loss: 0.4139 (0.4216)  time: 0.5121  data: 0.0002  max mem: 9341
[18:16:08.109854] Epoch: [170]  [ 60/195]  eta: 0:01:11  lr: 0.002230  loss: 0.4162 (0.4218)  time: 0.5108  data: 0.0002  max mem: 9341
[18:16:18.368370] Epoch: [170]  [ 80/195]  eta: 0:01:00  lr: 0.002230  loss: 0.4329 (0.4258)  time: 0.5129  data: 0.0002  max mem: 9341
[18:16:28.581415] Epoch: [170]  [100/195]  eta: 0:00:49  lr: 0.002230  loss: 0.4244 (0.4266)  time: 0.5106  data: 0.0002  max mem: 9341
[18:16:38.797455] Epoch: [170]  [120/195]  eta: 0:00:39  lr: 0.002229  loss: 0.4199 (0.4248)  time: 0.5107  data: 0.0002  max mem: 9341
[18:16:49.010715] Epoch: [170]  [140/195]  eta: 0:00:28  lr: 0.002229  loss: 0.4206 (0.4241)  time: 0.5106  data: 0.0002  max mem: 9341
[18:16:59.271066] Epoch: [170]  [160/195]  eta: 0:00:18  lr: 0.002229  loss: 0.4175 (0.4240)  time: 0.5130  data: 0.0002  max mem: 9341
[18:17:09.442941] Epoch: [170]  [180/195]  eta: 0:00:07  lr: 0.002229  loss: 0.4317 (0.4240)  time: 0.5085  data: 0.0001  max mem: 9341
[18:17:16.576157] Epoch: [170]  [194/195]  eta: 0:00:00  lr: 0.002228  loss: 0.4285 (0.4235)  time: 0.5107  data: 0.0001  max mem: 9341
[18:17:16.752010] Epoch: [170] Total time: 0:01:40 (0.5177 s / it)
[18:17:16.773263] Averaged stats: lr: 0.002228  loss: 0.4285 (0.4244)
[18:17:21.465444] {"train_lr": 0.0022296954424225026, "train_loss": 0.4244060267622654, "epoch": 170}
[18:17:21.465699] [18:17:21.465782] Training epoch 170 for 0:01:45
[18:17:21.465835] [18:17:21.470332] log_dir: ./exp/debug/cifar100-LT/debug
[18:17:23.357499] Epoch: [171]  [  0/195]  eta: 0:06:07  lr: 0.002228  loss: 0.3829 (0.3829)  time: 1.8858  data: 1.3947  max mem: 9341
[18:17:33.580419] Epoch: [171]  [ 20/195]  eta: 0:01:40  lr: 0.002228  loss: 0.4082 (0.4168)  time: 0.5111  data: 0.0002  max mem: 9341
[18:17:43.793053] Epoch: [171]  [ 40/195]  eta: 0:01:24  lr: 0.002228  loss: 0.4199 (0.4183)  time: 0.5106  data: 0.0002  max mem: 9341
[18:17:54.007125] Epoch: [171]  [ 60/195]  eta: 0:01:11  lr: 0.002228  loss: 0.4173 (0.4181)  time: 0.5107  data: 0.0002  max mem: 9341
[18:18:04.268768] Epoch: [171]  [ 80/195]  eta: 0:01:00  lr: 0.002227  loss: 0.4227 (0.4182)  time: 0.5130  data: 0.0002  max mem: 9341
[18:18:14.485246] Epoch: [171]  [100/195]  eta: 0:00:49  lr: 0.002227  loss: 0.4244 (0.4202)  time: 0.5108  data: 0.0002  max mem: 9341
[18:18:24.705899] Epoch: [171]  [120/195]  eta: 0:00:39  lr: 0.002227  loss: 0.4350 (0.4228)  time: 0.5110  data: 0.0002  max mem: 9341
[18:18:34.923260] Epoch: [171]  [140/195]  eta: 0:00:28  lr: 0.002227  loss: 0.4166 (0.4227)  time: 0.5108  data: 0.0002  max mem: 9341
[18:18:45.185759] Epoch: [171]  [160/195]  eta: 0:00:18  lr: 0.002226  loss: 0.4209 (0.4229)  time: 0.5131  data: 0.0002  max mem: 9341
[18:18:55.366110] Epoch: [171]  [180/195]  eta: 0:00:07  lr: 0.002226  loss: 0.4156 (0.4221)  time: 0.5090  data: 0.0001  max mem: 9341
[18:19:02.494623] Epoch: [171]  [194/195]  eta: 0:00:00  lr: 0.002226  loss: 0.4271 (0.4221)  time: 0.5105  data: 0.0001  max mem: 9341
[18:19:02.664820] Epoch: [171] Total time: 0:01:41 (0.5189 s / it)
[18:19:02.681115] Averaged stats: lr: 0.002226  loss: 0.4271 (0.4215)
[18:19:07.381236] {"train_lr": 0.002227139405601116, "train_loss": 0.4214995243228399, "epoch": 171}
[18:19:07.381502] [18:19:07.381587] Training epoch 171 for 0:01:45
[18:19:07.381641] [18:19:07.386106] log_dir: ./exp/debug/cifar100-LT/debug
[18:19:09.071957] Epoch: [172]  [  0/195]  eta: 0:05:28  lr: 0.002226  loss: 0.4404 (0.4404)  time: 1.6845  data: 1.1765  max mem: 9341
[18:19:19.361255] Epoch: [172]  [ 20/195]  eta: 0:01:39  lr: 0.002226  loss: 0.4248 (0.4268)  time: 0.5144  data: 0.0002  max mem: 9341
[18:19:29.583883] Epoch: [172]  [ 40/195]  eta: 0:01:23  lr: 0.002225  loss: 0.4197 (0.4232)  time: 0.5111  data: 0.0002  max mem: 9341
[18:19:39.818779] Epoch: [172]  [ 60/195]  eta: 0:01:11  lr: 0.002225  loss: 0.4285 (0.4241)  time: 0.5117  data: 0.0002  max mem: 9341
[18:19:50.089890] Epoch: [172]  [ 80/195]  eta: 0:01:00  lr: 0.002225  loss: 0.4303 (0.4269)  time: 0.5135  data: 0.0003  max mem: 9341
[18:20:00.338883] Epoch: [172]  [100/195]  eta: 0:00:49  lr: 0.002224  loss: 0.4234 (0.4264)  time: 0.5124  data: 0.0002  max mem: 9341
[18:20:10.585262] Epoch: [172]  [120/195]  eta: 0:00:39  lr: 0.002224  loss: 0.4221 (0.4254)  time: 0.5123  data: 0.0002  max mem: 9341
[18:20:20.828184] Epoch: [172]  [140/195]  eta: 0:00:28  lr: 0.002224  loss: 0.4383 (0.4268)  time: 0.5121  data: 0.0002  max mem: 9341
[18:20:31.088642] Epoch: [172]  [160/195]  eta: 0:00:18  lr: 0.002224  loss: 0.4245 (0.4269)  time: 0.5130  data: 0.0002  max mem: 9341
[18:20:41.266224] Epoch: [172]  [180/195]  eta: 0:00:07  lr: 0.002223  loss: 0.4260 (0.4271)  time: 0.5088  data: 0.0001  max mem: 9341
[18:20:48.404830] Epoch: [172]  [194/195]  eta: 0:00:00  lr: 0.002223  loss: 0.4202 (0.4271)  time: 0.5109  data: 0.0001  max mem: 9341
[18:20:48.586464] Epoch: [172] Total time: 0:01:41 (0.5190 s / it)
[18:20:48.593190] Averaged stats: lr: 0.002223  loss: 0.4202 (0.4221)
[18:20:53.194820] {"train_lr": 0.0022245658178012533, "train_loss": 0.42209373811880746, "epoch": 172}
[18:20:53.195094] [18:20:53.195198] Training epoch 172 for 0:01:45
[18:20:53.195253] [18:20:53.199722] log_dir: ./exp/debug/cifar100-LT/debug
[18:20:54.896978] Epoch: [173]  [  0/195]  eta: 0:05:30  lr: 0.002223  loss: 0.3844 (0.3844)  time: 1.6964  data: 1.1862  max mem: 9341
[18:21:05.119718] Epoch: [173]  [ 20/195]  eta: 0:01:39  lr: 0.002223  loss: 0.4293 (0.4267)  time: 0.5111  data: 0.0002  max mem: 9341
[18:21:15.342012] Epoch: [173]  [ 40/195]  eta: 0:01:23  lr: 0.002223  loss: 0.4364 (0.4298)  time: 0.5111  data: 0.0002  max mem: 9341
[18:21:25.553839] Epoch: [173]  [ 60/195]  eta: 0:01:11  lr: 0.002223  loss: 0.4126 (0.4262)  time: 0.5105  data: 0.0002  max mem: 9341
[18:21:35.815775] Epoch: [173]  [ 80/195]  eta: 0:01:00  lr: 0.002222  loss: 0.4203 (0.4258)  time: 0.5130  data: 0.0002  max mem: 9341
[18:21:46.028052] Epoch: [173]  [100/195]  eta: 0:00:49  lr: 0.002222  loss: 0.4147 (0.4242)  time: 0.5105  data: 0.0002  max mem: 9341
[18:21:56.243627] Epoch: [173]  [120/195]  eta: 0:00:39  lr: 0.002222  loss: 0.4207 (0.4244)  time: 0.5107  data: 0.0002  max mem: 9341
[18:22:06.468330] Epoch: [173]  [140/195]  eta: 0:00:28  lr: 0.002221  loss: 0.4184 (0.4235)  time: 0.5112  data: 0.0002  max mem: 9341
[18:22:16.732072] Epoch: [173]  [160/195]  eta: 0:00:18  lr: 0.002221  loss: 0.4258 (0.4243)  time: 0.5131  data: 0.0002  max mem: 9341
[18:22:26.902059] Epoch: [173]  [180/195]  eta: 0:00:07  lr: 0.002221  loss: 0.4305 (0.4254)  time: 0.5084  data: 0.0002  max mem: 9341
[18:22:34.030772] Epoch: [173]  [194/195]  eta: 0:00:00  lr: 0.002221  loss: 0.4185 (0.4249)  time: 0.5104  data: 0.0001  max mem: 9341
[18:22:34.211222] Epoch: [173] Total time: 0:01:41 (0.5180 s / it)
[18:22:34.215830] Averaged stats: lr: 0.002221  loss: 0.4185 (0.4244)
[18:22:38.903976] {"train_lr": 0.002221974722998449, "train_loss": 0.42436643491188686, "epoch": 173}
[18:22:38.904281] [18:22:38.904369] Training epoch 173 for 0:01:45
[18:22:38.904424] [18:22:38.908909] log_dir: ./exp/debug/cifar100-LT/debug
[18:22:40.703447] Epoch: [174]  [  0/195]  eta: 0:05:49  lr: 0.002221  loss: 0.4033 (0.4033)  time: 1.7937  data: 1.2870  max mem: 9341
[18:22:50.967525] Epoch: [174]  [ 20/195]  eta: 0:01:40  lr: 0.002220  loss: 0.4205 (0.4214)  time: 0.5131  data: 0.0003  max mem: 9341
[18:23:01.193786] Epoch: [174]  [ 40/195]  eta: 0:01:24  lr: 0.002220  loss: 0.4190 (0.4259)  time: 0.5112  data: 0.0002  max mem: 9341
[18:23:11.418556] Epoch: [174]  [ 60/195]  eta: 0:01:11  lr: 0.002220  loss: 0.4152 (0.4246)  time: 0.5112  data: 0.0002  max mem: 9341
[18:23:21.685656] Epoch: [174]  [ 80/195]  eta: 0:01:00  lr: 0.002219  loss: 0.4201 (0.4233)  time: 0.5133  data: 0.0003  max mem: 9341
[18:23:31.905451] Epoch: [174]  [100/195]  eta: 0:00:49  lr: 0.002219  loss: 0.4361 (0.4251)  time: 0.5109  data: 0.0002  max mem: 9341
[18:23:42.121280] Epoch: [174]  [120/195]  eta: 0:00:39  lr: 0.002219  loss: 0.4152 (0.4239)  time: 0.5107  data: 0.0002  max mem: 9341
[18:23:52.333580] Epoch: [174]  [140/195]  eta: 0:00:28  lr: 0.002219  loss: 0.4174 (0.4243)  time: 0.5106  data: 0.0002  max mem: 9341
[18:24:02.589748] Epoch: [174]  [160/195]  eta: 0:00:18  lr: 0.002218  loss: 0.4227 (0.4248)  time: 0.5128  data: 0.0002  max mem: 9341
[18:24:12.759780] Epoch: [174]  [180/195]  eta: 0:00:07  lr: 0.002218  loss: 0.4264 (0.4253)  time: 0.5084  data: 0.0001  max mem: 9341
[18:24:19.896649] Epoch: [174]  [194/195]  eta: 0:00:00  lr: 0.002218  loss: 0.4228 (0.4249)  time: 0.5108  data: 0.0001  max mem: 9341
[18:24:20.081430] Epoch: [174] Total time: 0:01:41 (0.5188 s / it)
[18:24:20.082240] Averaged stats: lr: 0.002218  loss: 0.4228 (0.4233)
[18:24:24.830217] {"train_lr": 0.0022193661654673403, "train_loss": 0.42334585506946615, "epoch": 174}
[18:24:24.830488] [18:24:24.830590] Training epoch 174 for 0:01:45
[18:24:24.830643] [18:24:24.835078] log_dir: ./exp/debug/cifar100-LT/debug
[18:24:26.480371] Epoch: [175]  [  0/195]  eta: 0:05:20  lr: 0.002218  loss: 0.3812 (0.3812)  time: 1.6439  data: 1.1546  max mem: 9341
[18:24:36.701946] Epoch: [175]  [ 20/195]  eta: 0:01:38  lr: 0.002218  loss: 0.4068 (0.4147)  time: 0.5110  data: 0.0002  max mem: 9341
[18:24:46.940017] Epoch: [175]  [ 40/195]  eta: 0:01:23  lr: 0.002218  loss: 0.4276 (0.4232)  time: 0.5118  data: 0.0002  max mem: 9341
[18:24:57.181730] Epoch: [175]  [ 60/195]  eta: 0:01:11  lr: 0.002217  loss: 0.4273 (0.4240)  time: 0.5120  data: 0.0002  max mem: 9341
[18:25:07.490570] Epoch: [175]  [ 80/195]  eta: 0:01:00  lr: 0.002217  loss: 0.4238 (0.4240)  time: 0.5154  data: 0.0002  max mem: 9341
[18:25:17.723321] Epoch: [175]  [100/195]  eta: 0:00:49  lr: 0.002217  loss: 0.4193 (0.4227)  time: 0.5116  data: 0.0002  max mem: 9341
[18:25:27.955169] Epoch: [175]  [120/195]  eta: 0:00:39  lr: 0.002216  loss: 0.4229 (0.4229)  time: 0.5115  data: 0.0002  max mem: 9341
[18:25:38.198242] Epoch: [175]  [140/195]  eta: 0:00:28  lr: 0.002216  loss: 0.4207 (0.4226)  time: 0.5121  data: 0.0002  max mem: 9341
[18:25:48.498037] Epoch: [175]  [160/195]  eta: 0:00:18  lr: 0.002216  loss: 0.4137 (0.4216)  time: 0.5149  data: 0.0002  max mem: 9341
[18:25:58.695780] Epoch: [175]  [180/195]  eta: 0:00:07  lr: 0.002216  loss: 0.4173 (0.4220)  time: 0.5098  data: 0.0001  max mem: 9341
[18:26:05.849514] Epoch: [175]  [194/195]  eta: 0:00:00  lr: 0.002215  loss: 0.4292 (0.4223)  time: 0.5127  data: 0.0001  max mem: 9341
[18:26:06.012902] Epoch: [175] Total time: 0:01:41 (0.5189 s / it)
[18:26:06.030761] Averaged stats: lr: 0.002215  loss: 0.4292 (0.4246)
[18:26:10.730735] {"train_lr": 0.002216740189780999, "train_loss": 0.42458705325157214, "epoch": 175}
[18:26:10.731098] [18:26:10.731187] Training epoch 175 for 0:01:45
[18:26:10.731240] [18:26:10.736187] log_dir: ./exp/debug/cifar100-LT/debug
[18:26:12.343991] Epoch: [176]  [  0/195]  eta: 0:05:13  lr: 0.002215  loss: 0.4043 (0.4043)  time: 1.6064  data: 1.0906  max mem: 9341
[18:26:22.564415] Epoch: [176]  [ 20/195]  eta: 0:01:38  lr: 0.002215  loss: 0.4152 (0.4151)  time: 0.5110  data: 0.0002  max mem: 9341
[18:26:32.782009] Epoch: [176]  [ 40/195]  eta: 0:01:23  lr: 0.002215  loss: 0.4179 (0.4145)  time: 0.5108  data: 0.0002  max mem: 9341
[18:26:42.996966] Epoch: [176]  [ 60/195]  eta: 0:01:11  lr: 0.002215  loss: 0.4245 (0.4185)  time: 0.5107  data: 0.0002  max mem: 9341
[18:26:53.259432] Epoch: [176]  [ 80/195]  eta: 0:01:00  lr: 0.002214  loss: 0.4150 (0.4189)  time: 0.5131  data: 0.0002  max mem: 9341
[18:27:03.495895] Epoch: [176]  [100/195]  eta: 0:00:49  lr: 0.002214  loss: 0.4117 (0.4180)  time: 0.5118  data: 0.0002  max mem: 9341
[18:27:13.732474] Epoch: [176]  [120/195]  eta: 0:00:39  lr: 0.002214  loss: 0.4179 (0.4183)  time: 0.5118  data: 0.0002  max mem: 9341
[18:27:23.971260] Epoch: [176]  [140/195]  eta: 0:00:28  lr: 0.002214  loss: 0.4285 (0.4198)  time: 0.5119  data: 0.0002  max mem: 9341
[18:27:34.277259] Epoch: [176]  [160/195]  eta: 0:00:18  lr: 0.002213  loss: 0.4208 (0.4203)  time: 0.5152  data: 0.0002  max mem: 9341
[18:27:44.464904] Epoch: [176]  [180/195]  eta: 0:00:07  lr: 0.002213  loss: 0.4224 (0.4210)  time: 0.5093  data: 0.0001  max mem: 9341
[18:27:51.621137] Epoch: [176]  [194/195]  eta: 0:00:00  lr: 0.002213  loss: 0.4149 (0.4213)  time: 0.5126  data: 0.0001  max mem: 9341
[18:27:51.811185] Epoch: [176] Total time: 0:01:41 (0.5183 s / it)
[18:27:51.826511] Averaged stats: lr: 0.002213  loss: 0.4149 (0.4213)
[18:27:56.532884] {"train_lr": 0.0022140968408100965, "train_loss": 0.4212838189724164, "epoch": 176}
[18:27:56.533140] [18:27:56.533224] Training epoch 176 for 0:01:45
[18:27:56.533277] [18:27:56.537723] log_dir: ./exp/debug/cifar100-LT/debug
[18:27:58.386595] Epoch: [177]  [  0/195]  eta: 0:06:00  lr: 0.002213  loss: 0.4240 (0.4240)  time: 1.8478  data: 1.3401  max mem: 9341
[18:28:08.624529] Epoch: [177]  [ 20/195]  eta: 0:01:40  lr: 0.002212  loss: 0.4165 (0.4213)  time: 0.5118  data: 0.0002  max mem: 9341
[18:28:18.850482] Epoch: [177]  [ 40/195]  eta: 0:01:24  lr: 0.002212  loss: 0.4174 (0.4193)  time: 0.5112  data: 0.0002  max mem: 9341
[18:28:29.069289] Epoch: [177]  [ 60/195]  eta: 0:01:11  lr: 0.002212  loss: 0.4004 (0.4167)  time: 0.5109  data: 0.0002  max mem: 9341
[18:28:39.339950] Epoch: [177]  [ 80/195]  eta: 0:01:00  lr: 0.002212  loss: 0.4199 (0.4180)  time: 0.5135  data: 0.0002  max mem: 9341
[18:28:49.552592] Epoch: [177]  [100/195]  eta: 0:00:49  lr: 0.002211  loss: 0.4269 (0.4193)  time: 0.5106  data: 0.0002  max mem: 9341
[18:28:59.762770] Epoch: [177]  [120/195]  eta: 0:00:39  lr: 0.002211  loss: 0.4128 (0.4191)  time: 0.5105  data: 0.0002  max mem: 9341
[18:29:09.981892] Epoch: [177]  [140/195]  eta: 0:00:28  lr: 0.002211  loss: 0.4122 (0.4189)  time: 0.5109  data: 0.0002  max mem: 9341
[18:29:20.241917] Epoch: [177]  [160/195]  eta: 0:00:18  lr: 0.002210  loss: 0.3998 (0.4170)  time: 0.5129  data: 0.0002  max mem: 9341
[18:29:30.415582] Epoch: [177]  [180/195]  eta: 0:00:07  lr: 0.002210  loss: 0.4122 (0.4167)  time: 0.5086  data: 0.0001  max mem: 9341
[18:29:37.548316] Epoch: [177]  [194/195]  eta: 0:00:00  lr: 0.002210  loss: 0.4061 (0.4157)  time: 0.5108  data: 0.0001  max mem: 9341
[18:29:37.710010] Epoch: [177] Total time: 0:01:41 (0.5188 s / it)
[18:29:37.720379] Averaged stats: lr: 0.002210  loss: 0.4061 (0.4174)
[18:29:42.473295] {"train_lr": 0.0022114361637221667, "train_loss": 0.41739067213657577, "epoch": 177}
[18:29:42.481277] [18:29:42.481386] Training epoch 177 for 0:01:45
[18:29:42.481443] [18:29:42.486460] log_dir: ./exp/debug/cifar100-LT/debug
[18:29:44.271428] Epoch: [178]  [  0/195]  eta: 0:05:47  lr: 0.002210  loss: 0.3778 (0.3778)  time: 1.7839  data: 1.2874  max mem: 9341
[18:29:54.491893] Epoch: [178]  [ 20/195]  eta: 0:01:40  lr: 0.002210  loss: 0.4142 (0.4150)  time: 0.5109  data: 0.0002  max mem: 9341
[18:30:04.707612] Epoch: [178]  [ 40/195]  eta: 0:01:23  lr: 0.002210  loss: 0.4115 (0.4148)  time: 0.5107  data: 0.0002  max mem: 9341
[18:30:14.923176] Epoch: [178]  [ 60/195]  eta: 0:01:11  lr: 0.002209  loss: 0.4115 (0.4149)  time: 0.5107  data: 0.0002  max mem: 9341
[18:30:25.173105] Epoch: [178]  [ 80/195]  eta: 0:01:00  lr: 0.002209  loss: 0.4189 (0.4157)  time: 0.5124  data: 0.0002  max mem: 9341
[18:30:35.386164] Epoch: [178]  [100/195]  eta: 0:00:49  lr: 0.002209  loss: 0.4049 (0.4159)  time: 0.5106  data: 0.0002  max mem: 9341
[18:30:45.593602] Epoch: [178]  [120/195]  eta: 0:00:39  lr: 0.002208  loss: 0.4250 (0.4173)  time: 0.5103  data: 0.0002  max mem: 9341
[18:30:55.812859] Epoch: [178]  [140/195]  eta: 0:00:28  lr: 0.002208  loss: 0.4267 (0.4186)  time: 0.5109  data: 0.0002  max mem: 9341
[18:31:06.071689] Epoch: [178]  [160/195]  eta: 0:00:18  lr: 0.002208  loss: 0.4189 (0.4188)  time: 0.5129  data: 0.0002  max mem: 9341
[18:31:16.256871] Epoch: [178]  [180/195]  eta: 0:00:07  lr: 0.002208  loss: 0.4238 (0.4195)  time: 0.5092  data: 0.0001  max mem: 9341
[18:31:23.386637] Epoch: [178]  [194/195]  eta: 0:00:00  lr: 0.002207  loss: 0.4175 (0.4195)  time: 0.5108  data: 0.0001  max mem: 9341
[18:31:23.551161] Epoch: [178] Total time: 0:01:41 (0.5183 s / it)
[18:31:23.565434] Averaged stats: lr: 0.002207  loss: 0.4175 (0.4192)
[18:31:28.308228] {"train_lr": 0.00220875820398085, "train_loss": 0.4191569835711748, "epoch": 178}
[18:31:28.308490] [18:31:28.308573] Training epoch 178 for 0:01:45
[18:31:28.308627] [18:31:28.313158] log_dir: ./exp/debug/cifar100-LT/debug
[18:31:30.006339] Epoch: [179]  [  0/195]  eta: 0:05:29  lr: 0.002207  loss: 0.4650 (0.4650)  time: 1.6920  data: 1.1904  max mem: 9341
[18:31:40.224292] Epoch: [179]  [ 20/195]  eta: 0:01:39  lr: 0.002207  loss: 0.4208 (0.4268)  time: 0.5108  data: 0.0002  max mem: 9341
[18:31:50.445713] Epoch: [179]  [ 40/195]  eta: 0:01:23  lr: 0.002207  loss: 0.4189 (0.4260)  time: 0.5110  data: 0.0002  max mem: 9341
[18:32:00.662071] Epoch: [179]  [ 60/195]  eta: 0:01:11  lr: 0.002207  loss: 0.4155 (0.4226)  time: 0.5108  data: 0.0002  max mem: 9341
[18:32:10.917551] Epoch: [179]  [ 80/195]  eta: 0:01:00  lr: 0.002206  loss: 0.4147 (0.4213)  time: 0.5127  data: 0.0002  max mem: 9341
[18:32:21.132288] Epoch: [179]  [100/195]  eta: 0:00:49  lr: 0.002206  loss: 0.4232 (0.4219)  time: 0.5107  data: 0.0002  max mem: 9341
[18:32:31.350843] Epoch: [179]  [120/195]  eta: 0:00:39  lr: 0.002206  loss: 0.4163 (0.4205)  time: 0.5109  data: 0.0002  max mem: 9341
[18:32:41.571111] Epoch: [179]  [140/195]  eta: 0:00:28  lr: 0.002206  loss: 0.4066 (0.4196)  time: 0.5110  data: 0.0002  max mem: 9341
[18:32:51.831769] Epoch: [179]  [160/195]  eta: 0:00:18  lr: 0.002205  loss: 0.4206 (0.4193)  time: 0.5130  data: 0.0002  max mem: 9341
[18:33:02.005210] Epoch: [179]  [180/195]  eta: 0:00:07  lr: 0.002205  loss: 0.4102 (0.4186)  time: 0.5086  data: 0.0001  max mem: 9341
[18:33:09.137545] Epoch: [179]  [194/195]  eta: 0:00:00  lr: 0.002205  loss: 0.4207 (0.4190)  time: 0.5106  data: 0.0001  max mem: 9341
[18:33:09.320935] Epoch: [179] Total time: 0:01:41 (0.5180 s / it)
[18:33:09.325427] Averaged stats: lr: 0.002205  loss: 0.4207 (0.4198)
[18:33:13.955665] {"train_lr": 0.002206063007345089, "train_loss": 0.41983566123705646, "epoch": 179}
[18:33:13.956022] [18:33:13.956124] Training epoch 179 for 0:01:45
[18:33:13.956180] [18:33:13.960675] log_dir: ./exp/debug/cifar100-LT/debug
[18:33:15.680245] Epoch: [180]  [  0/195]  eta: 0:05:34  lr: 0.002205  loss: 0.4653 (0.4653)  time: 1.7173  data: 1.2113  max mem: 9341
[18:33:25.896920] Epoch: [180]  [ 20/195]  eta: 0:01:39  lr: 0.002204  loss: 0.4329 (0.4286)  time: 0.5108  data: 0.0002  max mem: 9341
[18:33:36.113531] Epoch: [180]  [ 40/195]  eta: 0:01:23  lr: 0.002204  loss: 0.4351 (0.4309)  time: 0.5108  data: 0.0002  max mem: 9341
[18:33:46.328673] Epoch: [180]  [ 60/195]  eta: 0:01:11  lr: 0.002204  loss: 0.4108 (0.4261)  time: 0.5107  data: 0.0002  max mem: 9341
[18:33:56.583992] Epoch: [180]  [ 80/195]  eta: 0:01:00  lr: 0.002203  loss: 0.4228 (0.4258)  time: 0.5127  data: 0.0002  max mem: 9341
[18:34:06.793901] Epoch: [180]  [100/195]  eta: 0:00:49  lr: 0.002203  loss: 0.4155 (0.4238)  time: 0.5104  data: 0.0002  max mem: 9341
[18:34:17.006943] Epoch: [180]  [120/195]  eta: 0:00:39  lr: 0.002203  loss: 0.4269 (0.4238)  time: 0.5106  data: 0.0002  max mem: 9341
[18:34:27.219869] Epoch: [180]  [140/195]  eta: 0:00:28  lr: 0.002203  loss: 0.4141 (0.4228)  time: 0.5106  data: 0.0002  max mem: 9341
[18:34:37.475399] Epoch: [180]  [160/195]  eta: 0:00:18  lr: 0.002202  loss: 0.4112 (0.4215)  time: 0.5127  data: 0.0002  max mem: 9341
[18:34:47.642808] Epoch: [180]  [180/195]  eta: 0:00:07  lr: 0.002202  loss: 0.4123 (0.4214)  time: 0.5083  data: 0.0002  max mem: 9341
[18:34:54.770753] Epoch: [180]  [194/195]  eta: 0:00:00  lr: 0.002202  loss: 0.4000 (0.4200)  time: 0.5103  data: 0.0001  max mem: 9341
[18:34:54.948127] Epoch: [180] Total time: 0:01:40 (0.5179 s / it)
[18:34:54.950676] Averaged stats: lr: 0.002202  loss: 0.4000 (0.4189)
[18:34:59.692995] {"train_lr": 0.0022033506198683487, "train_loss": 0.4189467143171873, "epoch": 180}
[18:34:59.693264] [18:34:59.693349] Training epoch 180 for 0:01:45
[18:34:59.693403] [18:34:59.697829] log_dir: ./exp/debug/cifar100-LT/debug
[18:35:01.429232] Epoch: [181]  [  0/195]  eta: 0:05:37  lr: 0.002202  loss: 0.4350 (0.4350)  time: 1.7301  data: 1.2176  max mem: 9341
[18:35:11.646379] Epoch: [181]  [ 20/195]  eta: 0:01:39  lr: 0.002202  loss: 0.4253 (0.4236)  time: 0.5108  data: 0.0002  max mem: 9341
[18:35:21.869760] Epoch: [181]  [ 40/195]  eta: 0:01:23  lr: 0.002201  loss: 0.4243 (0.4229)  time: 0.5111  data: 0.0002  max mem: 9341
[18:35:32.087620] Epoch: [181]  [ 60/195]  eta: 0:01:11  lr: 0.002201  loss: 0.4166 (0.4228)  time: 0.5108  data: 0.0002  max mem: 9341
[18:35:42.362000] Epoch: [181]  [ 80/195]  eta: 0:01:00  lr: 0.002201  loss: 0.4172 (0.4227)  time: 0.5137  data: 0.0002  max mem: 9341
[18:35:52.576940] Epoch: [181]  [100/195]  eta: 0:00:49  lr: 0.002201  loss: 0.4239 (0.4222)  time: 0.5107  data: 0.0002  max mem: 9341
[18:36:02.793870] Epoch: [181]  [120/195]  eta: 0:00:39  lr: 0.002200  loss: 0.4147 (0.4213)  time: 0.5108  data: 0.0002  max mem: 9341
[18:36:13.009058] Epoch: [181]  [140/195]  eta: 0:00:28  lr: 0.002200  loss: 0.4119 (0.4213)  time: 0.5107  data: 0.0002  max mem: 9341
[18:36:23.270200] Epoch: [181]  [160/195]  eta: 0:00:18  lr: 0.002200  loss: 0.4163 (0.4212)  time: 0.5130  data: 0.0002  max mem: 9341
[18:36:33.441961] Epoch: [181]  [180/195]  eta: 0:00:07  lr: 0.002199  loss: 0.4134 (0.4205)  time: 0.5085  data: 0.0002  max mem: 9341
[18:36:40.575257] Epoch: [181]  [194/195]  eta: 0:00:00  lr: 0.002199  loss: 0.4354 (0.4217)  time: 0.5108  data: 0.0001  max mem: 9341
[18:36:40.734524] Epoch: [181] Total time: 0:01:41 (0.5181 s / it)
[18:36:40.745930] Averaged stats: lr: 0.002199  loss: 0.4354 (0.4210)
[18:36:45.441105] {"train_lr": 0.002200621087897854, "train_loss": 0.4209839151455806, "epoch": 181}
[18:36:45.441366] [18:36:45.441451] Training epoch 181 for 0:01:45
[18:36:45.441505] [18:36:45.446015] log_dir: ./exp/debug/cifar100-LT/debug
[18:36:47.154236] Epoch: [182]  [  0/195]  eta: 0:05:32  lr: 0.002199  loss: 0.3949 (0.3949)  time: 1.7069  data: 1.1972  max mem: 9341
[18:36:57.395634] Epoch: [182]  [ 20/195]  eta: 0:01:39  lr: 0.002199  loss: 0.4185 (0.4213)  time: 0.5119  data: 0.0002  max mem: 9341
[18:37:07.635634] Epoch: [182]  [ 40/195]  eta: 0:01:23  lr: 0.002199  loss: 0.4319 (0.4217)  time: 0.5119  data: 0.0002  max mem: 9341
[18:37:17.856117] Epoch: [182]  [ 60/195]  eta: 0:01:11  lr: 0.002198  loss: 0.4173 (0.4215)  time: 0.5110  data: 0.0002  max mem: 9341
[18:37:28.112118] Epoch: [182]  [ 80/195]  eta: 0:01:00  lr: 0.002198  loss: 0.4166 (0.4214)  time: 0.5127  data: 0.0002  max mem: 9341
[18:37:38.324324] Epoch: [182]  [100/195]  eta: 0:00:49  lr: 0.002198  loss: 0.4261 (0.4222)  time: 0.5106  data: 0.0002  max mem: 9341
[18:37:48.533725] Epoch: [182]  [120/195]  eta: 0:00:39  lr: 0.002198  loss: 0.4215 (0.4213)  time: 0.5104  data: 0.0002  max mem: 9341
[18:37:58.768435] Epoch: [182]  [140/195]  eta: 0:00:28  lr: 0.002197  loss: 0.4239 (0.4208)  time: 0.5117  data: 0.0002  max mem: 9341
[18:38:09.031346] Epoch: [182]  [160/195]  eta: 0:00:18  lr: 0.002197  loss: 0.4138 (0.4205)  time: 0.5131  data: 0.0002  max mem: 9341
[18:38:19.209050] Epoch: [182]  [180/195]  eta: 0:00:07  lr: 0.002197  loss: 0.4146 (0.4194)  time: 0.5088  data: 0.0001  max mem: 9341
[18:38:26.342830] Epoch: [182]  [194/195]  eta: 0:00:00  lr: 0.002196  loss: 0.4318 (0.4200)  time: 0.5107  data: 0.0001  max mem: 9341
[18:38:26.517298] Epoch: [182] Total time: 0:01:41 (0.5183 s / it)
[18:38:26.525188] Averaged stats: lr: 0.002196  loss: 0.4318 (0.4217)
[18:38:31.295033] {"train_lr": 0.002197874458073776, "train_loss": 0.4216588425712708, "epoch": 182}
[18:38:31.295301] [18:38:31.295387] Training epoch 182 for 0:01:45
[18:38:31.295440] [18:38:31.299956] log_dir: ./exp/debug/cifar100-LT/debug
[18:38:32.909524] Epoch: [183]  [  0/195]  eta: 0:05:13  lr: 0.002196  loss: 0.4079 (0.4079)  time: 1.6086  data: 1.1180  max mem: 9341
[18:38:43.129585] Epoch: [183]  [ 20/195]  eta: 0:01:38  lr: 0.002196  loss: 0.4140 (0.4229)  time: 0.5109  data: 0.0002  max mem: 9341
[18:38:53.349334] Epoch: [183]  [ 40/195]  eta: 0:01:23  lr: 0.002196  loss: 0.4176 (0.4207)  time: 0.5109  data: 0.0002  max mem: 9341
[18:39:03.569606] Epoch: [183]  [ 60/195]  eta: 0:01:11  lr: 0.002196  loss: 0.4150 (0.4190)  time: 0.5110  data: 0.0002  max mem: 9341
[18:39:13.829246] Epoch: [183]  [ 80/195]  eta: 0:01:00  lr: 0.002195  loss: 0.4102 (0.4175)  time: 0.5129  data: 0.0002  max mem: 9341
[18:39:24.047896] Epoch: [183]  [100/195]  eta: 0:00:49  lr: 0.002195  loss: 0.4118 (0.4180)  time: 0.5109  data: 0.0002  max mem: 9341
[18:39:34.272231] Epoch: [183]  [120/195]  eta: 0:00:39  lr: 0.002195  loss: 0.4070 (0.4174)  time: 0.5112  data: 0.0002  max mem: 9341
[18:39:44.495670] Epoch: [183]  [140/195]  eta: 0:00:28  lr: 0.002195  loss: 0.4167 (0.4174)  time: 0.5111  data: 0.0002  max mem: 9341
[18:39:54.765241] Epoch: [183]  [160/195]  eta: 0:00:18  lr: 0.002194  loss: 0.4243 (0.4186)  time: 0.5134  data: 0.0002  max mem: 9341
[18:40:04.951724] Epoch: [183]  [180/195]  eta: 0:00:07  lr: 0.002194  loss: 0.4144 (0.4174)  time: 0.5093  data: 0.0001  max mem: 9341
[18:40:12.090936] Epoch: [183]  [194/195]  eta: 0:00:00  lr: 0.002194  loss: 0.4032 (0.4168)  time: 0.5112  data: 0.0001  max mem: 9341
[18:40:12.264910] Epoch: [183] Total time: 0:01:40 (0.5178 s / it)
[18:40:12.274893] Averaged stats: lr: 0.002194  loss: 0.4032 (0.4185)
[18:40:16.932242] {"train_lr": 0.0021951107773284413, "train_loss": 0.41845761300661627, "epoch": 183}
[18:40:16.932574] [18:40:16.932658] Training epoch 183 for 0:01:45
[18:40:16.932710] [18:40:16.937210] log_dir: ./exp/debug/cifar100-LT/debug
[18:40:18.624151] Epoch: [184]  [  0/195]  eta: 0:05:28  lr: 0.002194  loss: 0.3927 (0.3927)  time: 1.6856  data: 1.1800  max mem: 9341
[18:40:28.843792] Epoch: [184]  [ 20/195]  eta: 0:01:39  lr: 0.002193  loss: 0.4239 (0.4193)  time: 0.5109  data: 0.0002  max mem: 9341
[18:40:39.067062] Epoch: [184]  [ 40/195]  eta: 0:01:23  lr: 0.002193  loss: 0.4152 (0.4171)  time: 0.5111  data: 0.0002  max mem: 9341
[18:40:49.306359] Epoch: [184]  [ 60/195]  eta: 0:01:11  lr: 0.002193  loss: 0.4201 (0.4195)  time: 0.5119  data: 0.0002  max mem: 9341
[18:40:59.586865] Epoch: [184]  [ 80/195]  eta: 0:01:00  lr: 0.002192  loss: 0.4127 (0.4193)  time: 0.5140  data: 0.0002  max mem: 9341
[18:41:09.796954] Epoch: [184]  [100/195]  eta: 0:00:49  lr: 0.002192  loss: 0.4233 (0.4202)  time: 0.5104  data: 0.0002  max mem: 9341
[18:41:20.012776] Epoch: [184]  [120/195]  eta: 0:00:39  lr: 0.002192  loss: 0.4364 (0.4207)  time: 0.5107  data: 0.0002  max mem: 9341
[18:41:30.228009] Epoch: [184]  [140/195]  eta: 0:00:28  lr: 0.002192  loss: 0.4138 (0.4195)  time: 0.5107  data: 0.0002  max mem: 9341
[18:41:40.514240] Epoch: [184]  [160/195]  eta: 0:00:18  lr: 0.002191  loss: 0.4213 (0.4191)  time: 0.5143  data: 0.0002  max mem: 9341
[18:41:50.708343] Epoch: [184]  [180/195]  eta: 0:00:07  lr: 0.002191  loss: 0.4157 (0.4193)  time: 0.5097  data: 0.0001  max mem: 9341
[18:41:57.860276] Epoch: [184]  [194/195]  eta: 0:00:00  lr: 0.002191  loss: 0.4090 (0.4188)  time: 0.5125  data: 0.0001  max mem: 9341
[18:41:58.028198] Epoch: [184] Total time: 0:01:41 (0.5184 s / it)
[18:41:58.044772] Averaged stats: lr: 0.002191  loss: 0.4090 (0.4190)
[18:42:02.733706] {"train_lr": 0.002192330092885524, "train_loss": 0.41900455500835027, "epoch": 184}
[18:42:02.734029] [18:42:02.734111] Training epoch 184 for 0:01:45
[18:42:02.734165] [18:42:02.738604] log_dir: ./exp/debug/cifar100-LT/debug
[18:42:04.383783] Epoch: [185]  [  0/195]  eta: 0:05:20  lr: 0.002191  loss: 0.4129 (0.4129)  time: 1.6434  data: 1.1305  max mem: 9341
[18:42:14.604760] Epoch: [185]  [ 20/195]  eta: 0:01:38  lr: 0.002191  loss: 0.4233 (0.4201)  time: 0.5110  data: 0.0002  max mem: 9341
[18:42:24.826350] Epoch: [185]  [ 40/195]  eta: 0:01:23  lr: 0.002190  loss: 0.4291 (0.4198)  time: 0.5110  data: 0.0002  max mem: 9341
[18:42:35.042254] Epoch: [185]  [ 60/195]  eta: 0:01:11  lr: 0.002190  loss: 0.4172 (0.4198)  time: 0.5107  data: 0.0002  max mem: 9341
[18:42:45.299579] Epoch: [185]  [ 80/195]  eta: 0:01:00  lr: 0.002190  loss: 0.4184 (0.4190)  time: 0.5128  data: 0.0002  max mem: 9341
[18:42:55.506545] Epoch: [185]  [100/195]  eta: 0:00:49  lr: 0.002189  loss: 0.4102 (0.4180)  time: 0.5103  data: 0.0002  max mem: 9341
[18:43:05.719516] Epoch: [185]  [120/195]  eta: 0:00:39  lr: 0.002189  loss: 0.4183 (0.4179)  time: 0.5106  data: 0.0002  max mem: 9341
[18:43:15.940836] Epoch: [185]  [140/195]  eta: 0:00:28  lr: 0.002189  loss: 0.4181 (0.4186)  time: 0.5110  data: 0.0002  max mem: 9341
[18:43:26.199790] Epoch: [185]  [160/195]  eta: 0:00:18  lr: 0.002189  loss: 0.4098 (0.4177)  time: 0.5129  data: 0.0002  max mem: 9341
[18:43:36.372117] Epoch: [185]  [180/195]  eta: 0:00:07  lr: 0.002188  loss: 0.4135 (0.4169)  time: 0.5086  data: 0.0001  max mem: 9341
[18:43:43.502018] Epoch: [185]  [194/195]  eta: 0:00:00  lr: 0.002188  loss: 0.4110 (0.4166)  time: 0.5105  data: 0.0001  max mem: 9341
[18:43:43.673895] Epoch: [185] Total time: 0:01:40 (0.5176 s / it)
[18:43:43.696574] Averaged stats: lr: 0.002188  loss: 0.4110 (0.4181)
[18:43:48.566502] {"train_lr": 0.0021895324522592497, "train_loss": 0.4180929849545161, "epoch": 185}
[18:43:48.566848] [18:43:48.566930] Training epoch 185 for 0:01:45
[18:43:48.566984] [18:43:48.571487] log_dir: ./exp/debug/cifar100-LT/debug
[18:43:50.426748] Epoch: [186]  [  0/195]  eta: 0:06:01  lr: 0.002188  loss: 0.3698 (0.3698)  time: 1.8542  data: 1.3537  max mem: 9341
[18:44:00.645441] Epoch: [186]  [ 20/195]  eta: 0:01:40  lr: 0.002188  loss: 0.4303 (0.4270)  time: 0.5109  data: 0.0002  max mem: 9341
[18:44:10.860656] Epoch: [186]  [ 40/195]  eta: 0:01:24  lr: 0.002188  loss: 0.4068 (0.4187)  time: 0.5107  data: 0.0002  max mem: 9341
[18:44:21.075758] Epoch: [186]  [ 60/195]  eta: 0:01:11  lr: 0.002187  loss: 0.4214 (0.4214)  time: 0.5107  data: 0.0002  max mem: 9341
[18:44:31.343777] Epoch: [186]  [ 80/195]  eta: 0:01:00  lr: 0.002187  loss: 0.4182 (0.4193)  time: 0.5133  data: 0.0002  max mem: 9341
[18:44:41.553534] Epoch: [186]  [100/195]  eta: 0:00:49  lr: 0.002187  loss: 0.4132 (0.4187)  time: 0.5104  data: 0.0002  max mem: 9341
[18:44:51.774067] Epoch: [186]  [120/195]  eta: 0:00:39  lr: 0.002186  loss: 0.4202 (0.4186)  time: 0.5110  data: 0.0002  max mem: 9341
[18:45:01.990726] Epoch: [186]  [140/195]  eta: 0:00:28  lr: 0.002186  loss: 0.4206 (0.4181)  time: 0.5108  data: 0.0002  max mem: 9341
[18:45:12.246899] Epoch: [186]  [160/195]  eta: 0:00:18  lr: 0.002186  loss: 0.4230 (0.4183)  time: 0.5128  data: 0.0002  max mem: 9341
[18:45:22.422571] Epoch: [186]  [180/195]  eta: 0:00:07  lr: 0.002185  loss: 0.4210 (0.4182)  time: 0.5087  data: 0.0001  max mem: 9341
[18:45:29.553734] Epoch: [186]  [194/195]  eta: 0:00:00  lr: 0.002185  loss: 0.4215 (0.4182)  time: 0.5106  data: 0.0001  max mem: 9341
[18:45:29.717047] Epoch: [186] Total time: 0:01:41 (0.5187 s / it)
[18:45:29.738548] Averaged stats: lr: 0.002185  loss: 0.4215 (0.4175)
[18:45:34.537395] {"train_lr": 0.0021867179032535773, "train_loss": 0.41745449079152863, "epoch": 186}
[18:45:34.537862] [18:45:34.538016] Training epoch 186 for 0:01:45
[18:45:34.538110] [18:45:34.547330] log_dir: ./exp/debug/cifar100-LT/debug
[18:45:36.275453] Epoch: [187]  [  0/195]  eta: 0:05:36  lr: 0.002185  loss: 0.3955 (0.3955)  time: 1.7268  data: 1.2217  max mem: 9341
[18:45:46.490312] Epoch: [187]  [ 20/195]  eta: 0:01:39  lr: 0.002185  loss: 0.4095 (0.4093)  time: 0.5107  data: 0.0002  max mem: 9341
[18:45:56.713249] Epoch: [187]  [ 40/195]  eta: 0:01:23  lr: 0.002185  loss: 0.4180 (0.4119)  time: 0.5111  data: 0.0002  max mem: 9341
[18:46:06.933366] Epoch: [187]  [ 60/195]  eta: 0:01:11  lr: 0.002184  loss: 0.4140 (0.4128)  time: 0.5109  data: 0.0002  max mem: 9341
[18:46:17.195051] Epoch: [187]  [ 80/195]  eta: 0:01:00  lr: 0.002184  loss: 0.4135 (0.4132)  time: 0.5130  data: 0.0002  max mem: 9341
[18:46:27.408034] Epoch: [187]  [100/195]  eta: 0:00:49  lr: 0.002184  loss: 0.4043 (0.4125)  time: 0.5106  data: 0.0002  max mem: 9341
[18:46:37.620924] Epoch: [187]  [120/195]  eta: 0:00:39  lr: 0.002184  loss: 0.4070 (0.4113)  time: 0.5106  data: 0.0002  max mem: 9341
[18:46:47.833954] Epoch: [187]  [140/195]  eta: 0:00:28  lr: 0.002183  loss: 0.4123 (0.4109)  time: 0.5106  data: 0.0002  max mem: 9341
[18:46:58.083822] Epoch: [187]  [160/195]  eta: 0:00:18  lr: 0.002183  loss: 0.4197 (0.4123)  time: 0.5124  data: 0.0002  max mem: 9341
[18:47:08.259001] Epoch: [187]  [180/195]  eta: 0:00:07  lr: 0.002183  loss: 0.4208 (0.4126)  time: 0.5087  data: 0.0001  max mem: 9341
[18:47:15.386828] Epoch: [187]  [194/195]  eta: 0:00:00  lr: 0.002182  loss: 0.4051 (0.4119)  time: 0.5103  data: 0.0001  max mem: 9341
[18:47:15.545663] Epoch: [187] Total time: 0:01:40 (0.5179 s / it)
[18:47:15.577698] Averaged stats: lr: 0.002182  loss: 0.4051 (0.4137)
[18:47:20.365662] {"train_lr": 0.0021838864939613895, "train_loss": 0.4136671374623592, "epoch": 187}
[18:47:20.366019] [18:47:20.366103] Training epoch 187 for 0:01:45
[18:47:20.366156] [18:47:20.370637] log_dir: ./exp/debug/cifar100-LT/debug
[18:47:21.920403] Epoch: [188]  [  0/195]  eta: 0:05:01  lr: 0.002182  loss: 0.3989 (0.3989)  time: 1.5484  data: 1.0447  max mem: 9341
[18:47:32.138508] Epoch: [188]  [ 20/195]  eta: 0:01:38  lr: 0.002182  loss: 0.4175 (0.4178)  time: 0.5108  data: 0.0002  max mem: 9341
[18:47:42.350706] Epoch: [188]  [ 40/195]  eta: 0:01:23  lr: 0.002182  loss: 0.4172 (0.4164)  time: 0.5106  data: 0.0002  max mem: 9341
[18:47:52.567606] Epoch: [188]  [ 60/195]  eta: 0:01:11  lr: 0.002182  loss: 0.4063 (0.4141)  time: 0.5108  data: 0.0002  max mem: 9341
[18:48:02.823754] Epoch: [188]  [ 80/195]  eta: 0:01:00  lr: 0.002181  loss: 0.4226 (0.4158)  time: 0.5128  data: 0.0002  max mem: 9341
[18:48:13.040725] Epoch: [188]  [100/195]  eta: 0:00:49  lr: 0.002181  loss: 0.4111 (0.4156)  time: 0.5108  data: 0.0002  max mem: 9341
[18:48:23.255381] Epoch: [188]  [120/195]  eta: 0:00:38  lr: 0.002181  loss: 0.4169 (0.4155)  time: 0.5107  data: 0.0002  max mem: 9341
[18:48:33.464204] Epoch: [188]  [140/195]  eta: 0:00:28  lr: 0.002180  loss: 0.4127 (0.4154)  time: 0.5104  data: 0.0002  max mem: 9341
[18:48:43.720428] Epoch: [188]  [160/195]  eta: 0:00:18  lr: 0.002180  loss: 0.4107 (0.4147)  time: 0.5128  data: 0.0002  max mem: 9341
[18:48:53.897163] Epoch: [188]  [180/195]  eta: 0:00:07  lr: 0.002180  loss: 0.4120 (0.4146)  time: 0.5088  data: 0.0001  max mem: 9341
[18:49:01.026075] Epoch: [188]  [194/195]  eta: 0:00:00  lr: 0.002180  loss: 0.4078 (0.4141)  time: 0.5105  data: 0.0001  max mem: 9341
[18:49:01.190183] Epoch: [188] Total time: 0:01:40 (0.5170 s / it)
[18:49:01.207219] Averaged stats: lr: 0.002180  loss: 0.4078 (0.4167)
[18:49:05.879849] {"train_lr": 0.002181038272763656, "train_loss": 0.41665668349999646, "epoch": 188}
[18:49:05.880298] [18:49:05.880398] Training epoch 188 for 0:01:45
[18:49:05.880453] [18:49:05.885205] log_dir: ./exp/debug/cifar100-LT/debug
[18:49:07.776592] Epoch: [189]  [  0/195]  eta: 0:06:08  lr: 0.002179  loss: 0.4246 (0.4246)  time: 1.8895  data: 1.3758  max mem: 9341
[18:49:18.012236] Epoch: [189]  [ 20/195]  eta: 0:01:41  lr: 0.002179  loss: 0.4290 (0.4214)  time: 0.5117  data: 0.0002  max mem: 9341
[18:49:28.228052] Epoch: [189]  [ 40/195]  eta: 0:01:24  lr: 0.002179  loss: 0.4183 (0.4181)  time: 0.5107  data: 0.0002  max mem: 9341
[18:49:38.452072] Epoch: [189]  [ 60/195]  eta: 0:01:12  lr: 0.002179  loss: 0.4201 (0.4184)  time: 0.5111  data: 0.0002  max mem: 9341
[18:49:48.716418] Epoch: [189]  [ 80/195]  eta: 0:01:00  lr: 0.002178  loss: 0.4155 (0.4172)  time: 0.5132  data: 0.0002  max mem: 9341
[18:49:58.932360] Epoch: [189]  [100/195]  eta: 0:00:49  lr: 0.002178  loss: 0.4281 (0.4195)  time: 0.5107  data: 0.0002  max mem: 9341
[18:50:09.149347] Epoch: [189]  [120/195]  eta: 0:00:39  lr: 0.002178  loss: 0.4081 (0.4183)  time: 0.5108  data: 0.0002  max mem: 9341
[18:50:19.363084] Epoch: [189]  [140/195]  eta: 0:00:28  lr: 0.002178  loss: 0.4121 (0.4179)  time: 0.5106  data: 0.0002  max mem: 9341
[18:50:29.622544] Epoch: [189]  [160/195]  eta: 0:00:18  lr: 0.002177  loss: 0.4156 (0.4179)  time: 0.5129  data: 0.0002  max mem: 9341
[18:50:39.791439] Epoch: [189]  [180/195]  eta: 0:00:07  lr: 0.002177  loss: 0.4196 (0.4176)  time: 0.5084  data: 0.0001  max mem: 9341
[18:50:46.925370] Epoch: [189]  [194/195]  eta: 0:00:00  lr: 0.002177  loss: 0.4174 (0.4173)  time: 0.5107  data: 0.0001  max mem: 9341
[18:50:47.085297] Epoch: [189] Total time: 0:01:41 (0.5190 s / it)
[18:50:47.100569] Averaged stats: lr: 0.002177  loss: 0.4174 (0.4198)
[18:50:51.838203] {"train_lr": 0.0021781732883286134, "train_loss": 0.4198283626100956, "epoch": 189}
[18:50:51.838466] [18:50:51.838548] Training epoch 189 for 0:01:45
[18:50:51.838601] [18:50:51.843112] log_dir: ./exp/debug/cifar100-LT/debug
[18:50:53.575214] Epoch: [190]  [  0/195]  eta: 0:05:37  lr: 0.002177  loss: 0.4435 (0.4435)  time: 1.7313  data: 1.2327  max mem: 9341
[18:51:03.805806] Epoch: [190]  [ 20/195]  eta: 0:01:39  lr: 0.002176  loss: 0.4114 (0.4145)  time: 0.5115  data: 0.0002  max mem: 9341
[18:51:14.029278] Epoch: [190]  [ 40/195]  eta: 0:01:23  lr: 0.002176  loss: 0.4244 (0.4169)  time: 0.5111  data: 0.0002  max mem: 9341
[18:51:24.248701] Epoch: [190]  [ 60/195]  eta: 0:01:11  lr: 0.002176  loss: 0.4127 (0.4159)  time: 0.5109  data: 0.0002  max mem: 9341
[18:51:34.514696] Epoch: [190]  [ 80/195]  eta: 0:01:00  lr: 0.002175  loss: 0.4168 (0.4165)  time: 0.5132  data: 0.0002  max mem: 9341
[18:51:44.729287] Epoch: [190]  [100/195]  eta: 0:00:49  lr: 0.002175  loss: 0.4162 (0.4154)  time: 0.5107  data: 0.0002  max mem: 9341
[18:51:54.940657] Epoch: [190]  [120/195]  eta: 0:00:39  lr: 0.002175  loss: 0.4076 (0.4146)  time: 0.5105  data: 0.0002  max mem: 9341
[18:52:05.157019] Epoch: [190]  [140/195]  eta: 0:00:28  lr: 0.002175  loss: 0.4052 (0.4138)  time: 0.5108  data: 0.0002  max mem: 9341
[18:52:15.421052] Epoch: [190]  [160/195]  eta: 0:00:18  lr: 0.002174  loss: 0.4264 (0.4151)  time: 0.5131  data: 0.0002  max mem: 9341
[18:52:25.594765] Epoch: [190]  [180/195]  eta: 0:00:07  lr: 0.002174  loss: 0.4268 (0.4158)  time: 0.5086  data: 0.0001  max mem: 9341
[18:52:32.722923] Epoch: [190]  [194/195]  eta: 0:00:00  lr: 0.002174  loss: 0.4203 (0.4160)  time: 0.5105  data: 0.0001  max mem: 9341
[18:52:32.884648] Epoch: [190] Total time: 0:01:41 (0.5182 s / it)
[18:52:32.899845] Averaged stats: lr: 0.002174  loss: 0.4203 (0.4159)
[18:52:37.677375] {"train_lr": 0.0021752915896109514, "train_loss": 0.4158946923720531, "epoch": 190}
[18:52:37.677735] [18:52:37.677819] Training epoch 190 for 0:01:45
[18:52:37.677873] [18:52:37.682309] log_dir: ./exp/debug/cifar100-LT/debug
[18:52:39.341948] Epoch: [191]  [  0/195]  eta: 0:05:23  lr: 0.002174  loss: 0.4388 (0.4388)  time: 1.6580  data: 1.1578  max mem: 9341
[18:52:49.564393] Epoch: [191]  [ 20/195]  eta: 0:01:38  lr: 0.002173  loss: 0.4056 (0.4123)  time: 0.5111  data: 0.0002  max mem: 9341
[18:52:59.780954] Epoch: [191]  [ 40/195]  eta: 0:01:23  lr: 0.002173  loss: 0.4111 (0.4133)  time: 0.5108  data: 0.0002  max mem: 9341
[18:53:09.998486] Epoch: [191]  [ 60/195]  eta: 0:01:11  lr: 0.002173  loss: 0.4100 (0.4112)  time: 0.5108  data: 0.0002  max mem: 9341
[18:53:20.258712] Epoch: [191]  [ 80/195]  eta: 0:01:00  lr: 0.002173  loss: 0.4093 (0.4126)  time: 0.5129  data: 0.0002  max mem: 9341
[18:53:30.468124] Epoch: [191]  [100/195]  eta: 0:00:49  lr: 0.002172  loss: 0.3982 (0.4097)  time: 0.5104  data: 0.0002  max mem: 9341
[18:53:40.679321] Epoch: [191]  [120/195]  eta: 0:00:39  lr: 0.002172  loss: 0.4238 (0.4115)  time: 0.5105  data: 0.0002  max mem: 9341
[18:53:50.893479] Epoch: [191]  [140/195]  eta: 0:00:28  lr: 0.002172  loss: 0.3980 (0.4102)  time: 0.5107  data: 0.0002  max mem: 9341
[18:54:01.150396] Epoch: [191]  [160/195]  eta: 0:00:18  lr: 0.002171  loss: 0.4124 (0.4105)  time: 0.5128  data: 0.0002  max mem: 9341
[18:54:11.320274] Epoch: [191]  [180/195]  eta: 0:00:07  lr: 0.002171  loss: 0.4088 (0.4105)  time: 0.5084  data: 0.0002  max mem: 9341
[18:54:18.453678] Epoch: [191]  [194/195]  eta: 0:00:00  lr: 0.002171  loss: 0.4015 (0.4106)  time: 0.5106  data: 0.0001  max mem: 9341
[18:54:18.633204] Epoch: [191] Total time: 0:01:40 (0.5177 s / it)
[18:54:18.647051] Averaged stats: lr: 0.002171  loss: 0.4015 (0.4117)
[18:54:23.411922] {"train_lr": 0.00217239322585094, "train_loss": 0.4116969816959821, "epoch": 191}
[18:54:23.412210] [18:54:23.412304] Training epoch 191 for 0:01:45
[18:54:23.412356] [18:54:23.416829] log_dir: ./exp/debug/cifar100-LT/debug
[18:54:25.043542] Epoch: [192]  [  0/195]  eta: 0:05:16  lr: 0.002171  loss: 0.4543 (0.4543)  time: 1.6248  data: 1.1125  max mem: 9341
[18:54:35.267001] Epoch: [192]  [ 20/195]  eta: 0:01:38  lr: 0.002171  loss: 0.4251 (0.4173)  time: 0.5111  data: 0.0002  max mem: 9341
[18:54:45.481009] Epoch: [192]  [ 40/195]  eta: 0:01:23  lr: 0.002170  loss: 0.4240 (0.4229)  time: 0.5106  data: 0.0002  max mem: 9341
[18:54:55.691495] Epoch: [192]  [ 60/195]  eta: 0:01:11  lr: 0.002170  loss: 0.4337 (0.4272)  time: 0.5105  data: 0.0002  max mem: 9341
[18:55:05.947831] Epoch: [192]  [ 80/195]  eta: 0:01:00  lr: 0.002170  loss: 0.4401 (0.4280)  time: 0.5128  data: 0.0002  max mem: 9341
[18:55:16.160377] Epoch: [192]  [100/195]  eta: 0:00:49  lr: 0.002169  loss: 0.4203 (0.4259)  time: 0.5106  data: 0.0002  max mem: 9341
[18:55:26.366008] Epoch: [192]  [120/195]  eta: 0:00:39  lr: 0.002169  loss: 0.4316 (0.4268)  time: 0.5102  data: 0.0002  max mem: 9341
[18:55:36.572423] Epoch: [192]  [140/195]  eta: 0:00:28  lr: 0.002169  loss: 0.4288 (0.4270)  time: 0.5103  data: 0.0002  max mem: 9341
[18:55:46.831024] Epoch: [192]  [160/195]  eta: 0:00:18  lr: 0.002168  loss: 0.4255 (0.4266)  time: 0.5129  data: 0.0002  max mem: 9341
[18:55:56.996450] Epoch: [192]  [180/195]  eta: 0:00:07  lr: 0.002168  loss: 0.4310 (0.4266)  time: 0.5082  data: 0.0001  max mem: 9341
[18:56:04.125422] Epoch: [192]  [194/195]  eta: 0:00:00  lr: 0.002168  loss: 0.4223 (0.4266)  time: 0.5103  data: 0.0001  max mem: 9341
[18:56:04.286734] Epoch: [192] Total time: 0:01:40 (0.5173 s / it)
[18:56:04.293319] Averaged stats: lr: 0.002168  loss: 0.4223 (0.4289)
[18:56:09.034997] {"train_lr": 0.002169478246573622, "train_loss": 0.4288986704670466, "epoch": 192}
[18:56:09.035265] [18:56:09.035362] Training epoch 192 for 0:01:45
[18:56:09.035415] [18:56:09.039912] log_dir: ./exp/debug/cifar100-LT/debug
[18:56:10.858845] Epoch: [193]  [  0/195]  eta: 0:05:54  lr: 0.002168  loss: 0.4176 (0.4176)  time: 1.8180  data: 1.3209  max mem: 9341
[18:56:21.070157] Epoch: [193]  [ 20/195]  eta: 0:01:40  lr: 0.002168  loss: 0.4255 (0.4252)  time: 0.5105  data: 0.0002  max mem: 9341
[18:56:31.289206] Epoch: [193]  [ 40/195]  eta: 0:01:24  lr: 0.002167  loss: 0.4229 (0.4239)  time: 0.5109  data: 0.0002  max mem: 9341
[18:56:41.501708] Epoch: [193]  [ 60/195]  eta: 0:01:11  lr: 0.002167  loss: 0.4219 (0.4243)  time: 0.5106  data: 0.0002  max mem: 9341
[18:56:51.764022] Epoch: [193]  [ 80/195]  eta: 0:01:00  lr: 0.002167  loss: 0.4262 (0.4255)  time: 0.5130  data: 0.0002  max mem: 9341
[18:57:01.992643] Epoch: [193]  [100/195]  eta: 0:00:49  lr: 0.002166  loss: 0.4240 (0.4251)  time: 0.5114  data: 0.0002  max mem: 9341
[18:57:12.221578] Epoch: [193]  [120/195]  eta: 0:00:39  lr: 0.002166  loss: 0.4082 (0.4241)  time: 0.5114  data: 0.0002  max mem: 9341
[18:57:22.435726] Epoch: [193]  [140/195]  eta: 0:00:28  lr: 0.002166  loss: 0.4126 (0.4230)  time: 0.5106  data: 0.0002  max mem: 9341
[18:57:32.696150] Epoch: [193]  [160/195]  eta: 0:00:18  lr: 0.002165  loss: 0.4068 (0.4216)  time: 0.5129  data: 0.0002  max mem: 9341
[18:57:42.860541] Epoch: [193]  [180/195]  eta: 0:00:07  lr: 0.002165  loss: 0.4192 (0.4207)  time: 0.5082  data: 0.0002  max mem: 9341
[18:57:49.987421] Epoch: [193]  [194/195]  eta: 0:00:00  lr: 0.002165  loss: 0.4093 (0.4202)  time: 0.5102  data: 0.0001  max mem: 9341
[18:57:50.157354] Epoch: [193] Total time: 0:01:41 (0.5186 s / it)
[18:57:50.171668] Averaged stats: lr: 0.002165  loss: 0.4093 (0.4187)
[18:57:54.902936] {"train_lr": 0.002166546701587949, "train_loss": 0.4186701544966453, "epoch": 193}
[18:57:54.903279] [18:57:54.903365] Training epoch 193 for 0:01:45
[18:57:54.903419] [18:57:54.907883] log_dir: ./exp/debug/cifar100-LT/debug
[18:57:56.665972] Epoch: [194]  [  0/195]  eta: 0:05:42  lr: 0.002165  loss: 0.4225 (0.4225)  time: 1.7568  data: 1.2404  max mem: 9341
[18:58:06.881002] Epoch: [194]  [ 20/195]  eta: 0:01:39  lr: 0.002165  loss: 0.3988 (0.4013)  time: 0.5107  data: 0.0002  max mem: 9341
[18:58:17.093964] Epoch: [194]  [ 40/195]  eta: 0:01:23  lr: 0.002164  loss: 0.4183 (0.4073)  time: 0.5106  data: 0.0002  max mem: 9341
[18:58:27.307480] Epoch: [194]  [ 60/195]  eta: 0:01:11  lr: 0.002164  loss: 0.4130 (0.4100)  time: 0.5106  data: 0.0002  max mem: 9341
[18:58:37.576868] Epoch: [194]  [ 80/195]  eta: 0:01:00  lr: 0.002164  loss: 0.4044 (0.4095)  time: 0.5134  data: 0.0002  max mem: 9341
[18:58:47.794170] Epoch: [194]  [100/195]  eta: 0:00:49  lr: 0.002164  loss: 0.3948 (0.4083)  time: 0.5108  data: 0.0002  max mem: 9341
[18:58:58.015800] Epoch: [194]  [120/195]  eta: 0:00:39  lr: 0.002163  loss: 0.4041 (0.4091)  time: 0.5110  data: 0.0002  max mem: 9341
[18:59:08.237063] Epoch: [194]  [140/195]  eta: 0:00:28  lr: 0.002163  loss: 0.4139 (0.4098)  time: 0.5110  data: 0.0002  max mem: 9341
[18:59:18.504584] Epoch: [194]  [160/195]  eta: 0:00:18  lr: 0.002163  loss: 0.4031 (0.4094)  time: 0.5133  data: 0.0002  max mem: 9341
[18:59:28.687090] Epoch: [194]  [180/195]  eta: 0:00:07  lr: 0.002162  loss: 0.4208 (0.4107)  time: 0.5091  data: 0.0001  max mem: 9341
[18:59:35.820238] Epoch: [194]  [194/195]  eta: 0:00:00  lr: 0.002162  loss: 0.4121 (0.4115)  time: 0.5108  data: 0.0001  max mem: 9341
[18:59:35.972556] Epoch: [194] Total time: 0:01:41 (0.5183 s / it)
[18:59:35.993179] Averaged stats: lr: 0.002162  loss: 0.4121 (0.4122)
[18:59:40.788470] {"train_lr": 0.002163598640985952, "train_loss": 0.4122157014715366, "epoch": 194}
[18:59:40.788734] [18:59:40.788819] Training epoch 194 for 0:01:45
[18:59:40.788871] [18:59:40.793385] log_dir: ./exp/debug/cifar100-LT/debug
[18:59:42.639268] Epoch: [195]  [  0/195]  eta: 0:05:59  lr: 0.002162  loss: 0.4351 (0.4351)  time: 1.8451  data: 1.3458  max mem: 9341
[18:59:52.863052] Epoch: [195]  [ 20/195]  eta: 0:01:40  lr: 0.002162  loss: 0.4056 (0.4104)  time: 0.5111  data: 0.0002  max mem: 9341
[19:00:03.076847] Epoch: [195]  [ 40/195]  eta: 0:01:24  lr: 0.002162  loss: 0.4087 (0.4082)  time: 0.5106  data: 0.0002  max mem: 9341
[19:00:13.291074] Epoch: [195]  [ 60/195]  eta: 0:01:11  lr: 0.002161  loss: 0.4108 (0.4104)  time: 0.5107  data: 0.0002  max mem: 9341
[19:00:23.550066] Epoch: [195]  [ 80/195]  eta: 0:01:00  lr: 0.002161  loss: 0.4189 (0.4124)  time: 0.5129  data: 0.0002  max mem: 9341
[19:00:33.758487] Epoch: [195]  [100/195]  eta: 0:00:49  lr: 0.002161  loss: 0.4064 (0.4113)  time: 0.5104  data: 0.0002  max mem: 9341
[19:00:43.973968] Epoch: [195]  [120/195]  eta: 0:00:39  lr: 0.002160  loss: 0.4088 (0.4111)  time: 0.5107  data: 0.0002  max mem: 9341
[19:00:54.193469] Epoch: [195]  [140/195]  eta: 0:00:28  lr: 0.002160  loss: 0.4148 (0.4109)  time: 0.5109  data: 0.0002  max mem: 9341
[19:01:04.447509] Epoch: [195]  [160/195]  eta: 0:00:18  lr: 0.002160  loss: 0.4105 (0.4110)  time: 0.5126  data: 0.0002  max mem: 9341
[19:01:14.610590] Epoch: [195]  [180/195]  eta: 0:00:07  lr: 0.002159  loss: 0.4216 (0.4115)  time: 0.5081  data: 0.0001  max mem: 9341
[19:01:21.739669] Epoch: [195]  [194/195]  eta: 0:00:00  lr: 0.002159  loss: 0.4148 (0.4116)  time: 0.5103  data: 0.0001  max mem: 9341
[19:01:21.919383] Epoch: [195] Total time: 0:01:41 (0.5186 s / it)
[19:01:21.920221] Averaged stats: lr: 0.002159  loss: 0.4148 (0.4105)
[19:01:26.644877] {"train_lr": 0.002160634115141827, "train_loss": 0.41052521960093424, "epoch": 195}
[19:01:26.645139] [19:01:26.645224] Training epoch 195 for 0:01:45
[19:01:26.645277] [19:01:26.649713] log_dir: ./exp/debug/cifar100-LT/debug
[19:01:28.439633] Epoch: [196]  [  0/195]  eta: 0:05:48  lr: 0.002159  loss: 0.3908 (0.3908)  time: 1.7891  data: 1.2913  max mem: 9341
[19:01:38.654635] Epoch: [196]  [ 20/195]  eta: 0:01:40  lr: 0.002159  loss: 0.4118 (0.4130)  time: 0.5107  data: 0.0002  max mem: 9341
[19:01:48.868070] Epoch: [196]  [ 40/195]  eta: 0:01:23  lr: 0.002159  loss: 0.4078 (0.4118)  time: 0.5106  data: 0.0002  max mem: 9341
[19:01:59.083378] Epoch: [196]  [ 60/195]  eta: 0:01:11  lr: 0.002158  loss: 0.4110 (0.4122)  time: 0.5107  data: 0.0002  max mem: 9341
[19:02:09.338398] Epoch: [196]  [ 80/195]  eta: 0:01:00  lr: 0.002158  loss: 0.4021 (0.4106)  time: 0.5127  data: 0.0002  max mem: 9341
[19:02:19.548888] Epoch: [196]  [100/195]  eta: 0:00:49  lr: 0.002158  loss: 0.4050 (0.4105)  time: 0.5105  data: 0.0002  max mem: 9341
[19:02:29.774500] Epoch: [196]  [120/195]  eta: 0:00:39  lr: 0.002157  loss: 0.4059 (0.4106)  time: 0.5112  data: 0.0002  max mem: 9341
[19:02:39.990044] Epoch: [196]  [140/195]  eta: 0:00:28  lr: 0.002157  loss: 0.4104 (0.4107)  time: 0.5107  data: 0.0002  max mem: 9341
[19:02:50.248402] Epoch: [196]  [160/195]  eta: 0:00:18  lr: 0.002157  loss: 0.4187 (0.4114)  time: 0.5129  data: 0.0002  max mem: 9341
[19:03:00.416284] Epoch: [196]  [180/195]  eta: 0:00:07  lr: 0.002156  loss: 0.4192 (0.4123)  time: 0.5083  data: 0.0001  max mem: 9341
[19:03:07.547138] Epoch: [196]  [194/195]  eta: 0:00:00  lr: 0.002156  loss: 0.4073 (0.4116)  time: 0.5105  data: 0.0001  max mem: 9341
[19:03:07.704811] Epoch: [196] Total time: 0:01:41 (0.5182 s / it)
[19:03:07.723836] Averaged stats: lr: 0.002156  loss: 0.4073 (0.4093)
[19:03:12.499255] {"train_lr": 0.002157653174711156, "train_loss": 0.4093075991441042, "epoch": 196}
[19:03:12.499611] [19:03:12.499698] Training epoch 196 for 0:01:45
[19:03:12.499752] [19:03:12.504322] log_dir: ./exp/debug/cifar100-LT/debug
[19:03:14.303595] Epoch: [197]  [  0/195]  eta: 0:05:50  lr: 0.002156  loss: 0.4058 (0.4058)  time: 1.7976  data: 1.2860  max mem: 9341
[19:03:24.543853] Epoch: [197]  [ 20/195]  eta: 0:01:40  lr: 0.002156  loss: 0.4016 (0.4060)  time: 0.5120  data: 0.0002  max mem: 9341
[19:03:34.781988] Epoch: [197]  [ 40/195]  eta: 0:01:24  lr: 0.002156  loss: 0.4045 (0.4061)  time: 0.5119  data: 0.0002  max mem: 9341
[19:03:45.016859] Epoch: [197]  [ 60/195]  eta: 0:01:11  lr: 0.002155  loss: 0.4252 (0.4103)  time: 0.5117  data: 0.0002  max mem: 9341
[19:03:55.314074] Epoch: [197]  [ 80/195]  eta: 0:01:00  lr: 0.002155  loss: 0.4078 (0.4088)  time: 0.5148  data: 0.0002  max mem: 9341
[19:04:05.526206] Epoch: [197]  [100/195]  eta: 0:00:49  lr: 0.002155  loss: 0.4110 (0.4094)  time: 0.5105  data: 0.0002  max mem: 9341
[19:04:15.762159] Epoch: [197]  [120/195]  eta: 0:00:39  lr: 0.002154  loss: 0.3957 (0.4080)  time: 0.5117  data: 0.0002  max mem: 9341
[19:04:25.995188] Epoch: [197]  [140/195]  eta: 0:00:28  lr: 0.002154  loss: 0.4051 (0.4082)  time: 0.5116  data: 0.0002  max mem: 9341
[19:04:36.266470] Epoch: [197]  [160/195]  eta: 0:00:18  lr: 0.002154  loss: 0.4093 (0.4084)  time: 0.5135  data: 0.0002  max mem: 9341
[19:04:46.436137] Epoch: [197]  [180/195]  eta: 0:00:07  lr: 0.002153  loss: 0.4091 (0.4083)  time: 0.5084  data: 0.0001  max mem: 9341
[19:04:53.566063] Epoch: [197]  [194/195]  eta: 0:00:00  lr: 0.002153  loss: 0.4101 (0.4081)  time: 0.5104  data: 0.0001  max mem: 9341
[19:04:53.743257] Epoch: [197] Total time: 0:01:41 (0.5192 s / it)
[19:04:53.751669] Averaged stats: lr: 0.002153  loss: 0.4101 (0.4084)
[19:04:58.536530] {"train_lr": 0.0021546558706299875, "train_loss": 0.40836580598201505, "epoch": 197}
[19:04:58.536858] [19:04:58.536944] Training epoch 197 for 0:01:46
[19:04:58.536997] [19:04:58.541427] log_dir: ./exp/debug/cifar100-LT/debug
[19:05:00.309961] Epoch: [198]  [  0/195]  eta: 0:05:44  lr: 0.002153  loss: 0.4286 (0.4286)  time: 1.7675  data: 1.2795  max mem: 9341
[19:05:10.546336] Epoch: [198]  [ 20/195]  eta: 0:01:40  lr: 0.002153  loss: 0.4088 (0.4124)  time: 0.5118  data: 0.0002  max mem: 9341
[19:05:20.786984] Epoch: [198]  [ 40/195]  eta: 0:01:24  lr: 0.002153  loss: 0.4151 (0.4110)  time: 0.5120  data: 0.0002  max mem: 9341
[19:05:31.034700] Epoch: [198]  [ 60/195]  eta: 0:01:11  lr: 0.002152  loss: 0.4100 (0.4105)  time: 0.5123  data: 0.0002  max mem: 9341
[19:05:41.339674] Epoch: [198]  [ 80/195]  eta: 0:01:00  lr: 0.002152  loss: 0.4328 (0.4140)  time: 0.5152  data: 0.0002  max mem: 9341
[19:05:51.560855] Epoch: [198]  [100/195]  eta: 0:00:49  lr: 0.002152  loss: 0.4074 (0.4128)  time: 0.5110  data: 0.0002  max mem: 9341
[19:06:01.774751] Epoch: [198]  [120/195]  eta: 0:00:39  lr: 0.002151  loss: 0.4111 (0.4122)  time: 0.5106  data: 0.0002  max mem: 9341
[19:06:11.982655] Epoch: [198]  [140/195]  eta: 0:00:28  lr: 0.002151  loss: 0.4298 (0.4153)  time: 0.5103  data: 0.0002  max mem: 9341
[19:06:22.243782] Epoch: [198]  [160/195]  eta: 0:00:18  lr: 0.002151  loss: 0.4113 (0.4143)  time: 0.5130  data: 0.0002  max mem: 9341
[19:06:32.414771] Epoch: [198]  [180/195]  eta: 0:00:07  lr: 0.002150  loss: 0.3949 (0.4132)  time: 0.5085  data: 0.0001  max mem: 9341
[19:06:39.553283] Epoch: [198]  [194/195]  eta: 0:00:00  lr: 0.002150  loss: 0.4132 (0.4133)  time: 0.5109  data: 0.0001  max mem: 9341
[19:06:39.724071] Epoch: [198] Total time: 0:01:41 (0.5189 s / it)
[19:06:39.739352] Averaged stats: lr: 0.002150  loss: 0.4132 (0.4140)
[19:06:44.471552] {"train_lr": 0.002151642254113968, "train_loss": 0.41403130888938905, "epoch": 198}
[19:06:44.472020] [19:06:44.472129] Training epoch 198 for 0:01:45
[19:06:44.472186] [19:06:44.477471] log_dir: ./exp/debug/cifar100-LT/debug
[19:06:46.149613] Epoch: [199]  [  0/195]  eta: 0:05:25  lr: 0.002150  loss: 0.4105 (0.4105)  time: 1.6707  data: 1.1554  max mem: 9341
[19:06:56.367768] Epoch: [199]  [ 20/195]  eta: 0:01:39  lr: 0.002150  loss: 0.4138 (0.4131)  time: 0.5108  data: 0.0002  max mem: 9341
[19:07:06.590435] Epoch: [199]  [ 40/195]  eta: 0:01:23  lr: 0.002150  loss: 0.4092 (0.4097)  time: 0.5111  data: 0.0002  max mem: 9341
[19:07:16.810695] Epoch: [199]  [ 60/195]  eta: 0:01:11  lr: 0.002149  loss: 0.4055 (0.4091)  time: 0.5110  data: 0.0002  max mem: 9341
[19:07:27.068997] Epoch: [199]  [ 80/195]  eta: 0:01:00  lr: 0.002149  loss: 0.4174 (0.4107)  time: 0.5129  data: 0.0002  max mem: 9341
[19:07:37.345781] Epoch: [199]  [100/195]  eta: 0:00:49  lr: 0.002149  loss: 0.4037 (0.4102)  time: 0.5138  data: 0.0002  max mem: 9341
[19:07:47.577957] Epoch: [199]  [120/195]  eta: 0:00:39  lr: 0.002148  loss: 0.4138 (0.4114)  time: 0.5116  data: 0.0002  max mem: 9341
[19:07:57.815750] Epoch: [199]  [140/195]  eta: 0:00:28  lr: 0.002148  loss: 0.4132 (0.4118)  time: 0.5118  data: 0.0002  max mem: 9341
[19:08:08.118548] Epoch: [199]  [160/195]  eta: 0:00:18  lr: 0.002148  loss: 0.4060 (0.4115)  time: 0.5151  data: 0.0002  max mem: 9341
[19:08:18.313221] Epoch: [199]  [180/195]  eta: 0:00:07  lr: 0.002147  loss: 0.3999 (0.4106)  time: 0.5097  data: 0.0001  max mem: 9341
[19:08:25.468422] Epoch: [199]  [194/195]  eta: 0:00:00  lr: 0.002147  loss: 0.3999 (0.4101)  time: 0.5127  data: 0.0001  max mem: 9341
[19:08:25.642358] Epoch: [199] Total time: 0:01:41 (0.5188 s / it)
[19:08:25.647640] Averaged stats: lr: 0.002147  loss: 0.3999 (0.4097)
[19:08:30.417686] {"train_lr": 0.002148612376657504, "train_loss": 0.4096622799451535, "epoch": 199}
[19:08:30.417947] [19:08:30.418034] Training epoch 199 for 0:01:45
[19:08:30.418086] [19:08:30.422565] log_dir: ./exp/debug/cifar100-LT/debug
[19:08:32.103342] Epoch: [200]  [  0/195]  eta: 0:05:27  lr: 0.002147  loss: 0.4070 (0.4070)  time: 1.6792  data: 1.1583  max mem: 9341
[19:08:42.328589] Epoch: [200]  [ 20/195]  eta: 0:01:39  lr: 0.002147  loss: 0.4059 (0.4105)  time: 0.5112  data: 0.0002  max mem: 9341
[19:08:52.542209] Epoch: [200]  [ 40/195]  eta: 0:01:23  lr: 0.002146  loss: 0.4014 (0.4072)  time: 0.5106  data: 0.0002  max mem: 9341
[19:09:02.764308] Epoch: [200]  [ 60/195]  eta: 0:01:11  lr: 0.002146  loss: 0.4092 (0.4055)  time: 0.5110  data: 0.0002  max mem: 9341
[19:09:13.026641] Epoch: [200]  [ 80/195]  eta: 0:01:00  lr: 0.002146  loss: 0.4142 (0.4077)  time: 0.5131  data: 0.0002  max mem: 9341
[19:09:23.239616] Epoch: [200]  [100/195]  eta: 0:00:49  lr: 0.002145  loss: 0.3962 (0.4063)  time: 0.5106  data: 0.0002  max mem: 9341
[19:09:33.456848] Epoch: [200]  [120/195]  eta: 0:00:39  lr: 0.002145  loss: 0.3960 (0.4061)  time: 0.5108  data: 0.0002  max mem: 9341
[19:09:43.668073] Epoch: [200]  [140/195]  eta: 0:00:28  lr: 0.002145  loss: 0.4083 (0.4065)  time: 0.5105  data: 0.0002  max mem: 9341
[19:09:53.922997] Epoch: [200]  [160/195]  eta: 0:00:18  lr: 0.002144  loss: 0.4069 (0.4064)  time: 0.5127  data: 0.0002  max mem: 9341
[19:10:04.099346] Epoch: [200]  [180/195]  eta: 0:00:07  lr: 0.002144  loss: 0.4053 (0.4065)  time: 0.5088  data: 0.0001  max mem: 9341
[19:10:11.228163] Epoch: [200]  [194/195]  eta: 0:00:00  lr: 0.002144  loss: 0.3996 (0.4059)  time: 0.5105  data: 0.0001  max mem: 9341
[19:10:11.405338] Epoch: [200] Total time: 0:01:40 (0.5179 s / it)
[19:10:11.430019] Averaged stats: lr: 0.002144  loss: 0.3996 (0.4085)
[19:10:16.243224] {"train_lr": 0.002145566290032827, "train_loss": 0.4085492390088546, "epoch": 200}
[19:10:16.243637] [19:10:16.243734] Training epoch 200 for 0:01:45
[19:10:16.243789] [19:10:16.249240] log_dir: ./exp/debug/cifar100-LT/debug
[19:10:18.002022] Epoch: [201]  [  0/195]  eta: 0:05:41  lr: 0.002144  loss: 0.4173 (0.4173)  time: 1.7512  data: 1.2647  max mem: 9341
[19:10:28.218686] Epoch: [201]  [ 20/195]  eta: 0:01:39  lr: 0.002144  loss: 0.3952 (0.4000)  time: 0.5107  data: 0.0003  max mem: 9341
[19:10:38.454412] Epoch: [201]  [ 40/195]  eta: 0:01:23  lr: 0.002143  loss: 0.4047 (0.4031)  time: 0.5117  data: 0.0003  max mem: 9341
[19:10:48.692131] Epoch: [201]  [ 60/195]  eta: 0:01:11  lr: 0.002143  loss: 0.4085 (0.4065)  time: 0.5118  data: 0.0003  max mem: 9341
[19:10:58.992812] Epoch: [201]  [ 80/195]  eta: 0:01:00  lr: 0.002143  loss: 0.4143 (0.4090)  time: 0.5150  data: 0.0003  max mem: 9341
[19:11:09.232203] Epoch: [201]  [100/195]  eta: 0:00:49  lr: 0.002142  loss: 0.4129 (0.4106)  time: 0.5119  data: 0.0003  max mem: 9341
[19:11:19.473782] Epoch: [201]  [120/195]  eta: 0:00:39  lr: 0.002142  loss: 0.4167 (0.4114)  time: 0.5120  data: 0.0002  max mem: 9341
[19:11:29.714450] Epoch: [201]  [140/195]  eta: 0:00:28  lr: 0.002142  loss: 0.4245 (0.4120)  time: 0.5120  data: 0.0002  max mem: 9341
[19:11:39.976145] Epoch: [201]  [160/195]  eta: 0:00:18  lr: 0.002141  loss: 0.4146 (0.4124)  time: 0.5130  data: 0.0002  max mem: 9341
[19:11:50.158844] Epoch: [201]  [180/195]  eta: 0:00:07  lr: 0.002141  loss: 0.4248 (0.4133)  time: 0.5091  data: 0.0001  max mem: 9341
[19:11:57.295535] Epoch: [201]  [194/195]  eta: 0:00:00  lr: 0.002141  loss: 0.4178 (0.4144)  time: 0.5110  data: 0.0001  max mem: 9341
[19:11:57.476227] Epoch: [201] Total time: 0:01:41 (0.5191 s / it)
[19:11:57.487797] Averaged stats: lr: 0.002141  loss: 0.4178 (0.4123)
[19:12:02.255408] {"train_lr": 0.0021425040462891647, "train_loss": 0.41229348870424126, "epoch": 201}
[19:12:02.255763] [19:12:02.255877] Training epoch 201 for 0:01:46
[19:12:02.255932] [19:12:02.261018] log_dir: ./exp/debug/cifar100-LT/debug
[19:12:04.096446] Epoch: [202]  [  0/195]  eta: 0:05:57  lr: 0.002141  loss: 0.4116 (0.4116)  time: 1.8342  data: 1.3343  max mem: 9341
[19:12:14.318977] Epoch: [202]  [ 20/195]  eta: 0:01:40  lr: 0.002141  loss: 0.4127 (0.4183)  time: 0.5111  data: 0.0002  max mem: 9341
[19:12:24.537311] Epoch: [202]  [ 40/195]  eta: 0:01:24  lr: 0.002140  loss: 0.4067 (0.4139)  time: 0.5109  data: 0.0002  max mem: 9341
[19:12:34.757628] Epoch: [202]  [ 60/195]  eta: 0:01:11  lr: 0.002140  loss: 0.4109 (0.4143)  time: 0.5110  data: 0.0002  max mem: 9341
[19:12:45.012347] Epoch: [202]  [ 80/195]  eta: 0:01:00  lr: 0.002140  loss: 0.4086 (0.4139)  time: 0.5127  data: 0.0002  max mem: 9341
[19:12:55.225388] Epoch: [202]  [100/195]  eta: 0:00:49  lr: 0.002139  loss: 0.4263 (0.4160)  time: 0.5106  data: 0.0002  max mem: 9341
[19:13:05.437837] Epoch: [202]  [120/195]  eta: 0:00:39  lr: 0.002139  loss: 0.4100 (0.4154)  time: 0.5106  data: 0.0002  max mem: 9341
[19:13:15.656095] Epoch: [202]  [140/195]  eta: 0:00:28  lr: 0.002139  loss: 0.3973 (0.4130)  time: 0.5109  data: 0.0002  max mem: 9341
[19:13:25.912731] Epoch: [202]  [160/195]  eta: 0:00:18  lr: 0.002138  loss: 0.4040 (0.4118)  time: 0.5128  data: 0.0002  max mem: 9341
[19:13:36.083398] Epoch: [202]  [180/195]  eta: 0:00:07  lr: 0.002138  loss: 0.4209 (0.4122)  time: 0.5085  data: 0.0001  max mem: 9341
[19:13:43.214457] Epoch: [202]  [194/195]  eta: 0:00:00  lr: 0.002138  loss: 0.4127 (0.4122)  time: 0.5105  data: 0.0001  max mem: 9341
[19:13:43.393856] Epoch: [202] Total time: 0:01:41 (0.5186 s / it)
[19:13:43.403128] Averaged stats: lr: 0.002138  loss: 0.4127 (0.4120)
[19:13:48.151572] {"train_lr": 0.002139425697751806, "train_loss": 0.4119870445285088, "epoch": 202}
[19:13:48.151839] [19:13:48.151929] Training epoch 202 for 0:01:45
[19:13:48.151983] [19:13:48.156496] log_dir: ./exp/debug/cifar100-LT/debug
[19:13:49.766094] Epoch: [203]  [  0/195]  eta: 0:05:13  lr: 0.002138  loss: 0.4295 (0.4295)  time: 1.6083  data: 1.1007  max mem: 9341
[19:13:59.983912] Epoch: [203]  [ 20/195]  eta: 0:01:38  lr: 0.002138  loss: 0.4097 (0.4103)  time: 0.5108  data: 0.0002  max mem: 9341
[19:14:10.218531] Epoch: [203]  [ 40/195]  eta: 0:01:23  lr: 0.002137  loss: 0.4153 (0.4156)  time: 0.5116  data: 0.0002  max mem: 9341
[19:14:20.430086] Epoch: [203]  [ 60/195]  eta: 0:01:11  lr: 0.002137  loss: 0.4008 (0.4121)  time: 0.5105  data: 0.0002  max mem: 9341
[19:14:30.697352] Epoch: [203]  [ 80/195]  eta: 0:01:00  lr: 0.002136  loss: 0.4009 (0.4106)  time: 0.5133  data: 0.0002  max mem: 9341
[19:14:40.912320] Epoch: [203]  [100/195]  eta: 0:00:49  lr: 0.002136  loss: 0.4231 (0.4127)  time: 0.5107  data: 0.0002  max mem: 9341
[19:14:51.133003] Epoch: [203]  [120/195]  eta: 0:00:39  lr: 0.002136  loss: 0.4195 (0.4134)  time: 0.5110  data: 0.0002  max mem: 9341
[19:15:01.352328] Epoch: [203]  [140/195]  eta: 0:00:28  lr: 0.002136  loss: 0.4099 (0.4123)  time: 0.5109  data: 0.0002  max mem: 9341
[19:15:11.610180] Epoch: [203]  [160/195]  eta: 0:00:18  lr: 0.002135  loss: 0.4116 (0.4121)  time: 0.5128  data: 0.0002  max mem: 9341
[19:15:21.784441] Epoch: [203]  [180/195]  eta: 0:00:07  lr: 0.002135  loss: 0.4011 (0.4114)  time: 0.5087  data: 0.0001  max mem: 9341
[19:15:28.914530] Epoch: [203]  [194/195]  eta: 0:00:00  lr: 0.002135  loss: 0.4075 (0.4114)  time: 0.5106  data: 0.0001  max mem: 9341
[19:15:29.076072] Epoch: [203] Total time: 0:01:40 (0.5175 s / it)
[19:15:29.087883] Averaged stats: lr: 0.002135  loss: 0.4075 (0.4082)
[19:15:33.767107] {"train_lr": 0.0021363312970212383, "train_loss": 0.4081670319422697, "epoch": 203}
[19:15:33.767432] [19:15:33.767518] Training epoch 203 for 0:01:45
[19:15:33.767572] [19:15:33.772082] log_dir: ./exp/debug/cifar100-LT/debug
[19:15:35.624669] Epoch: [204]  [  0/195]  eta: 0:06:00  lr: 0.002135  loss: 0.3974 (0.3974)  time: 1.8510  data: 1.3522  max mem: 9341
[19:15:45.838139] Epoch: [204]  [ 20/195]  eta: 0:01:40  lr: 0.002134  loss: 0.4034 (0.4052)  time: 0.5106  data: 0.0002  max mem: 9341
[19:15:56.053738] Epoch: [204]  [ 40/195]  eta: 0:01:24  lr: 0.002134  loss: 0.3957 (0.4020)  time: 0.5107  data: 0.0002  max mem: 9341
[19:16:06.268823] Epoch: [204]  [ 60/195]  eta: 0:01:11  lr: 0.002134  loss: 0.4118 (0.4052)  time: 0.5107  data: 0.0002  max mem: 9341
[19:16:16.532143] Epoch: [204]  [ 80/195]  eta: 0:01:00  lr: 0.002133  loss: 0.4123 (0.4081)  time: 0.5131  data: 0.0002  max mem: 9341
[19:16:26.750521] Epoch: [204]  [100/195]  eta: 0:00:49  lr: 0.002133  loss: 0.4055 (0.4085)  time: 0.5109  data: 0.0002  max mem: 9341
[19:16:36.972530] Epoch: [204]  [120/195]  eta: 0:00:39  lr: 0.002133  loss: 0.3984 (0.4081)  time: 0.5110  data: 0.0002  max mem: 9341
[19:16:47.191925] Epoch: [204]  [140/195]  eta: 0:00:28  lr: 0.002133  loss: 0.4063 (0.4090)  time: 0.5109  data: 0.0002  max mem: 9341
[19:16:57.474979] Epoch: [204]  [160/195]  eta: 0:00:18  lr: 0.002132  loss: 0.3941 (0.4074)  time: 0.5141  data: 0.0002  max mem: 9341
[19:17:07.648794] Epoch: [204]  [180/195]  eta: 0:00:07  lr: 0.002132  loss: 0.4019 (0.4073)  time: 0.5086  data: 0.0001  max mem: 9341
[19:17:14.781634] Epoch: [204]  [194/195]  eta: 0:00:00  lr: 0.002132  loss: 0.4075 (0.4073)  time: 0.5108  data: 0.0001  max mem: 9341
[19:17:14.952986] Epoch: [204] Total time: 0:01:41 (0.5189 s / it)
[19:17:14.961206] Averaged stats: lr: 0.002132  loss: 0.4075 (0.4080)
[19:17:19.700262] {"train_lr": 0.0021332208969722308, "train_loss": 0.40802498222925726, "epoch": 204}
[19:17:19.700522] [19:17:19.700609] Training epoch 204 for 0:01:45
[19:17:19.700662] [19:17:19.705227] log_dir: ./exp/debug/cifar100-LT/debug
[19:17:21.388538] Epoch: [205]  [  0/195]  eta: 0:05:28  lr: 0.002132  loss: 0.3702 (0.3702)  time: 1.6825  data: 1.1793  max mem: 9341
[19:17:31.601306] Epoch: [205]  [ 20/195]  eta: 0:01:39  lr: 0.002131  loss: 0.4027 (0.4014)  time: 0.5106  data: 0.0002  max mem: 9341
[19:17:41.816369] Epoch: [205]  [ 40/195]  eta: 0:01:23  lr: 0.002131  loss: 0.3955 (0.3979)  time: 0.5107  data: 0.0002  max mem: 9341
[19:17:52.029194] Epoch: [205]  [ 60/195]  eta: 0:01:11  lr: 0.002131  loss: 0.4099 (0.4031)  time: 0.5106  data: 0.0002  max mem: 9341
[19:18:02.285026] Epoch: [205]  [ 80/195]  eta: 0:01:00  lr: 0.002130  loss: 0.4110 (0.4049)  time: 0.5127  data: 0.0002  max mem: 9341
[19:18:12.494739] Epoch: [205]  [100/195]  eta: 0:00:49  lr: 0.002130  loss: 0.4077 (0.4062)  time: 0.5104  data: 0.0002  max mem: 9341
[19:18:22.708059] Epoch: [205]  [120/195]  eta: 0:00:39  lr: 0.002130  loss: 0.4174 (0.4076)  time: 0.5106  data: 0.0002  max mem: 9341
[19:18:32.920460] Epoch: [205]  [140/195]  eta: 0:00:28  lr: 0.002129  loss: 0.4108 (0.4085)  time: 0.5105  data: 0.0002  max mem: 9341
[19:18:43.181282] Epoch: [205]  [160/195]  eta: 0:00:18  lr: 0.002129  loss: 0.4009 (0.4074)  time: 0.5130  data: 0.0002  max mem: 9341
[19:18:53.357907] Epoch: [205]  [180/195]  eta: 0:00:07  lr: 0.002129  loss: 0.4105 (0.4081)  time: 0.5088  data: 0.0001  max mem: 9341
[19:19:00.490682] Epoch: [205]  [194/195]  eta: 0:00:00  lr: 0.002128  loss: 0.4219 (0.4087)  time: 0.5107  data: 0.0001  max mem: 9341
[19:19:00.664515] Epoch: [205] Total time: 0:01:40 (0.5177 s / it)
[19:19:00.697401] Averaged stats: lr: 0.002128  loss: 0.4219 (0.4060)
[19:19:05.279896] {"train_lr": 0.002130094550752934, "train_loss": 0.4060168330486004, "epoch": 205}
[19:19:05.280243] [19:19:05.280335] Training epoch 205 for 0:01:45
[19:19:05.280388] [19:19:05.284896] log_dir: ./exp/debug/cifar100-LT/debug
[19:19:07.126838] Epoch: [206]  [  0/195]  eta: 0:05:58  lr: 0.002128  loss: 0.3779 (0.3779)  time: 1.8407  data: 1.3352  max mem: 9341
[19:19:17.344113] Epoch: [206]  [ 20/195]  eta: 0:01:40  lr: 0.002128  loss: 0.4049 (0.4009)  time: 0.5108  data: 0.0002  max mem: 9341
[19:19:27.555458] Epoch: [206]  [ 40/195]  eta: 0:01:24  lr: 0.002128  loss: 0.4025 (0.4023)  time: 0.5105  data: 0.0002  max mem: 9341
[19:19:37.771102] Epoch: [206]  [ 60/195]  eta: 0:01:11  lr: 0.002128  loss: 0.4021 (0.4032)  time: 0.5107  data: 0.0002  max mem: 9341
[19:19:48.030363] Epoch: [206]  [ 80/195]  eta: 0:01:00  lr: 0.002127  loss: 0.4119 (0.4053)  time: 0.5129  data: 0.0002  max mem: 9341
[19:19:58.246697] Epoch: [206]  [100/195]  eta: 0:00:49  lr: 0.002127  loss: 0.4071 (0.4063)  time: 0.5107  data: 0.0002  max mem: 9341
[19:20:08.471370] Epoch: [206]  [120/195]  eta: 0:00:39  lr: 0.002127  loss: 0.3950 (0.4059)  time: 0.5112  data: 0.0003  max mem: 9341
[19:20:18.689391] Epoch: [206]  [140/195]  eta: 0:00:28  lr: 0.002126  loss: 0.4094 (0.4063)  time: 0.5108  data: 0.0002  max mem: 9341
[19:20:28.956331] Epoch: [206]  [160/195]  eta: 0:00:18  lr: 0.002126  loss: 0.3994 (0.4048)  time: 0.5133  data: 0.0002  max mem: 9341
[19:20:39.141247] Epoch: [206]  [180/195]  eta: 0:00:07  lr: 0.002126  loss: 0.4136 (0.4049)  time: 0.5092  data: 0.0001  max mem: 9341
[19:20:46.279765] Epoch: [206]  [194/195]  eta: 0:00:00  lr: 0.002125  loss: 0.3996 (0.4045)  time: 0.5111  data: 0.0001  max mem: 9341
[19:20:46.452478] Epoch: [206] Total time: 0:01:41 (0.5188 s / it)
[19:20:46.464610] Averaged stats: lr: 0.002125  loss: 0.3996 (0.4043)
[19:20:51.243005] {"train_lr": 0.002126952311783989, "train_loss": 0.40429652692415774, "epoch": 206}
[19:20:51.243349] [19:20:51.243436] Training epoch 206 for 0:01:45
[19:20:51.243491] [19:20:51.248019] log_dir: ./exp/debug/cifar100-LT/debug
[19:20:52.981900] Epoch: [207]  [  0/195]  eta: 0:05:37  lr: 0.002125  loss: 0.4302 (0.4302)  time: 1.7323  data: 1.2227  max mem: 9341
[19:21:03.290186] Epoch: [207]  [ 20/195]  eta: 0:01:40  lr: 0.002125  loss: 0.3979 (0.4017)  time: 0.5154  data: 0.0002  max mem: 9341
[19:21:13.514990] Epoch: [207]  [ 40/195]  eta: 0:01:24  lr: 0.002125  loss: 0.3986 (0.4004)  time: 0.5112  data: 0.0002  max mem: 9341
[19:21:23.736735] Epoch: [207]  [ 60/195]  eta: 0:01:11  lr: 0.002124  loss: 0.4007 (0.4018)  time: 0.5110  data: 0.0002  max mem: 9341
[19:21:34.004378] Epoch: [207]  [ 80/195]  eta: 0:01:00  lr: 0.002124  loss: 0.4056 (0.4039)  time: 0.5133  data: 0.0002  max mem: 9341
[19:21:44.223909] Epoch: [207]  [100/195]  eta: 0:00:49  lr: 0.002124  loss: 0.4045 (0.4042)  time: 0.5109  data: 0.0002  max mem: 9341
[19:21:54.441563] Epoch: [207]  [120/195]  eta: 0:00:39  lr: 0.002123  loss: 0.4065 (0.4048)  time: 0.5108  data: 0.0002  max mem: 9341
[19:22:04.664541] Epoch: [207]  [140/195]  eta: 0:00:28  lr: 0.002123  loss: 0.4075 (0.4050)  time: 0.5111  data: 0.0002  max mem: 9341
[19:22:14.926925] Epoch: [207]  [160/195]  eta: 0:00:18  lr: 0.002123  loss: 0.4065 (0.4055)  time: 0.5131  data: 0.0002  max mem: 9341
[19:22:25.119081] Epoch: [207]  [180/195]  eta: 0:00:07  lr: 0.002122  loss: 0.4099 (0.4059)  time: 0.5096  data: 0.0001  max mem: 9341
[19:22:32.253388] Epoch: [207]  [194/195]  eta: 0:00:00  lr: 0.002122  loss: 0.3947 (0.4052)  time: 0.5110  data: 0.0001  max mem: 9341
[19:22:32.424446] Epoch: [207] Total time: 0:01:41 (0.5189 s / it)
[19:22:32.449193] Averaged stats: lr: 0.002122  loss: 0.3947 (0.4064)
[19:22:37.234012] {"train_lr": 0.00212379423375758, "train_loss": 0.40644368479649223, "epoch": 207}
[19:22:37.234309] [19:22:37.234399] Training epoch 207 for 0:01:45
[19:22:37.234453] [19:22:37.239060] log_dir: ./exp/debug/cifar100-LT/debug
[19:22:38.826853] Epoch: [208]  [  0/195]  eta: 0:05:09  lr: 0.002122  loss: 0.3832 (0.3832)  time: 1.5868  data: 1.0870  max mem: 9341
[19:22:49.069033] Epoch: [208]  [ 20/195]  eta: 0:01:38  lr: 0.002122  loss: 0.3995 (0.3999)  time: 0.5121  data: 0.0002  max mem: 9341
[19:22:59.313405] Epoch: [208]  [ 40/195]  eta: 0:01:23  lr: 0.002122  loss: 0.4133 (0.4086)  time: 0.5121  data: 0.0002  max mem: 9341
[19:23:09.529664] Epoch: [208]  [ 60/195]  eta: 0:01:11  lr: 0.002121  loss: 0.4156 (0.4111)  time: 0.5108  data: 0.0002  max mem: 9341
[19:23:19.795186] Epoch: [208]  [ 80/195]  eta: 0:01:00  lr: 0.002121  loss: 0.4194 (0.4135)  time: 0.5132  data: 0.0002  max mem: 9341
[19:23:30.010255] Epoch: [208]  [100/195]  eta: 0:00:49  lr: 0.002121  loss: 0.4123 (0.4130)  time: 0.5107  data: 0.0002  max mem: 9341
[19:23:40.223963] Epoch: [208]  [120/195]  eta: 0:00:39  lr: 0.002120  loss: 0.4177 (0.4142)  time: 0.5106  data: 0.0002  max mem: 9341
[19:23:50.437947] Epoch: [208]  [140/195]  eta: 0:00:28  lr: 0.002120  loss: 0.4179 (0.4144)  time: 0.5106  data: 0.0002  max mem: 9341
[19:24:00.697732] Epoch: [208]  [160/195]  eta: 0:00:18  lr: 0.002119  loss: 0.4166 (0.4153)  time: 0.5129  data: 0.0002  max mem: 9341
[19:24:10.878029] Epoch: [208]  [180/195]  eta: 0:00:07  lr: 0.002119  loss: 0.4222 (0.4169)  time: 0.5090  data: 0.0001  max mem: 9341
[19:24:18.016660] Epoch: [208]  [194/195]  eta: 0:00:00  lr: 0.002119  loss: 0.4116 (0.4165)  time: 0.5111  data: 0.0001  max mem: 9341
[19:24:18.185474] Epoch: [208] Total time: 0:01:40 (0.5177 s / it)
[19:24:18.195842] Averaged stats: lr: 0.002119  loss: 0.4116 (0.4180)
[19:24:22.889629] {"train_lr": 0.002120620370636552, "train_loss": 0.4179663133162719, "epoch": 208}
[19:24:22.889978] [19:24:22.890065] Training epoch 208 for 0:01:45
[19:24:22.890120] [19:24:22.894599] log_dir: ./exp/debug/cifar100-LT/debug
[19:24:24.590782] Epoch: [209]  [  0/195]  eta: 0:05:30  lr: 0.002119  loss: 0.4304 (0.4304)  time: 1.6949  data: 1.1827  max mem: 9341
[19:24:34.809887] Epoch: [209]  [ 20/195]  eta: 0:01:39  lr: 0.002119  loss: 0.4207 (0.4208)  time: 0.5109  data: 0.0002  max mem: 9341
[19:24:45.025095] Epoch: [209]  [ 40/195]  eta: 0:01:23  lr: 0.002118  loss: 0.4099 (0.4163)  time: 0.5107  data: 0.0002  max mem: 9341
[19:24:55.239368] Epoch: [209]  [ 60/195]  eta: 0:01:11  lr: 0.002118  loss: 0.4165 (0.4162)  time: 0.5107  data: 0.0002  max mem: 9341
[19:25:05.500878] Epoch: [209]  [ 80/195]  eta: 0:01:00  lr: 0.002118  loss: 0.4177 (0.4158)  time: 0.5130  data: 0.0002  max mem: 9341
[19:25:15.713040] Epoch: [209]  [100/195]  eta: 0:00:49  lr: 0.002117  loss: 0.4100 (0.4155)  time: 0.5106  data: 0.0002  max mem: 9341
[19:25:25.932270] Epoch: [209]  [120/195]  eta: 0:00:39  lr: 0.002117  loss: 0.4264 (0.4173)  time: 0.5109  data: 0.0002  max mem: 9341
[19:25:36.142348] Epoch: [209]  [140/195]  eta: 0:00:28  lr: 0.002117  loss: 0.4093 (0.4161)  time: 0.5104  data: 0.0002  max mem: 9341
[19:25:46.395733] Epoch: [209]  [160/195]  eta: 0:00:18  lr: 0.002116  loss: 0.4178 (0.4166)  time: 0.5126  data: 0.0002  max mem: 9341
[19:25:56.570861] Epoch: [209]  [180/195]  eta: 0:00:07  lr: 0.002116  loss: 0.4024 (0.4158)  time: 0.5087  data: 0.0001  max mem: 9341
[19:26:03.704951] Epoch: [209]  [194/195]  eta: 0:00:00  lr: 0.002116  loss: 0.4039 (0.4156)  time: 0.5107  data: 0.0001  max mem: 9341
[19:26:03.878583] Epoch: [209] Total time: 0:01:40 (0.5179 s / it)
[19:26:03.886462] Averaged stats: lr: 0.002116  loss: 0.4039 (0.4153)
[19:26:08.602435] {"train_lr": 0.002117430776653468, "train_loss": 0.4153130714327861, "epoch": 209}
[19:26:08.602706] [19:26:08.602795] Training epoch 209 for 0:01:45
[19:26:08.602850] [19:26:08.607825] log_dir: ./exp/debug/cifar100-LT/debug
[19:26:10.345815] Epoch: [210]  [  0/195]  eta: 0:05:38  lr: 0.002116  loss: 0.3936 (0.3936)  time: 1.7372  data: 1.2249  max mem: 9341
[19:26:20.589105] Epoch: [210]  [ 20/195]  eta: 0:01:39  lr: 0.002115  loss: 0.3974 (0.4046)  time: 0.5121  data: 0.0002  max mem: 9341
[19:26:30.814057] Epoch: [210]  [ 40/195]  eta: 0:01:23  lr: 0.002115  loss: 0.3943 (0.4031)  time: 0.5112  data: 0.0002  max mem: 9341
[19:26:41.026305] Epoch: [210]  [ 60/195]  eta: 0:01:11  lr: 0.002115  loss: 0.4053 (0.4059)  time: 0.5106  data: 0.0002  max mem: 9341
[19:26:51.278847] Epoch: [210]  [ 80/195]  eta: 0:01:00  lr: 0.002114  loss: 0.4040 (0.4048)  time: 0.5126  data: 0.0002  max mem: 9341
[19:27:01.494026] Epoch: [210]  [100/195]  eta: 0:00:49  lr: 0.002114  loss: 0.4050 (0.4057)  time: 0.5107  data: 0.0002  max mem: 9341
[19:27:11.705354] Epoch: [210]  [120/195]  eta: 0:00:39  lr: 0.002114  loss: 0.4123 (0.4063)  time: 0.5105  data: 0.0002  max mem: 9341
[19:27:21.920607] Epoch: [210]  [140/195]  eta: 0:00:28  lr: 0.002114  loss: 0.4051 (0.4063)  time: 0.5107  data: 0.0002  max mem: 9341
[19:27:32.177966] Epoch: [210]  [160/195]  eta: 0:00:18  lr: 0.002113  loss: 0.3910 (0.4055)  time: 0.5128  data: 0.0002  max mem: 9341
[19:27:42.348981] Epoch: [210]  [180/195]  eta: 0:00:07  lr: 0.002113  loss: 0.4108 (0.4065)  time: 0.5085  data: 0.0001  max mem: 9341
[19:27:49.478854] Epoch: [210]  [194/195]  eta: 0:00:00  lr: 0.002113  loss: 0.4073 (0.4069)  time: 0.5105  data: 0.0001  max mem: 9341
[19:27:49.646504] Epoch: [210] Total time: 0:01:41 (0.5181 s / it)
[19:27:49.670962] Averaged stats: lr: 0.002113  loss: 0.4073 (0.4082)
[19:27:54.391289] {"train_lr": 0.002114225506309689, "train_loss": 0.40817981033753126, "epoch": 210}
[19:27:54.391609] [19:27:54.391697] Training epoch 210 for 0:01:45
[19:27:54.391751] [19:27:54.396246] log_dir: ./exp/debug/cifar100-LT/debug
[19:27:56.171392] Epoch: [211]  [  0/195]  eta: 0:05:45  lr: 0.002112  loss: 0.4242 (0.4242)  time: 1.7737  data: 1.2565  max mem: 9341
[19:28:06.385324] Epoch: [211]  [ 20/195]  eta: 0:01:39  lr: 0.002112  loss: 0.3970 (0.4054)  time: 0.5106  data: 0.0002  max mem: 9341
[19:28:16.600692] Epoch: [211]  [ 40/195]  eta: 0:01:23  lr: 0.002112  loss: 0.3916 (0.4036)  time: 0.5107  data: 0.0002  max mem: 9341
[19:28:26.829023] Epoch: [211]  [ 60/195]  eta: 0:01:11  lr: 0.002112  loss: 0.4127 (0.4069)  time: 0.5113  data: 0.0002  max mem: 9341
[19:28:37.085646] Epoch: [211]  [ 80/195]  eta: 0:01:00  lr: 0.002111  loss: 0.4083 (0.4072)  time: 0.5128  data: 0.0002  max mem: 9341
[19:28:47.300674] Epoch: [211]  [100/195]  eta: 0:00:49  lr: 0.002111  loss: 0.4090 (0.4074)  time: 0.5107  data: 0.0002  max mem: 9341
[19:28:57.518598] Epoch: [211]  [120/195]  eta: 0:00:39  lr: 0.002111  loss: 0.4122 (0.4087)  time: 0.5108  data: 0.0002  max mem: 9341
[19:29:07.730454] Epoch: [211]  [140/195]  eta: 0:00:28  lr: 0.002110  loss: 0.4010 (0.4083)  time: 0.5105  data: 0.0002  max mem: 9341
[19:29:17.993545] Epoch: [211]  [160/195]  eta: 0:00:18  lr: 0.002110  loss: 0.4029 (0.4072)  time: 0.5131  data: 0.0002  max mem: 9341
[19:29:28.169044] Epoch: [211]  [180/195]  eta: 0:00:07  lr: 0.002110  loss: 0.4017 (0.4067)  time: 0.5087  data: 0.0001  max mem: 9341
[19:29:35.298725] Epoch: [211]  [194/195]  eta: 0:00:00  lr: 0.002109  loss: 0.4037 (0.4071)  time: 0.5106  data: 0.0001  max mem: 9341
[19:29:35.459625] Epoch: [211] Total time: 0:01:41 (0.5183 s / it)
[19:29:35.476932] Averaged stats: lr: 0.002109  loss: 0.4037 (0.4062)
[19:29:40.159983] {"train_lr": 0.0021110046143744403, "train_loss": 0.4062322388092677, "epoch": 211}
[19:29:40.160282] [19:29:40.160368] Training epoch 211 for 0:01:45
[19:29:40.160421] [19:29:40.164907] log_dir: ./exp/debug/cifar100-LT/debug
[19:29:41.939043] Epoch: [212]  [  0/195]  eta: 0:05:45  lr: 0.002109  loss: 0.3675 (0.3675)  time: 1.7734  data: 1.2799  max mem: 9341
[19:29:52.166573] Epoch: [212]  [ 20/195]  eta: 0:01:39  lr: 0.002109  loss: 0.4111 (0.4092)  time: 0.5113  data: 0.0002  max mem: 9341
[19:30:02.382126] Epoch: [212]  [ 40/195]  eta: 0:01:23  lr: 0.002109  loss: 0.4014 (0.4061)  time: 0.5107  data: 0.0002  max mem: 9341
[19:30:12.607795] Epoch: [212]  [ 60/195]  eta: 0:01:11  lr: 0.002108  loss: 0.4017 (0.4032)  time: 0.5112  data: 0.0002  max mem: 9341
[19:30:22.878464] Epoch: [212]  [ 80/195]  eta: 0:01:00  lr: 0.002108  loss: 0.3897 (0.4012)  time: 0.5135  data: 0.0002  max mem: 9341
[19:30:33.096386] Epoch: [212]  [100/195]  eta: 0:00:49  lr: 0.002108  loss: 0.4046 (0.4011)  time: 0.5108  data: 0.0002  max mem: 9341
[19:30:43.319382] Epoch: [212]  [120/195]  eta: 0:00:39  lr: 0.002107  loss: 0.3913 (0.4008)  time: 0.5111  data: 0.0002  max mem: 9341
[19:30:53.540765] Epoch: [212]  [140/195]  eta: 0:00:28  lr: 0.002107  loss: 0.4063 (0.4012)  time: 0.5110  data: 0.0002  max mem: 9341
[19:31:03.802492] Epoch: [212]  [160/195]  eta: 0:00:18  lr: 0.002107  loss: 0.3945 (0.4006)  time: 0.5130  data: 0.0002  max mem: 9341
[19:31:13.974140] Epoch: [212]  [180/195]  eta: 0:00:07  lr: 0.002106  loss: 0.4011 (0.4009)  time: 0.5085  data: 0.0001  max mem: 9341
[19:31:21.108424] Epoch: [212]  [194/195]  eta: 0:00:00  lr: 0.002106  loss: 0.4058 (0.4010)  time: 0.5107  data: 0.0001  max mem: 9341
[19:31:21.290542] Epoch: [212] Total time: 0:01:41 (0.5186 s / it)
[19:31:21.296393] Averaged stats: lr: 0.002106  loss: 0.4058 (0.4048)
[19:31:26.093227] {"train_lr": 0.0021077681558838808, "train_loss": 0.40477308520139793, "epoch": 212}
[19:31:26.093522] [19:31:26.093609] Training epoch 212 for 0:01:45
[19:31:26.093663] [19:31:26.098166] log_dir: ./exp/debug/cifar100-LT/debug
[19:31:27.706479] Epoch: [213]  [  0/195]  eta: 0:05:13  lr: 0.002106  loss: 0.3965 (0.3965)  time: 1.6069  data: 1.1132  max mem: 9341
[19:31:37.919811] Epoch: [213]  [ 20/195]  eta: 0:01:38  lr: 0.002106  loss: 0.4012 (0.4003)  time: 0.5106  data: 0.0002  max mem: 9341
[19:31:48.147231] Epoch: [213]  [ 40/195]  eta: 0:01:23  lr: 0.002105  loss: 0.4066 (0.4031)  time: 0.5113  data: 0.0002  max mem: 9341
[19:31:58.366150] Epoch: [213]  [ 60/195]  eta: 0:01:11  lr: 0.002105  loss: 0.4003 (0.4034)  time: 0.5109  data: 0.0002  max mem: 9341
[19:32:08.631520] Epoch: [213]  [ 80/195]  eta: 0:01:00  lr: 0.002105  loss: 0.4053 (0.4031)  time: 0.5132  data: 0.0002  max mem: 9341
[19:32:18.849074] Epoch: [213]  [100/195]  eta: 0:00:49  lr: 0.002104  loss: 0.4096 (0.4038)  time: 0.5108  data: 0.0002  max mem: 9341
[19:32:29.060020] Epoch: [213]  [120/195]  eta: 0:00:39  lr: 0.002104  loss: 0.4117 (0.4047)  time: 0.5105  data: 0.0002  max mem: 9341
[19:32:39.272632] Epoch: [213]  [140/195]  eta: 0:00:28  lr: 0.002104  loss: 0.3971 (0.4032)  time: 0.5106  data: 0.0002  max mem: 9341
[19:32:49.572005] Epoch: [213]  [160/195]  eta: 0:00:18  lr: 0.002103  loss: 0.3882 (0.4016)  time: 0.5149  data: 0.0002  max mem: 9341
[19:32:59.768488] Epoch: [213]  [180/195]  eta: 0:00:07  lr: 0.002103  loss: 0.3994 (0.4010)  time: 0.5098  data: 0.0001  max mem: 9341
[19:33:06.919114] Epoch: [213]  [194/195]  eta: 0:00:00  lr: 0.002103  loss: 0.4018 (0.4019)  time: 0.5127  data: 0.0001  max mem: 9341
[19:33:07.096592] Epoch: [213] Total time: 0:01:40 (0.5179 s / it)
[19:33:07.100107] Averaged stats: lr: 0.002103  loss: 0.4018 (0.4025)
[19:33:11.810259] {"train_lr": 0.002104516186140152, "train_loss": 0.40246097907806055, "epoch": 213}
[19:33:11.810530] [19:33:11.810615] Training epoch 213 for 0:01:45
[19:33:11.810668] [19:33:11.815236] log_dir: ./exp/debug/cifar100-LT/debug
[19:33:13.687366] Epoch: [214]  [  0/195]  eta: 0:06:04  lr: 0.002103  loss: 0.3921 (0.3921)  time: 1.8710  data: 1.3708  max mem: 9341
[19:33:23.904852] Epoch: [214]  [ 20/195]  eta: 0:01:40  lr: 0.002102  loss: 0.3989 (0.3959)  time: 0.5108  data: 0.0002  max mem: 9341
[19:33:34.132136] Epoch: [214]  [ 40/195]  eta: 0:01:24  lr: 0.002102  loss: 0.3886 (0.3941)  time: 0.5113  data: 0.0002  max mem: 9341
[19:33:44.354274] Epoch: [214]  [ 60/195]  eta: 0:01:12  lr: 0.002102  loss: 0.3947 (0.3941)  time: 0.5110  data: 0.0002  max mem: 9341
[19:33:54.614223] Epoch: [214]  [ 80/195]  eta: 0:01:00  lr: 0.002101  loss: 0.3951 (0.3971)  time: 0.5129  data: 0.0002  max mem: 9341
[19:34:04.832635] Epoch: [214]  [100/195]  eta: 0:00:49  lr: 0.002101  loss: 0.4113 (0.3990)  time: 0.5109  data: 0.0002  max mem: 9341
[19:34:15.054059] Epoch: [214]  [120/195]  eta: 0:00:39  lr: 0.002101  loss: 0.3935 (0.3987)  time: 0.5110  data: 0.0002  max mem: 9341
[19:34:25.270513] Epoch: [214]  [140/195]  eta: 0:00:28  lr: 0.002101  loss: 0.3917 (0.3983)  time: 0.5108  data: 0.0002  max mem: 9341
[19:34:35.530835] Epoch: [214]  [160/195]  eta: 0:00:18  lr: 0.002100  loss: 0.4034 (0.3988)  time: 0.5130  data: 0.0002  max mem: 9341
[19:34:45.708977] Epoch: [214]  [180/195]  eta: 0:00:07  lr: 0.002100  loss: 0.3976 (0.3993)  time: 0.5089  data: 0.0001  max mem: 9341
[19:34:52.846077] Epoch: [214]  [194/195]  eta: 0:00:00  lr: 0.002100  loss: 0.3970 (0.3999)  time: 0.5112  data: 0.0001  max mem: 9341
[19:34:53.012487] Epoch: [214] Total time: 0:01:41 (0.5190 s / it)
[19:34:53.024217] Averaged stats: lr: 0.002100  loss: 0.3970 (0.4022)
[19:34:57.718361] {"train_lr": 0.00210124876071045, "train_loss": 0.40222266897177084, "epoch": 214}
[19:34:57.718719] [19:34:57.718809] Training epoch 214 for 0:01:45
[19:34:57.718864] [19:34:57.723384] log_dir: ./exp/debug/cifar100-LT/debug
[19:34:59.446096] Epoch: [215]  [  0/195]  eta: 0:05:35  lr: 0.002099  loss: 0.3600 (0.3600)  time: 1.7214  data: 1.2182  max mem: 9341
[19:35:09.673437] Epoch: [215]  [ 20/195]  eta: 0:01:39  lr: 0.002099  loss: 0.4042 (0.4072)  time: 0.5113  data: 0.0002  max mem: 9341
[19:35:19.886261] Epoch: [215]  [ 40/195]  eta: 0:01:23  lr: 0.002099  loss: 0.4079 (0.4067)  time: 0.5106  data: 0.0002  max mem: 9341
[19:35:30.107445] Epoch: [215]  [ 60/195]  eta: 0:01:11  lr: 0.002099  loss: 0.4113 (0.4085)  time: 0.5110  data: 0.0002  max mem: 9341
[19:35:40.369463] Epoch: [215]  [ 80/195]  eta: 0:01:00  lr: 0.002098  loss: 0.3900 (0.4051)  time: 0.5130  data: 0.0002  max mem: 9341
[19:35:50.585929] Epoch: [215]  [100/195]  eta: 0:00:49  lr: 0.002098  loss: 0.4033 (0.4059)  time: 0.5108  data: 0.0002  max mem: 9341
[19:36:00.796980] Epoch: [215]  [120/195]  eta: 0:00:39  lr: 0.002098  loss: 0.4044 (0.4059)  time: 0.5105  data: 0.0002  max mem: 9341
[19:36:11.013159] Epoch: [215]  [140/195]  eta: 0:00:28  lr: 0.002097  loss: 0.4087 (0.4053)  time: 0.5108  data: 0.0002  max mem: 9341
[19:36:21.311762] Epoch: [215]  [160/195]  eta: 0:00:18  lr: 0.002097  loss: 0.4028 (0.4052)  time: 0.5149  data: 0.0002  max mem: 9341
[19:36:31.507243] Epoch: [215]  [180/195]  eta: 0:00:07  lr: 0.002097  loss: 0.4108 (0.4059)  time: 0.5097  data: 0.0001  max mem: 9341
[19:36:38.662375] Epoch: [215]  [194/195]  eta: 0:00:00  lr: 0.002096  loss: 0.3945 (0.4053)  time: 0.5129  data: 0.0001  max mem: 9341
[19:36:38.832464] Epoch: [215] Total time: 0:01:41 (0.5185 s / it)
[19:36:38.845470] Averaged stats: lr: 0.002096  loss: 0.3945 (0.4036)
[19:36:43.528947] {"train_lr": 0.0020979659354260533, "train_loss": 0.4035740136718139, "epoch": 215}
[19:36:43.529208] [19:36:43.529294] Training epoch 215 for 0:01:45
[19:36:43.529347] [19:36:43.533829] log_dir: ./exp/debug/cifar100-LT/debug
[19:36:45.146896] Epoch: [216]  [  0/195]  eta: 0:05:14  lr: 0.002096  loss: 0.4318 (0.4318)  time: 1.6120  data: 1.0996  max mem: 9341
[19:36:55.373658] Epoch: [216]  [ 20/195]  eta: 0:01:38  lr: 0.002096  loss: 0.3931 (0.3981)  time: 0.5113  data: 0.0003  max mem: 9341
[19:37:05.596066] Epoch: [216]  [ 40/195]  eta: 0:01:23  lr: 0.002096  loss: 0.4063 (0.4032)  time: 0.5110  data: 0.0002  max mem: 9341
[19:37:15.820417] Epoch: [216]  [ 60/195]  eta: 0:01:11  lr: 0.002095  loss: 0.4000 (0.4013)  time: 0.5111  data: 0.0002  max mem: 9341
[19:37:26.122309] Epoch: [216]  [ 80/195]  eta: 0:01:00  lr: 0.002095  loss: 0.4067 (0.4036)  time: 0.5150  data: 0.0002  max mem: 9341
[19:37:36.342068] Epoch: [216]  [100/195]  eta: 0:00:49  lr: 0.002095  loss: 0.4130 (0.4051)  time: 0.5109  data: 0.0002  max mem: 9341
[19:37:46.560722] Epoch: [216]  [120/195]  eta: 0:00:39  lr: 0.002094  loss: 0.4077 (0.4048)  time: 0.5109  data: 0.0002  max mem: 9341
[19:37:56.784561] Epoch: [216]  [140/195]  eta: 0:00:28  lr: 0.002094  loss: 0.4109 (0.4058)  time: 0.5111  data: 0.0002  max mem: 9341
[19:38:07.101558] Epoch: [216]  [160/195]  eta: 0:00:18  lr: 0.002093  loss: 0.4073 (0.4059)  time: 0.5158  data: 0.0002  max mem: 9341
[19:38:17.280050] Epoch: [216]  [180/195]  eta: 0:00:07  lr: 0.002093  loss: 0.3909 (0.4048)  time: 0.5089  data: 0.0001  max mem: 9341
[19:38:24.414764] Epoch: [216]  [194/195]  eta: 0:00:00  lr: 0.002093  loss: 0.3892 (0.4039)  time: 0.5107  data: 0.0001  max mem: 9341
[19:38:24.578231] Epoch: [216] Total time: 0:01:41 (0.5182 s / it)
[19:38:24.602897] Averaged stats: lr: 0.002093  loss: 0.3892 (0.4064)
[19:38:29.417675] {"train_lr": 0.0020946677663813912, "train_loss": 0.40644935675156424, "epoch": 216}
[19:38:29.417937] [19:38:29.418041] Training epoch 216 for 0:01:45
[19:38:29.418097] [19:38:29.422589] log_dir: ./exp/debug/cifar100-LT/debug
[19:38:31.142826] Epoch: [217]  [  0/195]  eta: 0:05:35  lr: 0.002093  loss: 0.4339 (0.4339)  time: 1.7189  data: 1.2186  max mem: 9341
[19:38:41.363424] Epoch: [217]  [ 20/195]  eta: 0:01:39  lr: 0.002093  loss: 0.4106 (0.4097)  time: 0.5110  data: 0.0002  max mem: 9341
[19:38:51.588739] Epoch: [217]  [ 40/195]  eta: 0:01:23  lr: 0.002092  loss: 0.4038 (0.4105)  time: 0.5112  data: 0.0002  max mem: 9341
[19:39:01.802476] Epoch: [217]  [ 60/195]  eta: 0:01:11  lr: 0.002092  loss: 0.3923 (0.4091)  time: 0.5106  data: 0.0002  max mem: 9341
[19:39:12.067914] Epoch: [217]  [ 80/195]  eta: 0:01:00  lr: 0.002092  loss: 0.4073 (0.4081)  time: 0.5132  data: 0.0002  max mem: 9341
[19:39:22.289547] Epoch: [217]  [100/195]  eta: 0:00:49  lr: 0.002091  loss: 0.3937 (0.4074)  time: 0.5110  data: 0.0002  max mem: 9341
[19:39:32.509943] Epoch: [217]  [120/195]  eta: 0:00:39  lr: 0.002091  loss: 0.4095 (0.4073)  time: 0.5110  data: 0.0002  max mem: 9341
[19:39:42.730770] Epoch: [217]  [140/195]  eta: 0:00:28  lr: 0.002091  loss: 0.4063 (0.4074)  time: 0.5110  data: 0.0002  max mem: 9341
[19:39:52.996152] Epoch: [217]  [160/195]  eta: 0:00:18  lr: 0.002090  loss: 0.4021 (0.4067)  time: 0.5132  data: 0.0002  max mem: 9341
[19:40:03.170291] Epoch: [217]  [180/195]  eta: 0:00:07  lr: 0.002090  loss: 0.4009 (0.4068)  time: 0.5087  data: 0.0001  max mem: 9341
[19:40:10.303091] Epoch: [217]  [194/195]  eta: 0:00:00  lr: 0.002090  loss: 0.3997 (0.4066)  time: 0.5108  data: 0.0001  max mem: 9341
[19:40:10.461637] Epoch: [217] Total time: 0:01:41 (0.5181 s / it)
[19:40:10.478964] Averaged stats: lr: 0.002090  loss: 0.3997 (0.4051)
[19:40:15.172415] {"train_lr": 0.002091354309933081, "train_loss": 0.40512521408307245, "epoch": 217}
[19:40:15.172674] [19:40:15.172764] Training epoch 217 for 0:01:45
[19:40:15.172817] [19:40:15.177370] log_dir: ./exp/debug/cifar100-LT/debug
[19:40:17.080694] Epoch: [218]  [  0/195]  eta: 0:06:10  lr: 0.002090  loss: 0.4430 (0.4430)  time: 1.9024  data: 1.4023  max mem: 9341
[19:40:27.299613] Epoch: [218]  [ 20/195]  eta: 0:01:40  lr: 0.002089  loss: 0.3966 (0.4016)  time: 0.5109  data: 0.0002  max mem: 9341
[19:40:37.520215] Epoch: [218]  [ 40/195]  eta: 0:01:24  lr: 0.002089  loss: 0.3931 (0.4001)  time: 0.5110  data: 0.0002  max mem: 9341
[19:40:47.739351] Epoch: [218]  [ 60/195]  eta: 0:01:12  lr: 0.002089  loss: 0.4003 (0.4012)  time: 0.5109  data: 0.0002  max mem: 9341
[19:40:58.044259] Epoch: [218]  [ 80/195]  eta: 0:01:00  lr: 0.002088  loss: 0.4117 (0.4047)  time: 0.5152  data: 0.0002  max mem: 9341
[19:41:08.260901] Epoch: [218]  [100/195]  eta: 0:00:49  lr: 0.002088  loss: 0.3852 (0.4016)  time: 0.5108  data: 0.0002  max mem: 9341
[19:41:18.467302] Epoch: [218]  [120/195]  eta: 0:00:39  lr: 0.002088  loss: 0.4083 (0.4031)  time: 0.5103  data: 0.0002  max mem: 9341
[19:41:28.678774] Epoch: [218]  [140/195]  eta: 0:00:28  lr: 0.002087  loss: 0.4065 (0.4033)  time: 0.5105  data: 0.0002  max mem: 9341
[19:41:38.938359] Epoch: [218]  [160/195]  eta: 0:00:18  lr: 0.002087  loss: 0.4039 (0.4039)  time: 0.5129  data: 0.0002  max mem: 9341
[19:41:49.118805] Epoch: [218]  [180/195]  eta: 0:00:07  lr: 0.002087  loss: 0.4027 (0.4040)  time: 0.5090  data: 0.0001  max mem: 9341
[19:41:56.253238] Epoch: [218]  [194/195]  eta: 0:00:00  lr: 0.002086  loss: 0.4040 (0.4040)  time: 0.5108  data: 0.0001  max mem: 9341
[19:41:56.434597] Epoch: [218] Total time: 0:01:41 (0.5193 s / it)
[19:41:56.441735] Averaged stats: lr: 0.002086  loss: 0.4040 (0.4029)
[19:42:01.333780] {"train_lr": 0.0020880256226989452, "train_loss": 0.40287336714756794, "epoch": 218}
[19:42:01.334037] [19:42:01.334122] Training epoch 218 for 0:01:46
[19:42:01.334175] [19:42:01.338747] log_dir: ./exp/debug/cifar100-LT/debug
[19:42:03.087339] Epoch: [219]  [  0/195]  eta: 0:05:40  lr: 0.002086  loss: 0.3941 (0.3941)  time: 1.7478  data: 1.2561  max mem: 9341
[19:42:13.306928] Epoch: [219]  [ 20/195]  eta: 0:01:39  lr: 0.002086  loss: 0.4034 (0.4040)  time: 0.5109  data: 0.0002  max mem: 9341
[19:42:23.533024] Epoch: [219]  [ 40/195]  eta: 0:01:23  lr: 0.002086  loss: 0.4076 (0.4025)  time: 0.5112  data: 0.0002  max mem: 9341
[19:42:33.754773] Epoch: [219]  [ 60/195]  eta: 0:01:11  lr: 0.002085  loss: 0.4068 (0.4035)  time: 0.5110  data: 0.0002  max mem: 9341
[19:42:44.012277] Epoch: [219]  [ 80/195]  eta: 0:01:00  lr: 0.002085  loss: 0.4005 (0.4026)  time: 0.5128  data: 0.0002  max mem: 9341
[19:42:54.248754] Epoch: [219]  [100/195]  eta: 0:00:49  lr: 0.002085  loss: 0.4097 (0.4048)  time: 0.5118  data: 0.0002  max mem: 9341
[19:43:04.476467] Epoch: [219]  [120/195]  eta: 0:00:39  lr: 0.002084  loss: 0.4109 (0.4065)  time: 0.5113  data: 0.0002  max mem: 9341
[19:43:14.698869] Epoch: [219]  [140/195]  eta: 0:00:28  lr: 0.002084  loss: 0.4082 (0.4075)  time: 0.5111  data: 0.0002  max mem: 9341
[19:43:24.955471] Epoch: [219]  [160/195]  eta: 0:00:18  lr: 0.002083  loss: 0.3933 (0.4062)  time: 0.5128  data: 0.0002  max mem: 9341
[19:43:35.124822] Epoch: [219]  [180/195]  eta: 0:00:07  lr: 0.002083  loss: 0.4150 (0.4066)  time: 0.5084  data: 0.0001  max mem: 9341
[19:43:42.253966] Epoch: [219]  [194/195]  eta: 0:00:00  lr: 0.002083  loss: 0.4160 (0.4065)  time: 0.5103  data: 0.0001  max mem: 9341
[19:43:42.434273] Epoch: [219] Total time: 0:01:41 (0.5184 s / it)
[19:43:42.434988] Averaged stats: lr: 0.002083  loss: 0.4160 (0.4058)
[19:43:47.130493] {"train_lr": 0.002084681761557052, "train_loss": 0.4057563209380859, "epoch": 219}
[19:43:47.130757] [19:43:47.130845] Training epoch 219 for 0:01:45
[19:43:47.130899] [19:43:47.135481] log_dir: ./exp/debug/cifar100-LT/debug
[19:43:48.857325] Epoch: [220]  [  0/195]  eta: 0:05:35  lr: 0.002083  loss: 0.4300 (0.4300)  time: 1.7210  data: 1.2192  max mem: 9341
[19:43:59.108231] Epoch: [220]  [ 20/195]  eta: 0:01:39  lr: 0.002083  loss: 0.4026 (0.4041)  time: 0.5125  data: 0.0002  max mem: 9341
[19:44:09.333632] Epoch: [220]  [ 40/195]  eta: 0:01:23  lr: 0.002082  loss: 0.4058 (0.4078)  time: 0.5112  data: 0.0002  max mem: 9341
[19:44:19.570451] Epoch: [220]  [ 60/195]  eta: 0:01:11  lr: 0.002082  loss: 0.4006 (0.4063)  time: 0.5118  data: 0.0002  max mem: 9341
[19:44:29.873102] Epoch: [220]  [ 80/195]  eta: 0:01:00  lr: 0.002081  loss: 0.3929 (0.4039)  time: 0.5151  data: 0.0002  max mem: 9341
[19:44:40.108602] Epoch: [220]  [100/195]  eta: 0:00:49  lr: 0.002081  loss: 0.4017 (0.4042)  time: 0.5117  data: 0.0002  max mem: 9341
[19:44:50.337235] Epoch: [220]  [120/195]  eta: 0:00:39  lr: 0.002081  loss: 0.4076 (0.4047)  time: 0.5114  data: 0.0002  max mem: 9341
[19:45:00.571730] Epoch: [220]  [140/195]  eta: 0:00:28  lr: 0.002081  loss: 0.4045 (0.4050)  time: 0.5117  data: 0.0002  max mem: 9341
[19:45:10.872880] Epoch: [220]  [160/195]  eta: 0:00:18  lr: 0.002080  loss: 0.4064 (0.4052)  time: 0.5150  data: 0.0002  max mem: 9341
[19:45:21.064707] Epoch: [220]  [180/195]  eta: 0:00:07  lr: 0.002080  loss: 0.4008 (0.4049)  time: 0.5095  data: 0.0001  max mem: 9341
[19:45:28.218163] Epoch: [220]  [194/195]  eta: 0:00:00  lr: 0.002080  loss: 0.4136 (0.4056)  time: 0.5127  data: 0.0001  max mem: 9341
[19:45:28.391384] Epoch: [220] Total time: 0:01:41 (0.5193 s / it)
[19:45:28.400793] Averaged stats: lr: 0.002080  loss: 0.4136 (0.4058)
[19:45:33.065436] {"train_lr": 0.00208132278364479, "train_loss": 0.40576718071332346, "epoch": 220}
[19:45:33.065758] [19:45:33.065843] Training epoch 220 for 0:01:45
[19:45:33.065896] [19:45:33.070433] log_dir: ./exp/debug/cifar100-LT/debug
[19:45:34.771639] Epoch: [221]  [  0/195]  eta: 0:05:31  lr: 0.002080  loss: 0.3725 (0.3725)  time: 1.7000  data: 1.1908  max mem: 9341
[19:45:44.993513] Epoch: [221]  [ 20/195]  eta: 0:01:39  lr: 0.002079  loss: 0.4149 (0.4158)  time: 0.5110  data: 0.0002  max mem: 9341
[19:45:55.226999] Epoch: [221]  [ 40/195]  eta: 0:01:23  lr: 0.002079  loss: 0.4116 (0.4105)  time: 0.5116  data: 0.0002  max mem: 9341
[19:46:05.446189] Epoch: [221]  [ 60/195]  eta: 0:01:11  lr: 0.002079  loss: 0.4004 (0.4083)  time: 0.5109  data: 0.0002  max mem: 9341
[19:46:15.710189] Epoch: [221]  [ 80/195]  eta: 0:01:00  lr: 0.002078  loss: 0.4033 (0.4079)  time: 0.5131  data: 0.0002  max mem: 9341
[19:46:25.921195] Epoch: [221]  [100/195]  eta: 0:00:49  lr: 0.002078  loss: 0.3930 (0.4063)  time: 0.5105  data: 0.0002  max mem: 9341
[19:46:36.138431] Epoch: [221]  [120/195]  eta: 0:00:39  lr: 0.002078  loss: 0.3998 (0.4064)  time: 0.5108  data: 0.0002  max mem: 9341
[19:46:46.353412] Epoch: [221]  [140/195]  eta: 0:00:28  lr: 0.002077  loss: 0.4016 (0.4064)  time: 0.5107  data: 0.0002  max mem: 9341
[19:46:56.616407] Epoch: [221]  [160/195]  eta: 0:00:18  lr: 0.002077  loss: 0.4130 (0.4078)  time: 0.5131  data: 0.0002  max mem: 9341
[19:47:06.789630] Epoch: [221]  [180/195]  eta: 0:00:07  lr: 0.002076  loss: 0.3990 (0.4071)  time: 0.5086  data: 0.0002  max mem: 9341
[19:47:13.923950] Epoch: [221]  [194/195]  eta: 0:00:00  lr: 0.002076  loss: 0.3966 (0.4066)  time: 0.5106  data: 0.0001  max mem: 9341
[19:47:14.096169] Epoch: [221] Total time: 0:01:41 (0.5181 s / it)
[19:47:14.112590] Averaged stats: lr: 0.002076  loss: 0.3966 (0.4028)
[19:47:18.804506] {"train_lr": 0.002077948746357792, "train_loss": 0.40279217874392487, "epoch": 221}
[19:47:18.804768] [19:47:18.804859] Training epoch 221 for 0:01:45
[19:47:18.804912] [19:47:18.809549] log_dir: ./exp/debug/cifar100-LT/debug
[19:47:20.428988] Epoch: [222]  [  0/195]  eta: 0:05:15  lr: 0.002076  loss: 0.4405 (0.4405)  time: 1.6186  data: 1.1263  max mem: 9341
[19:47:30.640392] Epoch: [222]  [ 20/195]  eta: 0:01:38  lr: 0.002076  loss: 0.4027 (0.4019)  time: 0.5105  data: 0.0002  max mem: 9341
[19:47:40.856905] Epoch: [222]  [ 40/195]  eta: 0:01:23  lr: 0.002076  loss: 0.4091 (0.4050)  time: 0.5108  data: 0.0002  max mem: 9341
[19:47:51.075137] Epoch: [222]  [ 60/195]  eta: 0:01:11  lr: 0.002075  loss: 0.4125 (0.4087)  time: 0.5109  data: 0.0002  max mem: 9341
[19:48:01.340069] Epoch: [222]  [ 80/195]  eta: 0:01:00  lr: 0.002075  loss: 0.3974 (0.4062)  time: 0.5132  data: 0.0002  max mem: 9341
[19:48:11.556451] Epoch: [222]  [100/195]  eta: 0:00:49  lr: 0.002074  loss: 0.4027 (0.4049)  time: 0.5108  data: 0.0002  max mem: 9341
[19:48:21.770544] Epoch: [222]  [120/195]  eta: 0:00:39  lr: 0.002074  loss: 0.4022 (0.4053)  time: 0.5106  data: 0.0002  max mem: 9341
[19:48:31.978211] Epoch: [222]  [140/195]  eta: 0:00:28  lr: 0.002074  loss: 0.4031 (0.4042)  time: 0.5103  data: 0.0002  max mem: 9341
[19:48:42.239064] Epoch: [222]  [160/195]  eta: 0:00:18  lr: 0.002073  loss: 0.4011 (0.4042)  time: 0.5130  data: 0.0002  max mem: 9341
[19:48:52.415511] Epoch: [222]  [180/195]  eta: 0:00:07  lr: 0.002073  loss: 0.4002 (0.4033)  time: 0.5088  data: 0.0001  max mem: 9341
[19:48:59.550369] Epoch: [222]  [194/195]  eta: 0:00:00  lr: 0.002073  loss: 0.4002 (0.4024)  time: 0.5108  data: 0.0001  max mem: 9341
[19:48:59.722746] Epoch: [222] Total time: 0:01:40 (0.5175 s / it)
[19:48:59.736164] Averaged stats: lr: 0.002073  loss: 0.4002 (0.4012)
[19:49:04.435861] {"train_lr": 0.0020745597073490714, "train_loss": 0.4011875524352758, "epoch": 222}
[19:49:04.436151] [19:49:04.436241] Training epoch 222 for 0:01:45
[19:49:04.436295] [19:49:04.440846] log_dir: ./exp/debug/cifar100-LT/debug
[19:49:06.156395] Epoch: [223]  [  0/195]  eta: 0:05:34  lr: 0.002073  loss: 0.4045 (0.4045)  time: 1.7147  data: 1.2207  max mem: 9341
[19:49:16.368725] Epoch: [223]  [ 20/195]  eta: 0:01:39  lr: 0.002072  loss: 0.3983 (0.3973)  time: 0.5106  data: 0.0002  max mem: 9341
[19:49:26.588222] Epoch: [223]  [ 40/195]  eta: 0:01:23  lr: 0.002072  loss: 0.3917 (0.3956)  time: 0.5109  data: 0.0002  max mem: 9341
[19:49:36.807907] Epoch: [223]  [ 60/195]  eta: 0:01:11  lr: 0.002072  loss: 0.4055 (0.3990)  time: 0.5109  data: 0.0002  max mem: 9341
[19:49:47.114860] Epoch: [223]  [ 80/195]  eta: 0:01:00  lr: 0.002071  loss: 0.3903 (0.3969)  time: 0.5153  data: 0.0002  max mem: 9341
[19:49:57.353454] Epoch: [223]  [100/195]  eta: 0:00:49  lr: 0.002071  loss: 0.4010 (0.3979)  time: 0.5119  data: 0.0002  max mem: 9341
[19:50:07.591435] Epoch: [223]  [120/195]  eta: 0:00:39  lr: 0.002071  loss: 0.3991 (0.3992)  time: 0.5118  data: 0.0002  max mem: 9341
[19:50:17.830515] Epoch: [223]  [140/195]  eta: 0:00:28  lr: 0.002070  loss: 0.3970 (0.3984)  time: 0.5119  data: 0.0002  max mem: 9341
[19:50:28.092399] Epoch: [223]  [160/195]  eta: 0:00:18  lr: 0.002070  loss: 0.3956 (0.3984)  time: 0.5130  data: 0.0002  max mem: 9341
[19:50:38.266019] Epoch: [223]  [180/195]  eta: 0:00:07  lr: 0.002070  loss: 0.3947 (0.3980)  time: 0.5086  data: 0.0001  max mem: 9341
[19:50:45.399544] Epoch: [223]  [194/195]  eta: 0:00:00  lr: 0.002069  loss: 0.3959 (0.3980)  time: 0.5107  data: 0.0001  max mem: 9341
[19:50:45.578891] Epoch: [223] Total time: 0:01:41 (0.5187 s / it)
[19:50:45.579704] Averaged stats: lr: 0.002069  loss: 0.3959 (0.3995)
[19:50:50.282234] {"train_lr": 0.0020711557245279406, "train_loss": 0.39950533719399034, "epoch": 223}
[19:50:50.282498] [19:50:50.282586] Training epoch 223 for 0:01:45
[19:50:50.282641] [19:50:50.287144] log_dir: ./exp/debug/cifar100-LT/debug
[19:50:52.008272] Epoch: [224]  [  0/195]  eta: 0:05:35  lr: 0.002069  loss: 0.4200 (0.4200)  time: 1.7201  data: 1.1992  max mem: 9341
[19:51:02.253348] Epoch: [224]  [ 20/195]  eta: 0:01:39  lr: 0.002069  loss: 0.4023 (0.3998)  time: 0.5122  data: 0.0002  max mem: 9341
[19:51:12.489139] Epoch: [224]  [ 40/195]  eta: 0:01:23  lr: 0.002069  loss: 0.3920 (0.3970)  time: 0.5117  data: 0.0002  max mem: 9341
[19:51:22.734940] Epoch: [224]  [ 60/195]  eta: 0:01:11  lr: 0.002068  loss: 0.4073 (0.3987)  time: 0.5122  data: 0.0002  max mem: 9341
[19:51:33.040654] Epoch: [224]  [ 80/195]  eta: 0:01:00  lr: 0.002068  loss: 0.3944 (0.3962)  time: 0.5152  data: 0.0002  max mem: 9341
[19:51:43.274670] Epoch: [224]  [100/195]  eta: 0:00:49  lr: 0.002068  loss: 0.3851 (0.3959)  time: 0.5116  data: 0.0002  max mem: 9341
[19:51:53.515243] Epoch: [224]  [120/195]  eta: 0:00:39  lr: 0.002067  loss: 0.3982 (0.3961)  time: 0.5120  data: 0.0002  max mem: 9341
[19:52:03.753260] Epoch: [224]  [140/195]  eta: 0:00:28  lr: 0.002067  loss: 0.3950 (0.3968)  time: 0.5118  data: 0.0002  max mem: 9341
[19:52:14.051797] Epoch: [224]  [160/195]  eta: 0:00:18  lr: 0.002066  loss: 0.4043 (0.3976)  time: 0.5148  data: 0.0002  max mem: 9341
[19:52:24.248900] Epoch: [224]  [180/195]  eta: 0:00:07  lr: 0.002066  loss: 0.3956 (0.3977)  time: 0.5098  data: 0.0002  max mem: 9341
[19:52:31.403806] Epoch: [224]  [194/195]  eta: 0:00:00  lr: 0.002066  loss: 0.3986 (0.3980)  time: 0.5129  data: 0.0001  max mem: 9341
[19:52:31.596435] Epoch: [224] Total time: 0:01:41 (0.5195 s / it)
[19:52:31.597395] Averaged stats: lr: 0.002066  loss: 0.3986 (0.3989)
[19:52:36.305261] {"train_lr": 0.0020677368560590714, "train_loss": 0.3989261302810449, "epoch": 224}
[19:52:36.305529] [19:52:36.305618] Training epoch 224 for 0:01:46
[19:52:36.305672] [19:52:36.310134] log_dir: ./exp/debug/cifar100-LT/debug
[19:52:37.965235] Epoch: [225]  [  0/195]  eta: 0:05:22  lr: 0.002066  loss: 0.3775 (0.3775)  time: 1.6541  data: 1.1399  max mem: 9341
[19:52:48.181381] Epoch: [225]  [ 20/195]  eta: 0:01:38  lr: 0.002066  loss: 0.3913 (0.3932)  time: 0.5107  data: 0.0002  max mem: 9341
[19:52:58.396605] Epoch: [225]  [ 40/195]  eta: 0:01:23  lr: 0.002065  loss: 0.3960 (0.3966)  time: 0.5107  data: 0.0002  max mem: 9341
[19:53:08.611111] Epoch: [225]  [ 60/195]  eta: 0:01:11  lr: 0.002065  loss: 0.4022 (0.3958)  time: 0.5107  data: 0.0002  max mem: 9341
[19:53:18.878632] Epoch: [225]  [ 80/195]  eta: 0:01:00  lr: 0.002064  loss: 0.3987 (0.3978)  time: 0.5133  data: 0.0002  max mem: 9341
[19:53:29.109263] Epoch: [225]  [100/195]  eta: 0:00:49  lr: 0.002064  loss: 0.4007 (0.3992)  time: 0.5115  data: 0.0002  max mem: 9341
[19:53:39.349971] Epoch: [225]  [120/195]  eta: 0:00:39  lr: 0.002064  loss: 0.3994 (0.3999)  time: 0.5120  data: 0.0002  max mem: 9341
[19:53:49.594555] Epoch: [225]  [140/195]  eta: 0:00:28  lr: 0.002064  loss: 0.4048 (0.4012)  time: 0.5122  data: 0.0002  max mem: 9341
[19:53:59.875163] Epoch: [225]  [160/195]  eta: 0:00:18  lr: 0.002063  loss: 0.4030 (0.4022)  time: 0.5140  data: 0.0002  max mem: 9341
[19:54:10.047641] Epoch: [225]  [180/195]  eta: 0:00:07  lr: 0.002063  loss: 0.4051 (0.4028)  time: 0.5086  data: 0.0001  max mem: 9341
[19:54:17.182410] Epoch: [225]  [194/195]  eta: 0:00:00  lr: 0.002062  loss: 0.4086 (0.4036)  time: 0.5108  data: 0.0001  max mem: 9341
[19:54:17.357962] Epoch: [225] Total time: 0:01:41 (0.5182 s / it)
[19:54:17.380197] Averaged stats: lr: 0.002062  loss: 0.4086 (0.3992)
[19:54:22.054488] {"train_lr": 0.002064303160361514, "train_loss": 0.39920155172928784, "epoch": 225}
[19:54:22.054749] [19:54:22.054833] Training epoch 225 for 0:01:45
[19:54:22.054887] [19:54:22.059375] log_dir: ./exp/debug/cifar100-LT/debug
[19:54:23.705951] Epoch: [226]  [  0/195]  eta: 0:05:20  lr: 0.002062  loss: 0.4511 (0.4511)  time: 1.6456  data: 1.1208  max mem: 9341
[19:54:33.949242] Epoch: [226]  [ 20/195]  eta: 0:01:39  lr: 0.002062  loss: 0.3954 (0.4028)  time: 0.5121  data: 0.0002  max mem: 9341
[19:54:44.189742] Epoch: [226]  [ 40/195]  eta: 0:01:23  lr: 0.002062  loss: 0.4043 (0.4052)  time: 0.5120  data: 0.0002  max mem: 9341
[19:54:54.436865] Epoch: [226]  [ 60/195]  eta: 0:01:11  lr: 0.002062  loss: 0.4029 (0.4048)  time: 0.5123  data: 0.0002  max mem: 9341
[19:55:04.744856] Epoch: [226]  [ 80/195]  eta: 0:01:00  lr: 0.002061  loss: 0.4025 (0.4041)  time: 0.5153  data: 0.0002  max mem: 9341
[19:55:15.010050] Epoch: [226]  [100/195]  eta: 0:00:49  lr: 0.002061  loss: 0.4033 (0.4048)  time: 0.5132  data: 0.0002  max mem: 9341
[19:55:25.234220] Epoch: [226]  [120/195]  eta: 0:00:39  lr: 0.002060  loss: 0.4017 (0.4026)  time: 0.5112  data: 0.0002  max mem: 9341
[19:55:35.455076] Epoch: [226]  [140/195]  eta: 0:00:28  lr: 0.002060  loss: 0.3824 (0.4012)  time: 0.5110  data: 0.0002  max mem: 9341
[19:55:45.747547] Epoch: [226]  [160/195]  eta: 0:00:18  lr: 0.002060  loss: 0.3907 (0.4009)  time: 0.5146  data: 0.0002  max mem: 9341
[19:55:55.935372] Epoch: [226]  [180/195]  eta: 0:00:07  lr: 0.002059  loss: 0.4007 (0.4008)  time: 0.5093  data: 0.0002  max mem: 9341
[19:56:03.086886] Epoch: [226]  [194/195]  eta: 0:00:00  lr: 0.002059  loss: 0.4048 (0.4007)  time: 0.5125  data: 0.0001  max mem: 9341
[19:56:03.267644] Epoch: [226] Total time: 0:01:41 (0.5190 s / it)
[19:56:03.274100] Averaged stats: lr: 0.002059  loss: 0.4048 (0.4002)
[19:56:08.110471] {"train_lr": 0.002060854696107633, "train_loss": 0.4001613999788578, "epoch": 226}
[19:56:08.110801] [19:56:08.110885] Training epoch 226 for 0:01:46
[19:56:08.110939] [19:56:08.115383] log_dir: ./exp/debug/cifar100-LT/debug
[19:56:09.805642] Epoch: [227]  [  0/195]  eta: 0:05:29  lr: 0.002059  loss: 0.3949 (0.3949)  time: 1.6890  data: 1.1904  max mem: 9341
[19:56:20.025095] Epoch: [227]  [ 20/195]  eta: 0:01:39  lr: 0.002059  loss: 0.3855 (0.3851)  time: 0.5109  data: 0.0002  max mem: 9341
[19:56:30.241539] Epoch: [227]  [ 40/195]  eta: 0:01:23  lr: 0.002058  loss: 0.3939 (0.3902)  time: 0.5108  data: 0.0002  max mem: 9341
[19:56:40.454163] Epoch: [227]  [ 60/195]  eta: 0:01:11  lr: 0.002058  loss: 0.3953 (0.3918)  time: 0.5106  data: 0.0002  max mem: 9341
[19:56:50.711127] Epoch: [227]  [ 80/195]  eta: 0:01:00  lr: 0.002058  loss: 0.3965 (0.3932)  time: 0.5128  data: 0.0002  max mem: 9341
[19:57:00.924770] Epoch: [227]  [100/195]  eta: 0:00:49  lr: 0.002057  loss: 0.3948 (0.3939)  time: 0.5106  data: 0.0002  max mem: 9341
[19:57:11.140286] Epoch: [227]  [120/195]  eta: 0:00:39  lr: 0.002057  loss: 0.3909 (0.3940)  time: 0.5107  data: 0.0002  max mem: 9341
[19:57:21.358855] Epoch: [227]  [140/195]  eta: 0:00:28  lr: 0.002057  loss: 0.3886 (0.3932)  time: 0.5109  data: 0.0002  max mem: 9341
[19:57:31.614986] Epoch: [227]  [160/195]  eta: 0:00:18  lr: 0.002056  loss: 0.3938 (0.3922)  time: 0.5128  data: 0.0002  max mem: 9341
[19:57:41.786787] Epoch: [227]  [180/195]  eta: 0:00:07  lr: 0.002056  loss: 0.3898 (0.3932)  time: 0.5085  data: 0.0001  max mem: 9341
[19:57:48.919018] Epoch: [227]  [194/195]  eta: 0:00:00  lr: 0.002056  loss: 0.3968 (0.3934)  time: 0.5106  data: 0.0001  max mem: 9341
[19:57:49.081652] Epoch: [227] Total time: 0:01:40 (0.5178 s / it)
[19:57:49.099252] Averaged stats: lr: 0.002056  loss: 0.3968 (0.3954)
[19:57:53.888958] {"train_lr": 0.002057391522222169, "train_loss": 0.3953981049549885, "epoch": 227}
[19:57:53.889209] [19:57:53.889294] Training epoch 227 for 0:01:45
[19:57:53.889348] [19:57:53.893806] log_dir: ./exp/debug/cifar100-LT/debug
[19:57:55.742529] Epoch: [228]  [  0/195]  eta: 0:06:00  lr: 0.002056  loss: 0.3769 (0.3769)  time: 1.8480  data: 1.3626  max mem: 9341
[19:58:05.969665] Epoch: [228]  [ 20/195]  eta: 0:01:40  lr: 0.002055  loss: 0.4048 (0.3984)  time: 0.5113  data: 0.0002  max mem: 9341
[19:58:16.190994] Epoch: [228]  [ 40/195]  eta: 0:01:24  lr: 0.002055  loss: 0.3889 (0.3982)  time: 0.5110  data: 0.0002  max mem: 9341
[19:58:26.409923] Epoch: [228]  [ 60/195]  eta: 0:01:11  lr: 0.002055  loss: 0.4095 (0.4015)  time: 0.5109  data: 0.0002  max mem: 9341
[19:58:36.672552] Epoch: [228]  [ 80/195]  eta: 0:01:00  lr: 0.002054  loss: 0.3930 (0.3979)  time: 0.5131  data: 0.0002  max mem: 9341
[19:58:46.889196] Epoch: [228]  [100/195]  eta: 0:00:49  lr: 0.002054  loss: 0.4009 (0.3988)  time: 0.5108  data: 0.0002  max mem: 9341
[19:58:57.098369] Epoch: [228]  [120/195]  eta: 0:00:39  lr: 0.002054  loss: 0.3879 (0.3976)  time: 0.5104  data: 0.0002  max mem: 9341
[19:59:07.314310] Epoch: [228]  [140/195]  eta: 0:00:28  lr: 0.002053  loss: 0.4010 (0.3981)  time: 0.5107  data: 0.0002  max mem: 9341
[19:59:17.576158] Epoch: [228]  [160/195]  eta: 0:00:18  lr: 0.002053  loss: 0.4055 (0.3992)  time: 0.5130  data: 0.0002  max mem: 9341
[19:59:27.743374] Epoch: [228]  [180/195]  eta: 0:00:07  lr: 0.002052  loss: 0.3943 (0.3984)  time: 0.5083  data: 0.0001  max mem: 9341
[19:59:34.873145] Epoch: [228]  [194/195]  eta: 0:00:00  lr: 0.002052  loss: 0.4012 (0.3980)  time: 0.5104  data: 0.0001  max mem: 9341
[19:59:35.040028] Epoch: [228] Total time: 0:01:41 (0.5187 s / it)
[19:59:35.060414] Averaged stats: lr: 0.002052  loss: 0.4012 (0.3969)
[19:59:39.651795] {"train_lr": 0.002053913697881224, "train_loss": 0.3968524879752061, "epoch": 228}
[19:59:39.652170] [19:59:39.652262] Training epoch 228 for 0:01:45
[19:59:39.652317] [19:59:39.656831] log_dir: ./exp/debug/cifar100-LT/debug
[19:59:41.228950] Epoch: [229]  [  0/195]  eta: 0:05:06  lr: 0.002052  loss: 0.3439 (0.3439)  time: 1.5699  data: 1.0589  max mem: 9341
[19:59:51.445816] Epoch: [229]  [ 20/195]  eta: 0:01:38  lr: 0.002052  loss: 0.4030 (0.4056)  time: 0.5108  data: 0.0002  max mem: 9341
[20:00:01.660584] Epoch: [229]  [ 40/195]  eta: 0:01:23  lr: 0.002051  loss: 0.3959 (0.3999)  time: 0.5107  data: 0.0002  max mem: 9341
[20:00:11.874039] Epoch: [229]  [ 60/195]  eta: 0:01:11  lr: 0.002051  loss: 0.4017 (0.4012)  time: 0.5106  data: 0.0002  max mem: 9341
[20:00:22.131104] Epoch: [229]  [ 80/195]  eta: 0:01:00  lr: 0.002051  loss: 0.3941 (0.3996)  time: 0.5128  data: 0.0002  max mem: 9341
[20:00:32.342466] Epoch: [229]  [100/195]  eta: 0:00:49  lr: 0.002050  loss: 0.3922 (0.3988)  time: 0.5105  data: 0.0002  max mem: 9341
[20:00:42.558653] Epoch: [229]  [120/195]  eta: 0:00:38  lr: 0.002050  loss: 0.4037 (0.3994)  time: 0.5108  data: 0.0002  max mem: 9341
[20:00:52.775616] Epoch: [229]  [140/195]  eta: 0:00:28  lr: 0.002050  loss: 0.3910 (0.3984)  time: 0.5108  data: 0.0002  max mem: 9341
[20:01:03.033851] Epoch: [229]  [160/195]  eta: 0:00:18  lr: 0.002049  loss: 0.4082 (0.3991)  time: 0.5129  data: 0.0002  max mem: 9341
[20:01:13.205842] Epoch: [229]  [180/195]  eta: 0:00:07  lr: 0.002049  loss: 0.3917 (0.3986)  time: 0.5086  data: 0.0001  max mem: 9341
[20:01:20.336184] Epoch: [229]  [194/195]  eta: 0:00:00  lr: 0.002049  loss: 0.3894 (0.3984)  time: 0.5106  data: 0.0001  max mem: 9341
[20:01:20.492278] Epoch: [229] Total time: 0:01:40 (0.5171 s / it)
[20:01:20.505446] Averaged stats: lr: 0.002049  loss: 0.3894 (0.3980)
[20:01:25.217916] {"train_lr": 0.002050421282511205, "train_loss": 0.3979611305854259, "epoch": 229}
[20:01:25.218181] [20:01:25.218264] Training epoch 229 for 0:01:45
[20:01:25.218317] [20:01:25.222768] log_dir: ./exp/debug/cifar100-LT/debug
[20:01:26.978730] Epoch: [230]  [  0/195]  eta: 0:05:42  lr: 0.002049  loss: 0.4270 (0.4270)  time: 1.7550  data: 1.2404  max mem: 9341
[20:01:37.230223] Epoch: [230]  [ 20/195]  eta: 0:01:40  lr: 0.002048  loss: 0.4009 (0.3988)  time: 0.5125  data: 0.0002  max mem: 9341
[20:01:47.476580] Epoch: [230]  [ 40/195]  eta: 0:01:24  lr: 0.002048  loss: 0.4034 (0.3997)  time: 0.5123  data: 0.0002  max mem: 9341
[20:01:57.720351] Epoch: [230]  [ 60/195]  eta: 0:01:11  lr: 0.002048  loss: 0.3869 (0.3963)  time: 0.5121  data: 0.0002  max mem: 9341
[20:02:08.018983] Epoch: [230]  [ 80/195]  eta: 0:01:00  lr: 0.002047  loss: 0.3901 (0.3939)  time: 0.5149  data: 0.0002  max mem: 9341
[20:02:18.261600] Epoch: [230]  [100/195]  eta: 0:00:49  lr: 0.002047  loss: 0.3836 (0.3916)  time: 0.5121  data: 0.0002  max mem: 9341
[20:02:28.504720] Epoch: [230]  [120/195]  eta: 0:00:39  lr: 0.002047  loss: 0.3963 (0.3927)  time: 0.5121  data: 0.0002  max mem: 9341
[20:02:38.720259] Epoch: [230]  [140/195]  eta: 0:00:28  lr: 0.002046  loss: 0.3966 (0.3936)  time: 0.5107  data: 0.0002  max mem: 9341
[20:02:48.983339] Epoch: [230]  [160/195]  eta: 0:00:18  lr: 0.002046  loss: 0.4047 (0.3951)  time: 0.5131  data: 0.0002  max mem: 9341
[20:02:59.164920] Epoch: [230]  [180/195]  eta: 0:00:07  lr: 0.002045  loss: 0.4169 (0.3973)  time: 0.5090  data: 0.0001  max mem: 9341
[20:03:06.305446] Epoch: [230]  [194/195]  eta: 0:00:00  lr: 0.002045  loss: 0.4018 (0.3979)  time: 0.5113  data: 0.0001  max mem: 9341
[20:03:06.471013] Epoch: [230] Total time: 0:01:41 (0.5192 s / it)
[20:03:06.493157] Averaged stats: lr: 0.002045  loss: 0.4018 (0.4003)
[20:03:11.196123] {"train_lr": 0.002046914335787865, "train_loss": 0.4003294538611021, "epoch": 230}
[20:03:11.196499] [20:03:11.196594] Training epoch 230 for 0:01:45
[20:03:11.196649] [20:03:11.201319] log_dir: ./exp/debug/cifar100-LT/debug
[20:03:12.930362] Epoch: [231]  [  0/195]  eta: 0:05:36  lr: 0.002045  loss: 0.4138 (0.4138)  time: 1.7277  data: 1.2146  max mem: 9341
[20:03:23.151890] Epoch: [231]  [ 20/195]  eta: 0:01:39  lr: 0.002045  loss: 0.4272 (0.4199)  time: 0.5110  data: 0.0002  max mem: 9341
[20:03:33.362190] Epoch: [231]  [ 40/195]  eta: 0:01:23  lr: 0.002044  loss: 0.4195 (0.4183)  time: 0.5105  data: 0.0002  max mem: 9341
[20:03:43.584683] Epoch: [231]  [ 60/195]  eta: 0:01:11  lr: 0.002044  loss: 0.4046 (0.4132)  time: 0.5111  data: 0.0002  max mem: 9341
[20:03:53.876436] Epoch: [231]  [ 80/195]  eta: 0:01:00  lr: 0.002044  loss: 0.4183 (0.4151)  time: 0.5145  data: 0.0002  max mem: 9341
[20:04:04.102529] Epoch: [231]  [100/195]  eta: 0:00:49  lr: 0.002043  loss: 0.3991 (0.4129)  time: 0.5112  data: 0.0002  max mem: 9341
[20:04:14.324424] Epoch: [231]  [120/195]  eta: 0:00:39  lr: 0.002043  loss: 0.3997 (0.4118)  time: 0.5110  data: 0.0002  max mem: 9341
[20:04:24.544858] Epoch: [231]  [140/195]  eta: 0:00:28  lr: 0.002043  loss: 0.4011 (0.4101)  time: 0.5110  data: 0.0002  max mem: 9341
[20:04:34.810464] Epoch: [231]  [160/195]  eta: 0:00:18  lr: 0.002042  loss: 0.4047 (0.4097)  time: 0.5132  data: 0.0002  max mem: 9341
[20:04:44.992150] Epoch: [231]  [180/195]  eta: 0:00:07  lr: 0.002042  loss: 0.3943 (0.4089)  time: 0.5090  data: 0.0002  max mem: 9341
[20:04:52.126689] Epoch: [231]  [194/195]  eta: 0:00:00  lr: 0.002042  loss: 0.3993 (0.4085)  time: 0.5108  data: 0.0001  max mem: 9341
[20:04:52.294534] Epoch: [231] Total time: 0:01:41 (0.5184 s / it)
[20:04:52.303896] Averaged stats: lr: 0.002042  loss: 0.3993 (0.4097)
[20:04:56.954892] {"train_lr": 0.002043392917635237, "train_loss": 0.40974099120268453, "epoch": 231}
[20:04:56.955158] [20:04:56.955241] Training epoch 231 for 0:01:45
[20:04:56.955294] [20:04:56.959761] log_dir: ./exp/debug/cifar100-LT/debug
[20:04:58.628938] Epoch: [232]  [  0/195]  eta: 0:05:25  lr: 0.002041  loss: 0.4043 (0.4043)  time: 1.6682  data: 1.1631  max mem: 9341
[20:05:08.850675] Epoch: [232]  [ 20/195]  eta: 0:01:39  lr: 0.002041  loss: 0.3984 (0.3998)  time: 0.5110  data: 0.0002  max mem: 9341
[20:05:19.064619] Epoch: [232]  [ 40/195]  eta: 0:01:23  lr: 0.002041  loss: 0.4006 (0.4050)  time: 0.5106  data: 0.0002  max mem: 9341
[20:05:29.293596] Epoch: [232]  [ 60/195]  eta: 0:01:11  lr: 0.002041  loss: 0.4042 (0.4050)  time: 0.5114  data: 0.0002  max mem: 9341
[20:05:39.562850] Epoch: [232]  [ 80/195]  eta: 0:01:00  lr: 0.002040  loss: 0.3980 (0.4029)  time: 0.5134  data: 0.0002  max mem: 9341
[20:05:49.777171] Epoch: [232]  [100/195]  eta: 0:00:49  lr: 0.002040  loss: 0.3999 (0.4018)  time: 0.5107  data: 0.0002  max mem: 9341
[20:05:59.992046] Epoch: [232]  [120/195]  eta: 0:00:39  lr: 0.002039  loss: 0.3929 (0.4008)  time: 0.5107  data: 0.0002  max mem: 9341
[20:06:10.205102] Epoch: [232]  [140/195]  eta: 0:00:28  lr: 0.002039  loss: 0.3937 (0.4007)  time: 0.5106  data: 0.0002  max mem: 9341
[20:06:20.457936] Epoch: [232]  [160/195]  eta: 0:00:18  lr: 0.002039  loss: 0.3894 (0.4004)  time: 0.5126  data: 0.0002  max mem: 9341
[20:06:30.628305] Epoch: [232]  [180/195]  eta: 0:00:07  lr: 0.002038  loss: 0.3855 (0.3982)  time: 0.5085  data: 0.0001  max mem: 9341
[20:06:37.759662] Epoch: [232]  [194/195]  eta: 0:00:00  lr: 0.002038  loss: 0.3927 (0.3987)  time: 0.5104  data: 0.0001  max mem: 9341
[20:06:37.925827] Epoch: [232] Total time: 0:01:40 (0.5178 s / it)
[20:06:37.939353] Averaged stats: lr: 0.002038  loss: 0.3927 (0.3993)
[20:06:42.728709] {"train_lr": 0.0020398570882246506, "train_loss": 0.39934266129365337, "epoch": 232}
[20:06:42.728974] [20:06:42.729058] Training epoch 232 for 0:01:45
[20:06:42.729110] [20:06:42.733580] log_dir: ./exp/debug/cifar100-LT/debug
[20:06:44.547738] Epoch: [233]  [  0/195]  eta: 0:05:53  lr: 0.002038  loss: 0.3712 (0.3712)  time: 1.8126  data: 1.3247  max mem: 9341
[20:06:54.769432] Epoch: [233]  [ 20/195]  eta: 0:01:40  lr: 0.002038  loss: 0.3981 (0.4045)  time: 0.5110  data: 0.0002  max mem: 9341
[20:07:04.990166] Epoch: [233]  [ 40/195]  eta: 0:01:24  lr: 0.002037  loss: 0.3868 (0.3979)  time: 0.5110  data: 0.0002  max mem: 9341
[20:07:15.208700] Epoch: [233]  [ 60/195]  eta: 0:01:11  lr: 0.002037  loss: 0.3903 (0.3969)  time: 0.5109  data: 0.0002  max mem: 9341
[20:07:25.466946] Epoch: [233]  [ 80/195]  eta: 0:01:00  lr: 0.002036  loss: 0.4020 (0.3979)  time: 0.5129  data: 0.0002  max mem: 9341
[20:07:35.675384] Epoch: [233]  [100/195]  eta: 0:00:49  lr: 0.002036  loss: 0.3897 (0.3963)  time: 0.5104  data: 0.0002  max mem: 9341
[20:07:45.885591] Epoch: [233]  [120/195]  eta: 0:00:39  lr: 0.002036  loss: 0.3868 (0.3960)  time: 0.5105  data: 0.0002  max mem: 9341
[20:07:56.099335] Epoch: [233]  [140/195]  eta: 0:00:28  lr: 0.002036  loss: 0.3975 (0.3967)  time: 0.5106  data: 0.0002  max mem: 9341
[20:08:06.358759] Epoch: [233]  [160/195]  eta: 0:00:18  lr: 0.002035  loss: 0.3928 (0.3964)  time: 0.5129  data: 0.0002  max mem: 9341
[20:08:16.534980] Epoch: [233]  [180/195]  eta: 0:00:07  lr: 0.002035  loss: 0.3880 (0.3952)  time: 0.5088  data: 0.0001  max mem: 9341
[20:08:23.665538] Epoch: [233]  [194/195]  eta: 0:00:00  lr: 0.002034  loss: 0.3954 (0.3952)  time: 0.5106  data: 0.0001  max mem: 9341
[20:08:23.830794] Epoch: [233] Total time: 0:01:41 (0.5184 s / it)
[20:08:23.845653] Averaged stats: lr: 0.002034  loss: 0.3954 (0.3947)
[20:08:28.583914] {"train_lr": 0.0020363069079736653, "train_loss": 0.3946864353540616, "epoch": 233}
[20:08:28.584200] [20:08:28.584287] Training epoch 233 for 0:01:45
[20:08:28.584341] [20:08:28.588772] log_dir: ./exp/debug/cifar100-LT/debug
[20:08:30.285566] Epoch: [234]  [  0/195]  eta: 0:05:30  lr: 0.002034  loss: 0.3612 (0.3612)  time: 1.6958  data: 1.2021  max mem: 9341
[20:08:40.507907] Epoch: [234]  [ 20/195]  eta: 0:01:39  lr: 0.002034  loss: 0.3851 (0.3879)  time: 0.5110  data: 0.0002  max mem: 9341
[20:08:50.748334] Epoch: [234]  [ 40/195]  eta: 0:01:23  lr: 0.002034  loss: 0.3956 (0.3916)  time: 0.5120  data: 0.0002  max mem: 9341
[20:09:00.964505] Epoch: [234]  [ 60/195]  eta: 0:01:11  lr: 0.002034  loss: 0.3862 (0.3904)  time: 0.5108  data: 0.0002  max mem: 9341
[20:09:11.224800] Epoch: [234]  [ 80/195]  eta: 0:01:00  lr: 0.002033  loss: 0.3825 (0.3887)  time: 0.5130  data: 0.0002  max mem: 9341
[20:09:21.439047] Epoch: [234]  [100/195]  eta: 0:00:49  lr: 0.002033  loss: 0.4009 (0.3906)  time: 0.5107  data: 0.0002  max mem: 9341
[20:09:31.659068] Epoch: [234]  [120/195]  eta: 0:00:39  lr: 0.002032  loss: 0.4038 (0.3920)  time: 0.5109  data: 0.0002  max mem: 9341
[20:09:41.874188] Epoch: [234]  [140/195]  eta: 0:00:28  lr: 0.002032  loss: 0.3900 (0.3919)  time: 0.5107  data: 0.0002  max mem: 9341
[20:09:52.135220] Epoch: [234]  [160/195]  eta: 0:00:18  lr: 0.002031  loss: 0.3987 (0.3930)  time: 0.5130  data: 0.0002  max mem: 9341
[20:10:02.308020] Epoch: [234]  [180/195]  eta: 0:00:07  lr: 0.002031  loss: 0.3806 (0.3916)  time: 0.5086  data: 0.0001  max mem: 9341
[20:10:09.441438] Epoch: [234]  [194/195]  eta: 0:00:00  lr: 0.002031  loss: 0.3732 (0.3914)  time: 0.5106  data: 0.0001  max mem: 9341
[20:10:09.613669] Epoch: [234] Total time: 0:01:41 (0.5181 s / it)
[20:10:09.639773] Averaged stats: lr: 0.002031  loss: 0.3732 (0.3916)
[20:10:14.356617] {"train_lr": 0.002032742437545072, "train_loss": 0.3916149270076018, "epoch": 234}
[20:10:14.356880] [20:10:14.356971] Training epoch 234 for 0:01:45
[20:10:14.357025] [20:10:14.361907] log_dir: ./exp/debug/cifar100-LT/debug
[20:10:16.052000] Epoch: [235]  [  0/195]  eta: 0:05:29  lr: 0.002031  loss: 0.4010 (0.4010)  time: 1.6889  data: 1.1873  max mem: 9341
[20:10:26.267831] Epoch: [235]  [ 20/195]  eta: 0:01:39  lr: 0.002031  loss: 0.3974 (0.3989)  time: 0.5107  data: 0.0002  max mem: 9341
[20:10:36.484331] Epoch: [235]  [ 40/195]  eta: 0:01:23  lr: 0.002030  loss: 0.3826 (0.3903)  time: 0.5108  data: 0.0002  max mem: 9341
[20:10:46.699584] Epoch: [235]  [ 60/195]  eta: 0:01:11  lr: 0.002030  loss: 0.3915 (0.3916)  time: 0.5107  data: 0.0002  max mem: 9341
[20:10:57.002976] Epoch: [235]  [ 80/195]  eta: 0:01:00  lr: 0.002029  loss: 0.3961 (0.3927)  time: 0.5151  data: 0.0002  max mem: 9341
[20:11:07.240258] Epoch: [235]  [100/195]  eta: 0:00:49  lr: 0.002029  loss: 0.3774 (0.3911)  time: 0.5118  data: 0.0002  max mem: 9341
[20:11:17.468894] Epoch: [235]  [120/195]  eta: 0:00:39  lr: 0.002029  loss: 0.3914 (0.3913)  time: 0.5114  data: 0.0002  max mem: 9341
[20:11:27.706710] Epoch: [235]  [140/195]  eta: 0:00:28  lr: 0.002028  loss: 0.3793 (0.3900)  time: 0.5118  data: 0.0002  max mem: 9341
[20:11:38.010995] Epoch: [235]  [160/195]  eta: 0:00:18  lr: 0.002028  loss: 0.3878 (0.3901)  time: 0.5151  data: 0.0002  max mem: 9341
[20:11:48.200492] Epoch: [235]  [180/195]  eta: 0:00:07  lr: 0.002028  loss: 0.3866 (0.3900)  time: 0.5094  data: 0.0001  max mem: 9341
[20:11:55.349371] Epoch: [235]  [194/195]  eta: 0:00:00  lr: 0.002027  loss: 0.3802 (0.3898)  time: 0.5124  data: 0.0001  max mem: 9341
[20:11:55.532498] Epoch: [235] Total time: 0:01:41 (0.5188 s / it)
[20:11:55.538099] Averaged stats: lr: 0.002027  loss: 0.3802 (0.3912)
[20:12:00.326465] {"train_lr": 0.0020291637378458374, "train_loss": 0.39122951821638985, "epoch": 235}
[20:12:00.326834] [20:12:00.326920] Training epoch 235 for 0:01:45
[20:12:00.326974] [20:12:00.331454] log_dir: ./exp/debug/cifar100-LT/debug
[20:12:02.014146] Epoch: [236]  [  0/195]  eta: 0:05:27  lr: 0.002027  loss: 0.3242 (0.3242)  time: 1.6814  data: 1.1928  max mem: 9341
[20:12:12.225266] Epoch: [236]  [ 20/195]  eta: 0:01:39  lr: 0.002027  loss: 0.4023 (0.3984)  time: 0.5105  data: 0.0002  max mem: 9341
[20:12:22.435720] Epoch: [236]  [ 40/195]  eta: 0:01:23  lr: 0.002027  loss: 0.3845 (0.3931)  time: 0.5105  data: 0.0002  max mem: 9341
[20:12:32.649737] Epoch: [236]  [ 60/195]  eta: 0:01:11  lr: 0.002026  loss: 0.3877 (0.3920)  time: 0.5106  data: 0.0002  max mem: 9341
[20:12:42.910492] Epoch: [236]  [ 80/195]  eta: 0:01:00  lr: 0.002026  loss: 0.3876 (0.3925)  time: 0.5130  data: 0.0002  max mem: 9341
[20:12:53.129706] Epoch: [236]  [100/195]  eta: 0:00:49  lr: 0.002025  loss: 0.3937 (0.3924)  time: 0.5109  data: 0.0002  max mem: 9341
[20:13:03.347188] Epoch: [236]  [120/195]  eta: 0:00:39  lr: 0.002025  loss: 0.3889 (0.3921)  time: 0.5108  data: 0.0002  max mem: 9341
[20:13:13.563830] Epoch: [236]  [140/195]  eta: 0:00:28  lr: 0.002025  loss: 0.3863 (0.3914)  time: 0.5108  data: 0.0002  max mem: 9341
[20:13:23.824632] Epoch: [236]  [160/195]  eta: 0:00:18  lr: 0.002024  loss: 0.3952 (0.3920)  time: 0.5130  data: 0.0002  max mem: 9341
[20:13:34.000886] Epoch: [236]  [180/195]  eta: 0:00:07  lr: 0.002024  loss: 0.3867 (0.3917)  time: 0.5088  data: 0.0001  max mem: 9341
[20:13:41.130857] Epoch: [236]  [194/195]  eta: 0:00:00  lr: 0.002024  loss: 0.3983 (0.3917)  time: 0.5104  data: 0.0001  max mem: 9341
[20:13:41.303291] Epoch: [236] Total time: 0:01:40 (0.5178 s / it)
[20:13:41.311388] Averaged stats: lr: 0.002024  loss: 0.3983 (0.3927)
[20:13:45.963261] {"train_lr": 0.002025570870026066, "train_loss": 0.3927355531316537, "epoch": 236}
[20:13:45.963577] [20:13:45.963669] Training epoch 236 for 0:01:45
[20:13:45.963725] [20:13:45.968382] log_dir: ./exp/debug/cifar100-LT/debug
[20:13:47.515408] Epoch: [237]  [  0/195]  eta: 0:05:01  lr: 0.002024  loss: 0.3923 (0.3923)  time: 1.5460  data: 1.0405  max mem: 9341
[20:13:57.726307] Epoch: [237]  [ 20/195]  eta: 0:01:37  lr: 0.002023  loss: 0.3902 (0.3914)  time: 0.5105  data: 0.0002  max mem: 9341
[20:14:07.936298] Epoch: [237]  [ 40/195]  eta: 0:01:23  lr: 0.002023  loss: 0.3974 (0.3948)  time: 0.5104  data: 0.0002  max mem: 9341
[20:14:18.154810] Epoch: [237]  [ 60/195]  eta: 0:01:11  lr: 0.002023  loss: 0.3841 (0.3929)  time: 0.5109  data: 0.0002  max mem: 9341
[20:14:28.413395] Epoch: [237]  [ 80/195]  eta: 0:01:00  lr: 0.002022  loss: 0.3852 (0.3929)  time: 0.5129  data: 0.0002  max mem: 9341
[20:14:38.628392] Epoch: [237]  [100/195]  eta: 0:00:49  lr: 0.002022  loss: 0.3894 (0.3922)  time: 0.5107  data: 0.0002  max mem: 9341
[20:14:48.837457] Epoch: [237]  [120/195]  eta: 0:00:38  lr: 0.002022  loss: 0.3842 (0.3914)  time: 0.5104  data: 0.0002  max mem: 9341
[20:14:59.050193] Epoch: [237]  [140/195]  eta: 0:00:28  lr: 0.002021  loss: 0.3911 (0.3921)  time: 0.5106  data: 0.0002  max mem: 9341
[20:15:09.309312] Epoch: [237]  [160/195]  eta: 0:00:18  lr: 0.002021  loss: 0.3899 (0.3916)  time: 0.5129  data: 0.0002  max mem: 9341
[20:15:19.483136] Epoch: [237]  [180/195]  eta: 0:00:07  lr: 0.002020  loss: 0.3856 (0.3910)  time: 0.5086  data: 0.0001  max mem: 9341
[20:15:26.613737] Epoch: [237]  [194/195]  eta: 0:00:00  lr: 0.002020  loss: 0.3935 (0.3917)  time: 0.5105  data: 0.0001  max mem: 9341
[20:15:26.781941] Epoch: [237] Total time: 0:01:40 (0.5170 s / it)
[20:15:26.807710] Averaged stats: lr: 0.002020  loss: 0.3935 (0.3904)
[20:15:31.634187] {"train_lr": 0.0020219638954779586, "train_loss": 0.3904477631434416, "epoch": 237}
[20:15:31.634481] [20:15:31.634570] Training epoch 237 for 0:01:45
[20:15:31.634623] [20:15:31.639242] log_dir: ./exp/debug/cifar100-LT/debug
[20:15:33.260859] Epoch: [238]  [  0/195]  eta: 0:05:15  lr: 0.002020  loss: 0.3824 (0.3824)  time: 1.6203  data: 1.0998  max mem: 9341
[20:15:43.504238] Epoch: [238]  [ 20/195]  eta: 0:01:38  lr: 0.002020  loss: 0.3871 (0.3940)  time: 0.5121  data: 0.0002  max mem: 9341
[20:15:53.743718] Epoch: [238]  [ 40/195]  eta: 0:01:23  lr: 0.002019  loss: 0.3784 (0.3883)  time: 0.5119  data: 0.0002  max mem: 9341
[20:16:03.988069] Epoch: [238]  [ 60/195]  eta: 0:01:11  lr: 0.002019  loss: 0.3841 (0.3864)  time: 0.5122  data: 0.0002  max mem: 9341
[20:16:14.290396] Epoch: [238]  [ 80/195]  eta: 0:01:00  lr: 0.002019  loss: 0.3774 (0.3851)  time: 0.5151  data: 0.0002  max mem: 9341
[20:16:24.531238] Epoch: [238]  [100/195]  eta: 0:00:49  lr: 0.002018  loss: 0.3913 (0.3849)  time: 0.5120  data: 0.0002  max mem: 9341
[20:16:34.752416] Epoch: [238]  [120/195]  eta: 0:00:39  lr: 0.002018  loss: 0.3853 (0.3852)  time: 0.5110  data: 0.0002  max mem: 9341
[20:16:44.983124] Epoch: [238]  [140/195]  eta: 0:00:28  lr: 0.002018  loss: 0.3805 (0.3847)  time: 0.5115  data: 0.0002  max mem: 9341
[20:16:55.245359] Epoch: [238]  [160/195]  eta: 0:00:18  lr: 0.002017  loss: 0.3957 (0.3862)  time: 0.5131  data: 0.0002  max mem: 9341
[20:17:05.423422] Epoch: [238]  [180/195]  eta: 0:00:07  lr: 0.002017  loss: 0.3826 (0.3858)  time: 0.5089  data: 0.0001  max mem: 9341
[20:17:12.558383] Epoch: [238]  [194/195]  eta: 0:00:00  lr: 0.002016  loss: 0.3913 (0.3866)  time: 0.5109  data: 0.0001  max mem: 9341
[20:17:12.732004] Epoch: [238] Total time: 0:01:41 (0.5184 s / it)
[20:17:12.754922] Averaged stats: lr: 0.002016  loss: 0.3913 (0.3881)
[20:17:17.472554] {"train_lr": 0.0020183428758347444, "train_loss": 0.3881297264343653, "epoch": 238}
[20:17:17.472813] [20:17:17.472900] Training epoch 238 for 0:01:45
[20:17:17.472953] [20:17:17.477432] log_dir: ./exp/debug/cifar100-LT/debug
[20:17:19.169601] Epoch: [239]  [  0/195]  eta: 0:05:29  lr: 0.002016  loss: 0.3548 (0.3548)  time: 1.6914  data: 1.1739  max mem: 9341
[20:17:29.414827] Epoch: [239]  [ 20/195]  eta: 0:01:39  lr: 0.002016  loss: 0.3891 (0.3918)  time: 0.5122  data: 0.0002  max mem: 9341
[20:17:39.630377] Epoch: [239]  [ 40/195]  eta: 0:01:23  lr: 0.002016  loss: 0.3907 (0.3929)  time: 0.5107  data: 0.0002  max mem: 9341
[20:17:49.867348] Epoch: [239]  [ 60/195]  eta: 0:01:11  lr: 0.002015  loss: 0.3885 (0.3912)  time: 0.5118  data: 0.0002  max mem: 9341
[20:18:00.165791] Epoch: [239]  [ 80/195]  eta: 0:01:00  lr: 0.002015  loss: 0.3908 (0.3913)  time: 0.5149  data: 0.0002  max mem: 9341
[20:18:10.401577] Epoch: [239]  [100/195]  eta: 0:00:49  lr: 0.002015  loss: 0.3884 (0.3904)  time: 0.5117  data: 0.0002  max mem: 9341
[20:18:20.610695] Epoch: [239]  [120/195]  eta: 0:00:39  lr: 0.002014  loss: 0.3875 (0.3892)  time: 0.5104  data: 0.0002  max mem: 9341
[20:18:30.827684] Epoch: [239]  [140/195]  eta: 0:00:28  lr: 0.002014  loss: 0.3807 (0.3885)  time: 0.5108  data: 0.0002  max mem: 9341
[20:18:41.090352] Epoch: [239]  [160/195]  eta: 0:00:18  lr: 0.002013  loss: 0.3936 (0.3887)  time: 0.5131  data: 0.0002  max mem: 9341
[20:18:51.259535] Epoch: [239]  [180/195]  eta: 0:00:07  lr: 0.002013  loss: 0.3865 (0.3889)  time: 0.5084  data: 0.0001  max mem: 9341
[20:18:58.386434] Epoch: [239]  [194/195]  eta: 0:00:00  lr: 0.002013  loss: 0.3938 (0.3891)  time: 0.5101  data: 0.0001  max mem: 9341
[20:18:58.555217] Epoch: [239] Total time: 0:01:41 (0.5183 s / it)
[20:18:58.572339] Averaged stats: lr: 0.002013  loss: 0.3938 (0.3908)
[20:19:03.394949] {"train_lr": 0.002014707872969673, "train_loss": 0.39076457631129485, "epoch": 239}
[20:19:03.395183] [20:19:03.395268] Training epoch 239 for 0:01:45
[20:19:03.395321] [20:19:03.399812] log_dir: ./exp/debug/cifar100-LT/debug
[20:19:05.278567] Epoch: [240]  [  0/195]  eta: 0:06:06  lr: 0.002013  loss: 0.3846 (0.3846)  time: 1.8778  data: 1.3901  max mem: 9341
[20:19:15.500006] Epoch: [240]  [ 20/195]  eta: 0:01:40  lr: 0.002012  loss: 0.3858 (0.3884)  time: 0.5110  data: 0.0002  max mem: 9341
[20:19:25.723048] Epoch: [240]  [ 40/195]  eta: 0:01:24  lr: 0.002012  loss: 0.3913 (0.3908)  time: 0.5111  data: 0.0002  max mem: 9341
[20:19:35.938608] Epoch: [240]  [ 60/195]  eta: 0:01:12  lr: 0.002012  loss: 0.3882 (0.3908)  time: 0.5107  data: 0.0002  max mem: 9341
[20:19:46.199113] Epoch: [240]  [ 80/195]  eta: 0:01:00  lr: 0.002011  loss: 0.3872 (0.3911)  time: 0.5130  data: 0.0002  max mem: 9341
[20:19:56.415166] Epoch: [240]  [100/195]  eta: 0:00:49  lr: 0.002011  loss: 0.3778 (0.3882)  time: 0.5107  data: 0.0002  max mem: 9341
[20:20:06.629367] Epoch: [240]  [120/195]  eta: 0:00:39  lr: 0.002011  loss: 0.3739 (0.3855)  time: 0.5107  data: 0.0003  max mem: 9341
[20:20:16.870328] Epoch: [240]  [140/195]  eta: 0:00:28  lr: 0.002010  loss: 0.3843 (0.3855)  time: 0.5120  data: 0.0002  max mem: 9341
[20:20:27.172173] Epoch: [240]  [160/195]  eta: 0:00:18  lr: 0.002010  loss: 0.3872 (0.3852)  time: 0.5150  data: 0.0002  max mem: 9341
[20:20:37.362396] Epoch: [240]  [180/195]  eta: 0:00:07  lr: 0.002009  loss: 0.3868 (0.3857)  time: 0.5095  data: 0.0001  max mem: 9341
[20:20:44.517004] Epoch: [240]  [194/195]  eta: 0:00:00  lr: 0.002009  loss: 0.3950 (0.3861)  time: 0.5127  data: 0.0001  max mem: 9341
[20:20:44.684361] Epoch: [240] Total time: 0:01:41 (0.5194 s / it)
[20:20:44.694546] Averaged stats: lr: 0.002009  loss: 0.3950 (0.3888)
[20:20:49.314102] {"train_lr": 0.002011058948994897, "train_loss": 0.3888372852634161, "epoch": 240}
[20:20:49.314439] [20:20:49.314524] Training epoch 240 for 0:01:45
[20:20:49.314577] [20:20:49.319066] log_dir: ./exp/debug/cifar100-LT/debug
[20:20:51.145274] Epoch: [241]  [  0/195]  eta: 0:05:55  lr: 0.002009  loss: 0.4008 (0.4008)  time: 1.8248  data: 1.3299  max mem: 9341
[20:21:01.380150] Epoch: [241]  [ 20/195]  eta: 0:01:40  lr: 0.002009  loss: 0.3897 (0.3919)  time: 0.5117  data: 0.0002  max mem: 9341
[20:21:11.620995] Epoch: [241]  [ 40/195]  eta: 0:01:24  lr: 0.002008  loss: 0.3767 (0.3891)  time: 0.5120  data: 0.0002  max mem: 9341
[20:21:21.842276] Epoch: [241]  [ 60/195]  eta: 0:01:11  lr: 0.002008  loss: 0.3831 (0.3861)  time: 0.5110  data: 0.0002  max mem: 9341
[20:21:32.107593] Epoch: [241]  [ 80/195]  eta: 0:01:00  lr: 0.002008  loss: 0.3923 (0.3887)  time: 0.5132  data: 0.0002  max mem: 9341
[20:21:42.324484] Epoch: [241]  [100/195]  eta: 0:00:49  lr: 0.002007  loss: 0.3850 (0.3885)  time: 0.5108  data: 0.0002  max mem: 9341
[20:21:52.542999] Epoch: [241]  [120/195]  eta: 0:00:39  lr: 0.002007  loss: 0.4052 (0.3919)  time: 0.5109  data: 0.0002  max mem: 9341
[20:22:02.769695] Epoch: [241]  [140/195]  eta: 0:00:28  lr: 0.002007  loss: 0.3920 (0.3923)  time: 0.5113  data: 0.0002  max mem: 9341
[20:22:13.031340] Epoch: [241]  [160/195]  eta: 0:00:18  lr: 0.002006  loss: 0.3854 (0.3915)  time: 0.5130  data: 0.0002  max mem: 9341
[20:22:23.212506] Epoch: [241]  [180/195]  eta: 0:00:07  lr: 0.002006  loss: 0.3913 (0.3912)  time: 0.5090  data: 0.0001  max mem: 9341
[20:22:30.350212] Epoch: [241]  [194/195]  eta: 0:00:00  lr: 0.002005  loss: 0.3817 (0.3905)  time: 0.5110  data: 0.0001  max mem: 9341
[20:22:30.515170] Epoch: [241] Total time: 0:01:41 (0.5190 s / it)
[20:22:30.527850] Averaged stats: lr: 0.002005  loss: 0.3817 (0.3885)
[20:22:35.243737] {"train_lr": 0.002007396166260479, "train_loss": 0.38851942404722556, "epoch": 241}
[20:22:35.244108] [20:22:35.244200] Training epoch 241 for 0:01:45
[20:22:35.244254] [20:22:35.248718] log_dir: ./exp/debug/cifar100-LT/debug
[20:22:36.961720] Epoch: [242]  [  0/195]  eta: 0:05:33  lr: 0.002005  loss: 0.3726 (0.3726)  time: 1.7116  data: 1.2227  max mem: 9341
[20:22:47.179834] Epoch: [242]  [ 20/195]  eta: 0:01:39  lr: 0.002005  loss: 0.3919 (0.3894)  time: 0.5108  data: 0.0002  max mem: 9341
[20:22:57.390773] Epoch: [242]  [ 40/195]  eta: 0:01:23  lr: 0.002005  loss: 0.3901 (0.3914)  time: 0.5105  data: 0.0002  max mem: 9341
[20:23:07.601847] Epoch: [242]  [ 60/195]  eta: 0:01:11  lr: 0.002005  loss: 0.3809 (0.3885)  time: 0.5105  data: 0.0002  max mem: 9341
[20:23:17.864947] Epoch: [242]  [ 80/195]  eta: 0:01:00  lr: 0.002004  loss: 0.3872 (0.3892)  time: 0.5131  data: 0.0002  max mem: 9341
[20:23:28.079943] Epoch: [242]  [100/195]  eta: 0:00:49  lr: 0.002004  loss: 0.3968 (0.3905)  time: 0.5107  data: 0.0002  max mem: 9341
[20:23:38.305246] Epoch: [242]  [120/195]  eta: 0:00:39  lr: 0.002003  loss: 0.3968 (0.3918)  time: 0.5112  data: 0.0002  max mem: 9341
[20:23:48.527047] Epoch: [242]  [140/195]  eta: 0:00:28  lr: 0.002003  loss: 0.3755 (0.3904)  time: 0.5110  data: 0.0002  max mem: 9341
[20:23:58.806346] Epoch: [242]  [160/195]  eta: 0:00:18  lr: 0.002002  loss: 0.3889 (0.3903)  time: 0.5139  data: 0.0002  max mem: 9341
[20:24:08.975296] Epoch: [242]  [180/195]  eta: 0:00:07  lr: 0.002002  loss: 0.3873 (0.3909)  time: 0.5084  data: 0.0001  max mem: 9341
[20:24:16.110639] Epoch: [242]  [194/195]  eta: 0:00:00  lr: 0.002002  loss: 0.3870 (0.3908)  time: 0.5108  data: 0.0001  max mem: 9341
[20:24:16.273184] Epoch: [242] Total time: 0:01:41 (0.5181 s / it)
[20:24:16.296311] Averaged stats: lr: 0.002002  loss: 0.3870 (0.3894)
[20:24:21.004462] {"train_lr": 0.0020037195873532677, "train_loss": 0.38943957598545614, "epoch": 242}
[20:24:21.004769] [20:24:21.004859] Training epoch 242 for 0:01:45
[20:24:21.004913] [20:24:21.009514] log_dir: ./exp/debug/cifar100-LT/debug
[20:24:22.661560] Epoch: [243]  [  0/195]  eta: 0:05:21  lr: 0.002002  loss: 0.4014 (0.4014)  time: 1.6510  data: 1.1433  max mem: 9341
[20:24:32.879353] Epoch: [243]  [ 20/195]  eta: 0:01:38  lr: 0.002001  loss: 0.3948 (0.3987)  time: 0.5108  data: 0.0002  max mem: 9341
[20:24:43.096783] Epoch: [243]  [ 40/195]  eta: 0:01:23  lr: 0.002001  loss: 0.3827 (0.3950)  time: 0.5108  data: 0.0002  max mem: 9341
[20:24:53.319219] Epoch: [243]  [ 60/195]  eta: 0:01:11  lr: 0.002001  loss: 0.3856 (0.3941)  time: 0.5110  data: 0.0002  max mem: 9341
[20:25:03.597867] Epoch: [243]  [ 80/195]  eta: 0:01:00  lr: 0.002000  loss: 0.3840 (0.3923)  time: 0.5139  data: 0.0002  max mem: 9341
[20:25:13.822385] Epoch: [243]  [100/195]  eta: 0:00:49  lr: 0.002000  loss: 0.3855 (0.3917)  time: 0.5112  data: 0.0002  max mem: 9341
[20:25:24.037691] Epoch: [243]  [120/195]  eta: 0:00:39  lr: 0.002000  loss: 0.3924 (0.3920)  time: 0.5107  data: 0.0002  max mem: 9341
[20:25:34.253768] Epoch: [243]  [140/195]  eta: 0:00:28  lr: 0.001999  loss: 0.3930 (0.3921)  time: 0.5107  data: 0.0002  max mem: 9341
[20:25:44.506049] Epoch: [243]  [160/195]  eta: 0:00:18  lr: 0.001999  loss: 0.3857 (0.3914)  time: 0.5126  data: 0.0002  max mem: 9341
[20:25:54.678757] Epoch: [243]  [180/195]  eta: 0:00:07  lr: 0.001998  loss: 0.3926 (0.3915)  time: 0.5086  data: 0.0001  max mem: 9341
[20:26:01.812310] Epoch: [243]  [194/195]  eta: 0:00:00  lr: 0.001998  loss: 0.3822 (0.3904)  time: 0.5106  data: 0.0001  max mem: 9341
[20:26:01.981922] Epoch: [243] Total time: 0:01:40 (0.5178 s / it)
[20:26:01.995731] Averaged stats: lr: 0.001998  loss: 0.3822 (0.3899)
[20:26:06.691666] {"train_lr": 0.0020000292750958354, "train_loss": 0.3898594054656151, "epoch": 243}
[20:26:06.692019] [20:26:06.692115] Training epoch 243 for 0:01:45
[20:26:06.692172] [20:26:06.696668] log_dir: ./exp/debug/cifar100-LT/debug
[20:26:08.437435] Epoch: [244]  [  0/195]  eta: 0:05:39  lr: 0.001998  loss: 0.3641 (0.3641)  time: 1.7391  data: 1.2294  max mem: 9341
[20:26:18.656275] Epoch: [244]  [ 20/195]  eta: 0:01:39  lr: 0.001998  loss: 0.3860 (0.3855)  time: 0.5109  data: 0.0002  max mem: 9341
[20:26:28.881625] Epoch: [244]  [ 40/195]  eta: 0:01:23  lr: 0.001997  loss: 0.3937 (0.3867)  time: 0.5112  data: 0.0002  max mem: 9341
[20:26:39.094446] Epoch: [244]  [ 60/195]  eta: 0:01:11  lr: 0.001997  loss: 0.3877 (0.3882)  time: 0.5106  data: 0.0002  max mem: 9341
[20:26:49.353962] Epoch: [244]  [ 80/195]  eta: 0:01:00  lr: 0.001997  loss: 0.3981 (0.3911)  time: 0.5128  data: 0.0002  max mem: 9341
[20:26:59.559569] Epoch: [244]  [100/195]  eta: 0:00:49  lr: 0.001996  loss: 0.3848 (0.3905)  time: 0.5102  data: 0.0002  max mem: 9341
[20:27:09.770101] Epoch: [244]  [120/195]  eta: 0:00:39  lr: 0.001996  loss: 0.3875 (0.3897)  time: 0.5105  data: 0.0002  max mem: 9341
[20:27:19.984860] Epoch: [244]  [140/195]  eta: 0:00:28  lr: 0.001996  loss: 0.3840 (0.3904)  time: 0.5107  data: 0.0002  max mem: 9341
[20:27:30.243960] Epoch: [244]  [160/195]  eta: 0:00:18  lr: 0.001995  loss: 0.3812 (0.3893)  time: 0.5129  data: 0.0002  max mem: 9341
[20:27:40.417420] Epoch: [244]  [180/195]  eta: 0:00:07  lr: 0.001995  loss: 0.3789 (0.3891)  time: 0.5086  data: 0.0001  max mem: 9341
[20:27:47.550621] Epoch: [244]  [194/195]  eta: 0:00:00  lr: 0.001994  loss: 0.3945 (0.3893)  time: 0.5107  data: 0.0001  max mem: 9341
[20:27:47.730271] Epoch: [244] Total time: 0:01:41 (0.5181 s / it)
[20:27:47.741422] Averaged stats: lr: 0.001994  loss: 0.3945 (0.3885)
[20:27:52.471513] {"train_lr": 0.0019963252925454455, "train_loss": 0.3884908746832456, "epoch": 244}
[20:27:52.471810] [20:27:52.471900] Training epoch 244 for 0:01:45
[20:27:52.471953] [20:27:52.476596] log_dir: ./exp/debug/cifar100-LT/debug
[20:27:54.248372] Epoch: [245]  [  0/195]  eta: 0:05:45  lr: 0.001994  loss: 0.3637 (0.3637)  time: 1.7708  data: 1.2822  max mem: 9341
[20:28:04.463782] Epoch: [245]  [ 20/195]  eta: 0:01:39  lr: 0.001994  loss: 0.3934 (0.3903)  time: 0.5107  data: 0.0002  max mem: 9341
[20:28:14.729899] Epoch: [245]  [ 40/195]  eta: 0:01:24  lr: 0.001994  loss: 0.3958 (0.3910)  time: 0.5133  data: 0.0002  max mem: 9341
[20:28:24.954833] Epoch: [245]  [ 60/195]  eta: 0:01:11  lr: 0.001993  loss: 0.3916 (0.3905)  time: 0.5112  data: 0.0002  max mem: 9341
[20:28:35.213073] Epoch: [245]  [ 80/195]  eta: 0:01:00  lr: 0.001993  loss: 0.3928 (0.3904)  time: 0.5129  data: 0.0002  max mem: 9341
[20:28:45.427152] Epoch: [245]  [100/195]  eta: 0:00:49  lr: 0.001992  loss: 0.3760 (0.3885)  time: 0.5106  data: 0.0002  max mem: 9341
[20:28:55.645149] Epoch: [245]  [120/195]  eta: 0:00:39  lr: 0.001992  loss: 0.3972 (0.3894)  time: 0.5108  data: 0.0002  max mem: 9341
[20:29:05.862612] Epoch: [245]  [140/195]  eta: 0:00:28  lr: 0.001992  loss: 0.3700 (0.3883)  time: 0.5108  data: 0.0002  max mem: 9341
[20:29:16.162980] Epoch: [245]  [160/195]  eta: 0:00:18  lr: 0.001991  loss: 0.3910 (0.3885)  time: 0.5150  data: 0.0002  max mem: 9341
[20:29:26.359373] Epoch: [245]  [180/195]  eta: 0:00:07  lr: 0.001991  loss: 0.3887 (0.3885)  time: 0.5098  data: 0.0001  max mem: 9341
[20:29:33.513440] Epoch: [245]  [194/195]  eta: 0:00:00  lr: 0.001991  loss: 0.3886 (0.3886)  time: 0.5126  data: 0.0001  max mem: 9341
[20:29:33.681853] Epoch: [245] Total time: 0:01:41 (0.5190 s / it)
[20:29:33.707185] Averaged stats: lr: 0.001991  loss: 0.3886 (0.3869)
[20:29:38.427857] {"train_lr": 0.0019926077029929455, "train_loss": 0.38691620223033124, "epoch": 245}
[20:29:38.428215] [20:29:38.428300] Training epoch 245 for 0:01:45
[20:29:38.428354] [20:29:38.432797] log_dir: ./exp/debug/cifar100-LT/debug
[20:29:40.291159] Epoch: [246]  [  0/195]  eta: 0:06:02  lr: 0.001991  loss: 0.4073 (0.4073)  time: 1.8571  data: 1.3560  max mem: 9341
[20:29:50.509126] Epoch: [246]  [ 20/195]  eta: 0:01:40  lr: 0.001990  loss: 0.3947 (0.3930)  time: 0.5108  data: 0.0002  max mem: 9341
[20:30:00.729426] Epoch: [246]  [ 40/195]  eta: 0:01:24  lr: 0.001990  loss: 0.3914 (0.3916)  time: 0.5109  data: 0.0002  max mem: 9341
[20:30:10.948614] Epoch: [246]  [ 60/195]  eta: 0:01:11  lr: 0.001990  loss: 0.3785 (0.3890)  time: 0.5109  data: 0.0002  max mem: 9341
[20:30:21.207798] Epoch: [246]  [ 80/195]  eta: 0:01:00  lr: 0.001989  loss: 0.3913 (0.3897)  time: 0.5129  data: 0.0002  max mem: 9341
[20:30:31.427418] Epoch: [246]  [100/195]  eta: 0:00:49  lr: 0.001989  loss: 0.3891 (0.3910)  time: 0.5109  data: 0.0002  max mem: 9341
[20:30:41.638430] Epoch: [246]  [120/195]  eta: 0:00:39  lr: 0.001988  loss: 0.3964 (0.3920)  time: 0.5105  data: 0.0002  max mem: 9341
[20:30:51.869016] Epoch: [246]  [140/195]  eta: 0:00:28  lr: 0.001988  loss: 0.3910 (0.3915)  time: 0.5115  data: 0.0002  max mem: 9341
[20:31:02.130368] Epoch: [246]  [160/195]  eta: 0:00:18  lr: 0.001988  loss: 0.3869 (0.3912)  time: 0.5130  data: 0.0002  max mem: 9341
[20:31:12.304961] Epoch: [246]  [180/195]  eta: 0:00:07  lr: 0.001987  loss: 0.3966 (0.3921)  time: 0.5087  data: 0.0001  max mem: 9341
[20:31:19.437969] Epoch: [246]  [194/195]  eta: 0:00:00  lr: 0.001987  loss: 0.3873 (0.3918)  time: 0.5106  data: 0.0001  max mem: 9341
[20:31:19.595471] Epoch: [246] Total time: 0:01:41 (0.5188 s / it)
[20:31:19.628021] Averaged stats: lr: 0.001987  loss: 0.3873 (0.3918)
[20:31:24.425971] {"train_lr": 0.0019888765699616843, "train_loss": 0.3917546896598278, "epoch": 246}
[20:31:24.426379] [20:31:24.426473] Training epoch 246 for 0:01:45
[20:31:24.426528] [20:31:24.431284] log_dir: ./exp/debug/cifar100-LT/debug
[20:31:26.184793] Epoch: [247]  [  0/195]  eta: 0:05:41  lr: 0.001987  loss: 0.3554 (0.3554)  time: 1.7509  data: 1.2498  max mem: 9341
[20:31:36.415003] Epoch: [247]  [ 20/195]  eta: 0:01:39  lr: 0.001987  loss: 0.4016 (0.3932)  time: 0.5114  data: 0.0002  max mem: 9341
[20:31:46.754960] Epoch: [247]  [ 40/195]  eta: 0:01:24  lr: 0.001986  loss: 0.3821 (0.3873)  time: 0.5169  data: 0.0002  max mem: 9341
[20:31:56.981189] Epoch: [247]  [ 60/195]  eta: 0:01:12  lr: 0.001986  loss: 0.3768 (0.3854)  time: 0.5113  data: 0.0002  max mem: 9341
[20:32:07.285094] Epoch: [247]  [ 80/195]  eta: 0:01:00  lr: 0.001985  loss: 0.3875 (0.3855)  time: 0.5151  data: 0.0002  max mem: 9341
[20:32:17.521522] Epoch: [247]  [100/195]  eta: 0:00:49  lr: 0.001985  loss: 0.3883 (0.3867)  time: 0.5118  data: 0.0002  max mem: 9341
[20:32:27.757848] Epoch: [247]  [120/195]  eta: 0:00:39  lr: 0.001985  loss: 0.3774 (0.3871)  time: 0.5118  data: 0.0002  max mem: 9341
[20:32:37.985929] Epoch: [247]  [140/195]  eta: 0:00:28  lr: 0.001984  loss: 0.3969 (0.3883)  time: 0.5113  data: 0.0002  max mem: 9341
[20:32:48.241538] Epoch: [247]  [160/195]  eta: 0:00:18  lr: 0.001984  loss: 0.3872 (0.3879)  time: 0.5127  data: 0.0002  max mem: 9341
[20:32:58.423824] Epoch: [247]  [180/195]  eta: 0:00:07  lr: 0.001983  loss: 0.3713 (0.3871)  time: 0.5091  data: 0.0001  max mem: 9341
[20:33:05.560755] Epoch: [247]  [194/195]  eta: 0:00:00  lr: 0.001983  loss: 0.3729 (0.3869)  time: 0.5113  data: 0.0001  max mem: 9341
[20:33:05.731870] Epoch: [247] Total time: 0:01:41 (0.5195 s / it)
[20:33:05.750130] Averaged stats: lr: 0.001983  loss: 0.3729 (0.3861)
[20:33:10.514600] {"train_lr": 0.0019851319572064285, "train_loss": 0.3860838260406103, "epoch": 247}
[20:33:10.514939] [20:33:10.515025] Training epoch 247 for 0:01:46
[20:33:10.515079] [20:33:10.519571] log_dir: ./exp/debug/cifar100-LT/debug
[20:33:12.093802] Epoch: [248]  [  0/195]  eta: 0:05:06  lr: 0.001983  loss: 0.3651 (0.3651)  time: 1.5730  data: 1.0681  max mem: 9341
[20:33:22.317343] Epoch: [248]  [ 20/195]  eta: 0:01:38  lr: 0.001983  loss: 0.3933 (0.3858)  time: 0.5111  data: 0.0002  max mem: 9341
[20:33:32.534330] Epoch: [248]  [ 40/195]  eta: 0:01:23  lr: 0.001982  loss: 0.3882 (0.3858)  time: 0.5108  data: 0.0002  max mem: 9341
[20:33:42.750049] Epoch: [248]  [ 60/195]  eta: 0:01:11  lr: 0.001982  loss: 0.3921 (0.3864)  time: 0.5107  data: 0.0002  max mem: 9341
[20:33:53.008944] Epoch: [248]  [ 80/195]  eta: 0:01:00  lr: 0.001982  loss: 0.3794 (0.3852)  time: 0.5129  data: 0.0002  max mem: 9341
[20:34:03.229366] Epoch: [248]  [100/195]  eta: 0:00:49  lr: 0.001981  loss: 0.3885 (0.3865)  time: 0.5110  data: 0.0002  max mem: 9341
[20:34:13.452595] Epoch: [248]  [120/195]  eta: 0:00:39  lr: 0.001981  loss: 0.3976 (0.3873)  time: 0.5111  data: 0.0002  max mem: 9341
[20:34:23.678782] Epoch: [248]  [140/195]  eta: 0:00:28  lr: 0.001981  loss: 0.3726 (0.3862)  time: 0.5113  data: 0.0002  max mem: 9341
[20:34:33.950584] Epoch: [248]  [160/195]  eta: 0:00:18  lr: 0.001980  loss: 0.3787 (0.3850)  time: 0.5135  data: 0.0002  max mem: 9341
[20:34:44.134846] Epoch: [248]  [180/195]  eta: 0:00:07  lr: 0.001980  loss: 0.3821 (0.3850)  time: 0.5092  data: 0.0001  max mem: 9341
[20:34:51.269516] Epoch: [248]  [194/195]  eta: 0:00:00  lr: 0.001979  loss: 0.3825 (0.3852)  time: 0.5110  data: 0.0001  max mem: 9341
[20:34:51.453798] Epoch: [248] Total time: 0:01:40 (0.5176 s / it)
[20:34:51.454629] Averaged stats: lr: 0.001979  loss: 0.3825 (0.3848)
[20:34:56.113570] {"train_lr": 0.0019813739287122727, "train_loss": 0.3847662744995875, "epoch": 248}
[20:34:56.113916] [20:34:56.114001] Training epoch 248 for 0:01:45
[20:34:56.114055] [20:34:56.118505] log_dir: ./exp/debug/cifar100-LT/debug
[20:34:57.913872] Epoch: [249]  [  0/195]  eta: 0:05:49  lr: 0.001979  loss: 0.3751 (0.3751)  time: 1.7933  data: 1.2960  max mem: 9341
[20:35:08.148369] Epoch: [249]  [ 20/195]  eta: 0:01:40  lr: 0.001979  loss: 0.3781 (0.3848)  time: 0.5117  data: 0.0002  max mem: 9341
[20:35:18.368109] Epoch: [249]  [ 40/195]  eta: 0:01:24  lr: 0.001979  loss: 0.3833 (0.3885)  time: 0.5109  data: 0.0002  max mem: 9341
[20:35:28.586590] Epoch: [249]  [ 60/195]  eta: 0:01:11  lr: 0.001978  loss: 0.3842 (0.3871)  time: 0.5109  data: 0.0002  max mem: 9341
[20:35:38.853129] Epoch: [249]  [ 80/195]  eta: 0:01:00  lr: 0.001978  loss: 0.3881 (0.3865)  time: 0.5133  data: 0.0002  max mem: 9341
[20:35:49.068605] Epoch: [249]  [100/195]  eta: 0:00:49  lr: 0.001977  loss: 0.3958 (0.3878)  time: 0.5107  data: 0.0002  max mem: 9341
[20:35:59.291118] Epoch: [249]  [120/195]  eta: 0:00:39  lr: 0.001977  loss: 0.3902 (0.3882)  time: 0.5111  data: 0.0002  max mem: 9341
[20:36:09.505297] Epoch: [249]  [140/195]  eta: 0:00:28  lr: 0.001977  loss: 0.3802 (0.3866)  time: 0.5107  data: 0.0002  max mem: 9341
[20:36:19.784848] Epoch: [249]  [160/195]  eta: 0:00:18  lr: 0.001976  loss: 0.3809 (0.3866)  time: 0.5139  data: 0.0002  max mem: 9341
[20:36:29.982563] Epoch: [249]  [180/195]  eta: 0:00:07  lr: 0.001976  loss: 0.3878 (0.3871)  time: 0.5098  data: 0.0001  max mem: 9341
[20:36:37.138219] Epoch: [249]  [194/195]  eta: 0:00:00  lr: 0.001976  loss: 0.3891 (0.3872)  time: 0.5128  data: 0.0001  max mem: 9341
[20:36:37.315834] Epoch: [249] Total time: 0:01:41 (0.5190 s / it)
[20:36:37.328042] Averaged stats: lr: 0.001976  loss: 0.3891 (0.3849)
[20:36:42.032973] {"train_lr": 0.00197760254869356, "train_loss": 0.3848845368394485, "epoch": 249}
[20:36:42.033257] [20:36:42.033344] Training epoch 249 for 0:01:45
[20:36:42.033413] [20:36:42.038042] log_dir: ./exp/debug/cifar100-LT/debug
[20:36:43.852631] Epoch: [250]  [  0/195]  eta: 0:05:53  lr: 0.001976  loss: 0.4101 (0.4101)  time: 1.8135  data: 1.3147  max mem: 9341
[20:36:54.090575] Epoch: [250]  [ 20/195]  eta: 0:01:40  lr: 0.001975  loss: 0.3930 (0.3914)  time: 0.5118  data: 0.0002  max mem: 9341
[20:37:04.331572] Epoch: [250]  [ 40/195]  eta: 0:01:24  lr: 0.001975  loss: 0.3913 (0.3891)  time: 0.5120  data: 0.0002  max mem: 9341
[20:37:14.569461] Epoch: [250]  [ 60/195]  eta: 0:01:11  lr: 0.001975  loss: 0.3786 (0.3851)  time: 0.5118  data: 0.0002  max mem: 9341
[20:37:24.830435] Epoch: [250]  [ 80/195]  eta: 0:01:00  lr: 0.001974  loss: 0.3788 (0.3852)  time: 0.5130  data: 0.0002  max mem: 9341
[20:37:35.042245] Epoch: [250]  [100/195]  eta: 0:00:49  lr: 0.001974  loss: 0.3806 (0.3849)  time: 0.5105  data: 0.0002  max mem: 9341
[20:37:45.260952] Epoch: [250]  [120/195]  eta: 0:00:39  lr: 0.001973  loss: 0.3841 (0.3851)  time: 0.5109  data: 0.0002  max mem: 9341
[20:37:55.474957] Epoch: [250]  [140/195]  eta: 0:00:28  lr: 0.001973  loss: 0.3985 (0.3872)  time: 0.5106  data: 0.0002  max mem: 9341
[20:38:05.735931] Epoch: [250]  [160/195]  eta: 0:00:18  lr: 0.001972  loss: 0.3830 (0.3868)  time: 0.5130  data: 0.0002  max mem: 9341
[20:38:15.912828] Epoch: [250]  [180/195]  eta: 0:00:07  lr: 0.001972  loss: 0.3811 (0.3869)  time: 0.5088  data: 0.0001  max mem: 9341
[20:38:23.044650] Epoch: [250]  [194/195]  eta: 0:00:00  lr: 0.001972  loss: 0.3836 (0.3868)  time: 0.5107  data: 0.0001  max mem: 9341
[20:38:23.205557] Epoch: [250] Total time: 0:01:41 (0.5188 s / it)
[20:38:23.226245] Averaged stats: lr: 0.001972  loss: 0.3836 (0.3851)
[20:38:27.954741] {"train_lr": 0.00197381788159277, "train_loss": 0.38512770113272543, "epoch": 250}
[20:38:27.955019] [20:38:27.955105] Training epoch 250 for 0:01:45
[20:38:27.955158] [20:38:27.959758] log_dir: ./exp/debug/cifar100-LT/debug
[20:38:29.551844] Epoch: [251]  [  0/195]  eta: 0:05:10  lr: 0.001972  loss: 0.4364 (0.4364)  time: 1.5911  data: 1.0792  max mem: 9341
[20:38:39.772341] Epoch: [251]  [ 20/195]  eta: 0:01:38  lr: 0.001971  loss: 0.3770 (0.3807)  time: 0.5110  data: 0.0002  max mem: 9341
[20:38:49.988142] Epoch: [251]  [ 40/195]  eta: 0:01:23  lr: 0.001971  loss: 0.4047 (0.3903)  time: 0.5107  data: 0.0002  max mem: 9341
[20:39:00.204461] Epoch: [251]  [ 60/195]  eta: 0:01:11  lr: 0.001971  loss: 0.3874 (0.3902)  time: 0.5108  data: 0.0002  max mem: 9341
[20:39:10.467536] Epoch: [251]  [ 80/195]  eta: 0:01:00  lr: 0.001970  loss: 0.3756 (0.3875)  time: 0.5131  data: 0.0002  max mem: 9341
[20:39:20.679555] Epoch: [251]  [100/195]  eta: 0:00:49  lr: 0.001970  loss: 0.3730 (0.3848)  time: 0.5105  data: 0.0002  max mem: 9341
[20:39:30.900039] Epoch: [251]  [120/195]  eta: 0:00:39  lr: 0.001970  loss: 0.3841 (0.3843)  time: 0.5110  data: 0.0002  max mem: 9341
[20:39:41.114278] Epoch: [251]  [140/195]  eta: 0:00:28  lr: 0.001969  loss: 0.3849 (0.3844)  time: 0.5107  data: 0.0002  max mem: 9341
[20:39:51.375779] Epoch: [251]  [160/195]  eta: 0:00:18  lr: 0.001969  loss: 0.3845 (0.3841)  time: 0.5130  data: 0.0002  max mem: 9341
[20:40:01.552461] Epoch: [251]  [180/195]  eta: 0:00:07  lr: 0.001968  loss: 0.3786 (0.3842)  time: 0.5088  data: 0.0001  max mem: 9341
[20:40:08.688739] Epoch: [251]  [194/195]  eta: 0:00:00  lr: 0.001968  loss: 0.3832 (0.3845)  time: 0.5108  data: 0.0001  max mem: 9341
[20:40:08.854979] Epoch: [251] Total time: 0:01:40 (0.5174 s / it)
[20:40:08.871148] Averaged stats: lr: 0.001968  loss: 0.3832 (0.3868)
[20:40:13.573852] {"train_lr": 0.0019700199920794164, "train_loss": 0.38682791338517114, "epoch": 251}
[20:40:13.574115] [20:40:13.574200] Training epoch 251 for 0:01:45
[20:40:13.574255] [20:40:13.578739] log_dir: ./exp/debug/cifar100-LT/debug
[20:40:15.428050] Epoch: [252]  [  0/195]  eta: 0:06:00  lr: 0.001968  loss: 0.4024 (0.4024)  time: 1.8476  data: 1.3432  max mem: 9341
[20:40:25.676385] Epoch: [252]  [ 20/195]  eta: 0:01:40  lr: 0.001968  loss: 0.3810 (0.3867)  time: 0.5124  data: 0.0002  max mem: 9341
[20:40:35.895464] Epoch: [252]  [ 40/195]  eta: 0:01:24  lr: 0.001967  loss: 0.3878 (0.3824)  time: 0.5109  data: 0.0002  max mem: 9341
[20:40:46.107455] Epoch: [252]  [ 60/195]  eta: 0:01:11  lr: 0.001967  loss: 0.3780 (0.3802)  time: 0.5105  data: 0.0002  max mem: 9341
[20:40:56.374454] Epoch: [252]  [ 80/195]  eta: 0:01:00  lr: 0.001966  loss: 0.3817 (0.3810)  time: 0.5133  data: 0.0002  max mem: 9341
[20:41:06.585936] Epoch: [252]  [100/195]  eta: 0:00:49  lr: 0.001966  loss: 0.3835 (0.3827)  time: 0.5105  data: 0.0002  max mem: 9341
[20:41:16.804756] Epoch: [252]  [120/195]  eta: 0:00:39  lr: 0.001966  loss: 0.3855 (0.3829)  time: 0.5109  data: 0.0002  max mem: 9341
[20:41:27.011239] Epoch: [252]  [140/195]  eta: 0:00:28  lr: 0.001965  loss: 0.3867 (0.3838)  time: 0.5103  data: 0.0002  max mem: 9341
[20:41:37.271500] Epoch: [252]  [160/195]  eta: 0:00:18  lr: 0.001965  loss: 0.3777 (0.3834)  time: 0.5130  data: 0.0002  max mem: 9341
[20:41:47.442561] Epoch: [252]  [180/195]  eta: 0:00:07  lr: 0.001965  loss: 0.3740 (0.3827)  time: 0.5085  data: 0.0001  max mem: 9341
[20:41:54.575627] Epoch: [252]  [194/195]  eta: 0:00:00  lr: 0.001964  loss: 0.3746 (0.3827)  time: 0.5106  data: 0.0001  max mem: 9341
[20:41:54.750586] Epoch: [252] Total time: 0:01:41 (0.5188 s / it)
[20:41:54.759949] Averaged stats: lr: 0.001964  loss: 0.3746 (0.3838)
[20:41:59.557824] {"train_lr": 0.0019662089450489624, "train_loss": 0.38376092670055534, "epoch": 252}
[20:41:59.558100] [20:41:59.558186] Training epoch 252 for 0:01:45
[20:41:59.558240] [20:41:59.562781] log_dir: ./exp/debug/cifar100-LT/debug
[20:42:01.211202] Epoch: [253]  [  0/195]  eta: 0:05:21  lr: 0.001964  loss: 0.3999 (0.3999)  time: 1.6472  data: 1.1392  max mem: 9341
[20:42:11.427431] Epoch: [253]  [ 20/195]  eta: 0:01:38  lr: 0.001964  loss: 0.3780 (0.3767)  time: 0.5108  data: 0.0002  max mem: 9341
[20:42:21.649129] Epoch: [253]  [ 40/195]  eta: 0:01:23  lr: 0.001964  loss: 0.3604 (0.3729)  time: 0.5110  data: 0.0002  max mem: 9341
[20:42:31.868878] Epoch: [253]  [ 60/195]  eta: 0:01:11  lr: 0.001963  loss: 0.3744 (0.3739)  time: 0.5109  data: 0.0002  max mem: 9341
[20:42:42.122943] Epoch: [253]  [ 80/195]  eta: 0:01:00  lr: 0.001963  loss: 0.3816 (0.3757)  time: 0.5126  data: 0.0002  max mem: 9341
[20:42:52.363366] Epoch: [253]  [100/195]  eta: 0:00:49  lr: 0.001962  loss: 0.3911 (0.3775)  time: 0.5120  data: 0.0002  max mem: 9341
[20:43:02.577508] Epoch: [253]  [120/195]  eta: 0:00:39  lr: 0.001962  loss: 0.3869 (0.3789)  time: 0.5106  data: 0.0002  max mem: 9341
[20:43:12.790449] Epoch: [253]  [140/195]  eta: 0:00:28  lr: 0.001962  loss: 0.3823 (0.3800)  time: 0.5106  data: 0.0002  max mem: 9341
[20:43:23.044794] Epoch: [253]  [160/195]  eta: 0:00:18  lr: 0.001961  loss: 0.3801 (0.3811)  time: 0.5127  data: 0.0002  max mem: 9341
[20:43:33.211990] Epoch: [253]  [180/195]  eta: 0:00:07  lr: 0.001961  loss: 0.3900 (0.3816)  time: 0.5083  data: 0.0001  max mem: 9341
[20:43:40.340852] Epoch: [253]  [194/195]  eta: 0:00:00  lr: 0.001960  loss: 0.3900 (0.3820)  time: 0.5104  data: 0.0001  max mem: 9341
[20:43:40.510098] Epoch: [253] Total time: 0:01:40 (0.5177 s / it)
[20:43:40.522125] Averaged stats: lr: 0.001960  loss: 0.3900 (0.3821)
[20:43:45.203431] {"train_lr": 0.00196238480562167, "train_loss": 0.3820555492471426, "epoch": 253}
[20:43:45.203793] [20:43:45.203884] Training epoch 253 for 0:01:45
[20:43:45.203940] [20:43:45.208580] log_dir: ./exp/debug/cifar100-LT/debug
[20:43:46.844219] Epoch: [254]  [  0/195]  eta: 0:05:18  lr: 0.001960  loss: 0.3330 (0.3330)  time: 1.6340  data: 1.1107  max mem: 9341
[20:43:57.062935] Epoch: [254]  [ 20/195]  eta: 0:01:38  lr: 0.001960  loss: 0.3723 (0.3796)  time: 0.5108  data: 0.0002  max mem: 9341
[20:44:07.272469] Epoch: [254]  [ 40/195]  eta: 0:01:23  lr: 0.001960  loss: 0.3889 (0.3821)  time: 0.5104  data: 0.0002  max mem: 9341
[20:44:17.495191] Epoch: [254]  [ 60/195]  eta: 0:01:11  lr: 0.001959  loss: 0.3785 (0.3809)  time: 0.5111  data: 0.0002  max mem: 9341
[20:44:27.778361] Epoch: [254]  [ 80/195]  eta: 0:01:00  lr: 0.001959  loss: 0.3845 (0.3821)  time: 0.5141  data: 0.0002  max mem: 9341
[20:44:37.991692] Epoch: [254]  [100/195]  eta: 0:00:49  lr: 0.001958  loss: 0.3809 (0.3811)  time: 0.5106  data: 0.0002  max mem: 9341
[20:44:48.204067] Epoch: [254]  [120/195]  eta: 0:00:39  lr: 0.001958  loss: 0.3885 (0.3832)  time: 0.5106  data: 0.0002  max mem: 9341
[20:44:58.414945] Epoch: [254]  [140/195]  eta: 0:00:28  lr: 0.001958  loss: 0.3777 (0.3831)  time: 0.5105  data: 0.0002  max mem: 9341
[20:45:08.671526] Epoch: [254]  [160/195]  eta: 0:00:18  lr: 0.001957  loss: 0.3740 (0.3823)  time: 0.5128  data: 0.0002  max mem: 9341
[20:45:18.845838] Epoch: [254]  [180/195]  eta: 0:00:07  lr: 0.001957  loss: 0.3811 (0.3823)  time: 0.5087  data: 0.0001  max mem: 9341
[20:45:25.981445] Epoch: [254]  [194/195]  eta: 0:00:00  lr: 0.001957  loss: 0.3831 (0.3829)  time: 0.5108  data: 0.0001  max mem: 9341
[20:45:26.147609] Epoch: [254] Total time: 0:01:40 (0.5176 s / it)
[20:45:26.154843] Averaged stats: lr: 0.001957  loss: 0.3831 (0.3822)
[20:45:30.920862] {"train_lr": 0.001958547639141544, "train_loss": 0.38222040973412685, "epoch": 254}
[20:45:30.921168] [20:45:30.921251] Training epoch 254 for 0:01:45
[20:45:30.921304] [20:45:30.925901] log_dir: ./exp/debug/cifar100-LT/debug
[20:45:32.678621] Epoch: [255]  [  0/195]  eta: 0:05:41  lr: 0.001956  loss: 0.4010 (0.4010)  time: 1.7519  data: 1.2453  max mem: 9341
[20:45:42.897028] Epoch: [255]  [ 20/195]  eta: 0:01:39  lr: 0.001956  loss: 0.3749 (0.3835)  time: 0.5108  data: 0.0002  max mem: 9341
[20:45:53.108137] Epoch: [255]  [ 40/195]  eta: 0:01:23  lr: 0.001956  loss: 0.3870 (0.3883)  time: 0.5105  data: 0.0002  max mem: 9341
[20:46:03.322828] Epoch: [255]  [ 60/195]  eta: 0:01:11  lr: 0.001956  loss: 0.3825 (0.3885)  time: 0.5107  data: 0.0002  max mem: 9341
[20:46:13.603752] Epoch: [255]  [ 80/195]  eta: 0:01:00  lr: 0.001955  loss: 0.3762 (0.3855)  time: 0.5140  data: 0.0002  max mem: 9341
[20:46:23.814550] Epoch: [255]  [100/195]  eta: 0:00:49  lr: 0.001955  loss: 0.3900 (0.3873)  time: 0.5105  data: 0.0002  max mem: 9341
[20:46:34.024227] Epoch: [255]  [120/195]  eta: 0:00:39  lr: 0.001954  loss: 0.3992 (0.3883)  time: 0.5104  data: 0.0002  max mem: 9341
[20:46:44.235627] Epoch: [255]  [140/195]  eta: 0:00:28  lr: 0.001954  loss: 0.3804 (0.3876)  time: 0.5105  data: 0.0002  max mem: 9341
[20:46:54.494466] Epoch: [255]  [160/195]  eta: 0:00:18  lr: 0.001953  loss: 0.3755 (0.3858)  time: 0.5129  data: 0.0002  max mem: 9341
[20:47:04.671855] Epoch: [255]  [180/195]  eta: 0:00:07  lr: 0.001953  loss: 0.3802 (0.3859)  time: 0.5088  data: 0.0001  max mem: 9341
[20:47:11.809488] Epoch: [255]  [194/195]  eta: 0:00:00  lr: 0.001953  loss: 0.3844 (0.3860)  time: 0.5110  data: 0.0001  max mem: 9341
[20:47:11.987124] Epoch: [255] Total time: 0:01:41 (0.5183 s / it)
[20:47:11.994701] Averaged stats: lr: 0.001953  loss: 0.3844 (0.3837)
[20:47:16.767083] {"train_lr": 0.001954697511175171, "train_loss": 0.38365877817074456, "epoch": 255}
[20:47:16.767352] [20:47:16.767447] Training epoch 255 for 0:01:45
[20:47:16.767499] [20:47:16.772074] log_dir: ./exp/debug/cifar100-LT/debug
[20:47:18.448246] Epoch: [256]  [  0/195]  eta: 0:05:26  lr: 0.001953  loss: 0.4188 (0.4188)  time: 1.6748  data: 1.1593  max mem: 9341
[20:47:28.657373] Epoch: [256]  [ 20/195]  eta: 0:01:39  lr: 0.001952  loss: 0.3852 (0.3878)  time: 0.5104  data: 0.0002  max mem: 9341
[20:47:38.873747] Epoch: [256]  [ 40/195]  eta: 0:01:23  lr: 0.001952  loss: 0.3779 (0.3833)  time: 0.5108  data: 0.0002  max mem: 9341
[20:47:49.090660] Epoch: [256]  [ 60/195]  eta: 0:01:11  lr: 0.001952  loss: 0.3681 (0.3811)  time: 0.5108  data: 0.0002  max mem: 9341
[20:47:59.351714] Epoch: [256]  [ 80/195]  eta: 0:01:00  lr: 0.001951  loss: 0.3943 (0.3821)  time: 0.5130  data: 0.0002  max mem: 9341
[20:48:09.567211] Epoch: [256]  [100/195]  eta: 0:00:49  lr: 0.001951  loss: 0.3784 (0.3816)  time: 0.5107  data: 0.0002  max mem: 9341
[20:48:19.780636] Epoch: [256]  [120/195]  eta: 0:00:39  lr: 0.001950  loss: 0.3802 (0.3815)  time: 0.5106  data: 0.0002  max mem: 9341
[20:48:29.997473] Epoch: [256]  [140/195]  eta: 0:00:28  lr: 0.001950  loss: 0.3909 (0.3819)  time: 0.5108  data: 0.0002  max mem: 9341
[20:48:40.258789] Epoch: [256]  [160/195]  eta: 0:00:18  lr: 0.001949  loss: 0.3865 (0.3822)  time: 0.5130  data: 0.0002  max mem: 9341
[20:48:50.429162] Epoch: [256]  [180/195]  eta: 0:00:07  lr: 0.001949  loss: 0.3636 (0.3808)  time: 0.5085  data: 0.0001  max mem: 9341
[20:48:57.558732] Epoch: [256]  [194/195]  eta: 0:00:00  lr: 0.001949  loss: 0.3782 (0.3805)  time: 0.5104  data: 0.0001  max mem: 9341
[20:48:57.738099] Epoch: [256] Total time: 0:01:40 (0.5178 s / it)
[20:48:57.746513] Averaged stats: lr: 0.001949  loss: 0.3782 (0.3804)
[20:49:02.444164] {"train_lr": 0.0019508344875106137, "train_loss": 0.38035004505744346, "epoch": 256}
[20:49:02.444493] [20:49:02.444577] Training epoch 256 for 0:01:45
[20:49:02.444630] [20:49:02.449113] log_dir: ./exp/debug/cifar100-LT/debug
[20:49:04.358170] Epoch: [257]  [  0/195]  eta: 0:06:11  lr: 0.001949  loss: 0.3631 (0.3631)  time: 1.9073  data: 1.4179  max mem: 9341
[20:49:14.575134] Epoch: [257]  [ 20/195]  eta: 0:01:41  lr: 0.001948  loss: 0.3899 (0.3880)  time: 0.5108  data: 0.0002  max mem: 9341
[20:49:24.791896] Epoch: [257]  [ 40/195]  eta: 0:01:24  lr: 0.001948  loss: 0.3835 (0.3881)  time: 0.5108  data: 0.0002  max mem: 9341
[20:49:35.002177] Epoch: [257]  [ 60/195]  eta: 0:01:12  lr: 0.001948  loss: 0.3734 (0.3851)  time: 0.5105  data: 0.0002  max mem: 9341
[20:49:45.264584] Epoch: [257]  [ 80/195]  eta: 0:01:00  lr: 0.001947  loss: 0.3902 (0.3871)  time: 0.5131  data: 0.0002  max mem: 9341
[20:49:55.481043] Epoch: [257]  [100/195]  eta: 0:00:49  lr: 0.001947  loss: 0.3639 (0.3830)  time: 0.5107  data: 0.0002  max mem: 9341
[20:50:05.694908] Epoch: [257]  [120/195]  eta: 0:00:39  lr: 0.001947  loss: 0.3708 (0.3820)  time: 0.5106  data: 0.0002  max mem: 9341
[20:50:15.916311] Epoch: [257]  [140/195]  eta: 0:00:28  lr: 0.001946  loss: 0.4178 (0.3864)  time: 0.5110  data: 0.0002  max mem: 9341
[20:50:26.174994] Epoch: [257]  [160/195]  eta: 0:00:18  lr: 0.001946  loss: 0.3884 (0.3875)  time: 0.5129  data: 0.0002  max mem: 9341
[20:50:36.370597] Epoch: [257]  [180/195]  eta: 0:00:07  lr: 0.001945  loss: 0.4078 (0.3888)  time: 0.5097  data: 0.0001  max mem: 9341
[20:50:43.501984] Epoch: [257]  [194/195]  eta: 0:00:00  lr: 0.001945  loss: 0.3963 (0.3902)  time: 0.5117  data: 0.0001  max mem: 9341
[20:50:43.673626] Epoch: [257] Total time: 0:01:41 (0.5191 s / it)
[20:50:43.707960] Averaged stats: lr: 0.001945  loss: 0.3963 (0.3892)
[20:50:48.529426] {"train_lr": 0.001946958634156289, "train_loss": 0.38922133078941934, "epoch": 257}
[20:50:48.529703] [20:50:48.529787] Training epoch 257 for 0:01:46
[20:50:48.529839] [20:50:48.534397] log_dir: ./exp/debug/cifar100-LT/debug
[20:50:50.110869] Epoch: [258]  [  0/195]  eta: 0:05:07  lr: 0.001945  loss: 0.3843 (0.3843)  time: 1.5755  data: 1.0754  max mem: 9341
[20:51:00.336507] Epoch: [258]  [ 20/195]  eta: 0:01:38  lr: 0.001945  loss: 0.3950 (0.3938)  time: 0.5112  data: 0.0002  max mem: 9341
[20:51:10.554658] Epoch: [258]  [ 40/195]  eta: 0:01:23  lr: 0.001944  loss: 0.3959 (0.3978)  time: 0.5108  data: 0.0002  max mem: 9341
[20:51:20.769728] Epoch: [258]  [ 60/195]  eta: 0:01:11  lr: 0.001944  loss: 0.4028 (0.3992)  time: 0.5107  data: 0.0002  max mem: 9341
[20:51:31.072373] Epoch: [258]  [ 80/195]  eta: 0:01:00  lr: 0.001943  loss: 0.3885 (0.3974)  time: 0.5151  data: 0.0002  max mem: 9341
[20:51:41.282947] Epoch: [258]  [100/195]  eta: 0:00:49  lr: 0.001943  loss: 0.3835 (0.3962)  time: 0.5105  data: 0.0002  max mem: 9341
[20:51:51.501075] Epoch: [258]  [120/195]  eta: 0:00:39  lr: 0.001943  loss: 0.3985 (0.3961)  time: 0.5108  data: 0.0002  max mem: 9341
[20:52:01.714890] Epoch: [258]  [140/195]  eta: 0:00:28  lr: 0.001942  loss: 0.3846 (0.3949)  time: 0.5106  data: 0.0002  max mem: 9341
[20:52:11.975964] Epoch: [258]  [160/195]  eta: 0:00:18  lr: 0.001942  loss: 0.3808 (0.3939)  time: 0.5130  data: 0.0002  max mem: 9341
[20:52:22.146215] Epoch: [258]  [180/195]  eta: 0:00:07  lr: 0.001941  loss: 0.3826 (0.3931)  time: 0.5085  data: 0.0001  max mem: 9341
[20:52:29.277741] Epoch: [258]  [194/195]  eta: 0:00:00  lr: 0.001941  loss: 0.3873 (0.3922)  time: 0.5104  data: 0.0001  max mem: 9341
[20:52:29.460047] Epoch: [258] Total time: 0:01:40 (0.5176 s / it)
[20:52:29.475389] Averaged stats: lr: 0.001941  loss: 0.3873 (0.3920)
[20:52:34.217207] {"train_lr": 0.0019430700173398367, "train_loss": 0.3920199040419016, "epoch": 258}
[20:52:34.217576] [20:52:34.217666] Training epoch 258 for 0:01:45
[20:52:34.217720] [20:52:34.222341] log_dir: ./exp/debug/cifar100-LT/debug
[20:52:35.993876] Epoch: [259]  [  0/195]  eta: 0:05:45  lr: 0.001941  loss: 0.3839 (0.3839)  time: 1.7703  data: 1.2603  max mem: 9341
[20:52:46.215740] Epoch: [259]  [ 20/195]  eta: 0:01:39  lr: 0.001941  loss: 0.3873 (0.3827)  time: 0.5110  data: 0.0002  max mem: 9341
[20:52:56.432191] Epoch: [259]  [ 40/195]  eta: 0:01:23  lr: 0.001940  loss: 0.3721 (0.3807)  time: 0.5108  data: 0.0002  max mem: 9341
[20:53:06.648866] Epoch: [259]  [ 60/195]  eta: 0:01:11  lr: 0.001940  loss: 0.3816 (0.3819)  time: 0.5108  data: 0.0002  max mem: 9341
[20:53:16.901814] Epoch: [259]  [ 80/195]  eta: 0:01:00  lr: 0.001939  loss: 0.3831 (0.3819)  time: 0.5126  data: 0.0002  max mem: 9341
[20:53:27.138869] Epoch: [259]  [100/195]  eta: 0:00:49  lr: 0.001939  loss: 0.3707 (0.3803)  time: 0.5118  data: 0.0002  max mem: 9341
[20:53:37.379501] Epoch: [259]  [120/195]  eta: 0:00:39  lr: 0.001939  loss: 0.3869 (0.3808)  time: 0.5120  data: 0.0002  max mem: 9341
[20:53:47.618131] Epoch: [259]  [140/195]  eta: 0:00:28  lr: 0.001938  loss: 0.3824 (0.3810)  time: 0.5119  data: 0.0002  max mem: 9341
[20:53:57.880810] Epoch: [259]  [160/195]  eta: 0:00:18  lr: 0.001938  loss: 0.3751 (0.3810)  time: 0.5131  data: 0.0002  max mem: 9341
[20:54:08.054144] Epoch: [259]  [180/195]  eta: 0:00:07  lr: 0.001937  loss: 0.3836 (0.3814)  time: 0.5086  data: 0.0001  max mem: 9341
[20:54:15.180332] Epoch: [259]  [194/195]  eta: 0:00:00  lr: 0.001937  loss: 0.3821 (0.3807)  time: 0.5104  data: 0.0001  max mem: 9341
[20:54:15.333206] Epoch: [259] Total time: 0:01:41 (0.5185 s / it)
[20:54:15.364904] Averaged stats: lr: 0.001937  loss: 0.3821 (0.3814)
[20:54:20.059269] {"train_lr": 0.0019391687035069853, "train_loss": 0.3814401917350598, "epoch": 259}
[20:54:20.059561] [20:54:20.059650] Training epoch 259 for 0:01:45
[20:54:20.059702] [20:54:20.064745] log_dir: ./exp/debug/cifar100-LT/debug
[20:54:21.849689] Epoch: [260]  [  0/195]  eta: 0:05:47  lr: 0.001937  loss: 0.3159 (0.3159)  time: 1.7836  data: 1.2881  max mem: 9341
[20:54:32.064541] Epoch: [260]  [ 20/195]  eta: 0:01:39  lr: 0.001937  loss: 0.3643 (0.3678)  time: 0.5107  data: 0.0002  max mem: 9341
[20:54:42.279501] Epoch: [260]  [ 40/195]  eta: 0:01:23  lr: 0.001936  loss: 0.3755 (0.3703)  time: 0.5107  data: 0.0002  max mem: 9341
[20:54:52.494038] Epoch: [260]  [ 60/195]  eta: 0:01:11  lr: 0.001936  loss: 0.3591 (0.3674)  time: 0.5107  data: 0.0002  max mem: 9341
[20:55:02.750459] Epoch: [260]  [ 80/195]  eta: 0:01:00  lr: 0.001935  loss: 0.3727 (0.3697)  time: 0.5128  data: 0.0002  max mem: 9341
[20:55:12.959613] Epoch: [260]  [100/195]  eta: 0:00:49  lr: 0.001935  loss: 0.3743 (0.3699)  time: 0.5104  data: 0.0002  max mem: 9341
[20:55:23.174285] Epoch: [260]  [120/195]  eta: 0:00:39  lr: 0.001935  loss: 0.3693 (0.3704)  time: 0.5107  data: 0.0002  max mem: 9341
[20:55:33.388161] Epoch: [260]  [140/195]  eta: 0:00:28  lr: 0.001934  loss: 0.3778 (0.3721)  time: 0.5106  data: 0.0002  max mem: 9341
[20:55:43.639298] Epoch: [260]  [160/195]  eta: 0:00:18  lr: 0.001934  loss: 0.3807 (0.3738)  time: 0.5125  data: 0.0002  max mem: 9341
[20:55:53.813679] Epoch: [260]  [180/195]  eta: 0:00:07  lr: 0.001934  loss: 0.3694 (0.3733)  time: 0.5087  data: 0.0001  max mem: 9341
[20:56:00.942016] Epoch: [260]  [194/195]  eta: 0:00:00  lr: 0.001933  loss: 0.3695 (0.3733)  time: 0.5105  data: 0.0001  max mem: 9341
[20:56:01.140273] Epoch: [260] Total time: 0:01:41 (0.5183 s / it)
[20:56:01.151377] Averaged stats: lr: 0.001933  loss: 0.3695 (0.3747)
[20:56:05.866329] {"train_lr": 0.0019352547593204262, "train_loss": 0.37470749693039135, "epoch": 260}
[20:56:05.866593] [20:56:05.866675] Training epoch 260 for 0:01:45
[20:56:05.866728] [20:56:05.871302] log_dir: ./exp/debug/cifar100-LT/debug
[20:56:07.703545] Epoch: [261]  [  0/195]  eta: 0:05:57  lr: 0.001933  loss: 0.3942 (0.3942)  time: 1.8313  data: 1.3408  max mem: 9341
[20:56:17.921172] Epoch: [261]  [ 20/195]  eta: 0:01:40  lr: 0.001933  loss: 0.3708 (0.3786)  time: 0.5108  data: 0.0002  max mem: 9341
[20:56:28.138054] Epoch: [261]  [ 40/195]  eta: 0:01:24  lr: 0.001932  loss: 0.3865 (0.3836)  time: 0.5108  data: 0.0002  max mem: 9341
[20:56:38.359260] Epoch: [261]  [ 60/195]  eta: 0:01:11  lr: 0.001932  loss: 0.3797 (0.3841)  time: 0.5110  data: 0.0002  max mem: 9341
[20:56:48.619183] Epoch: [261]  [ 80/195]  eta: 0:01:00  lr: 0.001932  loss: 0.3718 (0.3810)  time: 0.5129  data: 0.0002  max mem: 9341
[20:56:58.854928] Epoch: [261]  [100/195]  eta: 0:00:49  lr: 0.001931  loss: 0.3924 (0.3822)  time: 0.5117  data: 0.0002  max mem: 9341
[20:57:09.072253] Epoch: [261]  [120/195]  eta: 0:00:39  lr: 0.001931  loss: 0.3727 (0.3813)  time: 0.5108  data: 0.0002  max mem: 9341
[20:57:19.292073] Epoch: [261]  [140/195]  eta: 0:00:28  lr: 0.001931  loss: 0.3742 (0.3807)  time: 0.5109  data: 0.0002  max mem: 9341
[20:57:29.572065] Epoch: [261]  [160/195]  eta: 0:00:18  lr: 0.001930  loss: 0.3844 (0.3803)  time: 0.5139  data: 0.0002  max mem: 9341
[20:57:39.748016] Epoch: [261]  [180/195]  eta: 0:00:07  lr: 0.001930  loss: 0.3834 (0.3805)  time: 0.5087  data: 0.0001  max mem: 9341
[20:57:46.881193] Epoch: [261]  [194/195]  eta: 0:00:00  lr: 0.001929  loss: 0.3725 (0.3799)  time: 0.5108  data: 0.0001  max mem: 9341
[20:57:47.048159] Epoch: [261] Total time: 0:01:41 (0.5189 s / it)
[20:57:47.059784] Averaged stats: lr: 0.001929  loss: 0.3725 (0.3804)
[20:57:51.747960] {"train_lr": 0.0019313282516586598, "train_loss": 0.3804347188044817, "epoch": 261}
[20:57:51.748331] [20:57:51.748437] Training epoch 261 for 0:01:45
[20:57:51.748494] [20:57:51.754718] log_dir: ./exp/debug/cifar100-LT/debug
[20:57:53.624731] Epoch: [262]  [  0/195]  eta: 0:06:04  lr: 0.001929  loss: 0.3592 (0.3592)  time: 1.8690  data: 1.3690  max mem: 9341
[20:58:03.876503] Epoch: [262]  [ 20/195]  eta: 0:01:40  lr: 0.001929  loss: 0.3763 (0.3738)  time: 0.5125  data: 0.0003  max mem: 9341
[20:58:14.109789] Epoch: [262]  [ 40/195]  eta: 0:01:24  lr: 0.001929  loss: 0.3638 (0.3729)  time: 0.5116  data: 0.0002  max mem: 9341
[20:58:24.355328] Epoch: [262]  [ 60/195]  eta: 0:01:12  lr: 0.001928  loss: 0.3713 (0.3733)  time: 0.5122  data: 0.0002  max mem: 9341
[20:58:34.656924] Epoch: [262]  [ 80/195]  eta: 0:01:00  lr: 0.001928  loss: 0.3681 (0.3723)  time: 0.5150  data: 0.0003  max mem: 9341
[20:58:44.888578] Epoch: [262]  [100/195]  eta: 0:00:49  lr: 0.001927  loss: 0.3688 (0.3719)  time: 0.5115  data: 0.0003  max mem: 9341
[20:58:55.133395] Epoch: [262]  [120/195]  eta: 0:00:39  lr: 0.001927  loss: 0.3841 (0.3737)  time: 0.5122  data: 0.0003  max mem: 9341
[20:59:05.370039] Epoch: [262]  [140/195]  eta: 0:00:28  lr: 0.001927  loss: 0.3737 (0.3742)  time: 0.5118  data: 0.0003  max mem: 9341
[20:59:15.674579] Epoch: [262]  [160/195]  eta: 0:00:18  lr: 0.001926  loss: 0.3670 (0.3734)  time: 0.5152  data: 0.0003  max mem: 9341
[20:59:25.864122] Epoch: [262]  [180/195]  eta: 0:00:07  lr: 0.001926  loss: 0.3918 (0.3745)  time: 0.5094  data: 0.0002  max mem: 9341
[20:59:33.012798] Epoch: [262]  [194/195]  eta: 0:00:00  lr: 0.001925  loss: 0.3850 (0.3747)  time: 0.5124  data: 0.0001  max mem: 9341
[20:59:33.195418] Epoch: [262] Total time: 0:01:41 (0.5202 s / it)
[20:59:33.207426] Averaged stats: lr: 0.001925  loss: 0.3850 (0.3746)
[20:59:38.037564] {"train_lr": 0.0019273892476148957, "train_loss": 0.3746247359193288, "epoch": 262}
[20:59:38.037861] [20:59:38.037967] Training epoch 262 for 0:01:46
[20:59:38.038022] [20:59:38.043556] log_dir: ./exp/debug/cifar100-LT/debug
[20:59:39.880455] Epoch: [263]  [  0/195]  eta: 0:05:58  lr: 0.001925  loss: 0.4390 (0.4390)  time: 1.8360  data: 1.3441  max mem: 9341
[20:59:50.139003] Epoch: [263]  [ 20/195]  eta: 0:01:40  lr: 0.001925  loss: 0.3698 (0.3766)  time: 0.5129  data: 0.0002  max mem: 9341
[21:00:00.350901] Epoch: [263]  [ 40/195]  eta: 0:01:24  lr: 0.001925  loss: 0.3645 (0.3731)  time: 0.5105  data: 0.0002  max mem: 9341
[21:00:10.558884] Epoch: [263]  [ 60/195]  eta: 0:01:11  lr: 0.001924  loss: 0.3578 (0.3699)  time: 0.5103  data: 0.0002  max mem: 9341
[21:00:20.816095] Epoch: [263]  [ 80/195]  eta: 0:01:00  lr: 0.001924  loss: 0.3619 (0.3696)  time: 0.5128  data: 0.0002  max mem: 9341
[21:00:31.021266] Epoch: [263]  [100/195]  eta: 0:00:49  lr: 0.001923  loss: 0.3779 (0.3704)  time: 0.5102  data: 0.0002  max mem: 9341
[21:00:41.230201] Epoch: [263]  [120/195]  eta: 0:00:39  lr: 0.001923  loss: 0.3789 (0.3721)  time: 0.5104  data: 0.0002  max mem: 9341
[21:00:51.434613] Epoch: [263]  [140/195]  eta: 0:00:28  lr: 0.001923  loss: 0.3698 (0.3719)  time: 0.5102  data: 0.0002  max mem: 9341
[21:01:01.689912] Epoch: [263]  [160/195]  eta: 0:00:18  lr: 0.001922  loss: 0.3735 (0.3720)  time: 0.5127  data: 0.0002  max mem: 9341
[21:01:11.851188] Epoch: [263]  [180/195]  eta: 0:00:07  lr: 0.001922  loss: 0.3731 (0.3723)  time: 0.5080  data: 0.0001  max mem: 9341
[21:01:18.974897] Epoch: [263]  [194/195]  eta: 0:00:00  lr: 0.001921  loss: 0.3663 (0.3721)  time: 0.5099  data: 0.0001  max mem: 9341
[21:01:19.149323] Epoch: [263] Total time: 0:01:41 (0.5185 s / it)
[21:01:19.162099] Averaged stats: lr: 0.001921  loss: 0.3663 (0.3738)
[21:01:23.851725] {"train_lr": 0.001923437814495803, "train_loss": 0.37383921788289, "epoch": 263}
[21:01:23.852039] [21:01:23.852151] Training epoch 263 for 0:01:45
[21:01:23.852208] [21:01:23.856648] log_dir: ./exp/debug/cifar100-LT/debug
[21:01:25.617336] Epoch: [264]  [  0/195]  eta: 0:05:43  lr: 0.001921  loss: 0.3835 (0.3835)  time: 1.7599  data: 1.2597  max mem: 9341
[21:01:35.833400] Epoch: [264]  [ 20/195]  eta: 0:01:39  lr: 0.001921  loss: 0.3733 (0.3782)  time: 0.5107  data: 0.0002  max mem: 9341
[21:01:46.052782] Epoch: [264]  [ 40/195]  eta: 0:01:23  lr: 0.001921  loss: 0.3741 (0.3750)  time: 0.5109  data: 0.0002  max mem: 9341
[21:01:56.261637] Epoch: [264]  [ 60/195]  eta: 0:01:11  lr: 0.001920  loss: 0.3791 (0.3760)  time: 0.5104  data: 0.0002  max mem: 9341
[21:02:06.519865] Epoch: [264]  [ 80/195]  eta: 0:01:00  lr: 0.001920  loss: 0.3654 (0.3731)  time: 0.5129  data: 0.0002  max mem: 9341
[21:02:16.731052] Epoch: [264]  [100/195]  eta: 0:00:49  lr: 0.001919  loss: 0.3700 (0.3737)  time: 0.5105  data: 0.0002  max mem: 9341
[21:02:26.942151] Epoch: [264]  [120/195]  eta: 0:00:39  lr: 0.001919  loss: 0.3702 (0.3733)  time: 0.5105  data: 0.0002  max mem: 9341
[21:02:37.154508] Epoch: [264]  [140/195]  eta: 0:00:28  lr: 0.001919  loss: 0.3679 (0.3737)  time: 0.5106  data: 0.0002  max mem: 9341
[21:02:47.413184] Epoch: [264]  [160/195]  eta: 0:00:18  lr: 0.001918  loss: 0.3683 (0.3731)  time: 0.5129  data: 0.0002  max mem: 9341
[21:02:57.586146] Epoch: [264]  [180/195]  eta: 0:00:07  lr: 0.001918  loss: 0.3790 (0.3738)  time: 0.5086  data: 0.0002  max mem: 9341
[21:03:04.719638] Epoch: [264]  [194/195]  eta: 0:00:00  lr: 0.001917  loss: 0.3783 (0.3731)  time: 0.5106  data: 0.0001  max mem: 9341
[21:03:04.885773] Epoch: [264] Total time: 0:01:41 (0.5181 s / it)
[21:03:04.894237] Averaged stats: lr: 0.001917  loss: 0.3783 (0.3725)
[21:03:09.608155] {"train_lr": 0.001919474019820509, "train_loss": 0.3724964123123731, "epoch": 264}
[21:03:09.608483] [21:03:09.608570] Training epoch 264 for 0:01:45
[21:03:09.608624] [21:03:09.613113] log_dir: ./exp/debug/cifar100-LT/debug
[21:03:11.152753] Epoch: [265]  [  0/195]  eta: 0:04:59  lr: 0.001917  loss: 0.3638 (0.3638)  time: 1.5383  data: 1.0506  max mem: 9341
[21:03:21.369954] Epoch: [265]  [ 20/195]  eta: 0:01:37  lr: 0.001917  loss: 0.3710 (0.3746)  time: 0.5108  data: 0.0002  max mem: 9341
[21:03:31.585588] Epoch: [265]  [ 40/195]  eta: 0:01:23  lr: 0.001917  loss: 0.3771 (0.3767)  time: 0.5107  data: 0.0002  max mem: 9341
[21:03:41.805878] Epoch: [265]  [ 60/195]  eta: 0:01:11  lr: 0.001916  loss: 0.3802 (0.3770)  time: 0.5110  data: 0.0002  max mem: 9341
[21:03:52.072067] Epoch: [265]  [ 80/195]  eta: 0:01:00  lr: 0.001916  loss: 0.3644 (0.3742)  time: 0.5133  data: 0.0002  max mem: 9341
[21:04:02.286100] Epoch: [265]  [100/195]  eta: 0:00:49  lr: 0.001915  loss: 0.3695 (0.3739)  time: 0.5106  data: 0.0002  max mem: 9341
[21:04:12.508374] Epoch: [265]  [120/195]  eta: 0:00:38  lr: 0.001915  loss: 0.3680 (0.3736)  time: 0.5111  data: 0.0002  max mem: 9341
[21:04:22.726161] Epoch: [265]  [140/195]  eta: 0:00:28  lr: 0.001915  loss: 0.3605 (0.3716)  time: 0.5108  data: 0.0002  max mem: 9341
[21:04:32.984935] Epoch: [265]  [160/195]  eta: 0:00:18  lr: 0.001914  loss: 0.3654 (0.3719)  time: 0.5129  data: 0.0002  max mem: 9341
[21:04:43.161800] Epoch: [265]  [180/195]  eta: 0:00:07  lr: 0.001914  loss: 0.3868 (0.3732)  time: 0.5088  data: 0.0001  max mem: 9341
[21:04:50.296592] Epoch: [265]  [194/195]  eta: 0:00:00  lr: 0.001913  loss: 0.3774 (0.3735)  time: 0.5106  data: 0.0001  max mem: 9341
[21:04:50.464995] Epoch: [265] Total time: 0:01:40 (0.5172 s / it)
[21:04:50.481345] Averaged stats: lr: 0.001913  loss: 0.3774 (0.3725)
[21:04:55.193667] {"train_lr": 0.001915497931319303, "train_loss": 0.3724721873035798, "epoch": 265}
[21:04:55.193937] [21:04:55.194021] Training epoch 265 for 0:01:45
[21:04:55.194074] [21:04:55.198606] log_dir: ./exp/debug/cifar100-LT/debug
[21:04:56.828322] Epoch: [266]  [  0/195]  eta: 0:05:17  lr: 0.001913  loss: 0.3416 (0.3416)  time: 1.6276  data: 1.1273  max mem: 9341
[21:05:07.047421] Epoch: [266]  [ 20/195]  eta: 0:01:38  lr: 0.001913  loss: 0.3722 (0.3737)  time: 0.5109  data: 0.0002  max mem: 9341
[21:05:17.287301] Epoch: [266]  [ 40/195]  eta: 0:01:23  lr: 0.001913  loss: 0.3669 (0.3733)  time: 0.5119  data: 0.0002  max mem: 9341
[21:05:27.516580] Epoch: [266]  [ 60/195]  eta: 0:01:11  lr: 0.001912  loss: 0.3677 (0.3736)  time: 0.5114  data: 0.0002  max mem: 9341
[21:05:37.814120] Epoch: [266]  [ 80/195]  eta: 0:01:00  lr: 0.001912  loss: 0.3808 (0.3745)  time: 0.5148  data: 0.0002  max mem: 9341
[21:05:48.045083] Epoch: [266]  [100/195]  eta: 0:00:49  lr: 0.001911  loss: 0.3641 (0.3717)  time: 0.5115  data: 0.0002  max mem: 9341
[21:05:58.281931] Epoch: [266]  [120/195]  eta: 0:00:39  lr: 0.001911  loss: 0.3743 (0.3725)  time: 0.5118  data: 0.0002  max mem: 9341
[21:06:08.515430] Epoch: [266]  [140/195]  eta: 0:00:28  lr: 0.001911  loss: 0.3650 (0.3723)  time: 0.5116  data: 0.0002  max mem: 9341
[21:06:18.813446] Epoch: [266]  [160/195]  eta: 0:00:18  lr: 0.001910  loss: 0.3807 (0.3732)  time: 0.5148  data: 0.0002  max mem: 9341
[21:06:28.999611] Epoch: [266]  [180/195]  eta: 0:00:07  lr: 0.001910  loss: 0.3632 (0.3727)  time: 0.5093  data: 0.0001  max mem: 9341
[21:06:36.149273] Epoch: [266]  [194/195]  eta: 0:00:00  lr: 0.001909  loss: 0.3604 (0.3724)  time: 0.5124  data: 0.0001  max mem: 9341
[21:06:36.325832] Epoch: [266] Total time: 0:01:41 (0.5186 s / it)
[21:06:36.355339] Averaged stats: lr: 0.001909  loss: 0.3604 (0.3740)
[21:06:41.282051] {"train_lr": 0.0019115096169325869, "train_loss": 0.374013420748405, "epoch": 266}
[21:06:41.282308] [21:06:41.282392] Training epoch 266 for 0:01:46
[21:06:41.282444] [21:06:41.286955] log_dir: ./exp/debug/cifar100-LT/debug
[21:06:42.916879] Epoch: [267]  [  0/195]  eta: 0:05:17  lr: 0.001909  loss: 0.4033 (0.4033)  time: 1.6289  data: 1.1274  max mem: 9341
[21:06:53.151920] Epoch: [267]  [ 20/195]  eta: 0:01:38  lr: 0.001909  loss: 0.3677 (0.3777)  time: 0.5117  data: 0.0002  max mem: 9341
[21:07:03.359602] Epoch: [267]  [ 40/195]  eta: 0:01:23  lr: 0.001909  loss: 0.3680 (0.3727)  time: 0.5103  data: 0.0002  max mem: 9341
[21:07:13.573851] Epoch: [267]  [ 60/195]  eta: 0:01:11  lr: 0.001908  loss: 0.3689 (0.3721)  time: 0.5106  data: 0.0002  max mem: 9341
[21:07:23.828256] Epoch: [267]  [ 80/195]  eta: 0:01:00  lr: 0.001908  loss: 0.3653 (0.3708)  time: 0.5126  data: 0.0002  max mem: 9341
[21:07:34.041973] Epoch: [267]  [100/195]  eta: 0:00:49  lr: 0.001907  loss: 0.3727 (0.3703)  time: 0.5106  data: 0.0002  max mem: 9341
[21:07:44.249479] Epoch: [267]  [120/195]  eta: 0:00:39  lr: 0.001907  loss: 0.3673 (0.3701)  time: 0.5103  data: 0.0002  max mem: 9341
[21:07:54.456968] Epoch: [267]  [140/195]  eta: 0:00:28  lr: 0.001907  loss: 0.3547 (0.3687)  time: 0.5103  data: 0.0002  max mem: 9341
[21:08:04.708502] Epoch: [267]  [160/195]  eta: 0:00:18  lr: 0.001906  loss: 0.3587 (0.3682)  time: 0.5125  data: 0.0002  max mem: 9341
[21:08:14.877754] Epoch: [267]  [180/195]  eta: 0:00:07  lr: 0.001906  loss: 0.3634 (0.3693)  time: 0.5084  data: 0.0001  max mem: 9341
[21:08:22.001275] Epoch: [267]  [194/195]  eta: 0:00:00  lr: 0.001905  loss: 0.3683 (0.3696)  time: 0.5101  data: 0.0001  max mem: 9341
[21:08:22.170900] Epoch: [267] Total time: 0:01:40 (0.5174 s / it)
[21:08:22.188860] Averaged stats: lr: 0.001905  loss: 0.3683 (0.3700)
[21:08:26.903941] {"train_lr": 0.0019075091448096297, "train_loss": 0.37000320851802826, "epoch": 267}
[21:08:26.904281] [21:08:26.904370] Training epoch 267 for 0:01:45
[21:08:26.904424] [21:08:26.908903] log_dir: ./exp/debug/cifar100-LT/debug
[21:08:28.525252] Epoch: [268]  [  0/195]  eta: 0:05:14  lr: 0.001905  loss: 0.3804 (0.3804)  time: 1.6149  data: 1.1080  max mem: 9341
[21:08:38.737168] Epoch: [268]  [ 20/195]  eta: 0:01:38  lr: 0.001905  loss: 0.3860 (0.3808)  time: 0.5105  data: 0.0002  max mem: 9341
[21:08:48.949835] Epoch: [268]  [ 40/195]  eta: 0:01:23  lr: 0.001905  loss: 0.3539 (0.3728)  time: 0.5106  data: 0.0002  max mem: 9341
[21:08:59.162168] Epoch: [268]  [ 60/195]  eta: 0:01:11  lr: 0.001904  loss: 0.3733 (0.3728)  time: 0.5106  data: 0.0002  max mem: 9341
[21:09:09.411993] Epoch: [268]  [ 80/195]  eta: 0:01:00  lr: 0.001904  loss: 0.3719 (0.3734)  time: 0.5124  data: 0.0002  max mem: 9341
[21:09:19.627229] Epoch: [268]  [100/195]  eta: 0:00:49  lr: 0.001903  loss: 0.3748 (0.3727)  time: 0.5107  data: 0.0002  max mem: 9341
[21:09:29.847331] Epoch: [268]  [120/195]  eta: 0:00:39  lr: 0.001903  loss: 0.3727 (0.3727)  time: 0.5109  data: 0.0002  max mem: 9341
[21:09:40.061730] Epoch: [268]  [140/195]  eta: 0:00:28  lr: 0.001903  loss: 0.3615 (0.3722)  time: 0.5107  data: 0.0002  max mem: 9341
[21:09:50.318828] Epoch: [268]  [160/195]  eta: 0:00:18  lr: 0.001902  loss: 0.3726 (0.3721)  time: 0.5128  data: 0.0002  max mem: 9341
[21:10:00.496434] Epoch: [268]  [180/195]  eta: 0:00:07  lr: 0.001902  loss: 0.3760 (0.3717)  time: 0.5088  data: 0.0001  max mem: 9341
[21:10:07.627753] Epoch: [268]  [194/195]  eta: 0:00:00  lr: 0.001901  loss: 0.3764 (0.3720)  time: 0.5106  data: 0.0001  max mem: 9341
[21:10:07.808833] Epoch: [268] Total time: 0:01:40 (0.5174 s / it)
[21:10:07.817828] Averaged stats: lr: 0.001901  loss: 0.3764 (0.3709)
[21:10:12.570929] {"train_lr": 0.0019034965833074752, "train_loss": 0.37093052508739327, "epoch": 268}
[21:10:12.571192] [21:10:12.571278] Training epoch 268 for 0:01:45
[21:10:12.571331] [21:10:12.575740] log_dir: ./exp/debug/cifar100-LT/debug
[21:10:14.344464] Epoch: [269]  [  0/195]  eta: 0:05:44  lr: 0.001901  loss: 0.3649 (0.3649)  time: 1.7675  data: 1.2787  max mem: 9341
[21:10:24.566568] Epoch: [269]  [ 20/195]  eta: 0:01:39  lr: 0.001901  loss: 0.3781 (0.3774)  time: 0.5110  data: 0.0002  max mem: 9341
[21:10:34.808759] Epoch: [269]  [ 40/195]  eta: 0:01:24  lr: 0.001901  loss: 0.3734 (0.3752)  time: 0.5120  data: 0.0002  max mem: 9341
[21:10:45.046303] Epoch: [269]  [ 60/195]  eta: 0:01:11  lr: 0.001900  loss: 0.3699 (0.3727)  time: 0.5118  data: 0.0002  max mem: 9341
[21:10:55.317858] Epoch: [269]  [ 80/195]  eta: 0:01:00  lr: 0.001900  loss: 0.3720 (0.3727)  time: 0.5135  data: 0.0002  max mem: 9341
[21:11:05.533241] Epoch: [269]  [100/195]  eta: 0:00:49  lr: 0.001899  loss: 0.3669 (0.3720)  time: 0.5107  data: 0.0002  max mem: 9341
[21:11:15.770323] Epoch: [269]  [120/195]  eta: 0:00:39  lr: 0.001899  loss: 0.3738 (0.3721)  time: 0.5118  data: 0.0002  max mem: 9341
[21:11:25.985002] Epoch: [269]  [140/195]  eta: 0:00:28  lr: 0.001899  loss: 0.3603 (0.3707)  time: 0.5107  data: 0.0002  max mem: 9341
[21:11:36.243203] Epoch: [269]  [160/195]  eta: 0:00:18  lr: 0.001898  loss: 0.3617 (0.3699)  time: 0.5129  data: 0.0002  max mem: 9341
[21:11:46.416639] Epoch: [269]  [180/195]  eta: 0:00:07  lr: 0.001898  loss: 0.3739 (0.3695)  time: 0.5086  data: 0.0001  max mem: 9341
[21:11:53.546739] Epoch: [269]  [194/195]  eta: 0:00:00  lr: 0.001897  loss: 0.3603 (0.3692)  time: 0.5105  data: 0.0001  max mem: 9341
[21:11:53.725751] Epoch: [269] Total time: 0:01:41 (0.5187 s / it)
[21:11:53.733125] Averaged stats: lr: 0.001897  loss: 0.3603 (0.3683)
[21:11:58.421202] {"train_lr": 0.0018994720009897258, "train_loss": 0.36832516136077736, "epoch": 269}
[21:11:58.421535] [21:11:58.421622] Training epoch 269 for 0:01:45
[21:11:58.421675] [21:11:58.426130] log_dir: ./exp/debug/cifar100-LT/debug
[21:12:00.213901] Epoch: [270]  [  0/195]  eta: 0:05:48  lr: 0.001897  loss: 0.3681 (0.3681)  time: 1.7861  data: 1.2675  max mem: 9341
[21:12:10.431136] Epoch: [270]  [ 20/195]  eta: 0:01:40  lr: 0.001897  loss: 0.3766 (0.3734)  time: 0.5108  data: 0.0002  max mem: 9341
[21:12:20.652974] Epoch: [270]  [ 40/195]  eta: 0:01:24  lr: 0.001897  loss: 0.3643 (0.3712)  time: 0.5110  data: 0.0002  max mem: 9341
[21:12:30.868501] Epoch: [270]  [ 60/195]  eta: 0:01:11  lr: 0.001896  loss: 0.3646 (0.3716)  time: 0.5107  data: 0.0002  max mem: 9341
[21:12:41.131467] Epoch: [270]  [ 80/195]  eta: 0:01:00  lr: 0.001896  loss: 0.3717 (0.3712)  time: 0.5131  data: 0.0002  max mem: 9341
[21:12:51.375207] Epoch: [270]  [100/195]  eta: 0:00:49  lr: 0.001895  loss: 0.3695 (0.3717)  time: 0.5121  data: 0.0002  max mem: 9341
[21:13:01.618211] Epoch: [270]  [120/195]  eta: 0:00:39  lr: 0.001895  loss: 0.3750 (0.3721)  time: 0.5121  data: 0.0002  max mem: 9341
[21:13:11.853379] Epoch: [270]  [140/195]  eta: 0:00:28  lr: 0.001895  loss: 0.3705 (0.3714)  time: 0.5117  data: 0.0002  max mem: 9341
[21:13:22.159962] Epoch: [270]  [160/195]  eta: 0:00:18  lr: 0.001894  loss: 0.3680 (0.3712)  time: 0.5153  data: 0.0002  max mem: 9341
[21:13:32.355177] Epoch: [270]  [180/195]  eta: 0:00:07  lr: 0.001894  loss: 0.3753 (0.3718)  time: 0.5097  data: 0.0001  max mem: 9341
[21:13:39.504802] Epoch: [270]  [194/195]  eta: 0:00:00  lr: 0.001893  loss: 0.3766 (0.3722)  time: 0.5127  data: 0.0001  max mem: 9341
[21:13:39.684404] Epoch: [270] Total time: 0:01:41 (0.5193 s / it)
[21:13:39.687825] Averaged stats: lr: 0.001893  loss: 0.3766 (0.3699)
[21:13:44.354706] {"train_lr": 0.0018954354666253866, "train_loss": 0.369898654291263, "epoch": 270}
[21:13:44.355051] [21:13:44.355136] Training epoch 270 for 0:01:45
[21:13:44.355189] [21:13:44.359592] log_dir: ./exp/debug/cifar100-LT/debug
[21:13:46.126263] Epoch: [271]  [  0/195]  eta: 0:05:44  lr: 0.001893  loss: 0.3966 (0.3966)  time: 1.7652  data: 1.2606  max mem: 9341
[21:13:56.339682] Epoch: [271]  [ 20/195]  eta: 0:01:39  lr: 0.001893  loss: 0.3501 (0.3570)  time: 0.5106  data: 0.0002  max mem: 9341
[21:14:06.557687] Epoch: [271]  [ 40/195]  eta: 0:01:23  lr: 0.001893  loss: 0.3618 (0.3583)  time: 0.5108  data: 0.0002  max mem: 9341
[21:14:16.794590] Epoch: [271]  [ 60/195]  eta: 0:01:11  lr: 0.001892  loss: 0.3708 (0.3626)  time: 0.5118  data: 0.0002  max mem: 9341
[21:14:27.096404] Epoch: [271]  [ 80/195]  eta: 0:01:00  lr: 0.001892  loss: 0.3806 (0.3668)  time: 0.5150  data: 0.0002  max mem: 9341
[21:14:37.331934] Epoch: [271]  [100/195]  eta: 0:00:49  lr: 0.001891  loss: 0.3692 (0.3681)  time: 0.5117  data: 0.0002  max mem: 9341
[21:14:47.571745] Epoch: [271]  [120/195]  eta: 0:00:39  lr: 0.001891  loss: 0.3709 (0.3697)  time: 0.5119  data: 0.0002  max mem: 9341
[21:14:57.808778] Epoch: [271]  [140/195]  eta: 0:00:28  lr: 0.001891  loss: 0.3809 (0.3712)  time: 0.5118  data: 0.0002  max mem: 9341
[21:15:08.110227] Epoch: [271]  [160/195]  eta: 0:00:18  lr: 0.001890  loss: 0.3693 (0.3713)  time: 0.5150  data: 0.0002  max mem: 9341
[21:15:18.311130] Epoch: [271]  [180/195]  eta: 0:00:07  lr: 0.001890  loss: 0.3658 (0.3708)  time: 0.5100  data: 0.0001  max mem: 9341
[21:15:25.469362] Epoch: [271]  [194/195]  eta: 0:00:00  lr: 0.001889  loss: 0.3636 (0.3703)  time: 0.5132  data: 0.0001  max mem: 9341
[21:15:25.644401] Epoch: [271] Total time: 0:01:41 (0.5194 s / it)
[21:15:25.645155] Averaged stats: lr: 0.001889  loss: 0.3636 (0.3707)
[21:15:30.223832] {"train_lr": 0.0018913870491876958, "train_loss": 0.37072088909454837, "epoch": 271}
[21:15:30.224329] [21:15:30.224446] Training epoch 271 for 0:01:45
[21:15:30.224521] [21:15:30.231421] log_dir: ./exp/debug/cifar100-LT/debug
[21:15:31.819945] Epoch: [272]  [  0/195]  eta: 0:05:09  lr: 0.001889  loss: 0.3488 (0.3488)  time: 1.5869  data: 1.0919  max mem: 9341
[21:15:42.043675] Epoch: [272]  [ 20/195]  eta: 0:01:38  lr: 0.001889  loss: 0.3601 (0.3660)  time: 0.5111  data: 0.0002  max mem: 9341
[21:15:52.257487] Epoch: [272]  [ 40/195]  eta: 0:01:23  lr: 0.001889  loss: 0.3606 (0.3651)  time: 0.5106  data: 0.0002  max mem: 9341
[21:16:02.468239] Epoch: [272]  [ 60/195]  eta: 0:01:11  lr: 0.001888  loss: 0.3654 (0.3652)  time: 0.5105  data: 0.0002  max mem: 9341
[21:16:12.718654] Epoch: [272]  [ 80/195]  eta: 0:01:00  lr: 0.001888  loss: 0.3701 (0.3667)  time: 0.5125  data: 0.0002  max mem: 9341
[21:16:22.931889] Epoch: [272]  [100/195]  eta: 0:00:49  lr: 0.001887  loss: 0.3820 (0.3689)  time: 0.5106  data: 0.0002  max mem: 9341
[21:16:33.145021] Epoch: [272]  [120/195]  eta: 0:00:38  lr: 0.001887  loss: 0.3668 (0.3691)  time: 0.5106  data: 0.0002  max mem: 9341
[21:16:43.360531] Epoch: [272]  [140/195]  eta: 0:00:28  lr: 0.001887  loss: 0.3739 (0.3696)  time: 0.5107  data: 0.0002  max mem: 9341
[21:16:53.615162] Epoch: [272]  [160/195]  eta: 0:00:18  lr: 0.001886  loss: 0.3584 (0.3691)  time: 0.5127  data: 0.0002  max mem: 9341
[21:17:03.786606] Epoch: [272]  [180/195]  eta: 0:00:07  lr: 0.001886  loss: 0.3655 (0.3695)  time: 0.5085  data: 0.0001  max mem: 9341
[21:17:10.916544] Epoch: [272]  [194/195]  eta: 0:00:00  lr: 0.001885  loss: 0.3649 (0.3696)  time: 0.5105  data: 0.0001  max mem: 9341
[21:17:11.080224] Epoch: [272] Total time: 0:01:40 (0.5172 s / it)
[21:17:11.099251] Averaged stats: lr: 0.001885  loss: 0.3649 (0.3688)
[21:17:15.833853] {"train_lr": 0.001887326817852932, "train_loss": 0.36881384540062684, "epoch": 272}
[21:17:15.834120] [21:17:15.834203] Training epoch 272 for 0:01:45
[21:17:15.834256] [21:17:15.838710] log_dir: ./exp/debug/cifar100-LT/debug
[21:17:17.606105] Epoch: [273]  [  0/195]  eta: 0:05:44  lr: 0.001885  loss: 0.3692 (0.3692)  time: 1.7663  data: 1.2599  max mem: 9341
[21:17:27.853354] Epoch: [273]  [ 20/195]  eta: 0:01:40  lr: 0.001885  loss: 0.3660 (0.3697)  time: 0.5123  data: 0.0002  max mem: 9341
[21:17:38.078046] Epoch: [273]  [ 40/195]  eta: 0:01:24  lr: 0.001884  loss: 0.3786 (0.3734)  time: 0.5112  data: 0.0002  max mem: 9341
[21:17:48.298118] Epoch: [273]  [ 60/195]  eta: 0:01:11  lr: 0.001884  loss: 0.3634 (0.3686)  time: 0.5109  data: 0.0002  max mem: 9341
[21:17:58.560168] Epoch: [273]  [ 80/195]  eta: 0:01:00  lr: 0.001883  loss: 0.3659 (0.3664)  time: 0.5130  data: 0.0002  max mem: 9341
[21:18:08.781512] Epoch: [273]  [100/195]  eta: 0:00:49  lr: 0.001883  loss: 0.3438 (0.3632)  time: 0.5110  data: 0.0002  max mem: 9341
[21:18:19.002673] Epoch: [273]  [120/195]  eta: 0:00:39  lr: 0.001883  loss: 0.3605 (0.3624)  time: 0.5110  data: 0.0002  max mem: 9341
[21:18:29.220542] Epoch: [273]  [140/195]  eta: 0:00:28  lr: 0.001882  loss: 0.3660 (0.3625)  time: 0.5108  data: 0.0002  max mem: 9341
[21:18:39.476824] Epoch: [273]  [160/195]  eta: 0:00:18  lr: 0.001882  loss: 0.3554 (0.3618)  time: 0.5128  data: 0.0002  max mem: 9341
[21:18:49.646666] Epoch: [273]  [180/195]  eta: 0:00:07  lr: 0.001881  loss: 0.3605 (0.3626)  time: 0.5084  data: 0.0001  max mem: 9341
[21:18:56.776157] Epoch: [273]  [194/195]  eta: 0:00:00  lr: 0.001881  loss: 0.3584 (0.3624)  time: 0.5104  data: 0.0001  max mem: 9341
[21:18:56.947878] Epoch: [273] Total time: 0:01:41 (0.5185 s / it)
[21:18:56.976303] Averaged stats: lr: 0.001881  loss: 0.3584 (0.3661)
[21:19:01.642108] {"train_lr": 0.0018832548419992529, "train_loss": 0.36610763271649677, "epoch": 273}
[21:19:01.642568] [21:19:01.642662] Training epoch 273 for 0:01:45
[21:19:01.642717] [21:19:01.647811] log_dir: ./exp/debug/cifar100-LT/debug
[21:19:03.510746] Epoch: [274]  [  0/195]  eta: 0:06:03  lr: 0.001881  loss: 0.3410 (0.3410)  time: 1.8616  data: 1.3470  max mem: 9341
[21:19:13.753897] Epoch: [274]  [ 20/195]  eta: 0:01:40  lr: 0.001881  loss: 0.3694 (0.3664)  time: 0.5121  data: 0.0002  max mem: 9341
[21:19:24.006429] Epoch: [274]  [ 40/195]  eta: 0:01:24  lr: 0.001880  loss: 0.3720 (0.3705)  time: 0.5126  data: 0.0002  max mem: 9341
[21:19:34.256286] Epoch: [274]  [ 60/195]  eta: 0:01:12  lr: 0.001880  loss: 0.3761 (0.3710)  time: 0.5124  data: 0.0002  max mem: 9341
[21:19:44.566923] Epoch: [274]  [ 80/195]  eta: 0:01:00  lr: 0.001879  loss: 0.3640 (0.3712)  time: 0.5155  data: 0.0002  max mem: 9341
[21:19:54.803203] Epoch: [274]  [100/195]  eta: 0:00:49  lr: 0.001879  loss: 0.3691 (0.3712)  time: 0.5118  data: 0.0002  max mem: 9341
[21:20:05.054328] Epoch: [274]  [120/195]  eta: 0:00:39  lr: 0.001879  loss: 0.3655 (0.3701)  time: 0.5125  data: 0.0002  max mem: 9341
[21:20:15.293770] Epoch: [274]  [140/195]  eta: 0:00:28  lr: 0.001878  loss: 0.3572 (0.3690)  time: 0.5119  data: 0.0002  max mem: 9341
[21:20:25.573462] Epoch: [274]  [160/195]  eta: 0:00:18  lr: 0.001878  loss: 0.3625 (0.3682)  time: 0.5139  data: 0.0002  max mem: 9341
[21:20:35.749408] Epoch: [274]  [180/195]  eta: 0:00:07  lr: 0.001877  loss: 0.3733 (0.3683)  time: 0.5087  data: 0.0001  max mem: 9341
[21:20:42.889093] Epoch: [274]  [194/195]  eta: 0:00:00  lr: 0.001877  loss: 0.3587 (0.3676)  time: 0.5111  data: 0.0001  max mem: 9341
[21:20:43.057849] Epoch: [274] Total time: 0:01:41 (0.5201 s / it)
[21:20:43.078074] Averaged stats: lr: 0.001877  loss: 0.3587 (0.3671)
[21:20:47.768995] {"train_lr": 0.001879171191205488, "train_loss": 0.3671165561064696, "epoch": 274}
[21:20:47.769318] [21:20:47.769407] Training epoch 274 for 0:01:46
[21:20:47.769461] [21:20:47.773881] log_dir: ./exp/debug/cifar100-LT/debug
[21:20:49.349992] Epoch: [275]  [  0/195]  eta: 0:05:07  lr: 0.001877  loss: 0.3610 (0.3610)  time: 1.5745  data: 1.0786  max mem: 9341
[21:20:59.579480] Epoch: [275]  [ 20/195]  eta: 0:01:38  lr: 0.001877  loss: 0.3619 (0.3659)  time: 0.5114  data: 0.0002  max mem: 9341
[21:21:09.819271] Epoch: [275]  [ 40/195]  eta: 0:01:23  lr: 0.001876  loss: 0.3553 (0.3634)  time: 0.5119  data: 0.0002  max mem: 9341
[21:21:20.063624] Epoch: [275]  [ 60/195]  eta: 0:01:11  lr: 0.001876  loss: 0.3696 (0.3669)  time: 0.5121  data: 0.0002  max mem: 9341
[21:21:30.344957] Epoch: [275]  [ 80/195]  eta: 0:01:00  lr: 0.001875  loss: 0.3640 (0.3668)  time: 0.5140  data: 0.0002  max mem: 9341
[21:21:40.585315] Epoch: [275]  [100/195]  eta: 0:00:49  lr: 0.001875  loss: 0.3650 (0.3675)  time: 0.5119  data: 0.0002  max mem: 9341
[21:21:50.823020] Epoch: [275]  [120/195]  eta: 0:00:39  lr: 0.001875  loss: 0.3760 (0.3686)  time: 0.5118  data: 0.0002  max mem: 9341
[21:22:01.061986] Epoch: [275]  [140/195]  eta: 0:00:28  lr: 0.001874  loss: 0.3428 (0.3663)  time: 0.5119  data: 0.0002  max mem: 9341
[21:22:11.345337] Epoch: [275]  [160/195]  eta: 0:00:18  lr: 0.001874  loss: 0.3636 (0.3654)  time: 0.5141  data: 0.0002  max mem: 9341
[21:22:21.523599] Epoch: [275]  [180/195]  eta: 0:00:07  lr: 0.001873  loss: 0.3556 (0.3646)  time: 0.5089  data: 0.0001  max mem: 9341
[21:22:28.657898] Epoch: [275]  [194/195]  eta: 0:00:00  lr: 0.001873  loss: 0.3589 (0.3648)  time: 0.5108  data: 0.0001  max mem: 9341
[21:22:28.840658] Epoch: [275] Total time: 0:01:41 (0.5183 s / it)
[21:22:28.851600] Averaged stats: lr: 0.001873  loss: 0.3589 (0.3651)
[21:22:33.536303] {"train_lr": 0.001875075935249973, "train_loss": 0.3651217098419483, "epoch": 275}
[21:22:33.536649] [21:22:33.536732] Training epoch 275 for 0:01:45
[21:22:33.536786] [21:22:33.541608] log_dir: ./exp/debug/cifar100-LT/debug
[21:22:35.089026] Epoch: [276]  [  0/195]  eta: 0:05:01  lr: 0.001873  loss: 0.4044 (0.4044)  time: 1.5457  data: 1.0254  max mem: 9341
[21:22:45.304486] Epoch: [276]  [ 20/195]  eta: 0:01:37  lr: 0.001873  loss: 0.3479 (0.3523)  time: 0.5107  data: 0.0002  max mem: 9341
[21:22:55.525807] Epoch: [276]  [ 40/195]  eta: 0:01:23  lr: 0.001872  loss: 0.3701 (0.3589)  time: 0.5110  data: 0.0002  max mem: 9341
[21:23:05.741832] Epoch: [276]  [ 60/195]  eta: 0:01:11  lr: 0.001872  loss: 0.3618 (0.3596)  time: 0.5107  data: 0.0002  max mem: 9341
[21:23:16.001953] Epoch: [276]  [ 80/195]  eta: 0:01:00  lr: 0.001871  loss: 0.3709 (0.3637)  time: 0.5129  data: 0.0002  max mem: 9341
[21:23:26.210984] Epoch: [276]  [100/195]  eta: 0:00:49  lr: 0.001871  loss: 0.3650 (0.3655)  time: 0.5104  data: 0.0002  max mem: 9341
[21:23:36.422878] Epoch: [276]  [120/195]  eta: 0:00:38  lr: 0.001870  loss: 0.3628 (0.3656)  time: 0.5105  data: 0.0002  max mem: 9341
[21:23:46.633220] Epoch: [276]  [140/195]  eta: 0:00:28  lr: 0.001870  loss: 0.3564 (0.3647)  time: 0.5105  data: 0.0002  max mem: 9341
[21:23:56.890542] Epoch: [276]  [160/195]  eta: 0:00:18  lr: 0.001869  loss: 0.3579 (0.3647)  time: 0.5128  data: 0.0002  max mem: 9341
[21:24:07.062390] Epoch: [276]  [180/195]  eta: 0:00:07  lr: 0.001869  loss: 0.3656 (0.3656)  time: 0.5085  data: 0.0001  max mem: 9341
[21:24:14.197325] Epoch: [276]  [194/195]  eta: 0:00:00  lr: 0.001869  loss: 0.3683 (0.3655)  time: 0.5106  data: 0.0001  max mem: 9341
[21:24:14.360968] Epoch: [276] Total time: 0:01:40 (0.5170 s / it)
[21:24:14.377899] Averaged stats: lr: 0.001869  loss: 0.3683 (0.3682)
[21:24:19.086529] {"train_lr": 0.0018709691441093105, "train_loss": 0.368173883893551, "epoch": 276}
[21:24:19.086955] [21:24:19.087043] Training epoch 276 for 0:01:45
[21:24:19.087097] [21:24:19.091642] log_dir: ./exp/debug/cifar100-LT/debug
[21:24:20.749658] Epoch: [277]  [  0/195]  eta: 0:05:23  lr: 0.001869  loss: 0.3398 (0.3398)  time: 1.6566  data: 1.1616  max mem: 9341
[21:24:30.968109] Epoch: [277]  [ 20/195]  eta: 0:01:38  lr: 0.001868  loss: 0.3669 (0.3700)  time: 0.5108  data: 0.0002  max mem: 9341
[21:24:41.178513] Epoch: [277]  [ 40/195]  eta: 0:01:23  lr: 0.001868  loss: 0.3627 (0.3661)  time: 0.5105  data: 0.0002  max mem: 9341
[21:24:51.397860] Epoch: [277]  [ 60/195]  eta: 0:01:11  lr: 0.001868  loss: 0.3740 (0.3657)  time: 0.5109  data: 0.0002  max mem: 9341
[21:25:01.662565] Epoch: [277]  [ 80/195]  eta: 0:01:00  lr: 0.001867  loss: 0.3566 (0.3636)  time: 0.5132  data: 0.0002  max mem: 9341
[21:25:11.874034] Epoch: [277]  [100/195]  eta: 0:00:49  lr: 0.001867  loss: 0.3581 (0.3639)  time: 0.5105  data: 0.0002  max mem: 9341
[21:25:22.084343] Epoch: [277]  [120/195]  eta: 0:00:39  lr: 0.001866  loss: 0.3645 (0.3644)  time: 0.5105  data: 0.0002  max mem: 9341
[21:25:32.298708] Epoch: [277]  [140/195]  eta: 0:00:28  lr: 0.001866  loss: 0.3548 (0.3642)  time: 0.5107  data: 0.0002  max mem: 9341
[21:25:42.564466] Epoch: [277]  [160/195]  eta: 0:00:18  lr: 0.001865  loss: 0.3587 (0.3635)  time: 0.5132  data: 0.0002  max mem: 9341
[21:25:52.741682] Epoch: [277]  [180/195]  eta: 0:00:07  lr: 0.001865  loss: 0.3687 (0.3633)  time: 0.5088  data: 0.0002  max mem: 9341
[21:25:59.870892] Epoch: [277]  [194/195]  eta: 0:00:00  lr: 0.001865  loss: 0.3663 (0.3643)  time: 0.5104  data: 0.0001  max mem: 9341
[21:26:00.032588] Epoch: [277] Total time: 0:01:40 (0.5176 s / it)
[21:26:00.055545] Averaged stats: lr: 0.001865  loss: 0.3663 (0.3648)
[21:26:04.887517] {"train_lr": 0.0018668508879572442, "train_loss": 0.36479112945305997, "epoch": 277}
[21:26:04.887845] [21:26:04.887937] Training epoch 277 for 0:01:45
[21:26:04.887992] [21:26:04.893353] log_dir: ./exp/debug/cifar100-LT/debug
[21:26:06.624951] Epoch: [278]  [  0/195]  eta: 0:05:37  lr: 0.001865  loss: 0.3457 (0.3457)  time: 1.7306  data: 1.2204  max mem: 9341
[21:26:16.851085] Epoch: [278]  [ 20/195]  eta: 0:01:39  lr: 0.001864  loss: 0.3648 (0.3701)  time: 0.5112  data: 0.0002  max mem: 9341
[21:26:27.064422] Epoch: [278]  [ 40/195]  eta: 0:01:23  lr: 0.001864  loss: 0.3532 (0.3642)  time: 0.5106  data: 0.0002  max mem: 9341
[21:26:37.286033] Epoch: [278]  [ 60/195]  eta: 0:01:11  lr: 0.001864  loss: 0.3590 (0.3652)  time: 0.5110  data: 0.0002  max mem: 9341
[21:26:47.546468] Epoch: [278]  [ 80/195]  eta: 0:01:00  lr: 0.001863  loss: 0.3730 (0.3658)  time: 0.5130  data: 0.0002  max mem: 9341
[21:26:57.762321] Epoch: [278]  [100/195]  eta: 0:00:49  lr: 0.001863  loss: 0.3462 (0.3643)  time: 0.5107  data: 0.0002  max mem: 9341
[21:27:07.976704] Epoch: [278]  [120/195]  eta: 0:00:39  lr: 0.001862  loss: 0.3740 (0.3656)  time: 0.5107  data: 0.0002  max mem: 9341
[21:27:18.192775] Epoch: [278]  [140/195]  eta: 0:00:28  lr: 0.001862  loss: 0.3670 (0.3661)  time: 0.5107  data: 0.0002  max mem: 9341
[21:27:28.451445] Epoch: [278]  [160/195]  eta: 0:00:18  lr: 0.001861  loss: 0.3769 (0.3670)  time: 0.5129  data: 0.0002  max mem: 9341
[21:27:38.625795] Epoch: [278]  [180/195]  eta: 0:00:07  lr: 0.001861  loss: 0.3598 (0.3663)  time: 0.5087  data: 0.0001  max mem: 9341
[21:27:45.758975] Epoch: [278]  [194/195]  eta: 0:00:00  lr: 0.001861  loss: 0.3603 (0.3664)  time: 0.5108  data: 0.0001  max mem: 9341
[21:27:45.914864] Epoch: [278] Total time: 0:01:41 (0.5181 s / it)
[21:27:45.945681] Averaged stats: lr: 0.001861  loss: 0.3603 (0.3651)
[21:27:50.800289] {"train_lr": 0.0018627212371634123, "train_loss": 0.36510880020184394, "epoch": 278}
[21:27:50.800635] [21:27:50.800720] Training epoch 278 for 0:01:45
[21:27:50.800774] [21:27:50.805329] log_dir: ./exp/debug/cifar100-LT/debug
[21:27:52.630449] Epoch: [279]  [  0/195]  eta: 0:05:55  lr: 0.001860  loss: 0.3877 (0.3877)  time: 1.8236  data: 1.3157  max mem: 9341
[21:28:02.872977] Epoch: [279]  [ 20/195]  eta: 0:01:40  lr: 0.001860  loss: 0.3620 (0.3686)  time: 0.5120  data: 0.0002  max mem: 9341
[21:28:13.097809] Epoch: [279]  [ 40/195]  eta: 0:01:24  lr: 0.001860  loss: 0.3728 (0.3703)  time: 0.5112  data: 0.0002  max mem: 9341
[21:28:23.346774] Epoch: [279]  [ 60/195]  eta: 0:01:12  lr: 0.001859  loss: 0.3643 (0.3700)  time: 0.5124  data: 0.0002  max mem: 9341
[21:28:33.649048] Epoch: [279]  [ 80/195]  eta: 0:01:00  lr: 0.001859  loss: 0.3554 (0.3688)  time: 0.5150  data: 0.0002  max mem: 9341
[21:28:43.863999] Epoch: [279]  [100/195]  eta: 0:00:49  lr: 0.001858  loss: 0.3491 (0.3661)  time: 0.5107  data: 0.0002  max mem: 9341
[21:28:54.083439] Epoch: [279]  [120/195]  eta: 0:00:39  lr: 0.001858  loss: 0.3653 (0.3652)  time: 0.5109  data: 0.0002  max mem: 9341
[21:29:04.305880] Epoch: [279]  [140/195]  eta: 0:00:28  lr: 0.001858  loss: 0.3665 (0.3650)  time: 0.5111  data: 0.0002  max mem: 9341
[21:29:14.564326] Epoch: [279]  [160/195]  eta: 0:00:18  lr: 0.001857  loss: 0.3701 (0.3654)  time: 0.5129  data: 0.0002  max mem: 9341
[21:29:24.750169] Epoch: [279]  [180/195]  eta: 0:00:07  lr: 0.001857  loss: 0.3666 (0.3660)  time: 0.5092  data: 0.0001  max mem: 9341
[21:29:31.887581] Epoch: [279]  [194/195]  eta: 0:00:00  lr: 0.001856  loss: 0.3639 (0.3664)  time: 0.5112  data: 0.0001  max mem: 9341
[21:29:32.044161] Epoch: [279] Total time: 0:01:41 (0.5192 s / it)
[21:29:32.063864] Averaged stats: lr: 0.001856  loss: 0.3639 (0.3672)
[21:29:36.743469] {"train_lr": 0.001858580262292137, "train_loss": 0.3671662321839577, "epoch": 279}
[21:29:36.743737] [21:29:36.743822] Training epoch 279 for 0:01:45
[21:29:36.743876] [21:29:36.748320] log_dir: ./exp/debug/cifar100-LT/debug
[21:29:38.530156] Epoch: [280]  [  0/195]  eta: 0:05:47  lr: 0.001856  loss: 0.3566 (0.3566)  time: 1.7804  data: 1.2830  max mem: 9341
[21:29:48.749169] Epoch: [280]  [ 20/195]  eta: 0:01:39  lr: 0.001856  loss: 0.3673 (0.3608)  time: 0.5109  data: 0.0002  max mem: 9341
[21:29:58.968410] Epoch: [280]  [ 40/195]  eta: 0:01:23  lr: 0.001856  loss: 0.3583 (0.3628)  time: 0.5109  data: 0.0002  max mem: 9341
[21:30:09.190402] Epoch: [280]  [ 60/195]  eta: 0:01:11  lr: 0.001855  loss: 0.3617 (0.3625)  time: 0.5110  data: 0.0002  max mem: 9341
[21:30:19.441789] Epoch: [280]  [ 80/195]  eta: 0:01:00  lr: 0.001855  loss: 0.3647 (0.3645)  time: 0.5125  data: 0.0004  max mem: 9341
[21:30:29.650603] Epoch: [280]  [100/195]  eta: 0:00:49  lr: 0.001854  loss: 0.3703 (0.3661)  time: 0.5104  data: 0.0002  max mem: 9341
[21:30:39.963794] Epoch: [280]  [120/195]  eta: 0:00:39  lr: 0.001854  loss: 0.3617 (0.3662)  time: 0.5156  data: 0.0002  max mem: 9341
[21:30:50.176033] Epoch: [280]  [140/195]  eta: 0:00:28  lr: 0.001854  loss: 0.3575 (0.3656)  time: 0.5106  data: 0.0002  max mem: 9341
[21:31:00.433431] Epoch: [280]  [160/195]  eta: 0:00:18  lr: 0.001853  loss: 0.3634 (0.3659)  time: 0.5128  data: 0.0002  max mem: 9341
[21:31:10.597771] Epoch: [280]  [180/195]  eta: 0:00:07  lr: 0.001853  loss: 0.3559 (0.3649)  time: 0.5082  data: 0.0001  max mem: 9341
[21:31:17.727611] Epoch: [280]  [194/195]  eta: 0:00:00  lr: 0.001852  loss: 0.3682 (0.3654)  time: 0.5104  data: 0.0001  max mem: 9341
[21:31:17.882452] Epoch: [280] Total time: 0:01:41 (0.5186 s / it)
[21:31:17.900720] Averaged stats: lr: 0.001852  loss: 0.3682 (0.3632)
[21:31:22.600673] {"train_lr": 0.001854428034101271, "train_loss": 0.3631998617679645, "epoch": 280}
[21:31:22.600970] [21:31:22.601060] Training epoch 280 for 0:01:45
[21:31:22.601115] [21:31:22.605590] log_dir: ./exp/debug/cifar100-LT/debug
[21:31:24.268482] Epoch: [281]  [  0/195]  eta: 0:05:23  lr: 0.001852  loss: 0.4150 (0.4150)  time: 1.6614  data: 1.1628  max mem: 9341
[21:31:34.495300] Epoch: [281]  [ 20/195]  eta: 0:01:39  lr: 0.001852  loss: 0.3696 (0.3680)  time: 0.5113  data: 0.0002  max mem: 9341
[21:31:44.720376] Epoch: [281]  [ 40/195]  eta: 0:01:23  lr: 0.001851  loss: 0.3749 (0.3679)  time: 0.5112  data: 0.0002  max mem: 9341
[21:31:54.935983] Epoch: [281]  [ 60/195]  eta: 0:01:11  lr: 0.001851  loss: 0.3560 (0.3640)  time: 0.5107  data: 0.0002  max mem: 9341
[21:32:05.194325] Epoch: [281]  [ 80/195]  eta: 0:01:00  lr: 0.001850  loss: 0.3479 (0.3616)  time: 0.5129  data: 0.0002  max mem: 9341
[21:32:15.411286] Epoch: [281]  [100/195]  eta: 0:00:49  lr: 0.001850  loss: 0.3682 (0.3616)  time: 0.5108  data: 0.0002  max mem: 9341
[21:32:25.625063] Epoch: [281]  [120/195]  eta: 0:00:39  lr: 0.001850  loss: 0.3563 (0.3624)  time: 0.5106  data: 0.0002  max mem: 9341
[21:32:35.839343] Epoch: [281]  [140/195]  eta: 0:00:28  lr: 0.001849  loss: 0.3576 (0.3614)  time: 0.5107  data: 0.0002  max mem: 9341
[21:32:46.099979] Epoch: [281]  [160/195]  eta: 0:00:18  lr: 0.001849  loss: 0.3603 (0.3615)  time: 0.5130  data: 0.0002  max mem: 9341
[21:32:56.267579] Epoch: [281]  [180/195]  eta: 0:00:07  lr: 0.001848  loss: 0.3726 (0.3627)  time: 0.5083  data: 0.0001  max mem: 9341
[21:33:03.400176] Epoch: [281]  [194/195]  eta: 0:00:00  lr: 0.001848  loss: 0.3598 (0.3623)  time: 0.5105  data: 0.0001  max mem: 9341
[21:33:03.577096] Epoch: [281] Total time: 0:01:40 (0.5178 s / it)
[21:33:03.582031] Averaged stats: lr: 0.001848  loss: 0.3598 (0.3627)
[21:33:08.297470] {"train_lr": 0.0018502646235409295, "train_loss": 0.36269609209818715, "epoch": 281}
[21:33:08.297806] [21:33:08.297893] Training epoch 281 for 0:01:45
[21:33:08.297947] [21:33:08.302398] log_dir: ./exp/debug/cifar100-LT/debug
[21:33:10.149459] Epoch: [282]  [  0/195]  eta: 0:05:59  lr: 0.001848  loss: 0.3806 (0.3806)  time: 1.8458  data: 1.3332  max mem: 9341
[21:33:20.372956] Epoch: [282]  [ 20/195]  eta: 0:01:40  lr: 0.001848  loss: 0.3638 (0.3625)  time: 0.5111  data: 0.0002  max mem: 9341
[21:33:30.586071] Epoch: [282]  [ 40/195]  eta: 0:01:24  lr: 0.001847  loss: 0.3562 (0.3588)  time: 0.5106  data: 0.0002  max mem: 9341
[21:33:40.804164] Epoch: [282]  [ 60/195]  eta: 0:01:11  lr: 0.001847  loss: 0.3521 (0.3574)  time: 0.5108  data: 0.0002  max mem: 9341
[21:33:51.061720] Epoch: [282]  [ 80/195]  eta: 0:01:00  lr: 0.001846  loss: 0.3600 (0.3587)  time: 0.5128  data: 0.0002  max mem: 9341
[21:34:01.289857] Epoch: [282]  [100/195]  eta: 0:00:49  lr: 0.001846  loss: 0.3500 (0.3583)  time: 0.5113  data: 0.0002  max mem: 9341
[21:34:11.508989] Epoch: [282]  [120/195]  eta: 0:00:39  lr: 0.001846  loss: 0.3551 (0.3578)  time: 0.5109  data: 0.0002  max mem: 9341
[21:34:21.728476] Epoch: [282]  [140/195]  eta: 0:00:28  lr: 0.001845  loss: 0.3496 (0.3578)  time: 0.5109  data: 0.0002  max mem: 9341
[21:34:31.992256] Epoch: [282]  [160/195]  eta: 0:00:18  lr: 0.001845  loss: 0.3523 (0.3574)  time: 0.5131  data: 0.0002  max mem: 9341
[21:34:42.172168] Epoch: [282]  [180/195]  eta: 0:00:07  lr: 0.001844  loss: 0.3631 (0.3583)  time: 0.5089  data: 0.0001  max mem: 9341
[21:34:49.310202] Epoch: [282]  [194/195]  eta: 0:00:00  lr: 0.001844  loss: 0.3541 (0.3586)  time: 0.5110  data: 0.0001  max mem: 9341
[21:34:49.484057] Epoch: [282] Total time: 0:01:41 (0.5189 s / it)
[21:34:49.509273] Averaged stats: lr: 0.001844  loss: 0.3541 (0.3619)
[21:34:54.204752] {"train_lr": 0.0018460901017523133, "train_loss": 0.3619461852388504, "epoch": 282}
[21:34:54.205021] [21:34:54.205114] Training epoch 282 for 0:01:45
[21:34:54.205166] [21:34:54.209637] log_dir: ./exp/debug/cifar100-LT/debug
[21:34:55.726265] Epoch: [283]  [  0/195]  eta: 0:04:55  lr: 0.001844  loss: 0.3945 (0.3945)  time: 1.5157  data: 1.0210  max mem: 9341
[21:35:05.970059] Epoch: [283]  [ 20/195]  eta: 0:01:37  lr: 0.001843  loss: 0.3479 (0.3529)  time: 0.5121  data: 0.0002  max mem: 9341
[21:35:16.188569] Epoch: [283]  [ 40/195]  eta: 0:01:23  lr: 0.001843  loss: 0.3615 (0.3559)  time: 0.5109  data: 0.0002  max mem: 9341
[21:35:26.425186] Epoch: [283]  [ 60/195]  eta: 0:01:11  lr: 0.001843  loss: 0.3565 (0.3563)  time: 0.5118  data: 0.0002  max mem: 9341
[21:35:36.685997] Epoch: [283]  [ 80/195]  eta: 0:01:00  lr: 0.001842  loss: 0.3607 (0.3568)  time: 0.5130  data: 0.0002  max mem: 9341
[21:35:46.908146] Epoch: [283]  [100/195]  eta: 0:00:49  lr: 0.001842  loss: 0.3554 (0.3565)  time: 0.5111  data: 0.0002  max mem: 9341
[21:35:57.125212] Epoch: [283]  [120/195]  eta: 0:00:38  lr: 0.001841  loss: 0.3546 (0.3574)  time: 0.5108  data: 0.0002  max mem: 9341
[21:36:07.337956] Epoch: [283]  [140/195]  eta: 0:00:28  lr: 0.001841  loss: 0.3553 (0.3576)  time: 0.5106  data: 0.0002  max mem: 9341
[21:36:17.593063] Epoch: [283]  [160/195]  eta: 0:00:18  lr: 0.001840  loss: 0.3569 (0.3584)  time: 0.5127  data: 0.0002  max mem: 9341
[21:36:27.759422] Epoch: [283]  [180/195]  eta: 0:00:07  lr: 0.001840  loss: 0.3683 (0.3595)  time: 0.5083  data: 0.0001  max mem: 9341
[21:36:34.886832] Epoch: [283]  [194/195]  eta: 0:00:00  lr: 0.001840  loss: 0.3708 (0.3597)  time: 0.5103  data: 0.0001  max mem: 9341
[21:36:35.052803] Epoch: [283] Total time: 0:01:40 (0.5171 s / it)
[21:36:35.066984] Averaged stats: lr: 0.001840  loss: 0.3708 (0.3622)
[21:36:39.766967] {"train_lr": 0.0018419045400664823, "train_loss": 0.3622176027450806, "epoch": 283}
[21:36:39.767367] [21:36:39.767450] Training epoch 283 for 0:01:45
[21:36:39.767503] [21:36:39.771919] log_dir: ./exp/debug/cifar100-LT/debug
[21:36:41.502349] Epoch: [284]  [  0/195]  eta: 0:05:37  lr: 0.001840  loss: 0.3349 (0.3349)  time: 1.7290  data: 1.2191  max mem: 9341
[21:36:51.721496] Epoch: [284]  [ 20/195]  eta: 0:01:39  lr: 0.001839  loss: 0.3732 (0.3710)  time: 0.5109  data: 0.0002  max mem: 9341
[21:37:01.940666] Epoch: [284]  [ 40/195]  eta: 0:01:23  lr: 0.001839  loss: 0.3523 (0.3668)  time: 0.5109  data: 0.0002  max mem: 9341
[21:37:12.155494] Epoch: [284]  [ 60/195]  eta: 0:01:11  lr: 0.001839  loss: 0.3583 (0.3652)  time: 0.5107  data: 0.0002  max mem: 9341
[21:37:22.418542] Epoch: [284]  [ 80/195]  eta: 0:01:00  lr: 0.001838  loss: 0.3645 (0.3653)  time: 0.5131  data: 0.0002  max mem: 9341
[21:37:32.636631] Epoch: [284]  [100/195]  eta: 0:00:49  lr: 0.001838  loss: 0.3583 (0.3640)  time: 0.5108  data: 0.0002  max mem: 9341
[21:37:42.874267] Epoch: [284]  [120/195]  eta: 0:00:39  lr: 0.001837  loss: 0.3600 (0.3632)  time: 0.5118  data: 0.0002  max mem: 9341
[21:37:53.083165] Epoch: [284]  [140/195]  eta: 0:00:28  lr: 0.001837  loss: 0.3605 (0.3637)  time: 0.5104  data: 0.0002  max mem: 9341
[21:38:03.351521] Epoch: [284]  [160/195]  eta: 0:00:18  lr: 0.001836  loss: 0.3658 (0.3641)  time: 0.5134  data: 0.0002  max mem: 9341
[21:38:13.532232] Epoch: [284]  [180/195]  eta: 0:00:07  lr: 0.001836  loss: 0.3495 (0.3641)  time: 0.5090  data: 0.0001  max mem: 9341
[21:38:20.672677] Epoch: [284]  [194/195]  eta: 0:00:00  lr: 0.001836  loss: 0.3580 (0.3641)  time: 0.5111  data: 0.0001  max mem: 9341
[21:38:20.845602] Epoch: [284] Total time: 0:01:41 (0.5183 s / it)
[21:38:20.860497] Averaged stats: lr: 0.001836  loss: 0.3580 (0.3616)
[21:38:25.549669] {"train_lr": 0.0018377080100031309, "train_loss": 0.3615710589366081, "epoch": 284}
[21:38:25.550009] [21:38:25.550094] Training epoch 284 for 0:01:45
[21:38:25.550149] [21:38:25.554542] log_dir: ./exp/debug/cifar100-LT/debug
[21:38:27.319472] Epoch: [285]  [  0/195]  eta: 0:05:43  lr: 0.001835  loss: 0.3524 (0.3524)  time: 1.7635  data: 1.2698  max mem: 9341
[21:38:37.535449] Epoch: [285]  [ 20/195]  eta: 0:01:39  lr: 0.001835  loss: 0.3556 (0.3592)  time: 0.5107  data: 0.0002  max mem: 9341
[21:38:47.756086] Epoch: [285]  [ 40/195]  eta: 0:01:23  lr: 0.001835  loss: 0.3498 (0.3539)  time: 0.5110  data: 0.0002  max mem: 9341
[21:38:57.967649] Epoch: [285]  [ 60/195]  eta: 0:01:11  lr: 0.001834  loss: 0.3681 (0.3561)  time: 0.5105  data: 0.0002  max mem: 9341
[21:39:08.230279] Epoch: [285]  [ 80/195]  eta: 0:01:00  lr: 0.001834  loss: 0.3597 (0.3570)  time: 0.5131  data: 0.0002  max mem: 9341
[21:39:18.449715] Epoch: [285]  [100/195]  eta: 0:00:49  lr: 0.001833  loss: 0.3619 (0.3592)  time: 0.5109  data: 0.0002  max mem: 9341
[21:39:28.663625] Epoch: [285]  [120/195]  eta: 0:00:39  lr: 0.001833  loss: 0.3538 (0.3594)  time: 0.5106  data: 0.0002  max mem: 9341
[21:39:38.871172] Epoch: [285]  [140/195]  eta: 0:00:28  lr: 0.001833  loss: 0.3618 (0.3595)  time: 0.5103  data: 0.0002  max mem: 9341
[21:39:49.129432] Epoch: [285]  [160/195]  eta: 0:00:18  lr: 0.001832  loss: 0.3503 (0.3588)  time: 0.5129  data: 0.0002  max mem: 9341
[21:39:59.302325] Epoch: [285]  [180/195]  eta: 0:00:07  lr: 0.001832  loss: 0.3634 (0.3597)  time: 0.5086  data: 0.0001  max mem: 9341
[21:40:06.431104] Epoch: [285]  [194/195]  eta: 0:00:00  lr: 0.001831  loss: 0.3709 (0.3606)  time: 0.5104  data: 0.0001  max mem: 9341
[21:40:06.616459] Epoch: [285] Total time: 0:01:41 (0.5183 s / it)
[21:40:06.622187] Averaged stats: lr: 0.001831  loss: 0.3709 (0.3613)
[21:40:11.346089] {"train_lr": 0.0018335005832693877, "train_loss": 0.3612751500346722, "epoch": 285}
[21:40:11.346346] [21:40:11.346429] Training epoch 285 for 0:01:45
[21:40:11.346482] [21:40:11.350925] log_dir: ./exp/debug/cifar100-LT/debug
[21:40:12.903234] Epoch: [286]  [  0/195]  eta: 0:05:02  lr: 0.001831  loss: 0.3845 (0.3845)  time: 1.5514  data: 1.0557  max mem: 9341
[21:40:23.169620] Epoch: [286]  [ 20/195]  eta: 0:01:38  lr: 0.001831  loss: 0.3663 (0.3712)  time: 0.5132  data: 0.0002  max mem: 9341
[21:40:33.387822] Epoch: [286]  [ 40/195]  eta: 0:01:23  lr: 0.001831  loss: 0.3649 (0.3689)  time: 0.5109  data: 0.0002  max mem: 9341
[21:40:43.609552] Epoch: [286]  [ 60/195]  eta: 0:01:11  lr: 0.001830  loss: 0.3568 (0.3665)  time: 0.5110  data: 0.0002  max mem: 9341
[21:40:53.864764] Epoch: [286]  [ 80/195]  eta: 0:01:00  lr: 0.001829  loss: 0.3630 (0.3654)  time: 0.5127  data: 0.0002  max mem: 9341
[21:41:04.078429] Epoch: [286]  [100/195]  eta: 0:00:49  lr: 0.001829  loss: 0.3607 (0.3646)  time: 0.5106  data: 0.0002  max mem: 9341
[21:41:14.293163] Epoch: [286]  [120/195]  eta: 0:00:39  lr: 0.001829  loss: 0.3614 (0.3642)  time: 0.5107  data: 0.0002  max mem: 9341
[21:41:24.508127] Epoch: [286]  [140/195]  eta: 0:00:28  lr: 0.001828  loss: 0.3476 (0.3627)  time: 0.5107  data: 0.0002  max mem: 9341
[21:41:34.773748] Epoch: [286]  [160/195]  eta: 0:00:18  lr: 0.001828  loss: 0.3551 (0.3620)  time: 0.5132  data: 0.0002  max mem: 9341
[21:41:44.945983] Epoch: [286]  [180/195]  eta: 0:00:07  lr: 0.001827  loss: 0.3521 (0.3617)  time: 0.5086  data: 0.0001  max mem: 9341
[21:41:52.078575] Epoch: [286]  [194/195]  eta: 0:00:00  lr: 0.001827  loss: 0.3642 (0.3618)  time: 0.5108  data: 0.0001  max mem: 9341
[21:41:52.239897] Epoch: [286] Total time: 0:01:40 (0.5174 s / it)
[21:41:52.264648] Averaged stats: lr: 0.001827  loss: 0.3642 (0.3630)
[21:41:56.916593] {"train_lr": 0.0018292823317585664, "train_loss": 0.36302198721812323, "epoch": 286}
[21:41:56.916925] [21:41:56.917008] Training epoch 286 for 0:01:45
[21:41:56.917061] [21:41:56.921481] log_dir: ./exp/debug/cifar100-LT/debug
[21:41:58.596109] Epoch: [287]  [  0/195]  eta: 0:05:26  lr: 0.001827  loss: 0.3631 (0.3631)  time: 1.6733  data: 1.1616  max mem: 9341
[21:42:08.815837] Epoch: [287]  [ 20/195]  eta: 0:01:39  lr: 0.001827  loss: 0.3597 (0.3594)  time: 0.5109  data: 0.0002  max mem: 9341
[21:42:19.033259] Epoch: [287]  [ 40/195]  eta: 0:01:23  lr: 0.001826  loss: 0.3502 (0.3576)  time: 0.5108  data: 0.0002  max mem: 9341
[21:42:29.266892] Epoch: [287]  [ 60/195]  eta: 0:01:11  lr: 0.001826  loss: 0.3554 (0.3571)  time: 0.5116  data: 0.0002  max mem: 9341
[21:42:39.573714] Epoch: [287]  [ 80/195]  eta: 0:01:00  lr: 0.001825  loss: 0.3583 (0.3576)  time: 0.5153  data: 0.0002  max mem: 9341
[21:42:49.817534] Epoch: [287]  [100/195]  eta: 0:00:49  lr: 0.001825  loss: 0.3624 (0.3596)  time: 0.5121  data: 0.0002  max mem: 9341
[21:43:00.057190] Epoch: [287]  [120/195]  eta: 0:00:39  lr: 0.001825  loss: 0.3628 (0.3604)  time: 0.5119  data: 0.0002  max mem: 9341
[21:43:10.299076] Epoch: [287]  [140/195]  eta: 0:00:28  lr: 0.001824  loss: 0.3624 (0.3600)  time: 0.5120  data: 0.0002  max mem: 9341
[21:43:20.608937] Epoch: [287]  [160/195]  eta: 0:00:18  lr: 0.001824  loss: 0.3503 (0.3594)  time: 0.5154  data: 0.0002  max mem: 9341
[21:43:30.809749] Epoch: [287]  [180/195]  eta: 0:00:07  lr: 0.001823  loss: 0.3591 (0.3596)  time: 0.5100  data: 0.0001  max mem: 9341
[21:43:37.980843] Epoch: [287]  [194/195]  eta: 0:00:00  lr: 0.001823  loss: 0.3558 (0.3600)  time: 0.5138  data: 0.0001  max mem: 9341
[21:43:38.149003] Epoch: [287] Total time: 0:01:41 (0.5191 s / it)
[21:43:38.167695] Averaged stats: lr: 0.001823  loss: 0.3558 (0.3591)
[21:43:43.020536] {"train_lr": 0.0018250533275489444, "train_loss": 0.35913570611140666, "epoch": 287}
[21:43:43.020807] [21:43:43.020892] Training epoch 287 for 0:01:46
[21:43:43.020946] [21:43:43.025404] log_dir: ./exp/debug/cifar100-LT/debug
[21:43:44.603403] Epoch: [288]  [  0/195]  eta: 0:05:07  lr: 0.001823  loss: 0.3717 (0.3717)  time: 1.5769  data: 1.0712  max mem: 9341
[21:43:54.855835] Epoch: [288]  [ 20/195]  eta: 0:01:38  lr: 0.001822  loss: 0.3642 (0.3678)  time: 0.5126  data: 0.0002  max mem: 9341
[21:44:05.075853] Epoch: [288]  [ 40/195]  eta: 0:01:23  lr: 0.001822  loss: 0.3660 (0.3683)  time: 0.5109  data: 0.0002  max mem: 9341
[21:44:15.318815] Epoch: [288]  [ 60/195]  eta: 0:01:11  lr: 0.001822  loss: 0.3513 (0.3638)  time: 0.5121  data: 0.0002  max mem: 9341
[21:44:25.625285] Epoch: [288]  [ 80/195]  eta: 0:01:00  lr: 0.001821  loss: 0.3639 (0.3639)  time: 0.5152  data: 0.0002  max mem: 9341
[21:44:35.863805] Epoch: [288]  [100/195]  eta: 0:00:49  lr: 0.001821  loss: 0.3571 (0.3623)  time: 0.5118  data: 0.0002  max mem: 9341
[21:44:46.100354] Epoch: [288]  [120/195]  eta: 0:00:39  lr: 0.001820  loss: 0.3555 (0.3623)  time: 0.5118  data: 0.0002  max mem: 9341
[21:44:56.337911] Epoch: [288]  [140/195]  eta: 0:00:28  lr: 0.001820  loss: 0.3606 (0.3624)  time: 0.5118  data: 0.0002  max mem: 9341
[21:45:06.643836] Epoch: [288]  [160/195]  eta: 0:00:18  lr: 0.001819  loss: 0.3507 (0.3616)  time: 0.5152  data: 0.0002  max mem: 9341
[21:45:16.846125] Epoch: [288]  [180/195]  eta: 0:00:07  lr: 0.001819  loss: 0.3605 (0.3615)  time: 0.5101  data: 0.0002  max mem: 9341
[21:45:24.003548] Epoch: [288]  [194/195]  eta: 0:00:00  lr: 0.001819  loss: 0.3541 (0.3609)  time: 0.5131  data: 0.0001  max mem: 9341
[21:45:24.159842] Epoch: [288] Total time: 0:01:41 (0.5186 s / it)
[21:45:24.194598] Averaged stats: lr: 0.001819  loss: 0.3541 (0.3607)
[21:45:28.912426] {"train_lr": 0.0018208136429025321, "train_loss": 0.3606738730118825, "epoch": 288}
[21:45:28.912699] [21:45:28.912783] Training epoch 288 for 0:01:45
[21:45:28.912837] [21:45:28.917279] log_dir: ./exp/debug/cifar100-LT/debug
[21:45:30.698522] Epoch: [289]  [  0/195]  eta: 0:05:47  lr: 0.001819  loss: 0.3371 (0.3371)  time: 1.7797  data: 1.2807  max mem: 9341
[21:45:40.936706] Epoch: [289]  [ 20/195]  eta: 0:01:40  lr: 0.001818  loss: 0.3509 (0.3566)  time: 0.5118  data: 0.0002  max mem: 9341
[21:45:51.178658] Epoch: [289]  [ 40/195]  eta: 0:01:24  lr: 0.001818  loss: 0.3673 (0.3613)  time: 0.5120  data: 0.0002  max mem: 9341
[21:46:01.411746] Epoch: [289]  [ 60/195]  eta: 0:01:11  lr: 0.001817  loss: 0.3534 (0.3588)  time: 0.5116  data: 0.0002  max mem: 9341
[21:46:11.706721] Epoch: [289]  [ 80/195]  eta: 0:01:00  lr: 0.001817  loss: 0.3618 (0.3598)  time: 0.5147  data: 0.0002  max mem: 9341
[21:46:21.943398] Epoch: [289]  [100/195]  eta: 0:00:49  lr: 0.001816  loss: 0.3536 (0.3594)  time: 0.5118  data: 0.0002  max mem: 9341
[21:46:32.175405] Epoch: [289]  [120/195]  eta: 0:00:39  lr: 0.001816  loss: 0.3564 (0.3596)  time: 0.5115  data: 0.0002  max mem: 9341
[21:46:42.411432] Epoch: [289]  [140/195]  eta: 0:00:28  lr: 0.001816  loss: 0.3597 (0.3601)  time: 0.5117  data: 0.0002  max mem: 9341
[21:46:52.714926] Epoch: [289]  [160/195]  eta: 0:00:18  lr: 0.001815  loss: 0.3757 (0.3611)  time: 0.5151  data: 0.0002  max mem: 9341
[21:47:02.908082] Epoch: [289]  [180/195]  eta: 0:00:07  lr: 0.001815  loss: 0.3588 (0.3610)  time: 0.5096  data: 0.0001  max mem: 9341
[21:47:10.059037] Epoch: [289]  [194/195]  eta: 0:00:00  lr: 0.001814  loss: 0.3507 (0.3599)  time: 0.5126  data: 0.0001  max mem: 9341
[21:47:10.228676] Epoch: [289] Total time: 0:01:41 (0.5195 s / it)
[21:47:10.233261] Averaged stats: lr: 0.001814  loss: 0.3507 (0.3594)
[21:47:14.933296] {"train_lr": 0.0018165633502638492, "train_loss": 0.3593606743675012, "epoch": 289}
[21:47:14.933557] [21:47:14.933641] Training epoch 289 for 0:01:46
[21:47:14.933695] [21:47:14.938477] log_dir: ./exp/debug/cifar100-LT/debug
[21:47:16.445716] Epoch: [290]  [  0/195]  eta: 0:04:53  lr: 0.001814  loss: 0.4069 (0.4069)  time: 1.5047  data: 1.0031  max mem: 9341
[21:47:26.683515] Epoch: [290]  [ 20/195]  eta: 0:01:37  lr: 0.001814  loss: 0.3576 (0.3555)  time: 0.5118  data: 0.0002  max mem: 9341
[21:47:36.902543] Epoch: [290]  [ 40/195]  eta: 0:01:23  lr: 0.001814  loss: 0.3639 (0.3591)  time: 0.5109  data: 0.0002  max mem: 9341
[21:47:47.140740] Epoch: [290]  [ 60/195]  eta: 0:01:11  lr: 0.001813  loss: 0.3600 (0.3613)  time: 0.5119  data: 0.0002  max mem: 9341
[21:47:57.394752] Epoch: [290]  [ 80/195]  eta: 0:01:00  lr: 0.001813  loss: 0.3578 (0.3605)  time: 0.5126  data: 0.0002  max mem: 9341
[21:48:07.606482] Epoch: [290]  [100/195]  eta: 0:00:49  lr: 0.001812  loss: 0.3577 (0.3602)  time: 0.5105  data: 0.0002  max mem: 9341
[21:48:17.819495] Epoch: [290]  [120/195]  eta: 0:00:38  lr: 0.001812  loss: 0.3527 (0.3586)  time: 0.5106  data: 0.0002  max mem: 9341
[21:48:28.026405] Epoch: [290]  [140/195]  eta: 0:00:28  lr: 0.001811  loss: 0.3495 (0.3579)  time: 0.5103  data: 0.0002  max mem: 9341
[21:48:38.280276] Epoch: [290]  [160/195]  eta: 0:00:18  lr: 0.001811  loss: 0.3583 (0.3581)  time: 0.5126  data: 0.0002  max mem: 9341
[21:48:48.451832] Epoch: [290]  [180/195]  eta: 0:00:07  lr: 0.001810  loss: 0.3580 (0.3575)  time: 0.5085  data: 0.0001  max mem: 9341
[21:48:55.584271] Epoch: [290]  [194/195]  eta: 0:00:00  lr: 0.001810  loss: 0.3575 (0.3576)  time: 0.5107  data: 0.0001  max mem: 9341
[21:48:55.741163] Epoch: [290] Total time: 0:01:40 (0.5169 s / it)
[21:48:55.768599] Averaged stats: lr: 0.001810  loss: 0.3575 (0.3586)
[21:49:00.460533] {"train_lr": 0.0018123025222586623, "train_loss": 0.3585881700500464, "epoch": 290}
[21:49:00.460799] [21:49:00.460883] Training epoch 290 for 0:01:45
[21:49:00.460938] [21:49:00.465344] log_dir: ./exp/debug/cifar100-LT/debug
[21:49:02.282892] Epoch: [291]  [  0/195]  eta: 0:05:54  lr: 0.001810  loss: 0.3950 (0.3950)  time: 1.8164  data: 1.3196  max mem: 9341
[21:49:12.503106] Epoch: [291]  [ 20/195]  eta: 0:01:40  lr: 0.001810  loss: 0.3477 (0.3530)  time: 0.5110  data: 0.0002  max mem: 9341
[21:49:22.721298] Epoch: [291]  [ 40/195]  eta: 0:01:24  lr: 0.001809  loss: 0.3408 (0.3521)  time: 0.5108  data: 0.0002  max mem: 9341
[21:49:32.932957] Epoch: [291]  [ 60/195]  eta: 0:01:11  lr: 0.001809  loss: 0.3623 (0.3564)  time: 0.5105  data: 0.0002  max mem: 9341
[21:49:43.192934] Epoch: [291]  [ 80/195]  eta: 0:01:00  lr: 0.001808  loss: 0.3481 (0.3544)  time: 0.5129  data: 0.0002  max mem: 9341
[21:49:53.410645] Epoch: [291]  [100/195]  eta: 0:00:49  lr: 0.001808  loss: 0.3535 (0.3546)  time: 0.5108  data: 0.0002  max mem: 9341
[21:50:03.627074] Epoch: [291]  [120/195]  eta: 0:00:39  lr: 0.001808  loss: 0.3554 (0.3556)  time: 0.5108  data: 0.0002  max mem: 9341
[21:50:13.840301] Epoch: [291]  [140/195]  eta: 0:00:28  lr: 0.001807  loss: 0.3585 (0.3555)  time: 0.5106  data: 0.0002  max mem: 9341
[21:50:24.094559] Epoch: [291]  [160/195]  eta: 0:00:18  lr: 0.001806  loss: 0.3572 (0.3562)  time: 0.5127  data: 0.0002  max mem: 9341
[21:50:34.262829] Epoch: [291]  [180/195]  eta: 0:00:07  lr: 0.001806  loss: 0.3430 (0.3553)  time: 0.5084  data: 0.0001  max mem: 9341
[21:50:41.394820] Epoch: [291]  [194/195]  eta: 0:00:00  lr: 0.001806  loss: 0.3502 (0.3550)  time: 0.5105  data: 0.0001  max mem: 9341
[21:50:41.555491] Epoch: [291] Total time: 0:01:41 (0.5184 s / it)
[21:50:41.574733] Averaged stats: lr: 0.001806  loss: 0.3502 (0.3567)
[21:50:46.317533] {"train_lr": 0.001808031231692776, "train_loss": 0.35666627838061404, "epoch": 291}
[21:50:46.317796] [21:50:46.317879] Training epoch 291 for 0:01:45
[21:50:46.317933] [21:50:46.322349] log_dir: ./exp/debug/cifar100-LT/debug
[21:50:47.988181] Epoch: [292]  [  0/195]  eta: 0:05:24  lr: 0.001806  loss: 0.3754 (0.3754)  time: 1.6645  data: 1.1550  max mem: 9341
[21:50:58.231540] Epoch: [292]  [ 20/195]  eta: 0:01:39  lr: 0.001805  loss: 0.3580 (0.3608)  time: 0.5121  data: 0.0002  max mem: 9341
[21:51:08.473739] Epoch: [292]  [ 40/195]  eta: 0:01:23  lr: 0.001805  loss: 0.3604 (0.3632)  time: 0.5120  data: 0.0002  max mem: 9341
[21:51:18.712980] Epoch: [292]  [ 60/195]  eta: 0:01:11  lr: 0.001805  loss: 0.3649 (0.3644)  time: 0.5119  data: 0.0002  max mem: 9341
[21:51:29.023049] Epoch: [292]  [ 80/195]  eta: 0:01:00  lr: 0.001804  loss: 0.3529 (0.3626)  time: 0.5154  data: 0.0002  max mem: 9341
[21:51:39.264247] Epoch: [292]  [100/195]  eta: 0:00:49  lr: 0.001804  loss: 0.3517 (0.3625)  time: 0.5120  data: 0.0002  max mem: 9341
[21:51:49.502460] Epoch: [292]  [120/195]  eta: 0:00:39  lr: 0.001803  loss: 0.3621 (0.3631)  time: 0.5118  data: 0.0002  max mem: 9341
[21:51:59.739004] Epoch: [292]  [140/195]  eta: 0:00:28  lr: 0.001803  loss: 0.3615 (0.3630)  time: 0.5118  data: 0.0002  max mem: 9341
[21:52:09.994866] Epoch: [292]  [160/195]  eta: 0:00:18  lr: 0.001802  loss: 0.3469 (0.3616)  time: 0.5127  data: 0.0002  max mem: 9341
[21:52:20.169075] Epoch: [292]  [180/195]  eta: 0:00:07  lr: 0.001802  loss: 0.3674 (0.3625)  time: 0.5087  data: 0.0001  max mem: 9341
[21:52:27.303532] Epoch: [292]  [194/195]  eta: 0:00:00  lr: 0.001801  loss: 0.3639 (0.3618)  time: 0.5107  data: 0.0001  max mem: 9341
[21:52:27.477095] Epoch: [292] Total time: 0:01:41 (0.5187 s / it)
[21:52:27.489766] Averaged stats: lr: 0.001801  loss: 0.3639 (0.3631)
[21:52:32.199920] {"train_lr": 0.0018037495515507507, "train_loss": 0.36306339028554085, "epoch": 292}
[21:52:32.200270] [21:52:32.200355] Training epoch 292 for 0:01:45
[21:52:32.200410] [21:52:32.204814] log_dir: ./exp/debug/cifar100-LT/debug
[21:52:34.049739] Epoch: [293]  [  0/195]  eta: 0:05:59  lr: 0.001801  loss: 0.3646 (0.3646)  time: 1.8433  data: 1.3488  max mem: 9341
[21:52:44.263975] Epoch: [293]  [ 20/195]  eta: 0:01:40  lr: 0.001801  loss: 0.3661 (0.3651)  time: 0.5107  data: 0.0002  max mem: 9341
[21:52:54.478360] Epoch: [293]  [ 40/195]  eta: 0:01:24  lr: 0.001801  loss: 0.3622 (0.3635)  time: 0.5107  data: 0.0002  max mem: 9341
[21:53:04.696790] Epoch: [293]  [ 60/195]  eta: 0:01:11  lr: 0.001800  loss: 0.3653 (0.3637)  time: 0.5108  data: 0.0002  max mem: 9341
[21:53:14.958368] Epoch: [293]  [ 80/195]  eta: 0:01:00  lr: 0.001800  loss: 0.3619 (0.3627)  time: 0.5130  data: 0.0002  max mem: 9341
[21:53:25.169115] Epoch: [293]  [100/195]  eta: 0:00:49  lr: 0.001799  loss: 0.3582 (0.3625)  time: 0.5105  data: 0.0002  max mem: 9341
[21:53:35.382735] Epoch: [293]  [120/195]  eta: 0:00:39  lr: 0.001799  loss: 0.3686 (0.3635)  time: 0.5106  data: 0.0002  max mem: 9341
[21:53:45.598537] Epoch: [293]  [140/195]  eta: 0:00:28  lr: 0.001799  loss: 0.3620 (0.3634)  time: 0.5107  data: 0.0002  max mem: 9341
[21:53:55.855812] Epoch: [293]  [160/195]  eta: 0:00:18  lr: 0.001798  loss: 0.3630 (0.3633)  time: 0.5128  data: 0.0002  max mem: 9341
[21:54:06.020628] Epoch: [293]  [180/195]  eta: 0:00:07  lr: 0.001798  loss: 0.3645 (0.3635)  time: 0.5082  data: 0.0001  max mem: 9341
[21:54:13.145910] Epoch: [293]  [194/195]  eta: 0:00:00  lr: 0.001797  loss: 0.3645 (0.3637)  time: 0.5101  data: 0.0001  max mem: 9341
[21:54:13.313849] Epoch: [293] Total time: 0:01:41 (0.5185 s / it)
[21:54:13.316024] Averaged stats: lr: 0.001797  loss: 0.3645 (0.3643)
[21:54:18.083971] {"train_lr": 0.001799457554994692, "train_loss": 0.36430850594471664, "epoch": 293}
[21:54:18.084313] [21:54:18.084398] Training epoch 293 for 0:01:45
[21:54:18.084451] [21:54:18.088835] log_dir: ./exp/debug/cifar100-LT/debug
[21:54:19.879743] Epoch: [294]  [  0/195]  eta: 0:05:48  lr: 0.001797  loss: 0.3411 (0.3411)  time: 1.7897  data: 1.2995  max mem: 9341
[21:54:30.139995] Epoch: [294]  [ 20/195]  eta: 0:01:40  lr: 0.001797  loss: 0.3591 (0.3619)  time: 0.5130  data: 0.0002  max mem: 9341
[21:54:40.375782] Epoch: [294]  [ 40/195]  eta: 0:01:24  lr: 0.001796  loss: 0.3665 (0.3647)  time: 0.5117  data: 0.0002  max mem: 9341
[21:54:50.618168] Epoch: [294]  [ 60/195]  eta: 0:01:11  lr: 0.001796  loss: 0.3500 (0.3607)  time: 0.5120  data: 0.0002  max mem: 9341
[21:55:00.897879] Epoch: [294]  [ 80/195]  eta: 0:01:00  lr: 0.001795  loss: 0.3473 (0.3597)  time: 0.5139  data: 0.0002  max mem: 9341
[21:55:11.113671] Epoch: [294]  [100/195]  eta: 0:00:49  lr: 0.001795  loss: 0.3643 (0.3605)  time: 0.5107  data: 0.0002  max mem: 9341
[21:55:21.332344] Epoch: [294]  [120/195]  eta: 0:00:39  lr: 0.001795  loss: 0.3521 (0.3599)  time: 0.5109  data: 0.0002  max mem: 9341
[21:55:31.548605] Epoch: [294]  [140/195]  eta: 0:00:28  lr: 0.001794  loss: 0.3528 (0.3595)  time: 0.5107  data: 0.0002  max mem: 9341
[21:55:41.811030] Epoch: [294]  [160/195]  eta: 0:00:18  lr: 0.001794  loss: 0.3613 (0.3591)  time: 0.5131  data: 0.0002  max mem: 9341
[21:55:51.977950] Epoch: [294]  [180/195]  eta: 0:00:07  lr: 0.001793  loss: 0.3671 (0.3602)  time: 0.5083  data: 0.0001  max mem: 9341
[21:55:59.109704] Epoch: [294]  [194/195]  eta: 0:00:00  lr: 0.001793  loss: 0.3671 (0.3608)  time: 0.5105  data: 0.0001  max mem: 9341
[21:55:59.293074] Epoch: [294] Total time: 0:01:41 (0.5190 s / it)
[21:55:59.312290] Averaged stats: lr: 0.001793  loss: 0.3671 (0.3599)
[21:56:04.046798] {"train_lr": 0.0017951553153629787, "train_loss": 0.3598753301379008, "epoch": 294}
[21:56:04.047206] [21:56:04.047298] Training epoch 294 for 0:01:45
[21:56:04.047351] [21:56:04.051953] log_dir: ./exp/debug/cifar100-LT/debug
[21:56:05.851683] Epoch: [295]  [  0/195]  eta: 0:05:50  lr: 0.001793  loss: 0.3260 (0.3260)  time: 1.7986  data: 1.2919  max mem: 9341
[21:56:16.095823] Epoch: [295]  [ 20/195]  eta: 0:01:40  lr: 0.001792  loss: 0.3582 (0.3575)  time: 0.5121  data: 0.0002  max mem: 9341
[21:56:26.346879] Epoch: [295]  [ 40/195]  eta: 0:01:24  lr: 0.001792  loss: 0.3516 (0.3568)  time: 0.5125  data: 0.0002  max mem: 9341
[21:56:36.588851] Epoch: [295]  [ 60/195]  eta: 0:01:11  lr: 0.001792  loss: 0.3612 (0.3576)  time: 0.5120  data: 0.0002  max mem: 9341
[21:56:46.850925] Epoch: [295]  [ 80/195]  eta: 0:01:00  lr: 0.001791  loss: 0.3541 (0.3560)  time: 0.5130  data: 0.0002  max mem: 9341
[21:56:57.076144] Epoch: [295]  [100/195]  eta: 0:00:49  lr: 0.001791  loss: 0.3513 (0.3562)  time: 0.5112  data: 0.0002  max mem: 9341
[21:57:07.294632] Epoch: [295]  [120/195]  eta: 0:00:39  lr: 0.001790  loss: 0.3530 (0.3553)  time: 0.5108  data: 0.0002  max mem: 9341
[21:57:17.510417] Epoch: [295]  [140/195]  eta: 0:00:28  lr: 0.001790  loss: 0.3580 (0.3558)  time: 0.5107  data: 0.0002  max mem: 9341
[21:57:27.774101] Epoch: [295]  [160/195]  eta: 0:00:18  lr: 0.001789  loss: 0.3513 (0.3552)  time: 0.5131  data: 0.0002  max mem: 9341
[21:57:37.947947] Epoch: [295]  [180/195]  eta: 0:00:07  lr: 0.001789  loss: 0.3509 (0.3548)  time: 0.5086  data: 0.0001  max mem: 9341
[21:57:45.086747] Epoch: [295]  [194/195]  eta: 0:00:00  lr: 0.001789  loss: 0.3478 (0.3549)  time: 0.5109  data: 0.0001  max mem: 9341
[21:57:45.260153] Epoch: [295] Total time: 0:01:41 (0.5190 s / it)
[21:57:45.275405] Averaged stats: lr: 0.001789  loss: 0.3478 (0.3546)
[21:57:49.999588] {"train_lr": 0.0017908429061690163, "train_loss": 0.35464324630223787, "epoch": 295}
[21:57:49.999851] [21:57:49.999935] Training epoch 295 for 0:01:45
[21:57:49.999988] [21:57:50.004538] log_dir: ./exp/debug/cifar100-LT/debug
[21:57:51.606563] Epoch: [296]  [  0/195]  eta: 0:05:12  lr: 0.001789  loss: 0.3572 (0.3572)  time: 1.6007  data: 1.0947  max mem: 9341
[21:58:01.861777] Epoch: [296]  [ 20/195]  eta: 0:01:38  lr: 0.001788  loss: 0.3516 (0.3564)  time: 0.5127  data: 0.0002  max mem: 9341
[21:58:12.087228] Epoch: [296]  [ 40/195]  eta: 0:01:23  lr: 0.001788  loss: 0.3515 (0.3520)  time: 0.5112  data: 0.0002  max mem: 9341
[21:58:22.304436] Epoch: [296]  [ 60/195]  eta: 0:01:11  lr: 0.001787  loss: 0.3701 (0.3558)  time: 0.5108  data: 0.0002  max mem: 9341
[21:58:32.559458] Epoch: [296]  [ 80/195]  eta: 0:01:00  lr: 0.001787  loss: 0.3582 (0.3560)  time: 0.5127  data: 0.0002  max mem: 9341
[21:58:42.773863] Epoch: [296]  [100/195]  eta: 0:00:49  lr: 0.001786  loss: 0.3540 (0.3553)  time: 0.5106  data: 0.0002  max mem: 9341
[21:58:52.981029] Epoch: [296]  [120/195]  eta: 0:00:39  lr: 0.001786  loss: 0.3597 (0.3564)  time: 0.5103  data: 0.0002  max mem: 9341
[21:59:03.188136] Epoch: [296]  [140/195]  eta: 0:00:28  lr: 0.001786  loss: 0.3533 (0.3557)  time: 0.5103  data: 0.0002  max mem: 9341
[21:59:13.448047] Epoch: [296]  [160/195]  eta: 0:00:18  lr: 0.001785  loss: 0.3527 (0.3559)  time: 0.5129  data: 0.0002  max mem: 9341
[21:59:23.627642] Epoch: [296]  [180/195]  eta: 0:00:07  lr: 0.001785  loss: 0.3506 (0.3559)  time: 0.5089  data: 0.0002  max mem: 9341
[21:59:30.761233] Epoch: [296]  [194/195]  eta: 0:00:00  lr: 0.001784  loss: 0.3629 (0.3564)  time: 0.5107  data: 0.0001  max mem: 9341
[21:59:30.936687] Epoch: [296] Total time: 0:01:40 (0.5176 s / it)
[21:59:30.948975] Averaged stats: lr: 0.001784  loss: 0.3629 (0.3557)
[21:59:35.547986] {"train_lr": 0.0017865204010999785, "train_loss": 0.3557002571530831, "epoch": 296}
[21:59:35.548333] [21:59:35.548421] Training epoch 296 for 0:01:45
[21:59:35.548475] [21:59:35.552964] log_dir: ./exp/debug/cifar100-LT/debug
[21:59:37.339174] Epoch: [297]  [  0/195]  eta: 0:05:48  lr: 0.001784  loss: 0.3321 (0.3321)  time: 1.7849  data: 1.2946  max mem: 9341
[21:59:47.563212] Epoch: [297]  [ 20/195]  eta: 0:01:40  lr: 0.001784  loss: 0.3638 (0.3562)  time: 0.5111  data: 0.0002  max mem: 9341
[21:59:57.778771] Epoch: [297]  [ 40/195]  eta: 0:01:24  lr: 0.001783  loss: 0.3579 (0.3562)  time: 0.5107  data: 0.0002  max mem: 9341
[22:00:07.991902] Epoch: [297]  [ 60/195]  eta: 0:01:11  lr: 0.001783  loss: 0.3539 (0.3561)  time: 0.5106  data: 0.0002  max mem: 9341
[22:00:18.254372] Epoch: [297]  [ 80/195]  eta: 0:01:00  lr: 0.001782  loss: 0.3547 (0.3556)  time: 0.5131  data: 0.0002  max mem: 9341
[22:00:28.468256] Epoch: [297]  [100/195]  eta: 0:00:49  lr: 0.001782  loss: 0.3529 (0.3551)  time: 0.5106  data: 0.0002  max mem: 9341
[22:00:38.709592] Epoch: [297]  [120/195]  eta: 0:00:39  lr: 0.001782  loss: 0.3622 (0.3564)  time: 0.5120  data: 0.0002  max mem: 9341
[22:00:48.946822] Epoch: [297]  [140/195]  eta: 0:00:28  lr: 0.001781  loss: 0.3533 (0.3565)  time: 0.5118  data: 0.0002  max mem: 9341
[22:00:59.247607] Epoch: [297]  [160/195]  eta: 0:00:18  lr: 0.001781  loss: 0.3573 (0.3561)  time: 0.5150  data: 0.0002  max mem: 9341
[22:01:09.444474] Epoch: [297]  [180/195]  eta: 0:00:07  lr: 0.001780  loss: 0.3483 (0.3560)  time: 0.5098  data: 0.0001  max mem: 9341
[22:01:16.597028] Epoch: [297]  [194/195]  eta: 0:00:00  lr: 0.001780  loss: 0.3577 (0.3558)  time: 0.5126  data: 0.0001  max mem: 9341
[22:01:16.765510] Epoch: [297] Total time: 0:01:41 (0.5190 s / it)
[22:01:16.772457] Averaged stats: lr: 0.001780  loss: 0.3577 (0.3558)
[22:01:21.475759] {"train_lr": 0.0017821878740155498, "train_loss": 0.3558293219942313, "epoch": 297}
[22:01:21.476023] [22:01:21.476131] Training epoch 297 for 0:01:45
[22:01:21.476188] [22:01:21.480619] log_dir: ./exp/debug/cifar100-LT/debug
[22:01:23.038293] Epoch: [298]  [  0/195]  eta: 0:05:03  lr: 0.001780  loss: 0.3894 (0.3894)  time: 1.5566  data: 1.0432  max mem: 9341
[22:01:33.255771] Epoch: [298]  [ 20/195]  eta: 0:01:38  lr: 0.001779  loss: 0.3505 (0.3525)  time: 0.5108  data: 0.0002  max mem: 9341
[22:01:43.476185] Epoch: [298]  [ 40/195]  eta: 0:01:23  lr: 0.001779  loss: 0.3615 (0.3587)  time: 0.5109  data: 0.0002  max mem: 9341
[22:01:53.688320] Epoch: [298]  [ 60/195]  eta: 0:01:11  lr: 0.001779  loss: 0.3584 (0.3584)  time: 0.5105  data: 0.0002  max mem: 9341
[22:02:03.947124] Epoch: [298]  [ 80/195]  eta: 0:01:00  lr: 0.001778  loss: 0.3583 (0.3575)  time: 0.5129  data: 0.0002  max mem: 9341
[22:02:14.155858] Epoch: [298]  [100/195]  eta: 0:00:49  lr: 0.001778  loss: 0.3567 (0.3573)  time: 0.5104  data: 0.0002  max mem: 9341
[22:02:24.370776] Epoch: [298]  [120/195]  eta: 0:00:38  lr: 0.001777  loss: 0.3534 (0.3569)  time: 0.5107  data: 0.0002  max mem: 9341
[22:02:34.587148] Epoch: [298]  [140/195]  eta: 0:00:28  lr: 0.001777  loss: 0.3498 (0.3563)  time: 0.5108  data: 0.0002  max mem: 9341
[22:02:44.844031] Epoch: [298]  [160/195]  eta: 0:00:18  lr: 0.001776  loss: 0.3535 (0.3558)  time: 0.5128  data: 0.0002  max mem: 9341
[22:02:55.022944] Epoch: [298]  [180/195]  eta: 0:00:07  lr: 0.001776  loss: 0.3584 (0.3560)  time: 0.5089  data: 0.0001  max mem: 9341
[22:03:02.152374] Epoch: [298]  [194/195]  eta: 0:00:00  lr: 0.001776  loss: 0.3621 (0.3562)  time: 0.5105  data: 0.0001  max mem: 9341
[22:03:02.323678] Epoch: [298] Total time: 0:01:40 (0.5171 s / it)
[22:03:02.346148] Averaged stats: lr: 0.001776  loss: 0.3621 (0.3548)
[22:03:07.007436] {"train_lr": 0.0017778453989466625, "train_loss": 0.3547811174621949, "epoch": 298}
[22:03:07.007816] [22:03:07.007908] Training epoch 298 for 0:01:45
[22:03:07.007962] [22:03:07.013030] log_dir: ./exp/debug/cifar100-LT/debug
[22:03:08.848306] Epoch: [299]  [  0/195]  eta: 0:05:57  lr: 0.001775  loss: 0.3175 (0.3175)  time: 1.8341  data: 1.3397  max mem: 9341
[22:03:19.092262] Epoch: [299]  [ 20/195]  eta: 0:01:40  lr: 0.001775  loss: 0.3487 (0.3521)  time: 0.5121  data: 0.0002  max mem: 9341
[22:03:29.335726] Epoch: [299]  [ 40/195]  eta: 0:01:24  lr: 0.001775  loss: 0.3399 (0.3468)  time: 0.5121  data: 0.0002  max mem: 9341
[22:03:39.579854] Epoch: [299]  [ 60/195]  eta: 0:01:12  lr: 0.001774  loss: 0.3467 (0.3462)  time: 0.5122  data: 0.0002  max mem: 9341
[22:03:49.888626] Epoch: [299]  [ 80/195]  eta: 0:01:00  lr: 0.001774  loss: 0.3570 (0.3491)  time: 0.5154  data: 0.0002  max mem: 9341
[22:04:00.127643] Epoch: [299]  [100/195]  eta: 0:00:49  lr: 0.001773  loss: 0.3569 (0.3503)  time: 0.5119  data: 0.0002  max mem: 9341
[22:04:10.360726] Epoch: [299]  [120/195]  eta: 0:00:39  lr: 0.001773  loss: 0.3598 (0.3523)  time: 0.5116  data: 0.0004  max mem: 9341
[22:04:20.590896] Epoch: [299]  [140/195]  eta: 0:00:28  lr: 0.001773  loss: 0.3484 (0.3523)  time: 0.5114  data: 0.0002  max mem: 9341
[22:04:30.890200] Epoch: [299]  [160/195]  eta: 0:00:18  lr: 0.001772  loss: 0.3490 (0.3518)  time: 0.5149  data: 0.0002  max mem: 9341
[22:04:41.076878] Epoch: [299]  [180/195]  eta: 0:00:07  lr: 0.001772  loss: 0.3466 (0.3515)  time: 0.5093  data: 0.0001  max mem: 9341
[22:04:48.233271] Epoch: [299]  [194/195]  eta: 0:00:00  lr: 0.001771  loss: 0.3571 (0.3520)  time: 0.5127  data: 0.0001  max mem: 9341
[22:04:48.398816] Epoch: [299] Total time: 0:01:41 (0.5199 s / it)
[22:04:48.411387] Averaged stats: lr: 0.001771  loss: 0.3571 (0.3536)
[22:04:53.084426] {"train_lr": 0.0017734930500942377, "train_loss": 0.3536351250914427, "epoch": 299}
[22:04:53.084687] [22:04:53.084780] Training epoch 299 for 0:01:46
[22:04:53.084834] [22:04:53.089226] log_dir: ./exp/debug/cifar100-LT/debug
[22:04:54.873153] Epoch: [300]  [  0/195]  eta: 0:05:47  lr: 0.001771  loss: 0.3969 (0.3969)  time: 1.7829  data: 1.2885  max mem: 9341
[22:05:05.089932] Epoch: [300]  [ 20/195]  eta: 0:01:39  lr: 0.001771  loss: 0.3599 (0.3629)  time: 0.5108  data: 0.0002  max mem: 9341
[22:05:15.307808] Epoch: [300]  [ 40/195]  eta: 0:01:23  lr: 0.001770  loss: 0.3580 (0.3613)  time: 0.5108  data: 0.0002  max mem: 9341
[22:05:25.525895] Epoch: [300]  [ 60/195]  eta: 0:01:11  lr: 0.001770  loss: 0.3472 (0.3596)  time: 0.5108  data: 0.0002  max mem: 9341
[22:05:35.792535] Epoch: [300]  [ 80/195]  eta: 0:01:00  lr: 0.001769  loss: 0.3499 (0.3577)  time: 0.5133  data: 0.0002  max mem: 9341
[22:05:46.010063] Epoch: [300]  [100/195]  eta: 0:00:49  lr: 0.001769  loss: 0.3490 (0.3564)  time: 0.5108  data: 0.0002  max mem: 9341
[22:05:56.229581] Epoch: [300]  [120/195]  eta: 0:00:39  lr: 0.001769  loss: 0.3387 (0.3542)  time: 0.5109  data: 0.0002  max mem: 9341
[22:06:06.443308] Epoch: [300]  [140/195]  eta: 0:00:28  lr: 0.001768  loss: 0.3376 (0.3524)  time: 0.5106  data: 0.0002  max mem: 9341
[22:06:16.698203] Epoch: [300]  [160/195]  eta: 0:00:18  lr: 0.001768  loss: 0.3492 (0.3522)  time: 0.5127  data: 0.0002  max mem: 9341
[22:06:26.870680] Epoch: [300]  [180/195]  eta: 0:00:07  lr: 0.001767  loss: 0.3432 (0.3517)  time: 0.5086  data: 0.0001  max mem: 9341
[22:06:34.003079] Epoch: [300]  [194/195]  eta: 0:00:00  lr: 0.001767  loss: 0.3499 (0.3517)  time: 0.5107  data: 0.0001  max mem: 9341
[22:06:34.172261] Epoch: [300] Total time: 0:01:41 (0.5184 s / it)
[22:06:34.186180] Averaged stats: lr: 0.001767  loss: 0.3499 (0.3521)
[22:06:38.925694] {"train_lr": 0.0017691309018279067, "train_loss": 0.3521495974980868, "epoch": 300}
[22:06:38.926054] [22:06:38.926140] Training epoch 300 for 0:01:45
[22:06:38.926195] [22:06:38.930617] log_dir: ./exp/debug/cifar100-LT/debug
[22:06:40.760624] Epoch: [301]  [  0/195]  eta: 0:05:56  lr: 0.001767  loss: 0.3291 (0.3291)  time: 1.8289  data: 1.3290  max mem: 9341
[22:06:51.000267] Epoch: [301]  [ 20/195]  eta: 0:01:40  lr: 0.001766  loss: 0.3605 (0.3596)  time: 0.5119  data: 0.0002  max mem: 9341
[22:07:01.220120] Epoch: [301]  [ 40/195]  eta: 0:01:24  lr: 0.001766  loss: 0.3500 (0.3585)  time: 0.5109  data: 0.0002  max mem: 9341
[22:07:11.436614] Epoch: [301]  [ 60/195]  eta: 0:01:11  lr: 0.001766  loss: 0.3594 (0.3580)  time: 0.5108  data: 0.0002  max mem: 9341
[22:07:21.711693] Epoch: [301]  [ 80/195]  eta: 0:01:00  lr: 0.001765  loss: 0.3568 (0.3584)  time: 0.5137  data: 0.0002  max mem: 9341
[22:07:31.951856] Epoch: [301]  [100/195]  eta: 0:00:49  lr: 0.001765  loss: 0.3572 (0.3581)  time: 0.5119  data: 0.0002  max mem: 9341
[22:07:42.186847] Epoch: [301]  [120/195]  eta: 0:00:39  lr: 0.001764  loss: 0.3613 (0.3582)  time: 0.5117  data: 0.0002  max mem: 9341
[22:07:52.423158] Epoch: [301]  [140/195]  eta: 0:00:28  lr: 0.001764  loss: 0.3607 (0.3585)  time: 0.5118  data: 0.0002  max mem: 9341
[22:08:02.724061] Epoch: [301]  [160/195]  eta: 0:00:18  lr: 0.001763  loss: 0.3612 (0.3583)  time: 0.5150  data: 0.0002  max mem: 9341
[22:08:12.920452] Epoch: [301]  [180/195]  eta: 0:00:07  lr: 0.001763  loss: 0.3485 (0.3571)  time: 0.5098  data: 0.0001  max mem: 9341
[22:08:20.076043] Epoch: [301]  [194/195]  eta: 0:00:00  lr: 0.001762  loss: 0.3461 (0.3565)  time: 0.5127  data: 0.0001  max mem: 9341
[22:08:20.241441] Epoch: [301] Total time: 0:01:41 (0.5195 s / it)
[22:08:20.253697] Averaged stats: lr: 0.001762  loss: 0.3461 (0.3552)
[22:08:24.986439] {"train_lr": 0.0017647590286847547, "train_loss": 0.3551894345726722, "epoch": 301}
[22:08:24.986652] [22:08:24.986736] Training epoch 301 for 0:01:46
[22:08:24.986844] [22:08:24.991324] log_dir: ./exp/debug/cifar100-LT/debug
[22:08:26.697003] Epoch: [302]  [  0/195]  eta: 0:05:32  lr: 0.001762  loss: 0.3264 (0.3264)  time: 1.7046  data: 1.1943  max mem: 9341
[22:08:36.916058] Epoch: [302]  [ 20/195]  eta: 0:01:39  lr: 0.001762  loss: 0.3510 (0.3532)  time: 0.5109  data: 0.0002  max mem: 9341
[22:08:47.137116] Epoch: [302]  [ 40/195]  eta: 0:01:23  lr: 0.001762  loss: 0.3424 (0.3503)  time: 0.5110  data: 0.0002  max mem: 9341
[22:08:57.352001] Epoch: [302]  [ 60/195]  eta: 0:01:11  lr: 0.001761  loss: 0.3528 (0.3511)  time: 0.5107  data: 0.0002  max mem: 9341
[22:09:07.609826] Epoch: [302]  [ 80/195]  eta: 0:01:00  lr: 0.001761  loss: 0.3456 (0.3502)  time: 0.5128  data: 0.0002  max mem: 9341
[22:09:17.831585] Epoch: [302]  [100/195]  eta: 0:00:49  lr: 0.001760  loss: 0.3496 (0.3502)  time: 0.5110  data: 0.0002  max mem: 9341
[22:09:28.054690] Epoch: [302]  [120/195]  eta: 0:00:39  lr: 0.001760  loss: 0.3480 (0.3506)  time: 0.5111  data: 0.0002  max mem: 9341
[22:09:38.271835] Epoch: [302]  [140/195]  eta: 0:00:28  lr: 0.001760  loss: 0.3472 (0.3502)  time: 0.5108  data: 0.0002  max mem: 9341
[22:09:48.530611] Epoch: [302]  [160/195]  eta: 0:00:18  lr: 0.001759  loss: 0.3593 (0.3516)  time: 0.5129  data: 0.0002  max mem: 9341
[22:09:58.702858] Epoch: [302]  [180/195]  eta: 0:00:07  lr: 0.001758  loss: 0.3413 (0.3513)  time: 0.5086  data: 0.0001  max mem: 9341
[22:10:05.833374] Epoch: [302]  [194/195]  eta: 0:00:00  lr: 0.001758  loss: 0.3541 (0.3514)  time: 0.5105  data: 0.0001  max mem: 9341
[22:10:06.000298] Epoch: [302] Total time: 0:01:41 (0.5180 s / it)
[22:10:06.024570] Averaged stats: lr: 0.001758  loss: 0.3541 (0.3521)
[22:10:10.755777] {"train_lr": 0.0017603775053680265, "train_loss": 0.35212554587767675, "epoch": 302}
[22:10:10.756156] [22:10:10.756252] Training epoch 302 for 0:01:45
[22:10:10.756308] [22:10:10.761264] log_dir: ./exp/debug/cifar100-LT/debug
[22:10:12.554314] Epoch: [303]  [  0/195]  eta: 0:05:49  lr: 0.001758  loss: 0.3880 (0.3880)  time: 1.7910  data: 1.2915  max mem: 9341
[22:10:22.769923] Epoch: [303]  [ 20/195]  eta: 0:01:40  lr: 0.001758  loss: 0.3420 (0.3438)  time: 0.5107  data: 0.0002  max mem: 9341
[22:10:32.989076] Epoch: [303]  [ 40/195]  eta: 0:01:24  lr: 0.001757  loss: 0.3551 (0.3487)  time: 0.5109  data: 0.0002  max mem: 9341
[22:10:43.231404] Epoch: [303]  [ 60/195]  eta: 0:01:11  lr: 0.001757  loss: 0.3460 (0.3505)  time: 0.5121  data: 0.0002  max mem: 9341
[22:10:53.495967] Epoch: [303]  [ 80/195]  eta: 0:01:00  lr: 0.001756  loss: 0.3403 (0.3498)  time: 0.5132  data: 0.0002  max mem: 9341
[22:11:03.711438] Epoch: [303]  [100/195]  eta: 0:00:49  lr: 0.001756  loss: 0.3512 (0.3509)  time: 0.5107  data: 0.0002  max mem: 9341
[22:11:13.929237] Epoch: [303]  [120/195]  eta: 0:00:39  lr: 0.001755  loss: 0.3578 (0.3520)  time: 0.5108  data: 0.0002  max mem: 9341
[22:11:24.150971] Epoch: [303]  [140/195]  eta: 0:00:28  lr: 0.001755  loss: 0.3456 (0.3512)  time: 0.5110  data: 0.0002  max mem: 9341
[22:11:34.413988] Epoch: [303]  [160/195]  eta: 0:00:18  lr: 0.001754  loss: 0.3483 (0.3519)  time: 0.5131  data: 0.0002  max mem: 9341
[22:11:44.589123] Epoch: [303]  [180/195]  eta: 0:00:07  lr: 0.001754  loss: 0.3539 (0.3525)  time: 0.5087  data: 0.0001  max mem: 9341
[22:11:51.721397] Epoch: [303]  [194/195]  eta: 0:00:00  lr: 0.001754  loss: 0.3585 (0.3525)  time: 0.5105  data: 0.0001  max mem: 9341
[22:11:51.890542] Epoch: [303] Total time: 0:01:41 (0.5186 s / it)
[22:11:51.899821] Averaged stats: lr: 0.001754  loss: 0.3585 (0.3534)
[22:11:56.627662] {"train_lr": 0.0017559864067458629, "train_loss": 0.35342577955661675, "epoch": 303}
[22:11:56.627991] [22:11:56.628083] Training epoch 303 for 0:01:45
[22:11:56.628152] [22:11:56.632606] log_dir: ./exp/debug/cifar100-LT/debug
[22:11:58.549015] Epoch: [304]  [  0/195]  eta: 0:06:13  lr: 0.001754  loss: 0.3416 (0.3416)  time: 1.9151  data: 1.4246  max mem: 9341
[22:12:08.772620] Epoch: [304]  [ 20/195]  eta: 0:01:41  lr: 0.001753  loss: 0.3510 (0.3512)  time: 0.5111  data: 0.0002  max mem: 9341
[22:12:18.985672] Epoch: [304]  [ 40/195]  eta: 0:01:24  lr: 0.001753  loss: 0.3545 (0.3523)  time: 0.5106  data: 0.0002  max mem: 9341
[22:12:29.196574] Epoch: [304]  [ 60/195]  eta: 0:01:12  lr: 0.001753  loss: 0.3593 (0.3553)  time: 0.5105  data: 0.0002  max mem: 9341
[22:12:39.453050] Epoch: [304]  [ 80/195]  eta: 0:01:00  lr: 0.001752  loss: 0.3414 (0.3535)  time: 0.5128  data: 0.0002  max mem: 9341
[22:12:49.665572] Epoch: [304]  [100/195]  eta: 0:00:49  lr: 0.001751  loss: 0.3414 (0.3527)  time: 0.5106  data: 0.0002  max mem: 9341
[22:12:59.875055] Epoch: [304]  [120/195]  eta: 0:00:39  lr: 0.001751  loss: 0.3543 (0.3529)  time: 0.5104  data: 0.0002  max mem: 9341
[22:13:10.090567] Epoch: [304]  [140/195]  eta: 0:00:28  lr: 0.001751  loss: 0.3491 (0.3526)  time: 0.5107  data: 0.0002  max mem: 9341
[22:13:20.348111] Epoch: [304]  [160/195]  eta: 0:00:18  lr: 0.001750  loss: 0.3500 (0.3524)  time: 0.5128  data: 0.0002  max mem: 9341
[22:13:30.521401] Epoch: [304]  [180/195]  eta: 0:00:07  lr: 0.001750  loss: 0.3567 (0.3527)  time: 0.5086  data: 0.0001  max mem: 9341
[22:13:37.647581] Epoch: [304]  [194/195]  eta: 0:00:00  lr: 0.001749  loss: 0.3529 (0.3531)  time: 0.5102  data: 0.0001  max mem: 9341
[22:13:37.820582] Epoch: [304] Total time: 0:01:41 (0.5189 s / it)
[22:13:37.837451] Averaged stats: lr: 0.001749  loss: 0.3529 (0.3521)
[22:13:42.608555] {"train_lr": 0.001751585807850027, "train_loss": 0.3520940259099007, "epoch": 304}
[22:13:42.608829] [22:13:42.608921] Training epoch 304 for 0:01:45
[22:13:42.608976] [22:13:42.613475] log_dir: ./exp/debug/cifar100-LT/debug
[22:13:44.411933] Epoch: [305]  [  0/195]  eta: 0:05:50  lr: 0.001749  loss: 0.3779 (0.3779)  time: 1.7975  data: 1.2863  max mem: 9341
[22:13:54.726815] Epoch: [305]  [ 20/195]  eta: 0:01:40  lr: 0.001749  loss: 0.3371 (0.3425)  time: 0.5157  data: 0.0002  max mem: 9341
[22:14:04.938947] Epoch: [305]  [ 40/195]  eta: 0:01:24  lr: 0.001748  loss: 0.3547 (0.3482)  time: 0.5106  data: 0.0002  max mem: 9341
[22:14:15.154237] Epoch: [305]  [ 60/195]  eta: 0:01:12  lr: 0.001748  loss: 0.3453 (0.3483)  time: 0.5107  data: 0.0002  max mem: 9341
[22:14:25.460909] Epoch: [305]  [ 80/195]  eta: 0:01:00  lr: 0.001747  loss: 0.3539 (0.3503)  time: 0.5153  data: 0.0002  max mem: 9341
[22:14:35.680957] Epoch: [305]  [100/195]  eta: 0:00:49  lr: 0.001747  loss: 0.3506 (0.3497)  time: 0.5109  data: 0.0002  max mem: 9341
[22:14:45.897414] Epoch: [305]  [120/195]  eta: 0:00:39  lr: 0.001747  loss: 0.3604 (0.3509)  time: 0.5108  data: 0.0002  max mem: 9341
[22:14:56.115154] Epoch: [305]  [140/195]  eta: 0:00:28  lr: 0.001746  loss: 0.3460 (0.3506)  time: 0.5108  data: 0.0002  max mem: 9341
[22:15:06.377957] Epoch: [305]  [160/195]  eta: 0:00:18  lr: 0.001746  loss: 0.3455 (0.3499)  time: 0.5131  data: 0.0002  max mem: 9341
[22:15:16.551384] Epoch: [305]  [180/195]  eta: 0:00:07  lr: 0.001745  loss: 0.3543 (0.3505)  time: 0.5086  data: 0.0001  max mem: 9341
[22:15:23.679419] Epoch: [305]  [194/195]  eta: 0:00:00  lr: 0.001745  loss: 0.3621 (0.3519)  time: 0.5106  data: 0.0001  max mem: 9341
[22:15:23.854379] Epoch: [305] Total time: 0:01:41 (0.5192 s / it)
[22:15:23.869304] Averaged stats: lr: 0.001745  loss: 0.3621 (0.3518)
[22:15:28.590450] {"train_lr": 0.0017471757838746064, "train_loss": 0.35177596918283366, "epoch": 305}
[22:15:28.590714] [22:15:28.590801] Training epoch 305 for 0:01:45
[22:15:28.590853] [22:15:28.595286] log_dir: ./exp/debug/cifar100-LT/debug
[22:15:30.292563] Epoch: [306]  [  0/195]  eta: 0:05:30  lr: 0.001745  loss: 0.3417 (0.3417)  time: 1.6963  data: 1.1872  max mem: 9341
[22:15:40.513084] Epoch: [306]  [ 20/195]  eta: 0:01:39  lr: 0.001744  loss: 0.3496 (0.3533)  time: 0.5109  data: 0.0002  max mem: 9341
[22:15:50.733594] Epoch: [306]  [ 40/195]  eta: 0:01:23  lr: 0.001744  loss: 0.3621 (0.3582)  time: 0.5110  data: 0.0002  max mem: 9341
[22:16:00.953346] Epoch: [306]  [ 60/195]  eta: 0:01:11  lr: 0.001744  loss: 0.3567 (0.3582)  time: 0.5109  data: 0.0002  max mem: 9341
[22:16:11.224546] Epoch: [306]  [ 80/195]  eta: 0:01:00  lr: 0.001743  loss: 0.3480 (0.3576)  time: 0.5135  data: 0.0002  max mem: 9341
[22:16:21.440807] Epoch: [306]  [100/195]  eta: 0:00:49  lr: 0.001743  loss: 0.3464 (0.3565)  time: 0.5108  data: 0.0002  max mem: 9341
[22:16:31.659937] Epoch: [306]  [120/195]  eta: 0:00:39  lr: 0.001742  loss: 0.3531 (0.3567)  time: 0.5109  data: 0.0002  max mem: 9341
[22:16:41.877463] Epoch: [306]  [140/195]  eta: 0:00:28  lr: 0.001742  loss: 0.3561 (0.3572)  time: 0.5108  data: 0.0002  max mem: 9341
[22:16:52.139421] Epoch: [306]  [160/195]  eta: 0:00:18  lr: 0.001741  loss: 0.3645 (0.3589)  time: 0.5130  data: 0.0002  max mem: 9341
[22:17:02.318850] Epoch: [306]  [180/195]  eta: 0:00:07  lr: 0.001741  loss: 0.3635 (0.3591)  time: 0.5089  data: 0.0001  max mem: 9341
[22:17:09.454367] Epoch: [306]  [194/195]  eta: 0:00:00  lr: 0.001740  loss: 0.3509 (0.3592)  time: 0.5110  data: 0.0001  max mem: 9341
[22:17:09.641712] Epoch: [306] Total time: 0:01:41 (0.5182 s / it)
[22:17:09.658815] Averaged stats: lr: 0.001740  loss: 0.3509 (0.3572)
[22:17:14.389556] {"train_lr": 0.0017427564101747497, "train_loss": 0.35718715974153614, "epoch": 306}
[22:17:14.389897] [22:17:14.389982] Training epoch 306 for 0:01:45
[22:17:14.390036] [22:17:14.394475] log_dir: ./exp/debug/cifar100-LT/debug
[22:17:16.045436] Epoch: [307]  [  0/195]  eta: 0:05:21  lr: 0.001740  loss: 0.3693 (0.3693)  time: 1.6496  data: 1.1437  max mem: 9341
[22:17:26.263552] Epoch: [307]  [ 20/195]  eta: 0:01:38  lr: 0.001740  loss: 0.3546 (0.3606)  time: 0.5108  data: 0.0002  max mem: 9341
[22:17:36.502542] Epoch: [307]  [ 40/195]  eta: 0:01:23  lr: 0.001740  loss: 0.3634 (0.3624)  time: 0.5119  data: 0.0002  max mem: 9341
[22:17:46.716400] Epoch: [307]  [ 60/195]  eta: 0:01:11  lr: 0.001739  loss: 0.3534 (0.3594)  time: 0.5106  data: 0.0002  max mem: 9341
[22:17:56.976552] Epoch: [307]  [ 80/195]  eta: 0:01:00  lr: 0.001739  loss: 0.3517 (0.3594)  time: 0.5129  data: 0.0002  max mem: 9341
[22:18:07.226288] Epoch: [307]  [100/195]  eta: 0:00:49  lr: 0.001738  loss: 0.3567 (0.3586)  time: 0.5124  data: 0.0002  max mem: 9341
[22:18:17.440251] Epoch: [307]  [120/195]  eta: 0:00:39  lr: 0.001738  loss: 0.3459 (0.3568)  time: 0.5106  data: 0.0002  max mem: 9341
[22:18:27.654614] Epoch: [307]  [140/195]  eta: 0:00:28  lr: 0.001737  loss: 0.3540 (0.3564)  time: 0.5107  data: 0.0002  max mem: 9341
[22:18:37.913688] Epoch: [307]  [160/195]  eta: 0:00:18  lr: 0.001737  loss: 0.3629 (0.3570)  time: 0.5129  data: 0.0002  max mem: 9341
[22:18:48.090957] Epoch: [307]  [180/195]  eta: 0:00:07  lr: 0.001736  loss: 0.3504 (0.3565)  time: 0.5088  data: 0.0001  max mem: 9341
[22:18:55.225207] Epoch: [307]  [194/195]  eta: 0:00:00  lr: 0.001736  loss: 0.3532 (0.3566)  time: 0.5106  data: 0.0001  max mem: 9341
[22:18:55.387744] Epoch: [307] Total time: 0:01:40 (0.5179 s / it)
[22:18:55.402426] Averaged stats: lr: 0.001736  loss: 0.3532 (0.3550)
[22:19:00.126228] {"train_lr": 0.001738327762265365, "train_loss": 0.35496641462429973, "epoch": 307}
[22:19:00.126597] [22:19:00.126687] Training epoch 307 for 0:01:45
[22:19:00.126739] [22:19:00.131807] log_dir: ./exp/debug/cifar100-LT/debug
[22:19:01.914803] Epoch: [308]  [  0/195]  eta: 0:05:47  lr: 0.001736  loss: 0.3496 (0.3496)  time: 1.7816  data: 1.2676  max mem: 9341
[22:19:12.137638] Epoch: [308]  [ 20/195]  eta: 0:01:40  lr: 0.001736  loss: 0.3572 (0.3585)  time: 0.5111  data: 0.0002  max mem: 9341
[22:19:22.380136] Epoch: [308]  [ 40/195]  eta: 0:01:24  lr: 0.001735  loss: 0.3336 (0.3500)  time: 0.5120  data: 0.0002  max mem: 9341
[22:19:32.621753] Epoch: [308]  [ 60/195]  eta: 0:01:11  lr: 0.001735  loss: 0.3562 (0.3519)  time: 0.5120  data: 0.0002  max mem: 9341
[22:19:42.927084] Epoch: [308]  [ 80/195]  eta: 0:01:00  lr: 0.001734  loss: 0.3492 (0.3519)  time: 0.5152  data: 0.0002  max mem: 9341
[22:19:53.165962] Epoch: [308]  [100/195]  eta: 0:00:49  lr: 0.001734  loss: 0.3481 (0.3513)  time: 0.5119  data: 0.0002  max mem: 9341
[22:20:03.406553] Epoch: [308]  [120/195]  eta: 0:00:39  lr: 0.001733  loss: 0.3410 (0.3502)  time: 0.5120  data: 0.0002  max mem: 9341
[22:20:13.648896] Epoch: [308]  [140/195]  eta: 0:00:28  lr: 0.001733  loss: 0.3510 (0.3504)  time: 0.5121  data: 0.0002  max mem: 9341
[22:20:23.951473] Epoch: [308]  [160/195]  eta: 0:00:18  lr: 0.001732  loss: 0.3489 (0.3503)  time: 0.5151  data: 0.0002  max mem: 9341
[22:20:34.155948] Epoch: [308]  [180/195]  eta: 0:00:07  lr: 0.001732  loss: 0.3366 (0.3493)  time: 0.5102  data: 0.0001  max mem: 9341
[22:20:41.310516] Epoch: [308]  [194/195]  eta: 0:00:00  lr: 0.001732  loss: 0.3306 (0.3488)  time: 0.5128  data: 0.0001  max mem: 9341
[22:20:41.482622] Epoch: [308] Total time: 0:01:41 (0.5197 s / it)
[22:20:41.490214] Averaged stats: lr: 0.001732  loss: 0.3306 (0.3493)
[22:20:46.333663] {"train_lr": 0.0017338899158198203, "train_loss": 0.349347389011811, "epoch": 308}
[22:20:46.334000] [22:20:46.334086] Training epoch 308 for 0:01:46
[22:20:46.334140] [22:20:46.338586] log_dir: ./exp/debug/cifar100-LT/debug
[22:20:47.846353] Epoch: [309]  [  0/195]  eta: 0:04:53  lr: 0.001731  loss: 0.3515 (0.3515)  time: 1.5060  data: 0.9959  max mem: 9341
[22:20:58.073251] Epoch: [309]  [ 20/195]  eta: 0:01:37  lr: 0.001731  loss: 0.3560 (0.3542)  time: 0.5113  data: 0.0012  max mem: 9341
[22:21:08.289668] Epoch: [309]  [ 40/195]  eta: 0:01:22  lr: 0.001731  loss: 0.3378 (0.3512)  time: 0.5108  data: 0.0002  max mem: 9341
[22:21:18.507843] Epoch: [309]  [ 60/195]  eta: 0:01:11  lr: 0.001730  loss: 0.3552 (0.3532)  time: 0.5109  data: 0.0002  max mem: 9341
[22:21:28.791343] Epoch: [309]  [ 80/195]  eta: 0:01:00  lr: 0.001730  loss: 0.3398 (0.3525)  time: 0.5141  data: 0.0002  max mem: 9341
[22:21:39.007529] Epoch: [309]  [100/195]  eta: 0:00:49  lr: 0.001729  loss: 0.3502 (0.3523)  time: 0.5107  data: 0.0002  max mem: 9341
[22:21:49.221661] Epoch: [309]  [120/195]  eta: 0:00:38  lr: 0.001729  loss: 0.3572 (0.3530)  time: 0.5107  data: 0.0002  max mem: 9341
[22:21:59.431664] Epoch: [309]  [140/195]  eta: 0:00:28  lr: 0.001729  loss: 0.3471 (0.3520)  time: 0.5104  data: 0.0002  max mem: 9341
[22:22:09.688261] Epoch: [309]  [160/195]  eta: 0:00:18  lr: 0.001728  loss: 0.3475 (0.3526)  time: 0.5128  data: 0.0002  max mem: 9341
[22:22:19.859217] Epoch: [309]  [180/195]  eta: 0:00:07  lr: 0.001727  loss: 0.3480 (0.3523)  time: 0.5085  data: 0.0001  max mem: 9341
[22:22:26.995704] Epoch: [309]  [194/195]  eta: 0:00:00  lr: 0.001727  loss: 0.3479 (0.3518)  time: 0.5108  data: 0.0001  max mem: 9341
[22:22:27.187119] Epoch: [309] Total time: 0:01:40 (0.5172 s / it)
[22:22:27.188167] Averaged stats: lr: 0.001727  loss: 0.3479 (0.3492)
[22:22:31.876336] {"train_lr": 0.0017294429466686559, "train_loss": 0.34917683070286726, "epoch": 309}
[22:22:31.876693] [22:22:31.876782] Training epoch 309 for 0:01:45
[22:22:31.876837] [22:22:31.881421] log_dir: ./exp/debug/cifar100-LT/debug
[22:22:33.807250] Epoch: [310]  [  0/195]  eta: 0:06:15  lr: 0.001727  loss: 0.3368 (0.3368)  time: 1.9234  data: 1.4369  max mem: 9341
[22:22:44.029402] Epoch: [310]  [ 20/195]  eta: 0:01:41  lr: 0.001727  loss: 0.3518 (0.3511)  time: 0.5110  data: 0.0002  max mem: 9341
[22:22:54.259556] Epoch: [310]  [ 40/195]  eta: 0:01:24  lr: 0.001726  loss: 0.3469 (0.3496)  time: 0.5114  data: 0.0002  max mem: 9341
[22:23:04.493549] Epoch: [310]  [ 60/195]  eta: 0:01:12  lr: 0.001726  loss: 0.3267 (0.3449)  time: 0.5116  data: 0.0002  max mem: 9341
[22:23:14.765640] Epoch: [310]  [ 80/195]  eta: 0:01:00  lr: 0.001725  loss: 0.3486 (0.3467)  time: 0.5135  data: 0.0002  max mem: 9341
[22:23:24.985564] Epoch: [310]  [100/195]  eta: 0:00:49  lr: 0.001725  loss: 0.3513 (0.3479)  time: 0.5109  data: 0.0002  max mem: 9341
[22:23:35.201075] Epoch: [310]  [120/195]  eta: 0:00:39  lr: 0.001724  loss: 0.3431 (0.3472)  time: 0.5107  data: 0.0002  max mem: 9341
[22:23:45.409463] Epoch: [310]  [140/195]  eta: 0:00:28  lr: 0.001724  loss: 0.3368 (0.3461)  time: 0.5104  data: 0.0002  max mem: 9341
[22:23:55.659793] Epoch: [310]  [160/195]  eta: 0:00:18  lr: 0.001723  loss: 0.3425 (0.3460)  time: 0.5125  data: 0.0002  max mem: 9341
[22:24:05.836431] Epoch: [310]  [180/195]  eta: 0:00:07  lr: 0.001723  loss: 0.3517 (0.3467)  time: 0.5088  data: 0.0001  max mem: 9341
[22:24:12.966829] Epoch: [310]  [194/195]  eta: 0:00:00  lr: 0.001723  loss: 0.3505 (0.3473)  time: 0.5105  data: 0.0001  max mem: 9341
[22:24:13.141045] Epoch: [310] Total time: 0:01:41 (0.5193 s / it)
[22:24:13.144758] Averaged stats: lr: 0.001723  loss: 0.3505 (0.3482)
[22:24:17.838771] {"train_lr": 0.0017249869307983294, "train_loss": 0.34824348993790455, "epoch": 310}
[22:24:17.839066] [22:24:17.839155] Training epoch 310 for 0:01:45
[22:24:17.839210] [22:24:17.843829] log_dir: ./exp/debug/cifar100-LT/debug
[22:24:19.605669] Epoch: [311]  [  0/195]  eta: 0:05:43  lr: 0.001723  loss: 0.3434 (0.3434)  time: 1.7609  data: 1.2626  max mem: 9341
[22:24:29.835029] Epoch: [311]  [ 20/195]  eta: 0:01:39  lr: 0.001722  loss: 0.3450 (0.3502)  time: 0.5114  data: 0.0002  max mem: 9341
[22:24:40.053391] Epoch: [311]  [ 40/195]  eta: 0:01:23  lr: 0.001722  loss: 0.3506 (0.3475)  time: 0.5109  data: 0.0002  max mem: 9341
[22:24:50.272534] Epoch: [311]  [ 60/195]  eta: 0:01:11  lr: 0.001721  loss: 0.3406 (0.3487)  time: 0.5109  data: 0.0002  max mem: 9341
[22:25:00.531911] Epoch: [311]  [ 80/195]  eta: 0:01:00  lr: 0.001721  loss: 0.3528 (0.3483)  time: 0.5129  data: 0.0002  max mem: 9341
[22:25:10.751011] Epoch: [311]  [100/195]  eta: 0:00:49  lr: 0.001720  loss: 0.3542 (0.3487)  time: 0.5109  data: 0.0002  max mem: 9341
[22:25:20.961183] Epoch: [311]  [120/195]  eta: 0:00:39  lr: 0.001720  loss: 0.3569 (0.3500)  time: 0.5105  data: 0.0002  max mem: 9341
[22:25:31.175812] Epoch: [311]  [140/195]  eta: 0:00:28  lr: 0.001720  loss: 0.3360 (0.3481)  time: 0.5107  data: 0.0002  max mem: 9341
[22:25:41.438836] Epoch: [311]  [160/195]  eta: 0:00:18  lr: 0.001719  loss: 0.3448 (0.3477)  time: 0.5131  data: 0.0002  max mem: 9341
[22:25:51.615117] Epoch: [311]  [180/195]  eta: 0:00:07  lr: 0.001719  loss: 0.3574 (0.3483)  time: 0.5088  data: 0.0001  max mem: 9341
[22:25:58.747027] Epoch: [311]  [194/195]  eta: 0:00:00  lr: 0.001718  loss: 0.3492 (0.3483)  time: 0.5107  data: 0.0001  max mem: 9341
[22:25:58.911029] Epoch: [311] Total time: 0:01:41 (0.5183 s / it)
[22:25:58.913529] Averaged stats: lr: 0.001718  loss: 0.3492 (0.3478)
[22:26:03.578394] {"train_lr": 0.001720521944349839, "train_loss": 0.3477603506965515, "epoch": 311}
[22:26:03.578751] [22:26:03.578834] Training epoch 311 for 0:01:45
[22:26:03.578887] [22:26:03.583363] log_dir: ./exp/debug/cifar100-LT/debug
[22:26:05.195615] Epoch: [312]  [  0/195]  eta: 0:05:14  lr: 0.001718  loss: 0.3380 (0.3380)  time: 1.6109  data: 1.1227  max mem: 9341
[22:26:15.419972] Epoch: [312]  [ 20/195]  eta: 0:01:38  lr: 0.001718  loss: 0.3403 (0.3451)  time: 0.5112  data: 0.0002  max mem: 9341
[22:26:25.646566] Epoch: [312]  [ 40/195]  eta: 0:01:23  lr: 0.001717  loss: 0.3534 (0.3490)  time: 0.5113  data: 0.0002  max mem: 9341
[22:26:35.868376] Epoch: [312]  [ 60/195]  eta: 0:01:11  lr: 0.001717  loss: 0.3466 (0.3487)  time: 0.5110  data: 0.0002  max mem: 9341
[22:26:46.134662] Epoch: [312]  [ 80/195]  eta: 0:01:00  lr: 0.001716  loss: 0.3445 (0.3490)  time: 0.5132  data: 0.0002  max mem: 9341
[22:26:56.347891] Epoch: [312]  [100/195]  eta: 0:00:49  lr: 0.001716  loss: 0.3509 (0.3495)  time: 0.5106  data: 0.0002  max mem: 9341
[22:27:06.562854] Epoch: [312]  [120/195]  eta: 0:00:39  lr: 0.001716  loss: 0.3550 (0.3504)  time: 0.5107  data: 0.0002  max mem: 9341
[22:27:16.773334] Epoch: [312]  [140/195]  eta: 0:00:28  lr: 0.001715  loss: 0.3478 (0.3509)  time: 0.5105  data: 0.0002  max mem: 9341
[22:27:27.031611] Epoch: [312]  [160/195]  eta: 0:00:18  lr: 0.001714  loss: 0.3367 (0.3503)  time: 0.5129  data: 0.0002  max mem: 9341
[22:27:37.202080] Epoch: [312]  [180/195]  eta: 0:00:07  lr: 0.001714  loss: 0.3485 (0.3503)  time: 0.5085  data: 0.0001  max mem: 9341
[22:27:44.331471] Epoch: [312]  [194/195]  eta: 0:00:00  lr: 0.001714  loss: 0.3576 (0.3506)  time: 0.5104  data: 0.0001  max mem: 9341
[22:27:44.510736] Epoch: [312] Total time: 0:01:40 (0.5176 s / it)
[22:27:44.545056] Averaged stats: lr: 0.001714  loss: 0.3576 (0.3470)
[22:27:49.212501] {"train_lr": 0.0017160480636174895, "train_loss": 0.34702789130119177, "epoch": 312}
[22:27:49.212828] [22:27:49.212917] Training epoch 312 for 0:01:45
[22:27:49.212971] [22:27:49.217431] log_dir: ./exp/debug/cifar100-LT/debug
[22:27:51.044927] Epoch: [313]  [  0/195]  eta: 0:05:56  lr: 0.001714  loss: 0.3533 (0.3533)  time: 1.8262  data: 1.3181  max mem: 9341
[22:28:01.264231] Epoch: [313]  [ 20/195]  eta: 0:01:40  lr: 0.001713  loss: 0.3282 (0.3408)  time: 0.5109  data: 0.0002  max mem: 9341
[22:28:11.480042] Epoch: [313]  [ 40/195]  eta: 0:01:24  lr: 0.001713  loss: 0.3388 (0.3452)  time: 0.5107  data: 0.0002  max mem: 9341
[22:28:21.717113] Epoch: [313]  [ 60/195]  eta: 0:01:11  lr: 0.001713  loss: 0.3475 (0.3463)  time: 0.5118  data: 0.0002  max mem: 9341
[22:28:32.019866] Epoch: [313]  [ 80/195]  eta: 0:01:00  lr: 0.001712  loss: 0.3353 (0.3451)  time: 0.5151  data: 0.0002  max mem: 9341
[22:28:42.261778] Epoch: [313]  [100/195]  eta: 0:00:49  lr: 0.001711  loss: 0.3345 (0.3434)  time: 0.5120  data: 0.0002  max mem: 9341
[22:28:52.500618] Epoch: [313]  [120/195]  eta: 0:00:39  lr: 0.001711  loss: 0.3351 (0.3423)  time: 0.5119  data: 0.0002  max mem: 9341
[22:29:02.735840] Epoch: [313]  [140/195]  eta: 0:00:28  lr: 0.001711  loss: 0.3400 (0.3420)  time: 0.5117  data: 0.0002  max mem: 9341
[22:29:13.038856] Epoch: [313]  [160/195]  eta: 0:00:18  lr: 0.001710  loss: 0.3468 (0.3427)  time: 0.5151  data: 0.0002  max mem: 9341
[22:29:23.228399] Epoch: [313]  [180/195]  eta: 0:00:07  lr: 0.001710  loss: 0.3488 (0.3434)  time: 0.5094  data: 0.0001  max mem: 9341
[22:29:30.377865] Epoch: [313]  [194/195]  eta: 0:00:00  lr: 0.001709  loss: 0.3384 (0.3429)  time: 0.5123  data: 0.0001  max mem: 9341
[22:29:30.543560] Epoch: [313] Total time: 0:01:41 (0.5196 s / it)
[22:29:30.565206] Averaged stats: lr: 0.001709  loss: 0.3384 (0.3469)
[22:29:35.314761] {"train_lr": 0.0017115653650475585, "train_loss": 0.346865408657453, "epoch": 313}
[22:29:35.315091] [22:29:35.315175] Training epoch 313 for 0:01:46
[22:29:35.315230] [22:29:35.319654] log_dir: ./exp/debug/cifar100-LT/debug
[22:29:37.125559] Epoch: [314]  [  0/195]  eta: 0:05:51  lr: 0.001709  loss: 0.3635 (0.3635)  time: 1.8049  data: 1.2976  max mem: 9341
[22:29:47.368365] Epoch: [314]  [ 20/195]  eta: 0:01:40  lr: 0.001709  loss: 0.3537 (0.3529)  time: 0.5121  data: 0.0002  max mem: 9341
[22:29:57.608448] Epoch: [314]  [ 40/195]  eta: 0:01:24  lr: 0.001708  loss: 0.3381 (0.3448)  time: 0.5119  data: 0.0002  max mem: 9341
[22:30:07.853058] Epoch: [314]  [ 60/195]  eta: 0:01:11  lr: 0.001708  loss: 0.3451 (0.3454)  time: 0.5122  data: 0.0002  max mem: 9341
[22:30:18.152976] Epoch: [314]  [ 80/195]  eta: 0:01:00  lr: 0.001707  loss: 0.3548 (0.3480)  time: 0.5149  data: 0.0002  max mem: 9341
[22:30:28.391200] Epoch: [314]  [100/195]  eta: 0:00:49  lr: 0.001707  loss: 0.3428 (0.3473)  time: 0.5119  data: 0.0002  max mem: 9341
[22:30:38.632423] Epoch: [314]  [120/195]  eta: 0:00:39  lr: 0.001707  loss: 0.3562 (0.3489)  time: 0.5120  data: 0.0002  max mem: 9341
[22:30:48.861749] Epoch: [314]  [140/195]  eta: 0:00:28  lr: 0.001706  loss: 0.3528 (0.3493)  time: 0.5114  data: 0.0002  max mem: 9341
[22:30:59.164661] Epoch: [314]  [160/195]  eta: 0:00:18  lr: 0.001705  loss: 0.3405 (0.3487)  time: 0.5151  data: 0.0002  max mem: 9341
[22:31:09.357837] Epoch: [314]  [180/195]  eta: 0:00:07  lr: 0.001705  loss: 0.3521 (0.3489)  time: 0.5096  data: 0.0001  max mem: 9341
[22:31:16.513191] Epoch: [314]  [194/195]  eta: 0:00:00  lr: 0.001705  loss: 0.3454 (0.3485)  time: 0.5129  data: 0.0001  max mem: 9341
[22:31:16.680132] Epoch: [314] Total time: 0:01:41 (0.5198 s / it)
[22:31:16.697963] Averaged stats: lr: 0.001705  loss: 0.3454 (0.3483)
[22:31:21.415225] {"train_lr": 0.0017070739252369807, "train_loss": 0.34834949932037257, "epoch": 314}
[22:31:21.415486] [22:31:21.415571] Training epoch 314 for 0:01:46
[22:31:21.415624] [22:31:21.420070] log_dir: ./exp/debug/cifar100-LT/debug
[22:31:23.203773] Epoch: [315]  [  0/195]  eta: 0:05:47  lr: 0.001705  loss: 0.3282 (0.3282)  time: 1.7824  data: 1.2844  max mem: 9341
[22:31:33.444956] Epoch: [315]  [ 20/195]  eta: 0:01:40  lr: 0.001704  loss: 0.3334 (0.3382)  time: 0.5120  data: 0.0002  max mem: 9341
[22:31:43.691399] Epoch: [315]  [ 40/195]  eta: 0:01:24  lr: 0.001704  loss: 0.3523 (0.3481)  time: 0.5122  data: 0.0002  max mem: 9341
[22:31:53.905486] Epoch: [315]  [ 60/195]  eta: 0:01:11  lr: 0.001704  loss: 0.3401 (0.3471)  time: 0.5106  data: 0.0002  max mem: 9341
[22:32:04.167822] Epoch: [315]  [ 80/195]  eta: 0:01:00  lr: 0.001703  loss: 0.3439 (0.3468)  time: 0.5131  data: 0.0002  max mem: 9341
[22:32:14.374815] Epoch: [315]  [100/195]  eta: 0:00:49  lr: 0.001702  loss: 0.3456 (0.3456)  time: 0.5103  data: 0.0002  max mem: 9341
[22:32:24.593118] Epoch: [315]  [120/195]  eta: 0:00:39  lr: 0.001702  loss: 0.3441 (0.3466)  time: 0.5109  data: 0.0002  max mem: 9341
[22:32:34.802293] Epoch: [315]  [140/195]  eta: 0:00:28  lr: 0.001702  loss: 0.3477 (0.3463)  time: 0.5104  data: 0.0002  max mem: 9341
[22:32:45.062647] Epoch: [315]  [160/195]  eta: 0:00:18  lr: 0.001701  loss: 0.3547 (0.3468)  time: 0.5130  data: 0.0002  max mem: 9341
[22:32:55.231153] Epoch: [315]  [180/195]  eta: 0:00:07  lr: 0.001701  loss: 0.3448 (0.3465)  time: 0.5084  data: 0.0001  max mem: 9341
[22:33:02.363000] Epoch: [315]  [194/195]  eta: 0:00:00  lr: 0.001700  loss: 0.3333 (0.3460)  time: 0.5106  data: 0.0001  max mem: 9341
[22:33:02.533624] Epoch: [315] Total time: 0:01:41 (0.5185 s / it)
[22:33:02.541425] Averaged stats: lr: 0.001700  loss: 0.3333 (0.3456)
[22:33:07.207534] {"train_lr": 0.0017025738209321022, "train_loss": 0.3455963308994587, "epoch": 315}
[22:33:07.207794] [22:33:07.207878] Training epoch 315 for 0:01:45
[22:33:07.207941] [22:33:07.212370] log_dir: ./exp/debug/cifar100-LT/debug
[22:33:08.935317] Epoch: [316]  [  0/195]  eta: 0:05:35  lr: 0.001700  loss: 0.3310 (0.3310)  time: 1.7215  data: 1.2167  max mem: 9341
[22:33:19.156923] Epoch: [316]  [ 20/195]  eta: 0:01:39  lr: 0.001700  loss: 0.3440 (0.3470)  time: 0.5110  data: 0.0003  max mem: 9341
[22:33:29.375265] Epoch: [316]  [ 40/195]  eta: 0:01:23  lr: 0.001699  loss: 0.3335 (0.3415)  time: 0.5109  data: 0.0002  max mem: 9341
[22:33:39.598144] Epoch: [316]  [ 60/195]  eta: 0:01:11  lr: 0.001699  loss: 0.3601 (0.3460)  time: 0.5111  data: 0.0002  max mem: 9341
[22:33:49.859376] Epoch: [316]  [ 80/195]  eta: 0:01:00  lr: 0.001698  loss: 0.3369 (0.3456)  time: 0.5130  data: 0.0002  max mem: 9341
[22:34:00.069120] Epoch: [316]  [100/195]  eta: 0:00:49  lr: 0.001698  loss: 0.3376 (0.3461)  time: 0.5104  data: 0.0002  max mem: 9341
[22:34:10.284696] Epoch: [316]  [120/195]  eta: 0:00:39  lr: 0.001698  loss: 0.3448 (0.3463)  time: 0.5107  data: 0.0002  max mem: 9341
[22:34:20.502067] Epoch: [316]  [140/195]  eta: 0:00:28  lr: 0.001697  loss: 0.3448 (0.3463)  time: 0.5108  data: 0.0002  max mem: 9341
[22:34:30.764747] Epoch: [316]  [160/195]  eta: 0:00:18  lr: 0.001696  loss: 0.3415 (0.3457)  time: 0.5131  data: 0.0002  max mem: 9341
[22:34:40.938870] Epoch: [316]  [180/195]  eta: 0:00:07  lr: 0.001696  loss: 0.3504 (0.3459)  time: 0.5087  data: 0.0001  max mem: 9341
[22:34:48.078141] Epoch: [316]  [194/195]  eta: 0:00:00  lr: 0.001696  loss: 0.3425 (0.3458)  time: 0.5109  data: 0.0001  max mem: 9341
[22:34:48.246141] Epoch: [316] Total time: 0:01:41 (0.5181 s / it)
[22:34:48.263273] Averaged stats: lr: 0.001696  loss: 0.3425 (0.3460)
[22:34:52.963854] {"train_lr": 0.0016980651290272636, "train_loss": 0.3459799207173861, "epoch": 316}
[22:34:52.964213] [22:34:52.964313] Training epoch 316 for 0:01:45
[22:34:52.964368] [22:34:52.968730] log_dir: ./exp/debug/cifar100-LT/debug
[22:34:54.868306] Epoch: [317]  [  0/195]  eta: 0:06:10  lr: 0.001696  loss: 0.3432 (0.3432)  time: 1.8982  data: 1.3962  max mem: 9341
[22:35:05.133913] Epoch: [317]  [ 20/195]  eta: 0:01:41  lr: 0.001695  loss: 0.3360 (0.3379)  time: 0.5132  data: 0.0002  max mem: 9341
[22:35:15.349993] Epoch: [317]  [ 40/195]  eta: 0:01:24  lr: 0.001695  loss: 0.3432 (0.3428)  time: 0.5107  data: 0.0002  max mem: 9341
[22:35:25.561814] Epoch: [317]  [ 60/195]  eta: 0:01:12  lr: 0.001695  loss: 0.3477 (0.3439)  time: 0.5105  data: 0.0002  max mem: 9341
[22:35:35.820727] Epoch: [317]  [ 80/195]  eta: 0:01:00  lr: 0.001694  loss: 0.3447 (0.3444)  time: 0.5129  data: 0.0002  max mem: 9341
[22:35:46.037829] Epoch: [317]  [100/195]  eta: 0:00:49  lr: 0.001693  loss: 0.3353 (0.3449)  time: 0.5108  data: 0.0002  max mem: 9341
[22:35:56.254531] Epoch: [317]  [120/195]  eta: 0:00:39  lr: 0.001693  loss: 0.3563 (0.3464)  time: 0.5108  data: 0.0002  max mem: 9341
[22:36:06.466229] Epoch: [317]  [140/195]  eta: 0:00:28  lr: 0.001693  loss: 0.3358 (0.3462)  time: 0.5105  data: 0.0002  max mem: 9341
[22:36:16.718212] Epoch: [317]  [160/195]  eta: 0:00:18  lr: 0.001692  loss: 0.3522 (0.3465)  time: 0.5125  data: 0.0002  max mem: 9341
[22:36:26.886671] Epoch: [317]  [180/195]  eta: 0:00:07  lr: 0.001692  loss: 0.3492 (0.3469)  time: 0.5084  data: 0.0001  max mem: 9341
[22:36:34.012870] Epoch: [317]  [194/195]  eta: 0:00:00  lr: 0.001691  loss: 0.3525 (0.3467)  time: 0.5102  data: 0.0001  max mem: 9341
[22:36:34.174572] Epoch: [317] Total time: 0:01:41 (0.5190 s / it)
[22:36:34.188074] Averaged stats: lr: 0.001691  loss: 0.3525 (0.3451)
[22:36:38.848106] {"train_lr": 0.0016935479265635864, "train_loss": 0.3450852152628776, "epoch": 317}
[22:36:38.848445] [22:36:38.848537] Training epoch 317 for 0:01:45
[22:36:38.848589] [22:36:38.853014] log_dir: ./exp/debug/cifar100-LT/debug
[22:36:40.596618] Epoch: [318]  [  0/195]  eta: 0:05:39  lr: 0.001691  loss: 0.3458 (0.3458)  time: 1.7416  data: 1.2435  max mem: 9341
[22:36:50.841666] Epoch: [318]  [ 20/195]  eta: 0:01:39  lr: 0.001691  loss: 0.3470 (0.3442)  time: 0.5122  data: 0.0002  max mem: 9341
[22:37:01.096981] Epoch: [318]  [ 40/195]  eta: 0:01:24  lr: 0.001690  loss: 0.3438 (0.3436)  time: 0.5127  data: 0.0002  max mem: 9341
[22:37:11.310114] Epoch: [318]  [ 60/195]  eta: 0:01:11  lr: 0.001690  loss: 0.3498 (0.3471)  time: 0.5106  data: 0.0002  max mem: 9341
[22:37:21.584746] Epoch: [318]  [ 80/195]  eta: 0:01:00  lr: 0.001689  loss: 0.3483 (0.3461)  time: 0.5137  data: 0.0002  max mem: 9341
[22:37:31.797168] Epoch: [318]  [100/195]  eta: 0:00:49  lr: 0.001689  loss: 0.3415 (0.3451)  time: 0.5106  data: 0.0002  max mem: 9341
[22:37:42.012582] Epoch: [318]  [120/195]  eta: 0:00:39  lr: 0.001689  loss: 0.3411 (0.3455)  time: 0.5107  data: 0.0002  max mem: 9341
[22:37:52.219954] Epoch: [318]  [140/195]  eta: 0:00:28  lr: 0.001688  loss: 0.3469 (0.3458)  time: 0.5103  data: 0.0002  max mem: 9341
[22:38:02.482232] Epoch: [318]  [160/195]  eta: 0:00:18  lr: 0.001687  loss: 0.3381 (0.3452)  time: 0.5131  data: 0.0002  max mem: 9341
[22:38:12.653935] Epoch: [318]  [180/195]  eta: 0:00:07  lr: 0.001687  loss: 0.3340 (0.3446)  time: 0.5085  data: 0.0001  max mem: 9341
[22:38:19.785594] Epoch: [318]  [194/195]  eta: 0:00:00  lr: 0.001687  loss: 0.3511 (0.3446)  time: 0.5105  data: 0.0001  max mem: 9341
[22:38:19.959415] Epoch: [318] Total time: 0:01:41 (0.5185 s / it)
[22:38:19.977492] Averaged stats: lr: 0.001687  loss: 0.3511 (0.3461)
[22:38:24.685354] {"train_lr": 0.0016890222907275842, "train_loss": 0.3461026103832783, "epoch": 318}
[22:38:24.685632] [22:38:24.685718] Training epoch 318 for 0:01:45
[22:38:24.685772] [22:38:24.690780] log_dir: ./exp/debug/cifar100-LT/debug
[22:38:26.305698] Epoch: [319]  [  0/195]  eta: 0:05:14  lr: 0.001687  loss: 0.3493 (0.3493)  time: 1.6134  data: 1.0934  max mem: 9341
[22:38:36.530242] Epoch: [319]  [ 20/195]  eta: 0:01:38  lr: 0.001686  loss: 0.3540 (0.3492)  time: 0.5112  data: 0.0002  max mem: 9341
[22:38:46.746792] Epoch: [319]  [ 40/195]  eta: 0:01:23  lr: 0.001686  loss: 0.3440 (0.3468)  time: 0.5108  data: 0.0002  max mem: 9341
[22:38:56.982608] Epoch: [319]  [ 60/195]  eta: 0:01:11  lr: 0.001685  loss: 0.3423 (0.3462)  time: 0.5117  data: 0.0002  max mem: 9341
[22:39:07.283620] Epoch: [319]  [ 80/195]  eta: 0:01:00  lr: 0.001685  loss: 0.3395 (0.3456)  time: 0.5150  data: 0.0002  max mem: 9341
[22:39:17.519090] Epoch: [319]  [100/195]  eta: 0:00:49  lr: 0.001684  loss: 0.3476 (0.3472)  time: 0.5117  data: 0.0002  max mem: 9341
[22:39:27.754065] Epoch: [319]  [120/195]  eta: 0:00:39  lr: 0.001684  loss: 0.3426 (0.3465)  time: 0.5117  data: 0.0002  max mem: 9341
[22:39:37.967154] Epoch: [319]  [140/195]  eta: 0:00:28  lr: 0.001684  loss: 0.3371 (0.3455)  time: 0.5106  data: 0.0002  max mem: 9341
[22:39:48.220837] Epoch: [319]  [160/195]  eta: 0:00:18  lr: 0.001683  loss: 0.3381 (0.3448)  time: 0.5126  data: 0.0002  max mem: 9341
[22:39:58.393761] Epoch: [319]  [180/195]  eta: 0:00:07  lr: 0.001682  loss: 0.3474 (0.3453)  time: 0.5086  data: 0.0001  max mem: 9341
[22:40:05.530231] Epoch: [319]  [194/195]  eta: 0:00:00  lr: 0.001682  loss: 0.3478 (0.3452)  time: 0.5108  data: 0.0001  max mem: 9341
[22:40:05.691386] Epoch: [319] Total time: 0:01:41 (0.5180 s / it)
[22:40:05.715857] Averaged stats: lr: 0.001682  loss: 0.3478 (0.3466)
[22:40:10.531466] {"train_lr": 0.0016844882988498943, "train_loss": 0.34658306218110596, "epoch": 319}
[22:40:10.531795] [22:40:10.531879] Training epoch 319 for 0:01:45
[22:40:10.531933] [22:40:10.536455] log_dir: ./exp/debug/cifar100-LT/debug
[22:40:12.444615] Epoch: [320]  [  0/195]  eta: 0:06:11  lr: 0.001682  loss: 0.3559 (0.3559)  time: 1.9066  data: 1.4084  max mem: 9341
[22:40:22.661336] Epoch: [320]  [ 20/195]  eta: 0:01:41  lr: 0.001682  loss: 0.3392 (0.3428)  time: 0.5108  data: 0.0002  max mem: 9341
[22:40:32.875064] Epoch: [320]  [ 40/195]  eta: 0:01:24  lr: 0.001681  loss: 0.3517 (0.3428)  time: 0.5106  data: 0.0002  max mem: 9341
[22:40:43.084117] Epoch: [320]  [ 60/195]  eta: 0:01:12  lr: 0.001681  loss: 0.3509 (0.3449)  time: 0.5104  data: 0.0002  max mem: 9341
[22:40:53.347304] Epoch: [320]  [ 80/195]  eta: 0:01:00  lr: 0.001680  loss: 0.3480 (0.3458)  time: 0.5131  data: 0.0002  max mem: 9341
[22:41:03.563853] Epoch: [320]  [100/195]  eta: 0:00:49  lr: 0.001680  loss: 0.3404 (0.3449)  time: 0.5108  data: 0.0002  max mem: 9341
[22:41:13.775405] Epoch: [320]  [120/195]  eta: 0:00:39  lr: 0.001679  loss: 0.3552 (0.3454)  time: 0.5105  data: 0.0002  max mem: 9341
[22:41:23.991044] Epoch: [320]  [140/195]  eta: 0:00:28  lr: 0.001679  loss: 0.3481 (0.3449)  time: 0.5107  data: 0.0002  max mem: 9341
[22:41:34.247396] Epoch: [320]  [160/195]  eta: 0:00:18  lr: 0.001678  loss: 0.3453 (0.3451)  time: 0.5128  data: 0.0002  max mem: 9341
[22:41:44.419780] Epoch: [320]  [180/195]  eta: 0:00:07  lr: 0.001678  loss: 0.3449 (0.3449)  time: 0.5086  data: 0.0001  max mem: 9341
[22:41:51.557208] Epoch: [320]  [194/195]  eta: 0:00:00  lr: 0.001678  loss: 0.3499 (0.3451)  time: 0.5108  data: 0.0001  max mem: 9341
[22:41:51.714361] Epoch: [320] Total time: 0:01:41 (0.5189 s / it)
[22:41:51.726047] Averaged stats: lr: 0.001678  loss: 0.3499 (0.3453)
[22:41:56.636347] {"train_lr": 0.0016799460284039256, "train_loss": 0.34528985569874443, "epoch": 320}
[22:41:56.636586] [22:41:56.636675] Training epoch 320 for 0:01:46
[22:41:56.636729] [22:41:56.641749] log_dir: ./exp/debug/cifar100-LT/debug
[22:41:58.361470] Epoch: [321]  [  0/195]  eta: 0:05:35  lr: 0.001677  loss: 0.3413 (0.3413)  time: 1.7189  data: 1.2268  max mem: 9341
[22:42:08.577542] Epoch: [321]  [ 20/195]  eta: 0:01:39  lr: 0.001677  loss: 0.3317 (0.3404)  time: 0.5108  data: 0.0002  max mem: 9341
[22:42:18.795197] Epoch: [321]  [ 40/195]  eta: 0:01:23  lr: 0.001677  loss: 0.3491 (0.3436)  time: 0.5108  data: 0.0002  max mem: 9341
[22:42:29.015490] Epoch: [321]  [ 60/195]  eta: 0:01:11  lr: 0.001676  loss: 0.3447 (0.3444)  time: 0.5110  data: 0.0002  max mem: 9341
[22:42:39.273302] Epoch: [321]  [ 80/195]  eta: 0:01:00  lr: 0.001676  loss: 0.3380 (0.3441)  time: 0.5128  data: 0.0002  max mem: 9341
[22:42:49.493802] Epoch: [321]  [100/195]  eta: 0:00:49  lr: 0.001675  loss: 0.3486 (0.3459)  time: 0.5110  data: 0.0002  max mem: 9341
[22:42:59.706452] Epoch: [321]  [120/195]  eta: 0:00:39  lr: 0.001675  loss: 0.3484 (0.3455)  time: 0.5106  data: 0.0002  max mem: 9341
[22:43:09.920258] Epoch: [321]  [140/195]  eta: 0:00:28  lr: 0.001674  loss: 0.3439 (0.3453)  time: 0.5106  data: 0.0002  max mem: 9341
[22:43:20.218617] Epoch: [321]  [160/195]  eta: 0:00:18  lr: 0.001674  loss: 0.3407 (0.3447)  time: 0.5149  data: 0.0002  max mem: 9341
[22:43:30.410221] Epoch: [321]  [180/195]  eta: 0:00:07  lr: 0.001673  loss: 0.3353 (0.3439)  time: 0.5094  data: 0.0001  max mem: 9341
[22:43:37.559981] Epoch: [321]  [194/195]  eta: 0:00:00  lr: 0.001673  loss: 0.3286 (0.3438)  time: 0.5124  data: 0.0001  max mem: 9341
[22:43:37.719154] Epoch: [321] Total time: 0:01:41 (0.5183 s / it)
[22:43:37.738577] Averaged stats: lr: 0.001673  loss: 0.3286 (0.3467)
[22:43:42.316907] {"train_lr": 0.001675395557004541, "train_loss": 0.34669698755710554, "epoch": 321}
[22:43:42.317261] [22:43:42.317367] Training epoch 321 for 0:01:45
[22:43:42.317421] [22:43:42.321876] log_dir: ./exp/debug/cifar100-LT/debug
[22:43:43.872203] Epoch: [322]  [  0/195]  eta: 0:05:01  lr: 0.001673  loss: 0.3309 (0.3309)  time: 1.5486  data: 1.0314  max mem: 9341
[22:43:54.111218] Epoch: [322]  [ 20/195]  eta: 0:01:38  lr: 0.001673  loss: 0.3370 (0.3427)  time: 0.5119  data: 0.0002  max mem: 9341
[22:44:04.329491] Epoch: [322]  [ 40/195]  eta: 0:01:23  lr: 0.001672  loss: 0.3325 (0.3375)  time: 0.5108  data: 0.0002  max mem: 9341
[22:44:14.541243] Epoch: [322]  [ 60/195]  eta: 0:01:11  lr: 0.001672  loss: 0.3450 (0.3407)  time: 0.5105  data: 0.0002  max mem: 9341
[22:44:24.798919] Epoch: [322]  [ 80/195]  eta: 0:01:00  lr: 0.001671  loss: 0.3430 (0.3418)  time: 0.5128  data: 0.0002  max mem: 9341
[22:44:35.028718] Epoch: [322]  [100/195]  eta: 0:00:49  lr: 0.001671  loss: 0.3369 (0.3424)  time: 0.5114  data: 0.0002  max mem: 9341
[22:44:45.261822] Epoch: [322]  [120/195]  eta: 0:00:39  lr: 0.001670  loss: 0.3328 (0.3419)  time: 0.5116  data: 0.0002  max mem: 9341
[22:44:55.496467] Epoch: [322]  [140/195]  eta: 0:00:28  lr: 0.001670  loss: 0.3394 (0.3421)  time: 0.5117  data: 0.0002  max mem: 9341
[22:45:05.796123] Epoch: [322]  [160/195]  eta: 0:00:18  lr: 0.001669  loss: 0.3309 (0.3421)  time: 0.5149  data: 0.0002  max mem: 9341
[22:45:15.989304] Epoch: [322]  [180/195]  eta: 0:00:07  lr: 0.001669  loss: 0.3448 (0.3418)  time: 0.5096  data: 0.0001  max mem: 9341
[22:45:23.140976] Epoch: [322]  [194/195]  eta: 0:00:00  lr: 0.001668  loss: 0.3284 (0.3406)  time: 0.5126  data: 0.0001  max mem: 9341
[22:45:23.311689] Epoch: [322] Total time: 0:01:40 (0.5179 s / it)
[22:45:23.327561] Averaged stats: lr: 0.001668  loss: 0.3284 (0.3436)
[22:45:28.050548] {"train_lr": 0.0016708369624067576, "train_loss": 0.3435660464259294, "epoch": 322}
[22:45:28.050809] [22:45:28.050902] Training epoch 322 for 0:01:45
[22:45:28.050957] [22:45:28.055374] log_dir: ./exp/debug/cifar100-LT/debug
[22:45:29.871439] Epoch: [323]  [  0/195]  eta: 0:05:53  lr: 0.001668  loss: 0.3428 (0.3428)  time: 1.8148  data: 1.3185  max mem: 9341
[22:45:40.088391] Epoch: [323]  [ 20/195]  eta: 0:01:40  lr: 0.001668  loss: 0.3542 (0.3508)  time: 0.5108  data: 0.0002  max mem: 9341
[22:45:50.320924] Epoch: [323]  [ 40/195]  eta: 0:01:24  lr: 0.001668  loss: 0.3464 (0.3500)  time: 0.5116  data: 0.0002  max mem: 9341
[22:46:00.562285] Epoch: [323]  [ 60/195]  eta: 0:01:11  lr: 0.001667  loss: 0.3315 (0.3441)  time: 0.5120  data: 0.0002  max mem: 9341
[22:46:10.822308] Epoch: [323]  [ 80/195]  eta: 0:01:00  lr: 0.001666  loss: 0.3416 (0.3446)  time: 0.5129  data: 0.0002  max mem: 9341
[22:46:21.074269] Epoch: [323]  [100/195]  eta: 0:00:49  lr: 0.001666  loss: 0.3360 (0.3428)  time: 0.5125  data: 0.0002  max mem: 9341
[22:46:31.290078] Epoch: [323]  [120/195]  eta: 0:00:39  lr: 0.001666  loss: 0.3439 (0.3432)  time: 0.5107  data: 0.0002  max mem: 9341
[22:46:41.499765] Epoch: [323]  [140/195]  eta: 0:00:28  lr: 0.001665  loss: 0.3581 (0.3447)  time: 0.5104  data: 0.0002  max mem: 9341
[22:46:51.750174] Epoch: [323]  [160/195]  eta: 0:00:18  lr: 0.001665  loss: 0.3452 (0.3449)  time: 0.5125  data: 0.0002  max mem: 9341
[22:47:01.921882] Epoch: [323]  [180/195]  eta: 0:00:07  lr: 0.001664  loss: 0.3628 (0.3466)  time: 0.5085  data: 0.0002  max mem: 9341
[22:47:09.054310] Epoch: [323]  [194/195]  eta: 0:00:00  lr: 0.001664  loss: 0.3557 (0.3466)  time: 0.5105  data: 0.0001  max mem: 9341
[22:47:09.225775] Epoch: [323] Total time: 0:01:41 (0.5188 s / it)
[22:47:09.239019] Averaged stats: lr: 0.001664  loss: 0.3557 (0.3459)
[22:47:14.063376] {"train_lr": 0.0016662703225043655, "train_loss": 0.3458676190712513, "epoch": 323}
[22:47:14.063712] [22:47:14.063800] Training epoch 323 for 0:01:46
[22:47:14.063853] [22:47:14.068311] log_dir: ./exp/debug/cifar100-LT/debug
[22:47:15.810206] Epoch: [324]  [  0/195]  eta: 0:05:39  lr: 0.001664  loss: 0.2955 (0.2955)  time: 1.7406  data: 1.2369  max mem: 9341
[22:47:26.025240] Epoch: [324]  [ 20/195]  eta: 0:01:39  lr: 0.001663  loss: 0.3453 (0.3458)  time: 0.5107  data: 0.0002  max mem: 9341
[22:47:36.242664] Epoch: [324]  [ 40/195]  eta: 0:01:23  lr: 0.001663  loss: 0.3417 (0.3454)  time: 0.5108  data: 0.0002  max mem: 9341
[22:47:46.456272] Epoch: [324]  [ 60/195]  eta: 0:01:11  lr: 0.001663  loss: 0.3445 (0.3474)  time: 0.5106  data: 0.0002  max mem: 9341
[22:47:56.708310] Epoch: [324]  [ 80/195]  eta: 0:01:00  lr: 0.001662  loss: 0.3374 (0.3457)  time: 0.5125  data: 0.0002  max mem: 9341
[22:48:06.915471] Epoch: [324]  [100/195]  eta: 0:00:49  lr: 0.001662  loss: 0.3499 (0.3459)  time: 0.5103  data: 0.0002  max mem: 9341
[22:48:17.127874] Epoch: [324]  [120/195]  eta: 0:00:39  lr: 0.001661  loss: 0.3471 (0.3459)  time: 0.5106  data: 0.0002  max mem: 9341
[22:48:27.340797] Epoch: [324]  [140/195]  eta: 0:00:28  lr: 0.001661  loss: 0.3403 (0.3458)  time: 0.5106  data: 0.0002  max mem: 9341
[22:48:37.589424] Epoch: [324]  [160/195]  eta: 0:00:18  lr: 0.001660  loss: 0.3294 (0.3446)  time: 0.5124  data: 0.0002  max mem: 9341
[22:48:47.760807] Epoch: [324]  [180/195]  eta: 0:00:07  lr: 0.001660  loss: 0.3468 (0.3450)  time: 0.5085  data: 0.0001  max mem: 9341
[22:48:54.887823] Epoch: [324]  [194/195]  eta: 0:00:00  lr: 0.001659  loss: 0.3377 (0.3446)  time: 0.5104  data: 0.0001  max mem: 9341
[22:48:55.063221] Epoch: [324] Total time: 0:01:40 (0.5179 s / it)
[22:48:55.064051] Averaged stats: lr: 0.001659  loss: 0.3377 (0.3437)
[22:48:59.783453] {"train_lr": 0.0016616957153286472, "train_loss": 0.34369987841600025, "epoch": 324}
[22:48:59.783718] [22:48:59.783806] Training epoch 324 for 0:01:45
[22:48:59.783860] [22:48:59.788346] log_dir: ./exp/debug/cifar100-LT/debug
[22:49:01.599618] Epoch: [325]  [  0/195]  eta: 0:05:52  lr: 0.001659  loss: 0.3270 (0.3270)  time: 1.8102  data: 1.2970  max mem: 9341
[22:49:11.825643] Epoch: [325]  [ 20/195]  eta: 0:01:40  lr: 0.001659  loss: 0.3397 (0.3468)  time: 0.5112  data: 0.0002  max mem: 9341
[22:49:22.031669] Epoch: [325]  [ 40/195]  eta: 0:01:24  lr: 0.001658  loss: 0.3383 (0.3442)  time: 0.5102  data: 0.0002  max mem: 9341
[22:49:32.240702] Epoch: [325]  [ 60/195]  eta: 0:01:11  lr: 0.001658  loss: 0.3410 (0.3442)  time: 0.5104  data: 0.0002  max mem: 9341
[22:49:42.495132] Epoch: [325]  [ 80/195]  eta: 0:01:00  lr: 0.001657  loss: 0.3416 (0.3450)  time: 0.5127  data: 0.0002  max mem: 9341
[22:49:52.709430] Epoch: [325]  [100/195]  eta: 0:00:49  lr: 0.001657  loss: 0.3396 (0.3443)  time: 0.5106  data: 0.0002  max mem: 9341
[22:50:02.918468] Epoch: [325]  [120/195]  eta: 0:00:39  lr: 0.001657  loss: 0.3357 (0.3433)  time: 0.5104  data: 0.0002  max mem: 9341
[22:50:13.133258] Epoch: [325]  [140/195]  eta: 0:00:28  lr: 0.001656  loss: 0.3440 (0.3443)  time: 0.5107  data: 0.0002  max mem: 9341
[22:50:23.391667] Epoch: [325]  [160/195]  eta: 0:00:18  lr: 0.001655  loss: 0.3552 (0.3453)  time: 0.5128  data: 0.0002  max mem: 9341
[22:50:33.556833] Epoch: [325]  [180/195]  eta: 0:00:07  lr: 0.001655  loss: 0.3333 (0.3443)  time: 0.5082  data: 0.0001  max mem: 9341
[22:50:40.682076] Epoch: [325]  [194/195]  eta: 0:00:00  lr: 0.001655  loss: 0.3380 (0.3445)  time: 0.5100  data: 0.0001  max mem: 9341
[22:50:40.858494] Epoch: [325] Total time: 0:01:41 (0.5183 s / it)
[22:50:40.867570] Averaged stats: lr: 0.001655  loss: 0.3380 (0.3422)
[22:50:45.612688] {"train_lr": 0.0016571132190470149, "train_loss": 0.3421938617642109, "epoch": 325}
[22:50:45.613030] [22:50:45.613117] Training epoch 325 for 0:01:45
[22:50:45.613170] [22:50:45.617563] log_dir: ./exp/debug/cifar100-LT/debug
[22:50:47.357615] Epoch: [326]  [  0/195]  eta: 0:05:39  lr: 0.001655  loss: 0.3447 (0.3447)  time: 1.7387  data: 1.2266  max mem: 9341
[22:50:57.590822] Epoch: [326]  [ 20/195]  eta: 0:01:39  lr: 0.001654  loss: 0.3450 (0.3536)  time: 0.5116  data: 0.0002  max mem: 9341
[22:51:07.807798] Epoch: [326]  [ 40/195]  eta: 0:01:23  lr: 0.001654  loss: 0.3551 (0.3545)  time: 0.5108  data: 0.0002  max mem: 9341
[22:51:18.022762] Epoch: [326]  [ 60/195]  eta: 0:01:11  lr: 0.001654  loss: 0.3390 (0.3500)  time: 0.5107  data: 0.0002  max mem: 9341
[22:51:28.281350] Epoch: [326]  [ 80/195]  eta: 0:01:00  lr: 0.001653  loss: 0.3399 (0.3461)  time: 0.5129  data: 0.0002  max mem: 9341
[22:51:38.489694] Epoch: [326]  [100/195]  eta: 0:00:49  lr: 0.001652  loss: 0.3477 (0.3457)  time: 0.5104  data: 0.0002  max mem: 9341
[22:51:48.704730] Epoch: [326]  [120/195]  eta: 0:00:39  lr: 0.001652  loss: 0.3398 (0.3452)  time: 0.5107  data: 0.0002  max mem: 9341
[22:51:58.916261] Epoch: [326]  [140/195]  eta: 0:00:28  lr: 0.001652  loss: 0.3440 (0.3454)  time: 0.5105  data: 0.0002  max mem: 9341
[22:52:09.167247] Epoch: [326]  [160/195]  eta: 0:00:18  lr: 0.001651  loss: 0.3512 (0.3458)  time: 0.5125  data: 0.0002  max mem: 9341
[22:52:19.336654] Epoch: [326]  [180/195]  eta: 0:00:07  lr: 0.001650  loss: 0.3400 (0.3455)  time: 0.5084  data: 0.0001  max mem: 9341
[22:52:26.466811] Epoch: [326]  [194/195]  eta: 0:00:00  lr: 0.001650  loss: 0.3512 (0.3457)  time: 0.5105  data: 0.0001  max mem: 9341
[22:52:26.636428] Epoch: [326] Total time: 0:01:41 (0.5180 s / it)
[22:52:26.646999] Averaged stats: lr: 0.001650  loss: 0.3512 (0.3441)
[22:52:31.232761] {"train_lr": 0.0016525229119616809, "train_loss": 0.34411237564606545, "epoch": 326}
[22:52:31.233045] [22:52:31.233129] Training epoch 326 for 0:01:45
[22:52:31.233184] [22:52:31.237647] log_dir: ./exp/debug/cifar100-LT/debug
[22:52:32.861405] Epoch: [327]  [  0/195]  eta: 0:05:16  lr: 0.001650  loss: 0.3706 (0.3706)  time: 1.6223  data: 1.1273  max mem: 9341
[22:52:43.072535] Epoch: [327]  [ 20/195]  eta: 0:01:38  lr: 0.001650  loss: 0.3372 (0.3424)  time: 0.5105  data: 0.0002  max mem: 9341
[22:52:53.293862] Epoch: [327]  [ 40/195]  eta: 0:01:23  lr: 0.001649  loss: 0.3358 (0.3388)  time: 0.5110  data: 0.0002  max mem: 9341
[22:53:03.508677] Epoch: [327]  [ 60/195]  eta: 0:01:11  lr: 0.001649  loss: 0.3362 (0.3405)  time: 0.5107  data: 0.0002  max mem: 9341
[22:53:13.762588] Epoch: [327]  [ 80/195]  eta: 0:01:00  lr: 0.001648  loss: 0.3323 (0.3397)  time: 0.5126  data: 0.0002  max mem: 9341
[22:53:23.976913] Epoch: [327]  [100/195]  eta: 0:00:49  lr: 0.001648  loss: 0.3433 (0.3408)  time: 0.5106  data: 0.0002  max mem: 9341
[22:53:34.188960] Epoch: [327]  [120/195]  eta: 0:00:39  lr: 0.001647  loss: 0.3506 (0.3420)  time: 0.5105  data: 0.0002  max mem: 9341
[22:53:44.398371] Epoch: [327]  [140/195]  eta: 0:00:28  lr: 0.001647  loss: 0.3546 (0.3439)  time: 0.5104  data: 0.0002  max mem: 9341
[22:53:54.661712] Epoch: [327]  [160/195]  eta: 0:00:18  lr: 0.001646  loss: 0.3320 (0.3431)  time: 0.5131  data: 0.0002  max mem: 9341
[22:54:04.831638] Epoch: [327]  [180/195]  eta: 0:00:07  lr: 0.001646  loss: 0.3367 (0.3431)  time: 0.5084  data: 0.0001  max mem: 9341
[22:54:11.981548] Epoch: [327]  [194/195]  eta: 0:00:00  lr: 0.001646  loss: 0.3339 (0.3430)  time: 0.5115  data: 0.0001  max mem: 9341
[22:54:12.159148] Epoch: [327] Total time: 0:01:40 (0.5175 s / it)
[22:54:12.169346] Averaged stats: lr: 0.001646  loss: 0.3339 (0.3423)
[22:54:16.839561] {"train_lr": 0.0016479248725083425, "train_loss": 0.3422627221315335, "epoch": 327}
[22:54:16.839826] [22:54:16.839913] Training epoch 327 for 0:01:45
[22:54:16.839967] [22:54:16.844379] log_dir: ./exp/debug/cifar100-LT/debug
[22:54:18.625882] Epoch: [328]  [  0/195]  eta: 0:05:47  lr: 0.001645  loss: 0.3608 (0.3608)  time: 1.7803  data: 1.2850  max mem: 9341
[22:54:28.842224] Epoch: [328]  [ 20/195]  eta: 0:01:39  lr: 0.001645  loss: 0.3423 (0.3468)  time: 0.5108  data: 0.0002  max mem: 9341
[22:54:39.059308] Epoch: [328]  [ 40/195]  eta: 0:01:23  lr: 0.001645  loss: 0.3451 (0.3453)  time: 0.5108  data: 0.0002  max mem: 9341
[22:54:49.302658] Epoch: [328]  [ 60/195]  eta: 0:01:11  lr: 0.001644  loss: 0.3572 (0.3493)  time: 0.5121  data: 0.0002  max mem: 9341
[22:54:59.564211] Epoch: [328]  [ 80/195]  eta: 0:01:00  lr: 0.001644  loss: 0.3372 (0.3457)  time: 0.5130  data: 0.0002  max mem: 9341
[22:55:09.780582] Epoch: [328]  [100/195]  eta: 0:00:49  lr: 0.001643  loss: 0.3371 (0.3458)  time: 0.5108  data: 0.0002  max mem: 9341
[22:55:19.990915] Epoch: [328]  [120/195]  eta: 0:00:39  lr: 0.001643  loss: 0.3281 (0.3427)  time: 0.5105  data: 0.0002  max mem: 9341
[22:55:30.204376] Epoch: [328]  [140/195]  eta: 0:00:28  lr: 0.001642  loss: 0.3428 (0.3431)  time: 0.5106  data: 0.0002  max mem: 9341
[22:55:40.459462] Epoch: [328]  [160/195]  eta: 0:00:18  lr: 0.001642  loss: 0.3383 (0.3426)  time: 0.5127  data: 0.0002  max mem: 9341
[22:55:50.626977] Epoch: [328]  [180/195]  eta: 0:00:07  lr: 0.001641  loss: 0.3346 (0.3422)  time: 0.5083  data: 0.0001  max mem: 9341
[22:55:57.756112] Epoch: [328]  [194/195]  eta: 0:00:00  lr: 0.001641  loss: 0.3467 (0.3426)  time: 0.5104  data: 0.0001  max mem: 9341
[22:55:57.932656] Epoch: [328] Total time: 0:01:41 (0.5184 s / it)
[22:55:57.946312] Averaged stats: lr: 0.001641  loss: 0.3467 (0.3415)
[22:56:02.818625] {"train_lr": 0.0016433191792548016, "train_loss": 0.3415461112673466, "epoch": 328}
[22:56:02.818960] [22:56:02.819045] Training epoch 328 for 0:01:45
[22:56:02.819098] [22:56:02.823811] log_dir: ./exp/debug/cifar100-LT/debug
[22:56:04.462558] Epoch: [329]  [  0/195]  eta: 0:05:19  lr: 0.001641  loss: 0.3459 (0.3459)  time: 1.6374  data: 1.1348  max mem: 9341
[22:56:14.679711] Epoch: [329]  [ 20/195]  eta: 0:01:38  lr: 0.001640  loss: 0.3436 (0.3421)  time: 0.5108  data: 0.0002  max mem: 9341
[22:56:24.901047] Epoch: [329]  [ 40/195]  eta: 0:01:23  lr: 0.001640  loss: 0.3422 (0.3417)  time: 0.5110  data: 0.0002  max mem: 9341
[22:56:35.116519] Epoch: [329]  [ 60/195]  eta: 0:01:11  lr: 0.001640  loss: 0.3451 (0.3424)  time: 0.5107  data: 0.0002  max mem: 9341
[22:56:45.393410] Epoch: [329]  [ 80/195]  eta: 0:01:00  lr: 0.001639  loss: 0.3472 (0.3436)  time: 0.5138  data: 0.0002  max mem: 9341
[22:56:55.604381] Epoch: [329]  [100/195]  eta: 0:00:49  lr: 0.001639  loss: 0.3399 (0.3436)  time: 0.5105  data: 0.0002  max mem: 9341
[22:57:05.821046] Epoch: [329]  [120/195]  eta: 0:00:39  lr: 0.001638  loss: 0.3337 (0.3423)  time: 0.5108  data: 0.0002  max mem: 9341
[22:57:16.037990] Epoch: [329]  [140/195]  eta: 0:00:28  lr: 0.001638  loss: 0.3440 (0.3423)  time: 0.5108  data: 0.0002  max mem: 9341
[22:57:26.299329] Epoch: [329]  [160/195]  eta: 0:00:18  lr: 0.001637  loss: 0.3468 (0.3426)  time: 0.5130  data: 0.0002  max mem: 9341
[22:57:36.467350] Epoch: [329]  [180/195]  eta: 0:00:07  lr: 0.001637  loss: 0.3526 (0.3431)  time: 0.5083  data: 0.0001  max mem: 9341
[22:57:43.597795] Epoch: [329]  [194/195]  eta: 0:00:00  lr: 0.001636  loss: 0.3275 (0.3421)  time: 0.5106  data: 0.0001  max mem: 9341
[22:57:43.769780] Epoch: [329] Total time: 0:01:40 (0.5177 s / it)
[22:57:43.807447] Averaged stats: lr: 0.001636  loss: 0.3275 (0.3406)
[22:57:48.499180] {"train_lr": 0.0016387059108996416, "train_loss": 0.3406438409517973, "epoch": 329}
[22:57:48.499446] [22:57:48.499533] Training epoch 329 for 0:01:45
[22:57:48.499585] [22:57:48.504023] log_dir: ./exp/debug/cifar100-LT/debug
[22:57:50.336519] Epoch: [330]  [  0/195]  eta: 0:05:57  lr: 0.001636  loss: 0.3844 (0.3844)  time: 1.8317  data: 1.3173  max mem: 9341
[22:58:00.578751] Epoch: [330]  [ 20/195]  eta: 0:01:40  lr: 0.001636  loss: 0.3387 (0.3387)  time: 0.5121  data: 0.0002  max mem: 9341
[22:58:10.798407] Epoch: [330]  [ 40/195]  eta: 0:01:24  lr: 0.001635  loss: 0.3447 (0.3422)  time: 0.5109  data: 0.0002  max mem: 9341
[22:58:21.031840] Epoch: [330]  [ 60/195]  eta: 0:01:11  lr: 0.001635  loss: 0.3429 (0.3414)  time: 0.5116  data: 0.0002  max mem: 9341
[22:58:31.285900] Epoch: [330]  [ 80/195]  eta: 0:01:00  lr: 0.001634  loss: 0.3393 (0.3412)  time: 0.5126  data: 0.0002  max mem: 9341
[22:58:41.506280] Epoch: [330]  [100/195]  eta: 0:00:49  lr: 0.001634  loss: 0.3356 (0.3413)  time: 0.5110  data: 0.0002  max mem: 9341
[22:58:51.720669] Epoch: [330]  [120/195]  eta: 0:00:39  lr: 0.001634  loss: 0.3345 (0.3406)  time: 0.5107  data: 0.0002  max mem: 9341
[22:59:01.933454] Epoch: [330]  [140/195]  eta: 0:00:28  lr: 0.001633  loss: 0.3580 (0.3423)  time: 0.5106  data: 0.0002  max mem: 9341
[22:59:12.191032] Epoch: [330]  [160/195]  eta: 0:00:18  lr: 0.001632  loss: 0.3379 (0.3426)  time: 0.5128  data: 0.0002  max mem: 9341
[22:59:22.368970] Epoch: [330]  [180/195]  eta: 0:00:07  lr: 0.001632  loss: 0.3469 (0.3433)  time: 0.5088  data: 0.0001  max mem: 9341
[22:59:29.500862] Epoch: [330]  [194/195]  eta: 0:00:00  lr: 0.001632  loss: 0.3421 (0.3434)  time: 0.5107  data: 0.0001  max mem: 9341
[22:59:29.703453] Epoch: [330] Total time: 0:01:41 (0.5190 s / it)
[22:59:29.718146] Averaged stats: lr: 0.001632  loss: 0.3421 (0.3405)
[22:59:34.422695] {"train_lr": 0.0016340851462708947, "train_loss": 0.34049475342035296, "epoch": 330}
[22:59:34.423067] [22:59:34.423157] Training epoch 330 for 0:01:45
[22:59:34.423210] [22:59:34.428245] log_dir: ./exp/debug/cifar100-LT/debug
[22:59:36.312278] Epoch: [331]  [  0/195]  eta: 0:06:07  lr: 0.001632  loss: 0.3392 (0.3392)  time: 1.8828  data: 1.3834  max mem: 9341
[22:59:46.528659] Epoch: [331]  [ 20/195]  eta: 0:01:40  lr: 0.001631  loss: 0.3429 (0.3449)  time: 0.5108  data: 0.0002  max mem: 9341
[22:59:56.743094] Epoch: [331]  [ 40/195]  eta: 0:01:24  lr: 0.001631  loss: 0.3463 (0.3436)  time: 0.5107  data: 0.0002  max mem: 9341
[23:00:06.953932] Epoch: [331]  [ 60/195]  eta: 0:01:11  lr: 0.001630  loss: 0.3358 (0.3415)  time: 0.5105  data: 0.0002  max mem: 9341
[23:00:17.211346] Epoch: [331]  [ 80/195]  eta: 0:01:00  lr: 0.001630  loss: 0.3316 (0.3395)  time: 0.5128  data: 0.0002  max mem: 9341
[23:00:27.424876] Epoch: [331]  [100/195]  eta: 0:00:49  lr: 0.001629  loss: 0.3341 (0.3384)  time: 0.5106  data: 0.0002  max mem: 9341
[23:00:37.637734] Epoch: [331]  [120/195]  eta: 0:00:39  lr: 0.001629  loss: 0.3307 (0.3376)  time: 0.5106  data: 0.0002  max mem: 9341
[23:00:47.850564] Epoch: [331]  [140/195]  eta: 0:00:28  lr: 0.001629  loss: 0.3353 (0.3382)  time: 0.5106  data: 0.0002  max mem: 9341
[23:00:58.100230] Epoch: [331]  [160/195]  eta: 0:00:18  lr: 0.001628  loss: 0.3337 (0.3385)  time: 0.5124  data: 0.0002  max mem: 9341
[23:01:08.278157] Epoch: [331]  [180/195]  eta: 0:00:07  lr: 0.001627  loss: 0.3385 (0.3386)  time: 0.5088  data: 0.0001  max mem: 9341
[23:01:15.408395] Epoch: [331]  [194/195]  eta: 0:00:00  lr: 0.001627  loss: 0.3342 (0.3391)  time: 0.5109  data: 0.0001  max mem: 9341
[23:01:15.577213] Epoch: [331] Total time: 0:01:41 (0.5187 s / it)
[23:01:15.593843] Averaged stats: lr: 0.001627  loss: 0.3342 (0.3396)
[23:01:20.401081] {"train_lr": 0.0016294569643246818, "train_loss": 0.3395868401496838, "epoch": 331}
[23:01:20.401359] [23:01:20.401446] Training epoch 331 for 0:01:45
[23:01:20.401499] [23:01:20.406035] log_dir: ./exp/debug/cifar100-LT/debug
[23:01:22.046705] Epoch: [332]  [  0/195]  eta: 0:05:19  lr: 0.001627  loss: 0.3523 (0.3523)  time: 1.6397  data: 1.1364  max mem: 9341
[23:01:32.286042] Epoch: [332]  [ 20/195]  eta: 0:01:38  lr: 0.001627  loss: 0.3423 (0.3471)  time: 0.5119  data: 0.0002  max mem: 9341
[23:01:42.505740] Epoch: [332]  [ 40/195]  eta: 0:01:23  lr: 0.001626  loss: 0.3408 (0.3448)  time: 0.5109  data: 0.0002  max mem: 9341
[23:01:52.720609] Epoch: [332]  [ 60/195]  eta: 0:01:11  lr: 0.001626  loss: 0.3333 (0.3422)  time: 0.5107  data: 0.0002  max mem: 9341
[23:02:03.029598] Epoch: [332]  [ 80/195]  eta: 0:01:00  lr: 0.001625  loss: 0.3345 (0.3407)  time: 0.5154  data: 0.0002  max mem: 9341
[23:02:13.267647] Epoch: [332]  [100/195]  eta: 0:00:49  lr: 0.001625  loss: 0.3566 (0.3430)  time: 0.5118  data: 0.0002  max mem: 9341
[23:02:23.509302] Epoch: [332]  [120/195]  eta: 0:00:39  lr: 0.001624  loss: 0.3404 (0.3424)  time: 0.5120  data: 0.0002  max mem: 9341
[23:02:33.748424] Epoch: [332]  [140/195]  eta: 0:00:28  lr: 0.001624  loss: 0.3568 (0.3446)  time: 0.5119  data: 0.0002  max mem: 9341
[23:02:44.049843] Epoch: [332]  [160/195]  eta: 0:00:18  lr: 0.001623  loss: 0.3500 (0.3459)  time: 0.5150  data: 0.0002  max mem: 9341
[23:02:54.242767] Epoch: [332]  [180/195]  eta: 0:00:07  lr: 0.001623  loss: 0.3421 (0.3456)  time: 0.5096  data: 0.0001  max mem: 9341
[23:03:01.389415] Epoch: [332]  [194/195]  eta: 0:00:00  lr: 0.001622  loss: 0.3466 (0.3456)  time: 0.5124  data: 0.0001  max mem: 9341
[23:03:01.547211] Epoch: [332] Total time: 0:01:41 (0.5187 s / it)
[23:03:01.561396] Averaged stats: lr: 0.001622  loss: 0.3466 (0.3412)
[23:03:06.243268] {"train_lr": 0.0016248214441438604, "train_loss": 0.3412240641239362, "epoch": 332}
[23:03:06.243601] [23:03:06.243686] Training epoch 332 for 0:01:45
[23:03:06.243740] [23:03:06.248282] log_dir: ./exp/debug/cifar100-LT/debug
[23:03:08.137857] Epoch: [333]  [  0/195]  eta: 0:06:08  lr: 0.001622  loss: 0.3137 (0.3137)  time: 1.8882  data: 1.3804  max mem: 9341
[23:03:18.351209] Epoch: [333]  [ 20/195]  eta: 0:01:40  lr: 0.001622  loss: 0.3459 (0.3532)  time: 0.5106  data: 0.0002  max mem: 9341
[23:03:28.568964] Epoch: [333]  [ 40/195]  eta: 0:01:24  lr: 0.001622  loss: 0.3413 (0.3479)  time: 0.5108  data: 0.0002  max mem: 9341
[23:03:38.780041] Epoch: [333]  [ 60/195]  eta: 0:01:11  lr: 0.001621  loss: 0.3312 (0.3443)  time: 0.5105  data: 0.0002  max mem: 9341
[23:03:49.039760] Epoch: [333]  [ 80/195]  eta: 0:01:00  lr: 0.001620  loss: 0.3399 (0.3441)  time: 0.5129  data: 0.0002  max mem: 9341
[23:03:59.249720] Epoch: [333]  [100/195]  eta: 0:00:49  lr: 0.001620  loss: 0.3508 (0.3448)  time: 0.5104  data: 0.0002  max mem: 9341
[23:04:09.462866] Epoch: [333]  [120/195]  eta: 0:00:39  lr: 0.001620  loss: 0.3550 (0.3463)  time: 0.5106  data: 0.0002  max mem: 9341
[23:04:19.672254] Epoch: [333]  [140/195]  eta: 0:00:28  lr: 0.001619  loss: 0.3464 (0.3473)  time: 0.5104  data: 0.0002  max mem: 9341
[23:04:29.933047] Epoch: [333]  [160/195]  eta: 0:00:18  lr: 0.001619  loss: 0.3473 (0.3474)  time: 0.5130  data: 0.0002  max mem: 9341
[23:04:40.107052] Epoch: [333]  [180/195]  eta: 0:00:07  lr: 0.001618  loss: 0.3330 (0.3460)  time: 0.5086  data: 0.0002  max mem: 9341
[23:04:47.236616] Epoch: [333]  [194/195]  eta: 0:00:00  lr: 0.001618  loss: 0.3568 (0.3469)  time: 0.5104  data: 0.0001  max mem: 9341
[23:04:47.407503] Epoch: [333] Total time: 0:01:41 (0.5188 s / it)
[23:04:47.416975] Averaged stats: lr: 0.001618  loss: 0.3568 (0.3438)
[23:04:52.269020] {"train_lr": 0.0016201786649366792, "train_loss": 0.3437622100114822, "epoch": 333}
[23:04:52.269294] [23:04:52.269380] Training epoch 333 for 0:01:46
[23:04:52.269434] [23:04:52.273888] log_dir: ./exp/debug/cifar100-LT/debug
[23:04:53.963760] Epoch: [334]  [  0/195]  eta: 0:05:29  lr: 0.001618  loss: 0.3014 (0.3014)  time: 1.6881  data: 1.1748  max mem: 9341
[23:05:04.180160] Epoch: [334]  [ 20/195]  eta: 0:01:39  lr: 0.001617  loss: 0.3357 (0.3387)  time: 0.5108  data: 0.0002  max mem: 9341
[23:05:14.399279] Epoch: [334]  [ 40/195]  eta: 0:01:23  lr: 0.001617  loss: 0.3491 (0.3419)  time: 0.5109  data: 0.0002  max mem: 9341
[23:05:24.639503] Epoch: [334]  [ 60/195]  eta: 0:01:11  lr: 0.001617  loss: 0.3438 (0.3407)  time: 0.5119  data: 0.0002  max mem: 9341
[23:05:34.936588] Epoch: [334]  [ 80/195]  eta: 0:01:00  lr: 0.001616  loss: 0.3382 (0.3417)  time: 0.5148  data: 0.0002  max mem: 9341
[23:05:45.148934] Epoch: [334]  [100/195]  eta: 0:00:49  lr: 0.001615  loss: 0.3488 (0.3422)  time: 0.5106  data: 0.0002  max mem: 9341
[23:05:55.359885] Epoch: [334]  [120/195]  eta: 0:00:39  lr: 0.001615  loss: 0.3442 (0.3429)  time: 0.5105  data: 0.0002  max mem: 9341
[23:06:05.567863] Epoch: [334]  [140/195]  eta: 0:00:28  lr: 0.001615  loss: 0.3461 (0.3436)  time: 0.5103  data: 0.0002  max mem: 9341
[23:06:15.825486] Epoch: [334]  [160/195]  eta: 0:00:18  lr: 0.001614  loss: 0.3324 (0.3433)  time: 0.5128  data: 0.0002  max mem: 9341
[23:06:25.997156] Epoch: [334]  [180/195]  eta: 0:00:07  lr: 0.001613  loss: 0.3449 (0.3435)  time: 0.5085  data: 0.0001  max mem: 9341
[23:06:33.122932] Epoch: [334]  [194/195]  eta: 0:00:00  lr: 0.001613  loss: 0.3426 (0.3429)  time: 0.5101  data: 0.0001  max mem: 9341
[23:06:33.288570] Epoch: [334] Total time: 0:01:41 (0.5180 s / it)
[23:06:33.301940] Averaged stats: lr: 0.001613  loss: 0.3426 (0.3419)
[23:06:38.021134] {"train_lr": 0.001615528706035426, "train_loss": 0.34187000634578557, "epoch": 334}
[23:06:38.021401] [23:06:38.021500] Training epoch 334 for 0:01:45
[23:06:38.021554] [23:06:38.026029] log_dir: ./exp/debug/cifar100-LT/debug
[23:06:39.772406] Epoch: [335]  [  0/195]  eta: 0:05:40  lr: 0.001613  loss: 0.3499 (0.3499)  time: 1.7452  data: 1.2304  max mem: 9341
[23:06:50.001652] Epoch: [335]  [ 20/195]  eta: 0:01:39  lr: 0.001613  loss: 0.3486 (0.3453)  time: 0.5114  data: 0.0003  max mem: 9341
[23:07:00.246326] Epoch: [335]  [ 40/195]  eta: 0:01:23  lr: 0.001612  loss: 0.3316 (0.3406)  time: 0.5122  data: 0.0002  max mem: 9341
[23:07:10.489801] Epoch: [335]  [ 60/195]  eta: 0:01:11  lr: 0.001612  loss: 0.3279 (0.3381)  time: 0.5121  data: 0.0002  max mem: 9341
[23:07:20.746083] Epoch: [335]  [ 80/195]  eta: 0:01:00  lr: 0.001611  loss: 0.3198 (0.3351)  time: 0.5127  data: 0.0002  max mem: 9341
[23:07:30.978195] Epoch: [335]  [100/195]  eta: 0:00:49  lr: 0.001611  loss: 0.3379 (0.3351)  time: 0.5115  data: 0.0002  max mem: 9341
[23:07:41.208856] Epoch: [335]  [120/195]  eta: 0:00:39  lr: 0.001610  loss: 0.3436 (0.3361)  time: 0.5115  data: 0.0002  max mem: 9341
[23:07:51.437876] Epoch: [335]  [140/195]  eta: 0:00:28  lr: 0.001610  loss: 0.3449 (0.3370)  time: 0.5114  data: 0.0002  max mem: 9341
[23:08:01.737300] Epoch: [335]  [160/195]  eta: 0:00:18  lr: 0.001609  loss: 0.3491 (0.3386)  time: 0.5149  data: 0.0002  max mem: 9341
[23:08:11.933211] Epoch: [335]  [180/195]  eta: 0:00:07  lr: 0.001609  loss: 0.3355 (0.3388)  time: 0.5097  data: 0.0002  max mem: 9341
[23:08:19.084985] Epoch: [335]  [194/195]  eta: 0:00:00  lr: 0.001608  loss: 0.3400 (0.3391)  time: 0.5127  data: 0.0001  max mem: 9341
[23:08:19.258244] Epoch: [335] Total time: 0:01:41 (0.5191 s / it)
[23:08:19.268267] Averaged stats: lr: 0.001608  loss: 0.3400 (0.3392)
[23:08:23.899572] {"train_lr": 0.001610871646895076, "train_loss": 0.339229135788404, "epoch": 335}
[23:08:23.899905] [23:08:23.899993] Training epoch 335 for 0:01:45
[23:08:23.900047] [23:08:23.904567] log_dir: ./exp/debug/cifar100-LT/debug
[23:08:25.622156] Epoch: [336]  [  0/195]  eta: 0:05:34  lr: 0.001608  loss: 0.3549 (0.3549)  time: 1.7160  data: 1.2196  max mem: 9341
[23:08:35.840368] Epoch: [336]  [ 20/195]  eta: 0:01:39  lr: 0.001608  loss: 0.3431 (0.3482)  time: 0.5108  data: 0.0002  max mem: 9341
[23:08:46.056525] Epoch: [336]  [ 40/195]  eta: 0:01:23  lr: 0.001608  loss: 0.3378 (0.3441)  time: 0.5107  data: 0.0002  max mem: 9341
[23:08:56.272408] Epoch: [336]  [ 60/195]  eta: 0:01:11  lr: 0.001607  loss: 0.3382 (0.3416)  time: 0.5107  data: 0.0002  max mem: 9341
[23:09:06.531973] Epoch: [336]  [ 80/195]  eta: 0:01:00  lr: 0.001606  loss: 0.3468 (0.3429)  time: 0.5129  data: 0.0002  max mem: 9341
[23:09:16.744943] Epoch: [336]  [100/195]  eta: 0:00:49  lr: 0.001606  loss: 0.3417 (0.3423)  time: 0.5106  data: 0.0002  max mem: 9341
[23:09:26.959981] Epoch: [336]  [120/195]  eta: 0:00:39  lr: 0.001606  loss: 0.3366 (0.3415)  time: 0.5107  data: 0.0002  max mem: 9341
[23:09:37.196374] Epoch: [336]  [140/195]  eta: 0:00:28  lr: 0.001605  loss: 0.3301 (0.3403)  time: 0.5118  data: 0.0002  max mem: 9341
[23:09:47.467193] Epoch: [336]  [160/195]  eta: 0:00:18  lr: 0.001605  loss: 0.3400 (0.3404)  time: 0.5135  data: 0.0002  max mem: 9341
[23:09:57.650407] Epoch: [336]  [180/195]  eta: 0:00:07  lr: 0.001604  loss: 0.3358 (0.3399)  time: 0.5091  data: 0.0001  max mem: 9341
[23:10:04.773098] Epoch: [336]  [194/195]  eta: 0:00:00  lr: 0.001604  loss: 0.3421 (0.3398)  time: 0.5103  data: 0.0001  max mem: 9341
[23:10:04.945833] Epoch: [336] Total time: 0:01:41 (0.5182 s / it)
[23:10:04.965074] Averaged stats: lr: 0.001604  loss: 0.3421 (0.3388)
[23:10:09.689760] {"train_lr": 0.0016062075670919016, "train_loss": 0.3388484164308279, "epoch": 336}
[23:10:09.690019] [23:10:09.690120] Training epoch 336 for 0:01:45
[23:10:09.690173] [23:10:09.694592] log_dir: ./exp/debug/cifar100-LT/debug
[23:10:11.493535] Epoch: [337]  [  0/195]  eta: 0:05:50  lr: 0.001604  loss: 0.3227 (0.3227)  time: 1.7979  data: 1.2827  max mem: 9341
[23:10:21.804847] Epoch: [337]  [ 20/195]  eta: 0:01:40  lr: 0.001603  loss: 0.3362 (0.3376)  time: 0.5155  data: 0.0002  max mem: 9341
[23:10:32.042192] Epoch: [337]  [ 40/195]  eta: 0:01:24  lr: 0.001603  loss: 0.3444 (0.3412)  time: 0.5118  data: 0.0002  max mem: 9341
[23:10:42.261272] Epoch: [337]  [ 60/195]  eta: 0:01:12  lr: 0.001603  loss: 0.3453 (0.3423)  time: 0.5109  data: 0.0002  max mem: 9341
[23:10:52.517331] Epoch: [337]  [ 80/195]  eta: 0:01:00  lr: 0.001602  loss: 0.3327 (0.3411)  time: 0.5127  data: 0.0002  max mem: 9341
[23:11:02.729343] Epoch: [337]  [100/195]  eta: 0:00:49  lr: 0.001601  loss: 0.3453 (0.3421)  time: 0.5105  data: 0.0002  max mem: 9341
[23:11:12.947834] Epoch: [337]  [120/195]  eta: 0:00:39  lr: 0.001601  loss: 0.3315 (0.3404)  time: 0.5109  data: 0.0002  max mem: 9341
[23:11:23.159563] Epoch: [337]  [140/195]  eta: 0:00:28  lr: 0.001601  loss: 0.3422 (0.3404)  time: 0.5105  data: 0.0002  max mem: 9341
[23:11:33.422065] Epoch: [337]  [160/195]  eta: 0:00:18  lr: 0.001600  loss: 0.3406 (0.3403)  time: 0.5131  data: 0.0002  max mem: 9341
[23:11:43.595323] Epoch: [337]  [180/195]  eta: 0:00:07  lr: 0.001599  loss: 0.3320 (0.3401)  time: 0.5086  data: 0.0001  max mem: 9341
[23:11:50.720975] Epoch: [337]  [194/195]  eta: 0:00:00  lr: 0.001599  loss: 0.3326 (0.3404)  time: 0.5103  data: 0.0001  max mem: 9341
[23:11:50.908857] Epoch: [337] Total time: 0:01:41 (0.5190 s / it)
[23:11:50.917169] Averaged stats: lr: 0.001599  loss: 0.3326 (0.3380)
[23:11:55.696244] {"train_lr": 0.0016015365463221795, "train_loss": 0.3379603647650817, "epoch": 337}
[23:11:55.696507] [23:11:55.696591] Training epoch 337 for 0:01:46
[23:11:55.696644] [23:11:55.701089] log_dir: ./exp/debug/cifar100-LT/debug
[23:11:57.556951] Epoch: [338]  [  0/195]  eta: 0:06:01  lr: 0.001599  loss: 0.3060 (0.3060)  time: 1.8540  data: 1.3562  max mem: 9341
[23:12:07.775004] Epoch: [338]  [ 20/195]  eta: 0:01:40  lr: 0.001599  loss: 0.3444 (0.3402)  time: 0.5108  data: 0.0002  max mem: 9341
[23:12:17.998609] Epoch: [338]  [ 40/195]  eta: 0:01:24  lr: 0.001598  loss: 0.3274 (0.3372)  time: 0.5111  data: 0.0002  max mem: 9341
[23:12:28.219320] Epoch: [338]  [ 60/195]  eta: 0:01:11  lr: 0.001598  loss: 0.3420 (0.3389)  time: 0.5110  data: 0.0002  max mem: 9341
[23:12:38.487060] Epoch: [338]  [ 80/195]  eta: 0:01:00  lr: 0.001597  loss: 0.3346 (0.3385)  time: 0.5133  data: 0.0002  max mem: 9341
[23:12:48.705247] Epoch: [338]  [100/195]  eta: 0:00:49  lr: 0.001597  loss: 0.3411 (0.3400)  time: 0.5109  data: 0.0002  max mem: 9341
[23:12:58.922526] Epoch: [338]  [120/195]  eta: 0:00:39  lr: 0.001596  loss: 0.3334 (0.3397)  time: 0.5108  data: 0.0002  max mem: 9341
[23:13:09.142107] Epoch: [338]  [140/195]  eta: 0:00:28  lr: 0.001596  loss: 0.3342 (0.3394)  time: 0.5109  data: 0.0002  max mem: 9341
[23:13:19.406335] Epoch: [338]  [160/195]  eta: 0:00:18  lr: 0.001595  loss: 0.3465 (0.3396)  time: 0.5131  data: 0.0002  max mem: 9341
[23:13:29.580580] Epoch: [338]  [180/195]  eta: 0:00:07  lr: 0.001595  loss: 0.3449 (0.3398)  time: 0.5086  data: 0.0002  max mem: 9341
[23:13:36.717629] Epoch: [338]  [194/195]  eta: 0:00:00  lr: 0.001594  loss: 0.3338 (0.3394)  time: 0.5108  data: 0.0001  max mem: 9341
[23:13:36.885861] Epoch: [338] Total time: 0:01:41 (0.5189 s / it)
[23:13:36.899580] Averaged stats: lr: 0.001594  loss: 0.3338 (0.3368)
[23:13:41.738161] {"train_lr": 0.001596858664400761, "train_loss": 0.33681250122877265, "epoch": 338}
[23:13:41.738475] [23:13:41.738567] Training epoch 338 for 0:01:46
[23:13:41.738696] [23:13:41.743818] log_dir: ./exp/debug/cifar100-LT/debug
[23:13:43.516572] Epoch: [339]  [  0/195]  eta: 0:05:45  lr: 0.001594  loss: 0.2869 (0.2869)  time: 1.7711  data: 1.2641  max mem: 9341
[23:13:53.739242] Epoch: [339]  [ 20/195]  eta: 0:01:39  lr: 0.001594  loss: 0.3480 (0.3469)  time: 0.5111  data: 0.0002  max mem: 9341
[23:14:03.964140] Epoch: [339]  [ 40/195]  eta: 0:01:23  lr: 0.001594  loss: 0.3335 (0.3409)  time: 0.5112  data: 0.0002  max mem: 9341
[23:14:14.179353] Epoch: [339]  [ 60/195]  eta: 0:01:11  lr: 0.001593  loss: 0.3257 (0.3387)  time: 0.5107  data: 0.0002  max mem: 9341
[23:14:24.477013] Epoch: [339]  [ 80/195]  eta: 0:01:00  lr: 0.001592  loss: 0.3337 (0.3382)  time: 0.5148  data: 0.0002  max mem: 9341
[23:14:34.715703] Epoch: [339]  [100/195]  eta: 0:00:49  lr: 0.001592  loss: 0.3400 (0.3389)  time: 0.5119  data: 0.0002  max mem: 9341
[23:14:44.956191] Epoch: [339]  [120/195]  eta: 0:00:39  lr: 0.001592  loss: 0.3358 (0.3390)  time: 0.5120  data: 0.0002  max mem: 9341
[23:14:55.187954] Epoch: [339]  [140/195]  eta: 0:00:28  lr: 0.001591  loss: 0.3385 (0.3400)  time: 0.5115  data: 0.0002  max mem: 9341
[23:15:05.443694] Epoch: [339]  [160/195]  eta: 0:00:18  lr: 0.001590  loss: 0.3368 (0.3397)  time: 0.5127  data: 0.0003  max mem: 9341
[23:15:15.618697] Epoch: [339]  [180/195]  eta: 0:00:07  lr: 0.001590  loss: 0.3261 (0.3387)  time: 0.5087  data: 0.0002  max mem: 9341
[23:15:22.758426] Epoch: [339]  [194/195]  eta: 0:00:00  lr: 0.001590  loss: 0.3291 (0.3387)  time: 0.5110  data: 0.0001  max mem: 9341
[23:15:22.927393] Epoch: [339] Total time: 0:01:41 (0.5189 s / it)
[23:15:22.935021] Averaged stats: lr: 0.001590  loss: 0.3291 (0.3386)
[23:15:27.695050] {"train_lr": 0.00159217400125973, "train_loss": 0.33856413819086856, "epoch": 339}
[23:15:27.695406] [23:15:27.695490] Training epoch 339 for 0:01:45
[23:15:27.695542] [23:15:27.699987] log_dir: ./exp/debug/cifar100-LT/debug
[23:15:29.328299] Epoch: [340]  [  0/195]  eta: 0:05:17  lr: 0.001590  loss: 0.3251 (0.3251)  time: 1.6271  data: 1.1357  max mem: 9341
[23:15:39.543747] Epoch: [340]  [ 20/195]  eta: 0:01:38  lr: 0.001589  loss: 0.3391 (0.3358)  time: 0.5107  data: 0.0002  max mem: 9341
[23:15:49.759273] Epoch: [340]  [ 40/195]  eta: 0:01:23  lr: 0.001589  loss: 0.3313 (0.3357)  time: 0.5107  data: 0.0002  max mem: 9341
[23:16:00.001432] Epoch: [340]  [ 60/195]  eta: 0:01:11  lr: 0.001588  loss: 0.3441 (0.3393)  time: 0.5121  data: 0.0002  max mem: 9341
[23:16:10.304061] Epoch: [340]  [ 80/195]  eta: 0:01:00  lr: 0.001588  loss: 0.3374 (0.3399)  time: 0.5151  data: 0.0002  max mem: 9341
[23:16:20.539598] Epoch: [340]  [100/195]  eta: 0:00:49  lr: 0.001587  loss: 0.3386 (0.3386)  time: 0.5117  data: 0.0002  max mem: 9341
[23:16:30.780609] Epoch: [340]  [120/195]  eta: 0:00:39  lr: 0.001587  loss: 0.3437 (0.3388)  time: 0.5120  data: 0.0002  max mem: 9341
[23:16:41.015119] Epoch: [340]  [140/195]  eta: 0:00:28  lr: 0.001587  loss: 0.3332 (0.3389)  time: 0.5117  data: 0.0002  max mem: 9341
[23:16:51.314417] Epoch: [340]  [160/195]  eta: 0:00:18  lr: 0.001586  loss: 0.3274 (0.3385)  time: 0.5149  data: 0.0002  max mem: 9341
[23:17:01.506044] Epoch: [340]  [180/195]  eta: 0:00:07  lr: 0.001585  loss: 0.3282 (0.3382)  time: 0.5095  data: 0.0001  max mem: 9341
[23:17:08.652226] Epoch: [340]  [194/195]  eta: 0:00:00  lr: 0.001585  loss: 0.3465 (0.3389)  time: 0.5122  data: 0.0001  max mem: 9341
[23:17:08.837522] Epoch: [340] Total time: 0:01:41 (0.5187 s / it)
[23:17:08.838269] Averaged stats: lr: 0.001585  loss: 0.3465 (0.3372)
[23:17:13.554504] {"train_lr": 0.0015874826369470694, "train_loss": 0.33719877612132293, "epoch": 340}
[23:17:13.554834] [23:17:13.554920] Training epoch 340 for 0:01:45
[23:17:13.554973] [23:17:13.559407] log_dir: ./exp/debug/cifar100-LT/debug
[23:17:15.181894] Epoch: [341]  [  0/195]  eta: 0:05:16  lr: 0.001585  loss: 0.4063 (0.4063)  time: 1.6206  data: 1.1231  max mem: 9341
[23:17:25.404294] Epoch: [341]  [ 20/195]  eta: 0:01:38  lr: 0.001585  loss: 0.3399 (0.3454)  time: 0.5111  data: 0.0002  max mem: 9341
[23:17:35.651147] Epoch: [341]  [ 40/195]  eta: 0:01:23  lr: 0.001584  loss: 0.3345 (0.3408)  time: 0.5123  data: 0.0002  max mem: 9341
[23:17:45.876149] Epoch: [341]  [ 60/195]  eta: 0:01:11  lr: 0.001584  loss: 0.3498 (0.3404)  time: 0.5112  data: 0.0002  max mem: 9341
[23:17:56.130132] Epoch: [341]  [ 80/195]  eta: 0:01:00  lr: 0.001583  loss: 0.3426 (0.3411)  time: 0.5126  data: 0.0002  max mem: 9341
[23:18:06.346359] Epoch: [341]  [100/195]  eta: 0:00:49  lr: 0.001583  loss: 0.3198 (0.3386)  time: 0.5107  data: 0.0002  max mem: 9341
[23:18:16.556709] Epoch: [341]  [120/195]  eta: 0:00:39  lr: 0.001582  loss: 0.3396 (0.3399)  time: 0.5104  data: 0.0002  max mem: 9341
[23:18:26.770037] Epoch: [341]  [140/195]  eta: 0:00:28  lr: 0.001582  loss: 0.3419 (0.3395)  time: 0.5106  data: 0.0002  max mem: 9341
[23:18:37.072710] Epoch: [341]  [160/195]  eta: 0:00:18  lr: 0.001581  loss: 0.3407 (0.3401)  time: 0.5151  data: 0.0002  max mem: 9341
[23:18:47.267411] Epoch: [341]  [180/195]  eta: 0:00:07  lr: 0.001581  loss: 0.3337 (0.3393)  time: 0.5097  data: 0.0001  max mem: 9341
[23:18:54.419719] Epoch: [341]  [194/195]  eta: 0:00:00  lr: 0.001580  loss: 0.3407 (0.3396)  time: 0.5127  data: 0.0001  max mem: 9341
[23:18:54.583824] Epoch: [341] Total time: 0:01:41 (0.5181 s / it)
[23:18:54.600759] Averaged stats: lr: 0.001580  loss: 0.3407 (0.3396)
[23:18:59.378627] {"train_lr": 0.0015827846516252347, "train_loss": 0.33963617040560795, "epoch": 341}
[23:18:59.378998] [23:18:59.379088] Training epoch 341 for 0:01:45
[23:18:59.379143] [23:18:59.384124] log_dir: ./exp/debug/cifar100-LT/debug
[23:19:01.112781] Epoch: [342]  [  0/195]  eta: 0:05:36  lr: 0.001580  loss: 0.3119 (0.3119)  time: 1.7271  data: 1.2338  max mem: 9341
[23:19:11.326388] Epoch: [342]  [ 20/195]  eta: 0:01:39  lr: 0.001580  loss: 0.3402 (0.3378)  time: 0.5106  data: 0.0002  max mem: 9341
[23:19:21.545818] Epoch: [342]  [ 40/195]  eta: 0:01:23  lr: 0.001579  loss: 0.3403 (0.3411)  time: 0.5109  data: 0.0002  max mem: 9341
[23:19:31.762966] Epoch: [342]  [ 60/195]  eta: 0:01:11  lr: 0.001579  loss: 0.3306 (0.3385)  time: 0.5108  data: 0.0002  max mem: 9341
[23:19:42.017947] Epoch: [342]  [ 80/195]  eta: 0:01:00  lr: 0.001578  loss: 0.3385 (0.3383)  time: 0.5127  data: 0.0002  max mem: 9341
[23:19:52.232353] Epoch: [342]  [100/195]  eta: 0:00:49  lr: 0.001578  loss: 0.3293 (0.3369)  time: 0.5106  data: 0.0002  max mem: 9341
[23:20:02.441169] Epoch: [342]  [120/195]  eta: 0:00:39  lr: 0.001578  loss: 0.3337 (0.3378)  time: 0.5104  data: 0.0002  max mem: 9341
[23:20:12.650219] Epoch: [342]  [140/195]  eta: 0:00:28  lr: 0.001577  loss: 0.3387 (0.3373)  time: 0.5104  data: 0.0002  max mem: 9341
[23:20:22.905958] Epoch: [342]  [160/195]  eta: 0:00:18  lr: 0.001576  loss: 0.3331 (0.3368)  time: 0.5127  data: 0.0002  max mem: 9341
[23:20:33.079476] Epoch: [342]  [180/195]  eta: 0:00:07  lr: 0.001576  loss: 0.3409 (0.3375)  time: 0.5086  data: 0.0001  max mem: 9341
[23:20:40.210139] Epoch: [342]  [194/195]  eta: 0:00:00  lr: 0.001576  loss: 0.3320 (0.3372)  time: 0.5105  data: 0.0001  max mem: 9341
[23:20:40.374065] Epoch: [342] Total time: 0:01:40 (0.5179 s / it)
[23:20:40.382083] Averaged stats: lr: 0.001576  loss: 0.3320 (0.3381)
[23:20:45.117053] {"train_lr": 0.0015780801255698533, "train_loss": 0.33814623172466574, "epoch": 342}
[23:20:45.117341] [23:20:45.117427] Training epoch 342 for 0:01:45
[23:20:45.117538] [23:20:45.124796] log_dir: ./exp/debug/cifar100-LT/debug
[23:20:46.696101] Epoch: [343]  [  0/195]  eta: 0:05:06  lr: 0.001576  loss: 0.3154 (0.3154)  time: 1.5705  data: 1.0705  max mem: 9341
[23:20:56.919443] Epoch: [343]  [ 20/195]  eta: 0:01:38  lr: 0.001575  loss: 0.3253 (0.3301)  time: 0.5111  data: 0.0002  max mem: 9341
[23:21:07.140333] Epoch: [343]  [ 40/195]  eta: 0:01:23  lr: 0.001575  loss: 0.3315 (0.3322)  time: 0.5110  data: 0.0002  max mem: 9341
[23:21:17.355723] Epoch: [343]  [ 60/195]  eta: 0:01:11  lr: 0.001574  loss: 0.3404 (0.3339)  time: 0.5107  data: 0.0002  max mem: 9341
[23:21:27.617173] Epoch: [343]  [ 80/195]  eta: 0:01:00  lr: 0.001574  loss: 0.3338 (0.3349)  time: 0.5130  data: 0.0002  max mem: 9341
[23:21:37.833533] Epoch: [343]  [100/195]  eta: 0:00:49  lr: 0.001573  loss: 0.3356 (0.3349)  time: 0.5108  data: 0.0002  max mem: 9341
[23:21:48.054332] Epoch: [343]  [120/195]  eta: 0:00:39  lr: 0.001573  loss: 0.3460 (0.3371)  time: 0.5110  data: 0.0002  max mem: 9341
[23:21:58.272978] Epoch: [343]  [140/195]  eta: 0:00:28  lr: 0.001572  loss: 0.3361 (0.3371)  time: 0.5109  data: 0.0002  max mem: 9341
[23:22:08.535230] Epoch: [343]  [160/195]  eta: 0:00:18  lr: 0.001572  loss: 0.3403 (0.3379)  time: 0.5131  data: 0.0002  max mem: 9341
[23:22:18.714690] Epoch: [343]  [180/195]  eta: 0:00:07  lr: 0.001571  loss: 0.3287 (0.3366)  time: 0.5089  data: 0.0002  max mem: 9341
[23:22:25.846120] Epoch: [343]  [194/195]  eta: 0:00:00  lr: 0.001571  loss: 0.3359 (0.3368)  time: 0.5107  data: 0.0001  max mem: 9341
[23:22:26.014923] Epoch: [343] Total time: 0:01:40 (0.5174 s / it)
[23:22:26.031492] Averaged stats: lr: 0.001571  loss: 0.3359 (0.3380)
[23:22:30.736780] {"train_lr": 0.0015733691391682776, "train_loss": 0.3379901096988947, "epoch": 343}
[23:22:30.737203] [23:22:30.737299] Training epoch 343 for 0:01:45
[23:22:30.737354] [23:22:30.742425] log_dir: ./exp/debug/cifar100-LT/debug
[23:22:32.542148] Epoch: [344]  [  0/195]  eta: 0:05:50  lr: 0.001571  loss: 0.3742 (0.3742)  time: 1.7984  data: 1.2936  max mem: 9341
[23:22:42.765995] Epoch: [344]  [ 20/195]  eta: 0:01:40  lr: 0.001570  loss: 0.3398 (0.3438)  time: 0.5111  data: 0.0002  max mem: 9341
[23:22:52.978398] Epoch: [344]  [ 40/195]  eta: 0:01:24  lr: 0.001570  loss: 0.3401 (0.3462)  time: 0.5106  data: 0.0002  max mem: 9341
[23:23:03.186122] Epoch: [344]  [ 60/195]  eta: 0:01:11  lr: 0.001570  loss: 0.3399 (0.3447)  time: 0.5103  data: 0.0002  max mem: 9341
[23:23:13.445862] Epoch: [344]  [ 80/195]  eta: 0:01:00  lr: 0.001569  loss: 0.3256 (0.3421)  time: 0.5129  data: 0.0002  max mem: 9341
[23:23:23.663590] Epoch: [344]  [100/195]  eta: 0:00:49  lr: 0.001568  loss: 0.3432 (0.3430)  time: 0.5108  data: 0.0002  max mem: 9341
[23:23:33.877809] Epoch: [344]  [120/195]  eta: 0:00:39  lr: 0.001568  loss: 0.3482 (0.3425)  time: 0.5107  data: 0.0002  max mem: 9341
[23:23:44.092532] Epoch: [344]  [140/195]  eta: 0:00:28  lr: 0.001568  loss: 0.3405 (0.3419)  time: 0.5107  data: 0.0002  max mem: 9341
[23:23:54.348812] Epoch: [344]  [160/195]  eta: 0:00:18  lr: 0.001567  loss: 0.3321 (0.3415)  time: 0.5128  data: 0.0002  max mem: 9341
[23:24:04.514393] Epoch: [344]  [180/195]  eta: 0:00:07  lr: 0.001567  loss: 0.3430 (0.3412)  time: 0.5082  data: 0.0001  max mem: 9341
[23:24:11.640648] Epoch: [344]  [194/195]  eta: 0:00:00  lr: 0.001566  loss: 0.3241 (0.3402)  time: 0.5102  data: 0.0001  max mem: 9341
[23:24:11.801244] Epoch: [344] Total time: 0:01:41 (0.5182 s / it)
[23:24:11.821262] Averaged stats: lr: 0.001566  loss: 0.3241 (0.3413)
[23:24:16.511342] {"train_lr": 0.0015686517729182794, "train_loss": 0.34132745306079204, "epoch": 344}
[23:24:16.511602] [23:24:16.511686] Training epoch 344 for 0:01:45
[23:24:16.511739] [23:24:16.516238] log_dir: ./exp/debug/cifar100-LT/debug
[23:24:18.089802] Epoch: [345]  [  0/195]  eta: 0:05:06  lr: 0.001566  loss: 0.3063 (0.3063)  time: 1.5721  data: 1.0644  max mem: 9341
[23:24:28.329671] Epoch: [345]  [ 20/195]  eta: 0:01:38  lr: 0.001566  loss: 0.3425 (0.3353)  time: 0.5119  data: 0.0002  max mem: 9341
[23:24:38.566759] Epoch: [345]  [ 40/195]  eta: 0:01:23  lr: 0.001565  loss: 0.3455 (0.3412)  time: 0.5118  data: 0.0002  max mem: 9341
[23:24:48.784043] Epoch: [345]  [ 60/195]  eta: 0:01:11  lr: 0.001565  loss: 0.3349 (0.3405)  time: 0.5108  data: 0.0002  max mem: 9341
[23:24:59.045626] Epoch: [345]  [ 80/195]  eta: 0:01:00  lr: 0.001564  loss: 0.3496 (0.3421)  time: 0.5130  data: 0.0002  max mem: 9341
[23:25:09.263502] Epoch: [345]  [100/195]  eta: 0:00:49  lr: 0.001564  loss: 0.3378 (0.3422)  time: 0.5108  data: 0.0002  max mem: 9341
[23:25:19.480129] Epoch: [345]  [120/195]  eta: 0:00:39  lr: 0.001563  loss: 0.3389 (0.3419)  time: 0.5108  data: 0.0002  max mem: 9341
[23:25:29.699839] Epoch: [345]  [140/195]  eta: 0:00:28  lr: 0.001563  loss: 0.3193 (0.3401)  time: 0.5109  data: 0.0002  max mem: 9341
[23:25:40.008855] Epoch: [345]  [160/195]  eta: 0:00:18  lr: 0.001562  loss: 0.3344 (0.3402)  time: 0.5154  data: 0.0002  max mem: 9341
[23:25:50.202277] Epoch: [345]  [180/195]  eta: 0:00:07  lr: 0.001562  loss: 0.3350 (0.3402)  time: 0.5096  data: 0.0002  max mem: 9341
[23:25:57.357257] Epoch: [345]  [194/195]  eta: 0:00:00  lr: 0.001561  loss: 0.3288 (0.3397)  time: 0.5126  data: 0.0001  max mem: 9341
[23:25:57.519872] Epoch: [345] Total time: 0:01:41 (0.5180 s / it)
[23:25:57.555916] Averaged stats: lr: 0.001561  loss: 0.3288 (0.3361)
[23:26:02.323658] {"train_lr": 0.0015639281074266201, "train_loss": 0.3361421697414838, "epoch": 345}
[23:26:02.324013] [23:26:02.324126] Training epoch 345 for 0:01:45
[23:26:02.324182] [23:26:02.328664] log_dir: ./exp/debug/cifar100-LT/debug
[23:26:03.917582] Epoch: [346]  [  0/195]  eta: 0:05:09  lr: 0.001561  loss: 0.3244 (0.3244)  time: 1.5876  data: 1.0868  max mem: 9341
[23:26:14.158095] Epoch: [346]  [ 20/195]  eta: 0:01:38  lr: 0.001561  loss: 0.3423 (0.3347)  time: 0.5120  data: 0.0002  max mem: 9341
[23:26:24.383075] Epoch: [346]  [ 40/195]  eta: 0:01:23  lr: 0.001561  loss: 0.3453 (0.3401)  time: 0.5112  data: 0.0002  max mem: 9341
[23:26:34.602653] Epoch: [346]  [ 60/195]  eta: 0:01:11  lr: 0.001560  loss: 0.3320 (0.3384)  time: 0.5109  data: 0.0002  max mem: 9341
[23:26:44.870331] Epoch: [346]  [ 80/195]  eta: 0:01:00  lr: 0.001559  loss: 0.3343 (0.3386)  time: 0.5133  data: 0.0002  max mem: 9341
[23:26:55.090528] Epoch: [346]  [100/195]  eta: 0:00:49  lr: 0.001559  loss: 0.3296 (0.3373)  time: 0.5110  data: 0.0002  max mem: 9341
[23:27:05.300573] Epoch: [346]  [120/195]  eta: 0:00:39  lr: 0.001559  loss: 0.3379 (0.3367)  time: 0.5104  data: 0.0002  max mem: 9341
[23:27:15.516613] Epoch: [346]  [140/195]  eta: 0:00:28  lr: 0.001558  loss: 0.3238 (0.3358)  time: 0.5107  data: 0.0002  max mem: 9341
[23:27:25.771055] Epoch: [346]  [160/195]  eta: 0:00:18  lr: 0.001557  loss: 0.3288 (0.3354)  time: 0.5127  data: 0.0002  max mem: 9341
[23:27:35.946574] Epoch: [346]  [180/195]  eta: 0:00:07  lr: 0.001557  loss: 0.3320 (0.3350)  time: 0.5087  data: 0.0001  max mem: 9341
[23:27:43.074817] Epoch: [346]  [194/195]  eta: 0:00:00  lr: 0.001557  loss: 0.3362 (0.3352)  time: 0.5104  data: 0.0001  max mem: 9341
[23:27:43.261127] Epoch: [346] Total time: 0:01:40 (0.5176 s / it)
[23:27:43.265807] Averaged stats: lr: 0.001557  loss: 0.3362 (0.3340)
[23:27:47.936056] {"train_lr": 0.0015591982234077204, "train_loss": 0.33403361349915844, "epoch": 346}
[23:27:47.936340] [23:27:47.936437] Training epoch 346 for 0:01:45
[23:27:47.936506] [23:27:47.941401] log_dir: ./exp/debug/cifar100-LT/debug
[23:27:49.634149] Epoch: [347]  [  0/195]  eta: 0:05:29  lr: 0.001557  loss: 0.3521 (0.3521)  time: 1.6915  data: 1.1815  max mem: 9341
[23:27:59.850334] Epoch: [347]  [ 20/195]  eta: 0:01:39  lr: 0.001556  loss: 0.3323 (0.3394)  time: 0.5107  data: 0.0002  max mem: 9341
[23:28:10.065488] Epoch: [347]  [ 40/195]  eta: 0:01:23  lr: 0.001556  loss: 0.3365 (0.3406)  time: 0.5107  data: 0.0002  max mem: 9341
[23:28:20.284163] Epoch: [347]  [ 60/195]  eta: 0:01:11  lr: 0.001555  loss: 0.3314 (0.3364)  time: 0.5109  data: 0.0002  max mem: 9341
[23:28:30.545055] Epoch: [347]  [ 80/195]  eta: 0:01:00  lr: 0.001555  loss: 0.3368 (0.3362)  time: 0.5130  data: 0.0002  max mem: 9341
[23:28:40.757477] Epoch: [347]  [100/195]  eta: 0:00:49  lr: 0.001554  loss: 0.3394 (0.3361)  time: 0.5106  data: 0.0002  max mem: 9341
[23:28:50.978317] Epoch: [347]  [120/195]  eta: 0:00:39  lr: 0.001554  loss: 0.3370 (0.3362)  time: 0.5110  data: 0.0002  max mem: 9341
[23:29:01.194064] Epoch: [347]  [140/195]  eta: 0:00:28  lr: 0.001554  loss: 0.3327 (0.3361)  time: 0.5107  data: 0.0002  max mem: 9341
[23:29:11.447630] Epoch: [347]  [160/195]  eta: 0:00:18  lr: 0.001553  loss: 0.3359 (0.3366)  time: 0.5126  data: 0.0002  max mem: 9341
[23:29:21.621272] Epoch: [347]  [180/195]  eta: 0:00:07  lr: 0.001552  loss: 0.3366 (0.3367)  time: 0.5086  data: 0.0001  max mem: 9341
[23:29:28.753906] Epoch: [347]  [194/195]  eta: 0:00:00  lr: 0.001552  loss: 0.3363 (0.3365)  time: 0.5107  data: 0.0001  max mem: 9341
[23:29:28.912579] Epoch: [347] Total time: 0:01:40 (0.5178 s / it)
[23:29:28.936131] Averaged stats: lr: 0.001552  loss: 0.3363 (0.3348)
[23:29:33.621692] {"train_lr": 0.001554462201682245, "train_loss": 0.33475976567237803, "epoch": 347}
[23:29:33.621953] [23:29:33.622039] Training epoch 347 for 0:01:45
[23:29:33.622093] [23:29:33.626569] log_dir: ./exp/debug/cifar100-LT/debug
[23:29:35.319288] Epoch: [348]  [  0/195]  eta: 0:05:29  lr: 0.001552  loss: 0.3306 (0.3306)  time: 1.6919  data: 1.1839  max mem: 9341
[23:29:45.538801] Epoch: [348]  [ 20/195]  eta: 0:01:39  lr: 0.001552  loss: 0.3346 (0.3351)  time: 0.5109  data: 0.0002  max mem: 9341
[23:29:55.761481] Epoch: [348]  [ 40/195]  eta: 0:01:23  lr: 0.001551  loss: 0.3393 (0.3364)  time: 0.5111  data: 0.0002  max mem: 9341
[23:30:05.980100] Epoch: [348]  [ 60/195]  eta: 0:01:11  lr: 0.001551  loss: 0.3252 (0.3337)  time: 0.5109  data: 0.0002  max mem: 9341
[23:30:16.236875] Epoch: [348]  [ 80/195]  eta: 0:01:00  lr: 0.001550  loss: 0.3364 (0.3344)  time: 0.5128  data: 0.0002  max mem: 9341
[23:30:26.452377] Epoch: [348]  [100/195]  eta: 0:00:49  lr: 0.001550  loss: 0.3255 (0.3340)  time: 0.5107  data: 0.0002  max mem: 9341
[23:30:36.662892] Epoch: [348]  [120/195]  eta: 0:00:39  lr: 0.001549  loss: 0.3395 (0.3339)  time: 0.5105  data: 0.0002  max mem: 9341
[23:30:46.875977] Epoch: [348]  [140/195]  eta: 0:00:28  lr: 0.001549  loss: 0.3315 (0.3330)  time: 0.5106  data: 0.0002  max mem: 9341
[23:30:57.136874] Epoch: [348]  [160/195]  eta: 0:00:18  lr: 0.001548  loss: 0.3323 (0.3333)  time: 0.5130  data: 0.0002  max mem: 9341
[23:31:07.312484] Epoch: [348]  [180/195]  eta: 0:00:07  lr: 0.001548  loss: 0.3429 (0.3342)  time: 0.5087  data: 0.0001  max mem: 9341
[23:31:14.448027] Epoch: [348]  [194/195]  eta: 0:00:00  lr: 0.001547  loss: 0.3284 (0.3340)  time: 0.5109  data: 0.0001  max mem: 9341
[23:31:14.610835] Epoch: [348] Total time: 0:01:40 (0.5179 s / it)
[23:31:14.630636] Averaged stats: lr: 0.001547  loss: 0.3284 (0.3349)
[23:31:19.309740] {"train_lr": 0.0015497201231757332, "train_loss": 0.3348674067319968, "epoch": 348}
[23:31:19.310007] [23:31:19.310108] Training epoch 348 for 0:01:45
[23:31:19.310160] [23:31:19.314606] log_dir: ./exp/debug/cifar100-LT/debug
[23:31:21.148050] Epoch: [349]  [  0/195]  eta: 0:05:57  lr: 0.001547  loss: 0.3286 (0.3286)  time: 1.8324  data: 1.3233  max mem: 9341
[23:31:31.386682] Epoch: [349]  [ 20/195]  eta: 0:01:40  lr: 0.001547  loss: 0.3396 (0.3399)  time: 0.5119  data: 0.0002  max mem: 9341
[23:31:41.624445] Epoch: [349]  [ 40/195]  eta: 0:01:24  lr: 0.001546  loss: 0.3382 (0.3401)  time: 0.5118  data: 0.0002  max mem: 9341
[23:31:51.855776] Epoch: [349]  [ 60/195]  eta: 0:01:12  lr: 0.001546  loss: 0.3309 (0.3357)  time: 0.5115  data: 0.0002  max mem: 9341
[23:32:02.155560] Epoch: [349]  [ 80/195]  eta: 0:01:00  lr: 0.001545  loss: 0.3265 (0.3331)  time: 0.5149  data: 0.0002  max mem: 9341
[23:32:12.392642] Epoch: [349]  [100/195]  eta: 0:00:49  lr: 0.001545  loss: 0.3224 (0.3328)  time: 0.5118  data: 0.0002  max mem: 9341
[23:32:22.627334] Epoch: [349]  [120/195]  eta: 0:00:39  lr: 0.001544  loss: 0.3376 (0.3335)  time: 0.5117  data: 0.0002  max mem: 9341
[23:32:32.863587] Epoch: [349]  [140/195]  eta: 0:00:28  lr: 0.001544  loss: 0.3224 (0.3330)  time: 0.5118  data: 0.0002  max mem: 9341
[23:32:43.162499] Epoch: [349]  [160/195]  eta: 0:00:18  lr: 0.001543  loss: 0.3334 (0.3330)  time: 0.5149  data: 0.0002  max mem: 9341
[23:32:53.359402] Epoch: [349]  [180/195]  eta: 0:00:07  lr: 0.001543  loss: 0.3331 (0.3335)  time: 0.5098  data: 0.0001  max mem: 9341
[23:33:00.510449] Epoch: [349]  [194/195]  eta: 0:00:00  lr: 0.001542  loss: 0.3352 (0.3339)  time: 0.5128  data: 0.0001  max mem: 9341
[23:33:00.685777] Epoch: [349] Total time: 0:01:41 (0.5199 s / it)
[23:33:00.695313] Averaged stats: lr: 0.001542  loss: 0.3352 (0.3346)
[23:33:05.412907] {"train_lr": 0.0015449720689172306, "train_loss": 0.33458331846273864, "epoch": 349}
[23:33:05.413241] [23:33:05.413327] Training epoch 349 for 0:01:46
[23:33:05.413380] [23:33:05.417859] log_dir: ./exp/debug/cifar100-LT/debug
[23:33:07.069530] Epoch: [350]  [  0/195]  eta: 0:05:21  lr: 0.001542  loss: 0.3768 (0.3768)  time: 1.6504  data: 1.1460  max mem: 9341
[23:33:17.282476] Epoch: [350]  [ 20/195]  eta: 0:01:38  lr: 0.001542  loss: 0.3296 (0.3370)  time: 0.5106  data: 0.0002  max mem: 9341
[23:33:27.507005] Epoch: [350]  [ 40/195]  eta: 0:01:23  lr: 0.001542  loss: 0.3289 (0.3346)  time: 0.5111  data: 0.0002  max mem: 9341
[23:33:37.716951] Epoch: [350]  [ 60/195]  eta: 0:01:11  lr: 0.001541  loss: 0.3347 (0.3335)  time: 0.5104  data: 0.0002  max mem: 9341
[23:33:47.977049] Epoch: [350]  [ 80/195]  eta: 0:01:00  lr: 0.001540  loss: 0.3312 (0.3342)  time: 0.5129  data: 0.0002  max mem: 9341
[23:33:58.186577] Epoch: [350]  [100/195]  eta: 0:00:49  lr: 0.001540  loss: 0.3347 (0.3345)  time: 0.5104  data: 0.0002  max mem: 9341
[23:34:08.401856] Epoch: [350]  [120/195]  eta: 0:00:39  lr: 0.001540  loss: 0.3288 (0.3350)  time: 0.5107  data: 0.0002  max mem: 9341
[23:34:18.619715] Epoch: [350]  [140/195]  eta: 0:00:28  lr: 0.001539  loss: 0.3312 (0.3347)  time: 0.5108  data: 0.0002  max mem: 9341
[23:34:28.876061] Epoch: [350]  [160/195]  eta: 0:00:18  lr: 0.001539  loss: 0.3347 (0.3341)  time: 0.5128  data: 0.0002  max mem: 9341
[23:34:39.049058] Epoch: [350]  [180/195]  eta: 0:00:07  lr: 0.001538  loss: 0.3309 (0.3341)  time: 0.5086  data: 0.0001  max mem: 9341
[23:34:46.177552] Epoch: [350]  [194/195]  eta: 0:00:00  lr: 0.001538  loss: 0.3337 (0.3343)  time: 0.5103  data: 0.0001  max mem: 9341
[23:34:46.340688] Epoch: [350] Total time: 0:01:40 (0.5176 s / it)
[23:34:46.357674] Averaged stats: lr: 0.001538  loss: 0.3337 (0.3359)
[23:34:51.059827] {"train_lr": 0.0015402181200378797, "train_loss": 0.3359334425666393, "epoch": 350}
[23:34:51.060189] [23:34:51.060276] Training epoch 350 for 0:01:45
[23:34:51.060330] [23:34:51.064792] log_dir: ./exp/debug/cifar100-LT/debug
[23:34:52.789563] Epoch: [351]  [  0/195]  eta: 0:05:36  lr: 0.001538  loss: 0.3553 (0.3553)  time: 1.7237  data: 1.2180  max mem: 9341
[23:35:03.009279] Epoch: [351]  [ 20/195]  eta: 0:01:39  lr: 0.001537  loss: 0.3362 (0.3352)  time: 0.5109  data: 0.0002  max mem: 9341
[23:35:13.227784] Epoch: [351]  [ 40/195]  eta: 0:01:23  lr: 0.001537  loss: 0.3260 (0.3307)  time: 0.5109  data: 0.0002  max mem: 9341
[23:35:23.444749] Epoch: [351]  [ 60/195]  eta: 0:01:11  lr: 0.001536  loss: 0.3318 (0.3311)  time: 0.5108  data: 0.0002  max mem: 9341
[23:35:33.711020] Epoch: [351]  [ 80/195]  eta: 0:01:00  lr: 0.001536  loss: 0.3308 (0.3323)  time: 0.5132  data: 0.0002  max mem: 9341
[23:35:43.922898] Epoch: [351]  [100/195]  eta: 0:00:49  lr: 0.001535  loss: 0.3249 (0.3308)  time: 0.5105  data: 0.0002  max mem: 9341
[23:35:54.133773] Epoch: [351]  [120/195]  eta: 0:00:39  lr: 0.001535  loss: 0.3252 (0.3309)  time: 0.5105  data: 0.0002  max mem: 9341
[23:36:04.346524] Epoch: [351]  [140/195]  eta: 0:00:28  lr: 0.001535  loss: 0.3329 (0.3316)  time: 0.5106  data: 0.0002  max mem: 9341
[23:36:14.604018] Epoch: [351]  [160/195]  eta: 0:00:18  lr: 0.001534  loss: 0.3382 (0.3324)  time: 0.5128  data: 0.0002  max mem: 9341
[23:36:24.778236] Epoch: [351]  [180/195]  eta: 0:00:07  lr: 0.001533  loss: 0.3328 (0.3330)  time: 0.5087  data: 0.0002  max mem: 9341
[23:36:31.913514] Epoch: [351]  [194/195]  eta: 0:00:00  lr: 0.001533  loss: 0.3334 (0.3329)  time: 0.5107  data: 0.0001  max mem: 9341
[23:36:32.089279] Epoch: [351] Total time: 0:01:41 (0.5181 s / it)
[23:36:32.090607] Averaged stats: lr: 0.001533  loss: 0.3334 (0.3330)
[23:36:36.794022] {"train_lr": 0.001535458357769556, "train_loss": 0.3330408011491482, "epoch": 351}
[23:36:36.794367] [23:36:36.794453] Training epoch 351 for 0:01:45
[23:36:36.794507] [23:36:36.798998] log_dir: ./exp/debug/cifar100-LT/debug
[23:36:38.514203] Epoch: [352]  [  0/195]  eta: 0:05:33  lr: 0.001533  loss: 0.3010 (0.3010)  time: 1.7128  data: 1.2044  max mem: 9341
[23:36:48.731886] Epoch: [352]  [ 20/195]  eta: 0:01:39  lr: 0.001532  loss: 0.3269 (0.3307)  time: 0.5108  data: 0.0002  max mem: 9341
[23:36:58.973022] Epoch: [352]  [ 40/195]  eta: 0:01:23  lr: 0.001532  loss: 0.3226 (0.3301)  time: 0.5120  data: 0.0002  max mem: 9341
[23:37:09.211449] Epoch: [352]  [ 60/195]  eta: 0:01:11  lr: 0.001532  loss: 0.3292 (0.3297)  time: 0.5118  data: 0.0002  max mem: 9341
[23:37:19.511212] Epoch: [352]  [ 80/195]  eta: 0:01:00  lr: 0.001531  loss: 0.3452 (0.3330)  time: 0.5149  data: 0.0002  max mem: 9341
[23:37:29.752066] Epoch: [352]  [100/195]  eta: 0:00:49  lr: 0.001531  loss: 0.3398 (0.3343)  time: 0.5120  data: 0.0002  max mem: 9341
[23:37:39.991909] Epoch: [352]  [120/195]  eta: 0:00:39  lr: 0.001530  loss: 0.3290 (0.3335)  time: 0.5119  data: 0.0002  max mem: 9341
[23:37:50.229574] Epoch: [352]  [140/195]  eta: 0:00:28  lr: 0.001530  loss: 0.3353 (0.3337)  time: 0.5118  data: 0.0002  max mem: 9341
[23:38:00.532783] Epoch: [352]  [160/195]  eta: 0:00:18  lr: 0.001529  loss: 0.3307 (0.3334)  time: 0.5151  data: 0.0002  max mem: 9341
[23:38:10.727499] Epoch: [352]  [180/195]  eta: 0:00:07  lr: 0.001529  loss: 0.3387 (0.3342)  time: 0.5097  data: 0.0002  max mem: 9341
[23:38:17.878392] Epoch: [352]  [194/195]  eta: 0:00:00  lr: 0.001528  loss: 0.3333 (0.3340)  time: 0.5128  data: 0.0001  max mem: 9341
[23:38:18.055919] Epoch: [352] Total time: 0:01:41 (0.5193 s / it)
[23:38:18.076840] Averaged stats: lr: 0.001528  loss: 0.3333 (0.3352)
[23:38:22.789561] {"train_lr": 0.001530692863443451, "train_loss": 0.3352441994043497, "epoch": 352}
[23:38:22.789897] [23:38:22.789990] Training epoch 352 for 0:01:45
[23:38:22.790051] [23:38:22.794733] log_dir: ./exp/debug/cifar100-LT/debug
[23:38:24.389785] Epoch: [353]  [  0/195]  eta: 0:05:10  lr: 0.001528  loss: 0.3117 (0.3117)  time: 1.5940  data: 1.0864  max mem: 9341
[23:38:34.640010] Epoch: [353]  [ 20/195]  eta: 0:01:38  lr: 0.001528  loss: 0.3312 (0.3262)  time: 0.5124  data: 0.0002  max mem: 9341
[23:38:44.874369] Epoch: [353]  [ 40/195]  eta: 0:01:23  lr: 0.001527  loss: 0.3384 (0.3322)  time: 0.5117  data: 0.0002  max mem: 9341
[23:38:55.109864] Epoch: [353]  [ 60/195]  eta: 0:01:11  lr: 0.001527  loss: 0.3464 (0.3360)  time: 0.5117  data: 0.0002  max mem: 9341
[23:39:05.405997] Epoch: [353]  [ 80/195]  eta: 0:01:00  lr: 0.001526  loss: 0.3313 (0.3367)  time: 0.5148  data: 0.0002  max mem: 9341
[23:39:15.647727] Epoch: [353]  [100/195]  eta: 0:00:49  lr: 0.001526  loss: 0.3445 (0.3379)  time: 0.5120  data: 0.0002  max mem: 9341
[23:39:25.884300] Epoch: [353]  [120/195]  eta: 0:00:39  lr: 0.001525  loss: 0.3398 (0.3384)  time: 0.5118  data: 0.0002  max mem: 9341
[23:39:36.116009] Epoch: [353]  [140/195]  eta: 0:00:28  lr: 0.001525  loss: 0.3316 (0.3367)  time: 0.5115  data: 0.0002  max mem: 9341
[23:39:46.417573] Epoch: [353]  [160/195]  eta: 0:00:18  lr: 0.001524  loss: 0.3507 (0.3378)  time: 0.5150  data: 0.0002  max mem: 9341
[23:39:56.613176] Epoch: [353]  [180/195]  eta: 0:00:07  lr: 0.001524  loss: 0.3389 (0.3377)  time: 0.5097  data: 0.0001  max mem: 9341
[23:40:03.763123] Epoch: [353]  [194/195]  eta: 0:00:00  lr: 0.001523  loss: 0.3415 (0.3379)  time: 0.5126  data: 0.0001  max mem: 9341
[23:40:03.925381] Epoch: [353] Total time: 0:01:41 (0.5186 s / it)
[23:40:03.932729] Averaged stats: lr: 0.001523  loss: 0.3415 (0.3351)
[23:40:08.745332] {"train_lr": 0.0015259217184887314, "train_loss": 0.3351060560498482, "epoch": 353}
[23:40:08.745684] [23:40:08.745770] Training epoch 353 for 0:01:45
[23:40:08.745822] [23:40:08.750245] log_dir: ./exp/debug/cifar100-LT/debug
[23:40:10.463070] Epoch: [354]  [  0/195]  eta: 0:05:33  lr: 0.001523  loss: 0.3057 (0.3057)  time: 1.7116  data: 1.2201  max mem: 9341
[23:40:20.709374] Epoch: [354]  [ 20/195]  eta: 0:01:39  lr: 0.001523  loss: 0.3378 (0.3375)  time: 0.5123  data: 0.0002  max mem: 9341
[23:40:30.956177] Epoch: [354]  [ 40/195]  eta: 0:01:23  lr: 0.001523  loss: 0.3438 (0.3394)  time: 0.5123  data: 0.0002  max mem: 9341
[23:40:41.196445] Epoch: [354]  [ 60/195]  eta: 0:01:11  lr: 0.001522  loss: 0.3409 (0.3415)  time: 0.5120  data: 0.0002  max mem: 9341
[23:40:51.493620] Epoch: [354]  [ 80/195]  eta: 0:01:00  lr: 0.001521  loss: 0.3250 (0.3391)  time: 0.5148  data: 0.0002  max mem: 9341
[23:41:01.727406] Epoch: [354]  [100/195]  eta: 0:00:49  lr: 0.001521  loss: 0.3303 (0.3367)  time: 0.5116  data: 0.0002  max mem: 9341
[23:41:11.965939] Epoch: [354]  [120/195]  eta: 0:00:39  lr: 0.001521  loss: 0.3330 (0.3369)  time: 0.5119  data: 0.0002  max mem: 9341
[23:41:22.185665] Epoch: [354]  [140/195]  eta: 0:00:28  lr: 0.001520  loss: 0.3296 (0.3362)  time: 0.5109  data: 0.0002  max mem: 9341
[23:41:32.465301] Epoch: [354]  [160/195]  eta: 0:00:18  lr: 0.001519  loss: 0.3407 (0.3363)  time: 0.5139  data: 0.0002  max mem: 9341
[23:41:42.641814] Epoch: [354]  [180/195]  eta: 0:00:07  lr: 0.001519  loss: 0.3220 (0.3353)  time: 0.5088  data: 0.0001  max mem: 9341
[23:41:49.774965] Epoch: [354]  [194/195]  eta: 0:00:00  lr: 0.001519  loss: 0.3371 (0.3360)  time: 0.5107  data: 0.0001  max mem: 9341
[23:41:49.940263] Epoch: [354] Total time: 0:01:41 (0.5189 s / it)
[23:41:49.953360] Averaged stats: lr: 0.001519  loss: 0.3371 (0.3329)
[23:41:54.682366] {"train_lr": 0.0015211450044310818, "train_loss": 0.3328783949598288, "epoch": 354}
[23:41:54.682730] [23:41:54.682817] Training epoch 354 for 0:01:45
[23:41:54.682871] [23:41:54.687291] log_dir: ./exp/debug/cifar100-LT/debug
[23:41:56.505924] Epoch: [355]  [  0/195]  eta: 0:05:54  lr: 0.001519  loss: 0.3562 (0.3562)  time: 1.8175  data: 1.3208  max mem: 9341
[23:42:06.777575] Epoch: [355]  [ 20/195]  eta: 0:01:40  lr: 0.001518  loss: 0.3431 (0.3405)  time: 0.5135  data: 0.0002  max mem: 9341
[23:42:16.995559] Epoch: [355]  [ 40/195]  eta: 0:01:24  lr: 0.001518  loss: 0.3252 (0.3345)  time: 0.5108  data: 0.0002  max mem: 9341
[23:42:27.211787] Epoch: [355]  [ 60/195]  eta: 0:01:11  lr: 0.001517  loss: 0.3164 (0.3310)  time: 0.5108  data: 0.0002  max mem: 9341
[23:42:37.469753] Epoch: [355]  [ 80/195]  eta: 0:01:00  lr: 0.001517  loss: 0.3313 (0.3311)  time: 0.5128  data: 0.0002  max mem: 9341
[23:42:47.684376] Epoch: [355]  [100/195]  eta: 0:00:49  lr: 0.001516  loss: 0.3312 (0.3311)  time: 0.5107  data: 0.0002  max mem: 9341
[23:42:57.899504] Epoch: [355]  [120/195]  eta: 0:00:39  lr: 0.001516  loss: 0.3329 (0.3313)  time: 0.5107  data: 0.0002  max mem: 9341
[23:43:08.110075] Epoch: [355]  [140/195]  eta: 0:00:28  lr: 0.001515  loss: 0.3238 (0.3320)  time: 0.5105  data: 0.0002  max mem: 9341
[23:43:18.367883] Epoch: [355]  [160/195]  eta: 0:00:18  lr: 0.001515  loss: 0.3203 (0.3311)  time: 0.5128  data: 0.0002  max mem: 9341
[23:43:28.545003] Epoch: [355]  [180/195]  eta: 0:00:07  lr: 0.001514  loss: 0.3350 (0.3313)  time: 0.5088  data: 0.0001  max mem: 9341
[23:43:35.677912] Epoch: [355]  [194/195]  eta: 0:00:00  lr: 0.001514  loss: 0.3306 (0.3310)  time: 0.5107  data: 0.0001  max mem: 9341
[23:43:35.855264] Epoch: [355] Total time: 0:01:41 (0.5188 s / it)
[23:43:35.861628] Averaged stats: lr: 0.001514  loss: 0.3306 (0.3315)
[23:43:40.583274] {"train_lr": 0.001516362802891378, "train_loss": 0.33145660406503924, "epoch": 355}
[23:43:40.583689] [23:43:40.583791] Training epoch 355 for 0:01:45
[23:43:40.583846] [23:43:40.588346] log_dir: ./exp/debug/cifar100-LT/debug
[23:43:42.176869] Epoch: [356]  [  0/195]  eta: 0:05:09  lr: 0.001514  loss: 0.3198 (0.3198)  time: 1.5871  data: 1.0939  max mem: 9341
[23:43:52.402595] Epoch: [356]  [ 20/195]  eta: 0:01:38  lr: 0.001513  loss: 0.3147 (0.3221)  time: 0.5112  data: 0.0002  max mem: 9341
[23:44:02.619337] Epoch: [356]  [ 40/195]  eta: 0:01:23  lr: 0.001513  loss: 0.3361 (0.3285)  time: 0.5108  data: 0.0002  max mem: 9341
[23:44:12.835390] Epoch: [356]  [ 60/195]  eta: 0:01:11  lr: 0.001513  loss: 0.3291 (0.3274)  time: 0.5107  data: 0.0002  max mem: 9341
[23:44:23.134701] Epoch: [356]  [ 80/195]  eta: 0:01:00  lr: 0.001512  loss: 0.3350 (0.3281)  time: 0.5149  data: 0.0002  max mem: 9341
[23:44:33.363809] Epoch: [356]  [100/195]  eta: 0:00:49  lr: 0.001511  loss: 0.3335 (0.3289)  time: 0.5114  data: 0.0002  max mem: 9341
[23:44:43.593062] Epoch: [356]  [120/195]  eta: 0:00:39  lr: 0.001511  loss: 0.3376 (0.3308)  time: 0.5114  data: 0.0002  max mem: 9341
[23:44:53.823148] Epoch: [356]  [140/195]  eta: 0:00:28  lr: 0.001511  loss: 0.3235 (0.3300)  time: 0.5114  data: 0.0002  max mem: 9341
[23:45:04.113139] Epoch: [356]  [160/195]  eta: 0:00:18  lr: 0.001510  loss: 0.3271 (0.3301)  time: 0.5144  data: 0.0002  max mem: 9341
[23:45:14.298911] Epoch: [356]  [180/195]  eta: 0:00:07  lr: 0.001509  loss: 0.3240 (0.3307)  time: 0.5092  data: 0.0001  max mem: 9341
[23:45:21.446279] Epoch: [356]  [194/195]  eta: 0:00:00  lr: 0.001509  loss: 0.3342 (0.3308)  time: 0.5121  data: 0.0001  max mem: 9341
[23:45:21.643207] Epoch: [356] Total time: 0:01:41 (0.5182 s / it)
[23:45:21.649594] Averaged stats: lr: 0.001509  loss: 0.3342 (0.3309)
[23:45:26.380452] {"train_lr": 0.001511575195584251, "train_loss": 0.33093941842134184, "epoch": 356}
[23:45:26.380717] [23:45:26.380804] Training epoch 356 for 0:01:45
[23:45:26.380858] [23:45:26.385356] log_dir: ./exp/debug/cifar100-LT/debug
[23:45:28.104748] Epoch: [357]  [  0/195]  eta: 0:05:35  lr: 0.001509  loss: 0.3988 (0.3988)  time: 1.7184  data: 1.2120  max mem: 9341
[23:45:38.338563] Epoch: [357]  [ 20/195]  eta: 0:01:39  lr: 0.001509  loss: 0.3284 (0.3355)  time: 0.5116  data: 0.0002  max mem: 9341
[23:45:48.545393] Epoch: [357]  [ 40/195]  eta: 0:01:23  lr: 0.001508  loss: 0.3349 (0.3357)  time: 0.5103  data: 0.0002  max mem: 9341
[23:45:58.759067] Epoch: [357]  [ 60/195]  eta: 0:01:11  lr: 0.001508  loss: 0.3381 (0.3356)  time: 0.5106  data: 0.0002  max mem: 9341
[23:46:09.014692] Epoch: [357]  [ 80/195]  eta: 0:01:00  lr: 0.001507  loss: 0.3384 (0.3363)  time: 0.5127  data: 0.0002  max mem: 9341
[23:46:19.222305] Epoch: [357]  [100/195]  eta: 0:00:49  lr: 0.001507  loss: 0.3456 (0.3379)  time: 0.5103  data: 0.0002  max mem: 9341
[23:46:29.446589] Epoch: [357]  [120/195]  eta: 0:00:39  lr: 0.001506  loss: 0.3362 (0.3381)  time: 0.5112  data: 0.0002  max mem: 9341
[23:46:39.673437] Epoch: [357]  [140/195]  eta: 0:00:28  lr: 0.001506  loss: 0.3329 (0.3381)  time: 0.5113  data: 0.0002  max mem: 9341
[23:46:49.961826] Epoch: [357]  [160/195]  eta: 0:00:18  lr: 0.001505  loss: 0.3241 (0.3370)  time: 0.5144  data: 0.0002  max mem: 9341
[23:47:00.147057] Epoch: [357]  [180/195]  eta: 0:00:07  lr: 0.001505  loss: 0.3341 (0.3371)  time: 0.5092  data: 0.0001  max mem: 9341
[23:47:07.291576] Epoch: [357]  [194/195]  eta: 0:00:00  lr: 0.001504  loss: 0.3377 (0.3367)  time: 0.5121  data: 0.0001  max mem: 9341
[23:47:07.460890] Epoch: [357] Total time: 0:01:41 (0.5183 s / it)
[23:47:07.479797] Averaged stats: lr: 0.001504  loss: 0.3377 (0.3343)
[23:47:12.201963] {"train_lr": 0.0015067822643166956, "train_loss": 0.3342536561000041, "epoch": 357}
[23:47:12.202261] [23:47:12.202362] Training epoch 357 for 0:01:45
[23:47:12.202416] [23:47:12.207025] log_dir: ./exp/debug/cifar100-LT/debug
[23:47:13.889888] Epoch: [358]  [  0/195]  eta: 0:05:27  lr: 0.001504  loss: 0.3468 (0.3468)  time: 1.6809  data: 1.1834  max mem: 9341
[23:47:24.129710] Epoch: [358]  [ 20/195]  eta: 0:01:39  lr: 0.001504  loss: 0.3359 (0.3426)  time: 0.5119  data: 0.0003  max mem: 9341
[23:47:34.361183] Epoch: [358]  [ 40/195]  eta: 0:01:23  lr: 0.001503  loss: 0.3508 (0.3454)  time: 0.5115  data: 0.0002  max mem: 9341
[23:47:44.603291] Epoch: [358]  [ 60/195]  eta: 0:01:11  lr: 0.001503  loss: 0.3449 (0.3434)  time: 0.5120  data: 0.0002  max mem: 9341
[23:47:54.861816] Epoch: [358]  [ 80/195]  eta: 0:01:00  lr: 0.001502  loss: 0.3269 (0.3392)  time: 0.5129  data: 0.0002  max mem: 9341
[23:48:05.073735] Epoch: [358]  [100/195]  eta: 0:00:49  lr: 0.001502  loss: 0.3257 (0.3359)  time: 0.5105  data: 0.0003  max mem: 9341
[23:48:15.288752] Epoch: [358]  [120/195]  eta: 0:00:39  lr: 0.001501  loss: 0.3291 (0.3358)  time: 0.5107  data: 0.0002  max mem: 9341
[23:48:25.499656] Epoch: [358]  [140/195]  eta: 0:00:28  lr: 0.001501  loss: 0.3264 (0.3346)  time: 0.5105  data: 0.0002  max mem: 9341
[23:48:35.753209] Epoch: [358]  [160/195]  eta: 0:00:18  lr: 0.001500  loss: 0.3418 (0.3351)  time: 0.5126  data: 0.0002  max mem: 9341
[23:48:45.922342] Epoch: [358]  [180/195]  eta: 0:00:07  lr: 0.001500  loss: 0.3282 (0.3347)  time: 0.5084  data: 0.0001  max mem: 9341
[23:48:53.047052] Epoch: [358]  [194/195]  eta: 0:00:00  lr: 0.001499  loss: 0.3241 (0.3341)  time: 0.5102  data: 0.0001  max mem: 9341
[23:48:53.231522] Epoch: [358] Total time: 0:01:41 (0.5181 s / it)
[23:48:53.235994] Averaged stats: lr: 0.001499  loss: 0.3241 (0.3330)
[23:48:57.941138] {"train_lr": 0.001501984090986682, "train_loss": 0.333024285810116, "epoch": 358}
[23:48:57.941496] [23:48:57.941603] Training epoch 358 for 0:01:45
[23:48:57.941659] [23:48:57.946149] log_dir: ./exp/debug/cifar100-LT/debug
[23:48:59.676230] Epoch: [359]  [  0/195]  eta: 0:05:37  lr: 0.001499  loss: 0.3193 (0.3193)  time: 1.7285  data: 1.2193  max mem: 9341
[23:49:09.901785] Epoch: [359]  [ 20/195]  eta: 0:01:39  lr: 0.001499  loss: 0.3280 (0.3311)  time: 0.5112  data: 0.0002  max mem: 9341
[23:49:20.144649] Epoch: [359]  [ 40/195]  eta: 0:01:23  lr: 0.001499  loss: 0.3167 (0.3274)  time: 0.5121  data: 0.0002  max mem: 9341
[23:49:30.383468] Epoch: [359]  [ 60/195]  eta: 0:01:11  lr: 0.001498  loss: 0.3256 (0.3279)  time: 0.5119  data: 0.0002  max mem: 9341
[23:49:40.692712] Epoch: [359]  [ 80/195]  eta: 0:01:00  lr: 0.001497  loss: 0.3284 (0.3283)  time: 0.5154  data: 0.0002  max mem: 9341
[23:49:50.933706] Epoch: [359]  [100/195]  eta: 0:00:49  lr: 0.001497  loss: 0.3194 (0.3270)  time: 0.5120  data: 0.0002  max mem: 9341
[23:50:01.175202] Epoch: [359]  [120/195]  eta: 0:00:39  lr: 0.001497  loss: 0.3383 (0.3284)  time: 0.5120  data: 0.0002  max mem: 9341
[23:50:11.416994] Epoch: [359]  [140/195]  eta: 0:00:28  lr: 0.001496  loss: 0.3191 (0.3281)  time: 0.5120  data: 0.0002  max mem: 9341
[23:50:21.724677] Epoch: [359]  [160/195]  eta: 0:00:18  lr: 0.001495  loss: 0.3192 (0.3276)  time: 0.5153  data: 0.0002  max mem: 9341
[23:50:31.920010] Epoch: [359]  [180/195]  eta: 0:00:07  lr: 0.001495  loss: 0.3167 (0.3266)  time: 0.5097  data: 0.0001  max mem: 9341
[23:50:39.073128] Epoch: [359]  [194/195]  eta: 0:00:00  lr: 0.001495  loss: 0.3209 (0.3263)  time: 0.5127  data: 0.0001  max mem: 9341
[23:50:39.256591] Epoch: [359] Total time: 0:01:41 (0.5195 s / it)
[23:50:39.279811] Averaged stats: lr: 0.001495  loss: 0.3209 (0.3271)
[23:50:43.895981] {"train_lr": 0.0014971807575817585, "train_loss": 0.3271403773090778, "epoch": 359}
[23:50:43.896271] [23:50:43.896363] Training epoch 359 for 0:01:45
[23:50:43.896417] [23:50:43.900911] log_dir: ./exp/debug/cifar100-LT/debug
[23:50:45.515092] Epoch: [360]  [  0/195]  eta: 0:05:14  lr: 0.001495  loss: 0.3290 (0.3290)  time: 1.6134  data: 1.1151  max mem: 9341
[23:50:55.732832] Epoch: [360]  [ 20/195]  eta: 0:01:38  lr: 0.001494  loss: 0.3197 (0.3216)  time: 0.5108  data: 0.0002  max mem: 9341
[23:51:05.944635] Epoch: [360]  [ 40/195]  eta: 0:01:23  lr: 0.001494  loss: 0.3295 (0.3250)  time: 0.5105  data: 0.0002  max mem: 9341
[23:51:16.162038] Epoch: [360]  [ 60/195]  eta: 0:01:11  lr: 0.001493  loss: 0.3258 (0.3273)  time: 0.5108  data: 0.0002  max mem: 9341
[23:51:26.425539] Epoch: [360]  [ 80/195]  eta: 0:01:00  lr: 0.001493  loss: 0.3241 (0.3265)  time: 0.5131  data: 0.0002  max mem: 9341
[23:51:36.638457] Epoch: [360]  [100/195]  eta: 0:00:49  lr: 0.001492  loss: 0.3169 (0.3252)  time: 0.5106  data: 0.0002  max mem: 9341
[23:51:46.851571] Epoch: [360]  [120/195]  eta: 0:00:39  lr: 0.001492  loss: 0.3207 (0.3249)  time: 0.5106  data: 0.0002  max mem: 9341
[23:51:57.069156] Epoch: [360]  [140/195]  eta: 0:00:28  lr: 0.001491  loss: 0.3371 (0.3264)  time: 0.5108  data: 0.0002  max mem: 9341
[23:52:07.331453] Epoch: [360]  [160/195]  eta: 0:00:18  lr: 0.001491  loss: 0.3284 (0.3269)  time: 0.5131  data: 0.0002  max mem: 9341
[23:52:17.509849] Epoch: [360]  [180/195]  eta: 0:00:07  lr: 0.001490  loss: 0.3396 (0.3278)  time: 0.5089  data: 0.0002  max mem: 9341
[23:52:24.647987] Epoch: [360]  [194/195]  eta: 0:00:00  lr: 0.001490  loss: 0.3297 (0.3281)  time: 0.5110  data: 0.0001  max mem: 9341
[23:52:24.832775] Epoch: [360] Total time: 0:01:40 (0.5176 s / it)
[23:52:24.853931] Averaged stats: lr: 0.001490  loss: 0.3297 (0.3302)
[23:52:29.715907] {"train_lr": 0.001492372346177639, "train_loss": 0.33019150976951306, "epoch": 360}
[23:52:29.716222] [23:52:29.716311] Training epoch 360 for 0:01:45
[23:52:29.716365] [23:52:29.720763] log_dir: ./exp/debug/cifar100-LT/debug
[23:52:31.265404] Epoch: [361]  [  0/195]  eta: 0:05:00  lr: 0.001490  loss: 0.3162 (0.3162)  time: 1.5430  data: 1.0442  max mem: 9341
[23:52:41.478769] Epoch: [361]  [ 20/195]  eta: 0:01:37  lr: 0.001489  loss: 0.3327 (0.3339)  time: 0.5106  data: 0.0002  max mem: 9341
[23:52:51.697834] Epoch: [361]  [ 40/195]  eta: 0:01:23  lr: 0.001489  loss: 0.3318 (0.3325)  time: 0.5109  data: 0.0002  max mem: 9341
[23:53:01.912909] Epoch: [361]  [ 60/195]  eta: 0:01:11  lr: 0.001489  loss: 0.3379 (0.3327)  time: 0.5107  data: 0.0002  max mem: 9341
[23:53:12.172023] Epoch: [361]  [ 80/195]  eta: 0:01:00  lr: 0.001488  loss: 0.3260 (0.3326)  time: 0.5129  data: 0.0002  max mem: 9341
[23:53:22.385210] Epoch: [361]  [100/195]  eta: 0:00:49  lr: 0.001487  loss: 0.3300 (0.3322)  time: 0.5106  data: 0.0002  max mem: 9341
[23:53:32.601964] Epoch: [361]  [120/195]  eta: 0:00:38  lr: 0.001487  loss: 0.3338 (0.3331)  time: 0.5108  data: 0.0002  max mem: 9341
[23:53:42.822830] Epoch: [361]  [140/195]  eta: 0:00:28  lr: 0.001487  loss: 0.3413 (0.3336)  time: 0.5110  data: 0.0002  max mem: 9341
[23:53:53.106495] Epoch: [361]  [160/195]  eta: 0:00:18  lr: 0.001486  loss: 0.3342 (0.3336)  time: 0.5141  data: 0.0002  max mem: 9341
[23:54:03.300900] Epoch: [361]  [180/195]  eta: 0:00:07  lr: 0.001485  loss: 0.3225 (0.3324)  time: 0.5097  data: 0.0001  max mem: 9341
[23:54:10.455643] Epoch: [361]  [194/195]  eta: 0:00:00  lr: 0.001485  loss: 0.3345 (0.3327)  time: 0.5128  data: 0.0001  max mem: 9341
[23:54:10.632466] Epoch: [361] Total time: 0:01:40 (0.5175 s / it)
[23:54:10.639006] Averaged stats: lr: 0.001485  loss: 0.3345 (0.3328)
[23:54:15.326108] {"train_lr": 0.0014875589389368024, "train_loss": 0.33277367336245683, "epoch": 361}
[23:54:15.326460] [23:54:15.326545] Training epoch 361 for 0:01:45
[23:54:15.326599] [23:54:15.331037] log_dir: ./exp/debug/cifar100-LT/debug
[23:54:17.070174] Epoch: [362]  [  0/195]  eta: 0:05:38  lr: 0.001485  loss: 0.3316 (0.3316)  time: 1.7377  data: 1.2338  max mem: 9341
[23:54:27.291329] Epoch: [362]  [ 20/195]  eta: 0:01:39  lr: 0.001485  loss: 0.3164 (0.3217)  time: 0.5110  data: 0.0002  max mem: 9341
[23:54:37.515412] Epoch: [362]  [ 40/195]  eta: 0:01:23  lr: 0.001484  loss: 0.3382 (0.3293)  time: 0.5112  data: 0.0002  max mem: 9341
[23:54:47.744690] Epoch: [362]  [ 60/195]  eta: 0:01:11  lr: 0.001484  loss: 0.3336 (0.3304)  time: 0.5114  data: 0.0002  max mem: 9341
[23:54:58.010818] Epoch: [362]  [ 80/195]  eta: 0:01:00  lr: 0.001483  loss: 0.3299 (0.3302)  time: 0.5133  data: 0.0002  max mem: 9341
[23:55:08.232030] Epoch: [362]  [100/195]  eta: 0:00:49  lr: 0.001483  loss: 0.3230 (0.3294)  time: 0.5110  data: 0.0002  max mem: 9341
[23:55:18.451155] Epoch: [362]  [120/195]  eta: 0:00:39  lr: 0.001482  loss: 0.3335 (0.3306)  time: 0.5109  data: 0.0002  max mem: 9341
[23:55:28.662796] Epoch: [362]  [140/195]  eta: 0:00:28  lr: 0.001482  loss: 0.3230 (0.3297)  time: 0.5105  data: 0.0002  max mem: 9341
[23:55:38.924277] Epoch: [362]  [160/195]  eta: 0:00:18  lr: 0.001481  loss: 0.3373 (0.3306)  time: 0.5130  data: 0.0002  max mem: 9341
[23:55:49.102841] Epoch: [362]  [180/195]  eta: 0:00:07  lr: 0.001481  loss: 0.3284 (0.3303)  time: 0.5089  data: 0.0001  max mem: 9341
[23:55:56.233216] Epoch: [362]  [194/195]  eta: 0:00:00  lr: 0.001480  loss: 0.3254 (0.3301)  time: 0.5105  data: 0.0001  max mem: 9341
[23:55:56.385361] Epoch: [362] Total time: 0:01:41 (0.5182 s / it)
[23:55:56.408108] Averaged stats: lr: 0.001480  loss: 0.3254 (0.3306)
[23:56:01.105883] {"train_lr": 0.001482740618107099, "train_loss": 0.33059586099325083, "epoch": 362}
[23:56:01.106141] [23:56:01.106226] Training epoch 362 for 0:01:45
[23:56:01.106278] [23:56:01.110673] log_dir: ./exp/debug/cifar100-LT/debug
[23:56:02.896499] Epoch: [363]  [  0/195]  eta: 0:05:48  lr: 0.001480  loss: 0.3169 (0.3169)  time: 1.7849  data: 1.2820  max mem: 9341
[23:56:13.118309] Epoch: [363]  [ 20/195]  eta: 0:01:40  lr: 0.001480  loss: 0.3267 (0.3283)  time: 0.5110  data: 0.0002  max mem: 9341
[23:56:23.341442] Epoch: [363]  [ 40/195]  eta: 0:01:24  lr: 0.001479  loss: 0.3198 (0.3247)  time: 0.5111  data: 0.0002  max mem: 9341
[23:56:33.557381] Epoch: [363]  [ 60/195]  eta: 0:01:11  lr: 0.001479  loss: 0.3418 (0.3303)  time: 0.5107  data: 0.0002  max mem: 9341
[23:56:43.816583] Epoch: [363]  [ 80/195]  eta: 0:01:00  lr: 0.001478  loss: 0.3339 (0.3307)  time: 0.5129  data: 0.0002  max mem: 9341
[23:56:54.039492] Epoch: [363]  [100/195]  eta: 0:00:49  lr: 0.001478  loss: 0.3235 (0.3310)  time: 0.5111  data: 0.0002  max mem: 9341
[23:57:04.262076] Epoch: [363]  [120/195]  eta: 0:00:39  lr: 0.001477  loss: 0.3299 (0.3313)  time: 0.5111  data: 0.0002  max mem: 9341
[23:57:14.469510] Epoch: [363]  [140/195]  eta: 0:00:28  lr: 0.001477  loss: 0.3359 (0.3322)  time: 0.5103  data: 0.0002  max mem: 9341
[23:57:24.724196] Epoch: [363]  [160/195]  eta: 0:00:18  lr: 0.001476  loss: 0.3144 (0.3310)  time: 0.5127  data: 0.0002  max mem: 9341
[23:57:34.891537] Epoch: [363]  [180/195]  eta: 0:00:07  lr: 0.001476  loss: 0.3294 (0.3306)  time: 0.5083  data: 0.0001  max mem: 9341
[23:57:42.023707] Epoch: [363]  [194/195]  eta: 0:00:00  lr: 0.001475  loss: 0.3294 (0.3308)  time: 0.5106  data: 0.0001  max mem: 9341
[23:57:42.200995] Epoch: [363] Total time: 0:01:41 (0.5184 s / it)
[23:57:42.209775] Averaged stats: lr: 0.001475  loss: 0.3294 (0.3314)
[23:57:46.987751] {"train_lr": 0.0014779174660203489, "train_loss": 0.3314394499628972, "epoch": 363}
[23:57:46.988462] [23:57:46.988585] Training epoch 363 for 0:01:45
[23:57:46.988660] [23:57:46.995467] log_dir: ./exp/debug/cifar100-LT/debug
[23:57:48.685116] Epoch: [364]  [  0/195]  eta: 0:05:29  lr: 0.001475  loss: 0.3050 (0.3050)  time: 1.6884  data: 1.1817  max mem: 9341
[23:57:58.899686] Epoch: [364]  [ 20/195]  eta: 0:01:39  lr: 0.001475  loss: 0.3257 (0.3250)  time: 0.5107  data: 0.0002  max mem: 9341
[23:58:09.125046] Epoch: [364]  [ 40/195]  eta: 0:01:23  lr: 0.001475  loss: 0.3347 (0.3303)  time: 0.5112  data: 0.0002  max mem: 9341
[23:58:19.348297] Epoch: [364]  [ 60/195]  eta: 0:01:11  lr: 0.001474  loss: 0.3255 (0.3309)  time: 0.5111  data: 0.0002  max mem: 9341
[23:58:29.629149] Epoch: [364]  [ 80/195]  eta: 0:01:00  lr: 0.001473  loss: 0.3285 (0.3299)  time: 0.5140  data: 0.0002  max mem: 9341
[23:58:39.844398] Epoch: [364]  [100/195]  eta: 0:00:49  lr: 0.001473  loss: 0.3174 (0.3281)  time: 0.5107  data: 0.0002  max mem: 9341
[23:58:50.052082] Epoch: [364]  [120/195]  eta: 0:00:39  lr: 0.001473  loss: 0.3294 (0.3282)  time: 0.5103  data: 0.0002  max mem: 9341
[23:59:00.261484] Epoch: [364]  [140/195]  eta: 0:00:28  lr: 0.001472  loss: 0.3426 (0.3298)  time: 0.5104  data: 0.0002  max mem: 9341
[23:59:10.513881] Epoch: [364]  [160/195]  eta: 0:00:18  lr: 0.001471  loss: 0.3298 (0.3296)  time: 0.5126  data: 0.0002  max mem: 9341
[23:59:20.681363] Epoch: [364]  [180/195]  eta: 0:00:07  lr: 0.001471  loss: 0.3387 (0.3308)  time: 0.5083  data: 0.0001  max mem: 9341
[23:59:27.811282] Epoch: [364]  [194/195]  eta: 0:00:00  lr: 0.001471  loss: 0.3377 (0.3310)  time: 0.5105  data: 0.0001  max mem: 9341
[23:59:27.976968] Epoch: [364] Total time: 0:01:40 (0.5179 s / it)
[23:59:27.991333] Averaged stats: lr: 0.001471  loss: 0.3377 (0.3296)
[23:59:32.638383] {"train_lr": 0.0014730895650908915, "train_loss": 0.3295744601732645, "epoch": 364}
[23:59:32.638827] [23:59:32.638929] Training epoch 364 for 0:01:45
[23:59:32.638984] [23:59:32.644312] log_dir: ./exp/debug/cifar100-LT/debug
[23:59:34.317819] Epoch: [365]  [  0/195]  eta: 0:05:26  lr: 0.001470  loss: 0.3665 (0.3665)  time: 1.6721  data: 1.1833  max mem: 9341
[23:59:44.536654] Epoch: [365]  [ 20/195]  eta: 0:01:39  lr: 0.001470  loss: 0.3309 (0.3321)  time: 0.5108  data: 0.0003  max mem: 9341
[23:59:54.761862] Epoch: [365]  [ 40/195]  eta: 0:01:23  lr: 0.001470  loss: 0.3280 (0.3307)  time: 0.5112  data: 0.0002  max mem: 9341
[00:00:04.985994] Epoch: [365]  [ 60/195]  eta: 0:01:11  lr: 0.001469  loss: 0.3280 (0.3313)  time: 0.5111  data: 0.0002  max mem: 9341
[00:00:15.247690] Epoch: [365]  [ 80/195]  eta: 0:01:00  lr: 0.001468  loss: 0.3264 (0.3317)  time: 0.5130  data: 0.0002  max mem: 9341
[00:00:25.455498] Epoch: [365]  [100/195]  eta: 0:00:49  lr: 0.001468  loss: 0.3362 (0.3315)  time: 0.5103  data: 0.0002  max mem: 9341
[00:00:35.667307] Epoch: [365]  [120/195]  eta: 0:00:39  lr: 0.001468  loss: 0.3310 (0.3314)  time: 0.5105  data: 0.0002  max mem: 9341
[00:00:45.892316] Epoch: [365]  [140/195]  eta: 0:00:28  lr: 0.001467  loss: 0.3285 (0.3322)  time: 0.5112  data: 0.0002  max mem: 9341
[00:00:56.156862] Epoch: [365]  [160/195]  eta: 0:00:18  lr: 0.001467  loss: 0.3391 (0.3331)  time: 0.5132  data: 0.0002  max mem: 9341
[00:01:06.331816] Epoch: [365]  [180/195]  eta: 0:00:07  lr: 0.001466  loss: 0.3286 (0.3328)  time: 0.5087  data: 0.0002  max mem: 9341
[00:01:13.462653] Epoch: [365]  [194/195]  eta: 0:00:00  lr: 0.001466  loss: 0.3403 (0.3333)  time: 0.5106  data: 0.0001  max mem: 9341
[00:01:13.646637] Epoch: [365] Total time: 0:01:41 (0.5180 s / it)
[00:01:13.654466] Averaged stats: lr: 0.001466  loss: 0.3403 (0.3311)
[00:01:18.471827] {"train_lr": 0.0014682569978142603, "train_loss": 0.3310624967973966, "epoch": 365}
[00:01:18.472220] [00:01:18.472325] Training epoch 365 for 0:01:45
[00:01:18.472382] [00:01:18.477029] log_dir: ./exp/debug/cifar100-LT/debug
[00:01:20.353129] Epoch: [366]  [  0/195]  eta: 0:06:05  lr: 0.001466  loss: 0.3377 (0.3377)  time: 1.8749  data: 1.3727  max mem: 9341
[00:01:30.561785] Epoch: [366]  [ 20/195]  eta: 0:01:40  lr: 0.001465  loss: 0.3159 (0.3174)  time: 0.5104  data: 0.0002  max mem: 9341
[00:01:40.801656] Epoch: [366]  [ 40/195]  eta: 0:01:24  lr: 0.001465  loss: 0.3285 (0.3207)  time: 0.5119  data: 0.0002  max mem: 9341
[00:01:51.036323] Epoch: [366]  [ 60/195]  eta: 0:01:12  lr: 0.001464  loss: 0.3158 (0.3212)  time: 0.5117  data: 0.0002  max mem: 9341
[00:02:01.294221] Epoch: [366]  [ 80/195]  eta: 0:01:00  lr: 0.001464  loss: 0.3264 (0.3239)  time: 0.5128  data: 0.0002  max mem: 9341
[00:02:11.512064] Epoch: [366]  [100/195]  eta: 0:00:49  lr: 0.001463  loss: 0.3332 (0.3255)  time: 0.5108  data: 0.0002  max mem: 9341
[00:02:21.727933] Epoch: [366]  [120/195]  eta: 0:00:39  lr: 0.001463  loss: 0.3255 (0.3260)  time: 0.5107  data: 0.0002  max mem: 9341
[00:02:31.965381] Epoch: [366]  [140/195]  eta: 0:00:28  lr: 0.001462  loss: 0.3296 (0.3271)  time: 0.5118  data: 0.0002  max mem: 9341
[00:02:42.266382] Epoch: [366]  [160/195]  eta: 0:00:18  lr: 0.001462  loss: 0.3320 (0.3276)  time: 0.5150  data: 0.0002  max mem: 9341
[00:02:52.460906] Epoch: [366]  [180/195]  eta: 0:00:07  lr: 0.001461  loss: 0.3259 (0.3280)  time: 0.5097  data: 0.0001  max mem: 9341
[00:02:59.611462] Epoch: [366]  [194/195]  eta: 0:00:00  lr: 0.001461  loss: 0.3347 (0.3283)  time: 0.5127  data: 0.0001  max mem: 9341
[00:02:59.770471] Epoch: [366] Total time: 0:01:41 (0.5195 s / it)
[00:02:59.784636] Averaged stats: lr: 0.001461  loss: 0.3347 (0.3298)
[00:03:04.517819] {"train_lr": 0.0014634198467656774, "train_loss": 0.32983050247033435, "epoch": 366}
[00:03:04.518171] [00:03:04.518261] Training epoch 366 for 0:01:46
[00:03:04.518314] [00:03:04.522892] log_dir: ./exp/debug/cifar100-LT/debug
[00:03:06.251768] Epoch: [367]  [  0/195]  eta: 0:05:36  lr: 0.001461  loss: 0.3436 (0.3436)  time: 1.7275  data: 1.2276  max mem: 9341
[00:03:16.489735] Epoch: [367]  [ 20/195]  eta: 0:01:39  lr: 0.001460  loss: 0.3303 (0.3290)  time: 0.5118  data: 0.0002  max mem: 9341
[00:03:26.703948] Epoch: [367]  [ 40/195]  eta: 0:01:23  lr: 0.001460  loss: 0.3311 (0.3316)  time: 0.5106  data: 0.0002  max mem: 9341
[00:03:36.917087] Epoch: [367]  [ 60/195]  eta: 0:01:11  lr: 0.001460  loss: 0.3268 (0.3314)  time: 0.5106  data: 0.0002  max mem: 9341
[00:03:47.170189] Epoch: [367]  [ 80/195]  eta: 0:01:00  lr: 0.001459  loss: 0.3337 (0.3321)  time: 0.5126  data: 0.0002  max mem: 9341
[00:03:57.382950] Epoch: [367]  [100/195]  eta: 0:00:49  lr: 0.001458  loss: 0.3209 (0.3309)  time: 0.5106  data: 0.0002  max mem: 9341
[00:04:07.591927] Epoch: [367]  [120/195]  eta: 0:00:39  lr: 0.001458  loss: 0.3309 (0.3306)  time: 0.5104  data: 0.0002  max mem: 9341
[00:04:17.808303] Epoch: [367]  [140/195]  eta: 0:00:28  lr: 0.001458  loss: 0.3358 (0.3305)  time: 0.5108  data: 0.0002  max mem: 9341
[00:04:28.065318] Epoch: [367]  [160/195]  eta: 0:00:18  lr: 0.001457  loss: 0.3369 (0.3315)  time: 0.5128  data: 0.0002  max mem: 9341
[00:04:38.237938] Epoch: [367]  [180/195]  eta: 0:00:07  lr: 0.001456  loss: 0.3331 (0.3317)  time: 0.5086  data: 0.0001  max mem: 9341
[00:04:45.380543] Epoch: [367]  [194/195]  eta: 0:00:00  lr: 0.001456  loss: 0.3414 (0.3319)  time: 0.5112  data: 0.0001  max mem: 9341
[00:04:45.567835] Epoch: [367] Total time: 0:01:41 (0.5182 s / it)
[00:04:45.577185] Averaged stats: lr: 0.001456  loss: 0.3414 (0.3310)
[00:04:50.192605] {"train_lr": 0.0014585781945987122, "train_loss": 0.3310293197631836, "epoch": 367}
[00:04:50.192882] [00:04:50.192970] Training epoch 367 for 0:01:45
[00:04:50.193024] [00:04:50.197422] log_dir: ./exp/debug/cifar100-LT/debug
[00:04:51.981798] Epoch: [368]  [  0/195]  eta: 0:05:47  lr: 0.001456  loss: 0.2967 (0.2967)  time: 1.7832  data: 1.2687  max mem: 9341
[00:05:02.191334] Epoch: [368]  [ 20/195]  eta: 0:01:39  lr: 0.001456  loss: 0.3279 (0.3274)  time: 0.5104  data: 0.0002  max mem: 9341
[00:05:12.411201] Epoch: [368]  [ 40/195]  eta: 0:01:23  lr: 0.001455  loss: 0.3284 (0.3298)  time: 0.5109  data: 0.0002  max mem: 9341
[00:05:22.629612] Epoch: [368]  [ 60/195]  eta: 0:01:11  lr: 0.001455  loss: 0.3305 (0.3291)  time: 0.5108  data: 0.0002  max mem: 9341
[00:05:32.889237] Epoch: [368]  [ 80/195]  eta: 0:01:00  lr: 0.001454  loss: 0.3307 (0.3295)  time: 0.5129  data: 0.0002  max mem: 9341
[00:05:43.104058] Epoch: [368]  [100/195]  eta: 0:00:49  lr: 0.001454  loss: 0.3238 (0.3285)  time: 0.5107  data: 0.0002  max mem: 9341
[00:05:53.325437] Epoch: [368]  [120/195]  eta: 0:00:39  lr: 0.001453  loss: 0.3249 (0.3284)  time: 0.5110  data: 0.0002  max mem: 9341
[00:06:03.542196] Epoch: [368]  [140/195]  eta: 0:00:28  lr: 0.001453  loss: 0.3349 (0.3290)  time: 0.5108  data: 0.0002  max mem: 9341
[00:06:13.798464] Epoch: [368]  [160/195]  eta: 0:00:18  lr: 0.001452  loss: 0.3216 (0.3286)  time: 0.5128  data: 0.0002  max mem: 9341
[00:06:23.992044] Epoch: [368]  [180/195]  eta: 0:00:07  lr: 0.001452  loss: 0.3314 (0.3288)  time: 0.5096  data: 0.0001  max mem: 9341
[00:06:31.149578] Epoch: [368]  [194/195]  eta: 0:00:00  lr: 0.001451  loss: 0.3225 (0.3285)  time: 0.5124  data: 0.0001  max mem: 9341
[00:06:31.322059] Epoch: [368] Total time: 0:01:41 (0.5186 s / it)
[00:06:31.339723] Averaged stats: lr: 0.001451  loss: 0.3225 (0.3279)
[00:06:37.934276] {"train_lr": 0.0014537321240438432, "train_loss": 0.32789575717388053, "epoch": 368}
[00:06:37.934602] [00:06:37.934685] Training epoch 368 for 0:01:47
[00:06:37.934737] [00:06:37.939178] log_dir: ./exp/debug/cifar100-LT/debug
[00:06:39.660552] Epoch: [369]  [  0/195]  eta: 0:05:35  lr: 0.001451  loss: 0.3479 (0.3479)  time: 1.7201  data: 1.2079  max mem: 9341
[00:06:50.084581] Epoch: [369]  [ 20/195]  eta: 0:01:41  lr: 0.001451  loss: 0.3354 (0.3326)  time: 0.5211  data: 0.0002  max mem: 9341
[00:07:00.297579] Epoch: [369]  [ 40/195]  eta: 0:01:24  lr: 0.001450  loss: 0.3184 (0.3271)  time: 0.5106  data: 0.0002  max mem: 9341
[00:07:10.510533] Epoch: [369]  [ 60/195]  eta: 0:01:12  lr: 0.001450  loss: 0.3318 (0.3283)  time: 0.5106  data: 0.0002  max mem: 9341
[00:07:20.776864] Epoch: [369]  [ 80/195]  eta: 0:01:00  lr: 0.001449  loss: 0.3336 (0.3297)  time: 0.5133  data: 0.0002  max mem: 9341
[00:07:30.993418] Epoch: [369]  [100/195]  eta: 0:00:49  lr: 0.001449  loss: 0.3218 (0.3294)  time: 0.5108  data: 0.0002  max mem: 9341
[00:07:41.209174] Epoch: [369]  [120/195]  eta: 0:00:39  lr: 0.001448  loss: 0.3298 (0.3287)  time: 0.5107  data: 0.0002  max mem: 9341
[00:07:51.419353] Epoch: [369]  [140/195]  eta: 0:00:28  lr: 0.001448  loss: 0.3357 (0.3297)  time: 0.5105  data: 0.0002  max mem: 9341
[00:08:01.676511] Epoch: [369]  [160/195]  eta: 0:00:18  lr: 0.001447  loss: 0.3337 (0.3304)  time: 0.5128  data: 0.0002  max mem: 9341
[00:08:11.842294] Epoch: [369]  [180/195]  eta: 0:00:07  lr: 0.001447  loss: 0.3282 (0.3299)  time: 0.5082  data: 0.0001  max mem: 9341
[00:08:18.968844] Epoch: [369]  [194/195]  eta: 0:00:00  lr: 0.001446  loss: 0.3283 (0.3302)  time: 0.5102  data: 0.0001  max mem: 9341
[00:08:19.138713] Epoch: [369] Total time: 0:01:41 (0.5190 s / it)
[00:08:19.155411] Averaged stats: lr: 0.001446  loss: 0.3283 (0.3283)
[00:08:23.908372] {"train_lr": 0.001448881717907042, "train_loss": 0.32827875365813575, "epoch": 369}
[00:08:23.908641] [00:08:23.908728] Training epoch 369 for 0:01:45
[00:08:23.908782] [00:08:23.913204] log_dir: ./exp/debug/cifar100-LT/debug
[00:08:25.567216] Epoch: [370]  [  0/195]  eta: 0:05:22  lr: 0.001446  loss: 0.3613 (0.3613)  time: 1.6530  data: 1.1404  max mem: 9341
[00:08:35.785780] Epoch: [370]  [ 20/195]  eta: 0:01:38  lr: 0.001446  loss: 0.3336 (0.3348)  time: 0.5109  data: 0.0002  max mem: 9341
[00:08:46.001296] Epoch: [370]  [ 40/195]  eta: 0:01:23  lr: 0.001445  loss: 0.3253 (0.3330)  time: 0.5107  data: 0.0003  max mem: 9341
[00:08:56.215379] Epoch: [370]  [ 60/195]  eta: 0:01:11  lr: 0.001445  loss: 0.3352 (0.3336)  time: 0.5106  data: 0.0003  max mem: 9341
[00:09:06.473938] Epoch: [370]  [ 80/195]  eta: 0:01:00  lr: 0.001444  loss: 0.3190 (0.3307)  time: 0.5129  data: 0.0003  max mem: 9341
[00:09:16.685986] Epoch: [370]  [100/195]  eta: 0:00:49  lr: 0.001444  loss: 0.3327 (0.3309)  time: 0.5105  data: 0.0003  max mem: 9341
[00:09:26.900462] Epoch: [370]  [120/195]  eta: 0:00:39  lr: 0.001443  loss: 0.3269 (0.3311)  time: 0.5107  data: 0.0003  max mem: 9341
[00:09:37.112464] Epoch: [370]  [140/195]  eta: 0:00:28  lr: 0.001443  loss: 0.3233 (0.3304)  time: 0.5105  data: 0.0002  max mem: 9341
[00:09:47.373785] Epoch: [370]  [160/195]  eta: 0:00:18  lr: 0.001442  loss: 0.3281 (0.3297)  time: 0.5130  data: 0.0003  max mem: 9341
[00:09:57.549417] Epoch: [370]  [180/195]  eta: 0:00:07  lr: 0.001442  loss: 0.3300 (0.3292)  time: 0.5087  data: 0.0002  max mem: 9341
[00:10:04.682848] Epoch: [370]  [194/195]  eta: 0:00:00  lr: 0.001441  loss: 0.3326 (0.3295)  time: 0.5108  data: 0.0001  max mem: 9341
[00:10:04.863239] Epoch: [370] Total time: 0:01:40 (0.5177 s / it)
[00:10:04.864871] Averaged stats: lr: 0.001441  loss: 0.3326 (0.3277)
[00:10:09.568016] {"train_lr": 0.0014440270590683727, "train_loss": 0.3277398388355206, "epoch": 370}
[00:10:09.568363] [00:10:09.568482] Training epoch 370 for 0:01:45
[00:10:09.568540] [00:10:09.572992] log_dir: ./exp/debug/cifar100-LT/debug
[00:10:11.191632] Epoch: [371]  [  0/195]  eta: 0:05:15  lr: 0.001441  loss: 0.3111 (0.3111)  time: 1.6172  data: 1.1093  max mem: 9341
[00:10:21.425490] Epoch: [371]  [ 20/195]  eta: 0:01:38  lr: 0.001441  loss: 0.3260 (0.3285)  time: 0.5116  data: 0.0002  max mem: 9341
[00:10:31.659044] Epoch: [371]  [ 40/195]  eta: 0:01:23  lr: 0.001441  loss: 0.3392 (0.3310)  time: 0.5116  data: 0.0002  max mem: 9341
[00:10:41.891256] Epoch: [371]  [ 60/195]  eta: 0:01:11  lr: 0.001440  loss: 0.3231 (0.3279)  time: 0.5116  data: 0.0002  max mem: 9341
[00:10:52.152845] Epoch: [371]  [ 80/195]  eta: 0:01:00  lr: 0.001439  loss: 0.3329 (0.3286)  time: 0.5130  data: 0.0002  max mem: 9341
[00:11:02.359501] Epoch: [371]  [100/195]  eta: 0:00:49  lr: 0.001439  loss: 0.3328 (0.3294)  time: 0.5103  data: 0.0003  max mem: 9341
[00:11:12.567279] Epoch: [371]  [120/195]  eta: 0:00:39  lr: 0.001439  loss: 0.3287 (0.3278)  time: 0.5103  data: 0.0003  max mem: 9341
[00:11:22.775503] Epoch: [371]  [140/195]  eta: 0:00:28  lr: 0.001438  loss: 0.3365 (0.3284)  time: 0.5104  data: 0.0002  max mem: 9341
[00:11:33.031314] Epoch: [371]  [160/195]  eta: 0:00:18  lr: 0.001437  loss: 0.3321 (0.3289)  time: 0.5127  data: 0.0002  max mem: 9341
[00:11:43.202010] Epoch: [371]  [180/195]  eta: 0:00:07  lr: 0.001437  loss: 0.3176 (0.3287)  time: 0.5085  data: 0.0002  max mem: 9341
[00:11:50.331308] Epoch: [371]  [194/195]  eta: 0:00:00  lr: 0.001437  loss: 0.3265 (0.3286)  time: 0.5106  data: 0.0001  max mem: 9341
[00:11:50.506268] Epoch: [371] Total time: 0:01:40 (0.5176 s / it)
[00:11:50.514783] Averaged stats: lr: 0.001437  loss: 0.3265 (0.3280)
[00:11:55.201810] {"train_lr": 0.0014391682304805598, "train_loss": 0.32803360112966634, "epoch": 371}
[00:11:55.202137] [00:11:55.202238] Training epoch 371 for 0:01:45
[00:11:55.202294] [00:11:55.206800] log_dir: ./exp/debug/cifar100-LT/debug
[00:11:56.855377] Epoch: [372]  [  0/195]  eta: 0:05:21  lr: 0.001437  loss: 0.3283 (0.3283)  time: 1.6462  data: 1.1382  max mem: 9341
[00:12:07.095601] Epoch: [372]  [ 20/195]  eta: 0:01:39  lr: 0.001436  loss: 0.3353 (0.3283)  time: 0.5120  data: 0.0003  max mem: 9341
[00:12:17.312682] Epoch: [372]  [ 40/195]  eta: 0:01:23  lr: 0.001436  loss: 0.3261 (0.3266)  time: 0.5108  data: 0.0002  max mem: 9341
[00:12:27.524219] Epoch: [372]  [ 60/195]  eta: 0:01:11  lr: 0.001435  loss: 0.3265 (0.3270)  time: 0.5105  data: 0.0003  max mem: 9341
[00:12:37.797373] Epoch: [372]  [ 80/195]  eta: 0:01:00  lr: 0.001435  loss: 0.3223 (0.3254)  time: 0.5136  data: 0.0003  max mem: 9341
[00:12:48.013174] Epoch: [372]  [100/195]  eta: 0:00:49  lr: 0.001434  loss: 0.3317 (0.3263)  time: 0.5107  data: 0.0003  max mem: 9341
[00:12:58.226364] Epoch: [372]  [120/195]  eta: 0:00:39  lr: 0.001434  loss: 0.3264 (0.3266)  time: 0.5106  data: 0.0003  max mem: 9341
[00:13:08.441245] Epoch: [372]  [140/195]  eta: 0:00:28  lr: 0.001433  loss: 0.3346 (0.3285)  time: 0.5107  data: 0.0002  max mem: 9341
[00:13:18.702068] Epoch: [372]  [160/195]  eta: 0:00:18  lr: 0.001433  loss: 0.3325 (0.3290)  time: 0.5130  data: 0.0002  max mem: 9341
[00:13:28.870014] Epoch: [372]  [180/195]  eta: 0:00:07  lr: 0.001432  loss: 0.3234 (0.3278)  time: 0.5083  data: 0.0002  max mem: 9341
[00:13:35.996304] Epoch: [372]  [194/195]  eta: 0:00:00  lr: 0.001432  loss: 0.3293 (0.3285)  time: 0.5102  data: 0.0001  max mem: 9341
[00:13:36.184899] Epoch: [372] Total time: 0:01:40 (0.5178 s / it)
[00:13:36.185677] Averaged stats: lr: 0.001432  loss: 0.3293 (0.3284)
[00:13:40.899475] {"train_lr": 0.0014343053151675718, "train_loss": 0.328409209388953, "epoch": 372}
[00:13:40.899752] [00:13:40.899857] Training epoch 372 for 0:01:45
[00:13:40.899912] [00:13:40.904453] log_dir: ./exp/debug/cifar100-LT/debug
[00:13:42.575307] Epoch: [373]  [  0/195]  eta: 0:05:25  lr: 0.001432  loss: 0.3010 (0.3010)  time: 1.6694  data: 1.1657  max mem: 9341
[00:13:52.790536] Epoch: [373]  [ 20/195]  eta: 0:01:39  lr: 0.001431  loss: 0.3351 (0.3317)  time: 0.5107  data: 0.0002  max mem: 9341
[00:14:03.005847] Epoch: [373]  [ 40/195]  eta: 0:01:23  lr: 0.001431  loss: 0.3316 (0.3299)  time: 0.5107  data: 0.0002  max mem: 9341
[00:14:13.226036] Epoch: [373]  [ 60/195]  eta: 0:01:11  lr: 0.001430  loss: 0.3342 (0.3308)  time: 0.5109  data: 0.0003  max mem: 9341
[00:14:23.480945] Epoch: [373]  [ 80/195]  eta: 0:01:00  lr: 0.001430  loss: 0.3211 (0.3289)  time: 0.5127  data: 0.0003  max mem: 9341
[00:14:33.693539] Epoch: [373]  [100/195]  eta: 0:00:49  lr: 0.001429  loss: 0.3265 (0.3280)  time: 0.5106  data: 0.0002  max mem: 9341
[00:14:43.909175] Epoch: [373]  [120/195]  eta: 0:00:39  lr: 0.001429  loss: 0.3222 (0.3280)  time: 0.5107  data: 0.0003  max mem: 9341
[00:14:54.119860] Epoch: [373]  [140/195]  eta: 0:00:28  lr: 0.001428  loss: 0.3258 (0.3284)  time: 0.5105  data: 0.0002  max mem: 9341
[00:15:04.379608] Epoch: [373]  [160/195]  eta: 0:00:18  lr: 0.001428  loss: 0.3284 (0.3294)  time: 0.5129  data: 0.0002  max mem: 9341
[00:15:14.557073] Epoch: [373]  [180/195]  eta: 0:00:07  lr: 0.001427  loss: 0.3276 (0.3293)  time: 0.5088  data: 0.0002  max mem: 9341
[00:15:21.687237] Epoch: [373]  [194/195]  eta: 0:00:00  lr: 0.001427  loss: 0.3309 (0.3290)  time: 0.5106  data: 0.0001  max mem: 9341
[00:15:21.870602] Epoch: [373] Total time: 0:01:40 (0.5178 s / it)
[00:15:21.884425] Averaged stats: lr: 0.001427  loss: 0.3309 (0.3292)
[00:15:26.585955] {"train_lr": 0.0014294383962232208, "train_loss": 0.3292150910466145, "epoch": 373}
[00:15:26.586221] [00:15:26.586326] Training epoch 373 for 0:01:45
[00:15:26.586380] [00:15:26.590823] log_dir: ./exp/debug/cifar100-LT/debug
[00:15:28.235929] Epoch: [374]  [  0/195]  eta: 0:05:20  lr: 0.001427  loss: 0.3116 (0.3116)  time: 1.6442  data: 1.1444  max mem: 9341
[00:15:38.453334] Epoch: [374]  [ 20/195]  eta: 0:01:38  lr: 0.001426  loss: 0.3201 (0.3228)  time: 0.5108  data: 0.0002  max mem: 9341
[00:15:48.672394] Epoch: [374]  [ 40/195]  eta: 0:01:23  lr: 0.001426  loss: 0.3239 (0.3222)  time: 0.5109  data: 0.0002  max mem: 9341
[00:15:58.888986] Epoch: [374]  [ 60/195]  eta: 0:01:11  lr: 0.001426  loss: 0.3321 (0.3274)  time: 0.5108  data: 0.0002  max mem: 9341
[00:16:09.144990] Epoch: [374]  [ 80/195]  eta: 0:01:00  lr: 0.001425  loss: 0.3214 (0.3269)  time: 0.5127  data: 0.0002  max mem: 9341
[00:16:19.359644] Epoch: [374]  [100/195]  eta: 0:00:49  lr: 0.001424  loss: 0.3268 (0.3276)  time: 0.5107  data: 0.0002  max mem: 9341
[00:16:29.577470] Epoch: [374]  [120/195]  eta: 0:00:39  lr: 0.001424  loss: 0.3261 (0.3275)  time: 0.5108  data: 0.0002  max mem: 9341
[00:16:39.787590] Epoch: [374]  [140/195]  eta: 0:00:28  lr: 0.001424  loss: 0.3299 (0.3282)  time: 0.5105  data: 0.0002  max mem: 9341
[00:16:50.048222] Epoch: [374]  [160/195]  eta: 0:00:18  lr: 0.001423  loss: 0.3180 (0.3272)  time: 0.5130  data: 0.0002  max mem: 9341
[00:17:00.220684] Epoch: [374]  [180/195]  eta: 0:00:07  lr: 0.001422  loss: 0.3277 (0.3272)  time: 0.5086  data: 0.0001  max mem: 9341
[00:17:07.352591] Epoch: [374]  [194/195]  eta: 0:00:00  lr: 0.001422  loss: 0.3251 (0.3275)  time: 0.5106  data: 0.0001  max mem: 9341
[00:17:07.520700] Epoch: [374] Total time: 0:01:40 (0.5176 s / it)
[00:17:07.539459] Averaged stats: lr: 0.001422  loss: 0.3251 (0.3267)
[00:17:12.207390] {"train_lr": 0.0014245675568097201, "train_loss": 0.32668336522884855, "epoch": 374}
[00:17:12.207652] [00:17:12.207736] Training epoch 374 for 0:01:45
[00:17:12.207790] [00:17:12.212241] log_dir: ./exp/debug/cifar100-LT/debug
[00:17:14.148152] Epoch: [375]  [  0/195]  eta: 0:06:17  lr: 0.001422  loss: 0.3317 (0.3317)  time: 1.9348  data: 1.4357  max mem: 9341
[00:17:24.368850] Epoch: [375]  [ 20/195]  eta: 0:01:41  lr: 0.001422  loss: 0.3231 (0.3218)  time: 0.5110  data: 0.0002  max mem: 9341
[00:17:34.588400] Epoch: [375]  [ 40/195]  eta: 0:01:24  lr: 0.001421  loss: 0.3248 (0.3252)  time: 0.5109  data: 0.0002  max mem: 9341
[00:17:44.807104] Epoch: [375]  [ 60/195]  eta: 0:01:12  lr: 0.001421  loss: 0.3131 (0.3215)  time: 0.5108  data: 0.0003  max mem: 9341
[00:17:55.065717] Epoch: [375]  [ 80/195]  eta: 0:01:00  lr: 0.001420  loss: 0.3170 (0.3219)  time: 0.5129  data: 0.0003  max mem: 9341
[00:18:05.279668] Epoch: [375]  [100/195]  eta: 0:00:49  lr: 0.001420  loss: 0.3122 (0.3214)  time: 0.5106  data: 0.0002  max mem: 9341
[00:18:15.497644] Epoch: [375]  [120/195]  eta: 0:00:39  lr: 0.001419  loss: 0.3140 (0.3203)  time: 0.5108  data: 0.0002  max mem: 9341
[00:18:25.711933] Epoch: [375]  [140/195]  eta: 0:00:28  lr: 0.001419  loss: 0.3330 (0.3218)  time: 0.5107  data: 0.0003  max mem: 9341
[00:18:35.965401] Epoch: [375]  [160/195]  eta: 0:00:18  lr: 0.001418  loss: 0.3251 (0.3225)  time: 0.5126  data: 0.0003  max mem: 9341
[00:18:46.139077] Epoch: [375]  [180/195]  eta: 0:00:07  lr: 0.001418  loss: 0.3199 (0.3230)  time: 0.5086  data: 0.0002  max mem: 9341
[00:18:53.262863] Epoch: [375]  [194/195]  eta: 0:00:00  lr: 0.001417  loss: 0.3255 (0.3234)  time: 0.5101  data: 0.0001  max mem: 9341
[00:18:53.442798] Epoch: [375] Total time: 0:01:41 (0.5191 s / it)
[00:18:53.464322] Averaged stats: lr: 0.001417  loss: 0.3255 (0.3248)
[00:18:58.305010] {"train_lr": 0.0014196928801562788, "train_loss": 0.32477370298061614, "epoch": 375}
[00:18:58.305304] [00:18:58.305404] Training epoch 375 for 0:01:46
[00:18:58.305459] [00:18:58.309962] log_dir: ./exp/debug/cifar100-LT/debug
[00:18:59.906341] Epoch: [376]  [  0/195]  eta: 0:05:10  lr: 0.001417  loss: 0.3406 (0.3406)  time: 1.5940  data: 1.0833  max mem: 9341
[00:19:10.119916] Epoch: [376]  [ 20/195]  eta: 0:01:38  lr: 0.001417  loss: 0.3155 (0.3204)  time: 0.5106  data: 0.0004  max mem: 9341
[00:19:20.334782] Epoch: [376]  [ 40/195]  eta: 0:01:23  lr: 0.001416  loss: 0.3214 (0.3234)  time: 0.5107  data: 0.0003  max mem: 9341
[00:19:30.553569] Epoch: [376]  [ 60/195]  eta: 0:01:11  lr: 0.001416  loss: 0.3201 (0.3221)  time: 0.5109  data: 0.0002  max mem: 9341
[00:19:40.814523] Epoch: [376]  [ 80/195]  eta: 0:01:00  lr: 0.001415  loss: 0.3377 (0.3252)  time: 0.5130  data: 0.0003  max mem: 9341
[00:19:51.029532] Epoch: [376]  [100/195]  eta: 0:00:49  lr: 0.001415  loss: 0.3302 (0.3262)  time: 0.5107  data: 0.0002  max mem: 9341
[00:20:01.244537] Epoch: [376]  [120/195]  eta: 0:00:39  lr: 0.001414  loss: 0.3249 (0.3259)  time: 0.5107  data: 0.0003  max mem: 9341
[00:20:11.458346] Epoch: [376]  [140/195]  eta: 0:00:28  lr: 0.001414  loss: 0.3237 (0.3260)  time: 0.5106  data: 0.0002  max mem: 9341
[00:20:21.722138] Epoch: [376]  [160/195]  eta: 0:00:18  lr: 0.001413  loss: 0.3323 (0.3271)  time: 0.5131  data: 0.0002  max mem: 9341
[00:20:31.912520] Epoch: [376]  [180/195]  eta: 0:00:07  lr: 0.001413  loss: 0.3279 (0.3275)  time: 0.5095  data: 0.0002  max mem: 9341
[00:20:39.047552] Epoch: [376]  [194/195]  eta: 0:00:00  lr: 0.001412  loss: 0.3301 (0.3271)  time: 0.5108  data: 0.0001  max mem: 9341
[00:20:39.225671] Epoch: [376] Total time: 0:01:40 (0.5175 s / it)
[00:20:39.229123] Averaged stats: lr: 0.001412  loss: 0.3301 (0.3274)
[00:20:43.877024] {"train_lr": 0.0014148144495576655, "train_loss": 0.3273963173994651, "epoch": 376}
[00:20:43.877405] [00:20:43.877521] Training epoch 376 for 0:01:45
[00:20:43.877577] [00:20:43.882099] log_dir: ./exp/debug/cifar100-LT/debug
[00:20:45.513405] Epoch: [377]  [  0/195]  eta: 0:05:17  lr: 0.001412  loss: 0.3447 (0.3447)  time: 1.6300  data: 1.1204  max mem: 9341
[00:20:55.726139] Epoch: [377]  [ 20/195]  eta: 0:01:38  lr: 0.001412  loss: 0.3346 (0.3355)  time: 0.5106  data: 0.0002  max mem: 9341
[00:21:05.941663] Epoch: [377]  [ 40/195]  eta: 0:01:23  lr: 0.001411  loss: 0.3251 (0.3272)  time: 0.5107  data: 0.0002  max mem: 9341
[00:21:16.167958] Epoch: [377]  [ 60/195]  eta: 0:01:11  lr: 0.001411  loss: 0.3253 (0.3274)  time: 0.5113  data: 0.0002  max mem: 9341
[00:21:26.431555] Epoch: [377]  [ 80/195]  eta: 0:01:00  lr: 0.001410  loss: 0.3266 (0.3279)  time: 0.5131  data: 0.0003  max mem: 9341
[00:21:36.643718] Epoch: [377]  [100/195]  eta: 0:00:49  lr: 0.001410  loss: 0.3227 (0.3276)  time: 0.5106  data: 0.0003  max mem: 9341
[00:21:46.858931] Epoch: [377]  [120/195]  eta: 0:00:39  lr: 0.001409  loss: 0.3243 (0.3277)  time: 0.5107  data: 0.0003  max mem: 9341
[00:21:57.073982] Epoch: [377]  [140/195]  eta: 0:00:28  lr: 0.001409  loss: 0.3290 (0.3282)  time: 0.5107  data: 0.0002  max mem: 9341
[00:22:07.339636] Epoch: [377]  [160/195]  eta: 0:00:18  lr: 0.001408  loss: 0.3184 (0.3275)  time: 0.5132  data: 0.0002  max mem: 9341
[00:22:17.514302] Epoch: [377]  [180/195]  eta: 0:00:07  lr: 0.001408  loss: 0.3367 (0.3283)  time: 0.5087  data: 0.0002  max mem: 9341
[00:22:24.665871] Epoch: [377]  [194/195]  eta: 0:00:00  lr: 0.001407  loss: 0.3131 (0.3274)  time: 0.5115  data: 0.0001  max mem: 9341
[00:22:24.846415] Epoch: [377] Total time: 0:01:40 (0.5178 s / it)
[00:22:24.852496] Averaged stats: lr: 0.001407  loss: 0.3131 (0.3263)
[00:22:29.559274] {"train_lr": 0.0014099323483728033, "train_loss": 0.3262908437695259, "epoch": 377}
[00:22:29.559534] [00:22:29.559639] Training epoch 377 for 0:01:45
[00:22:29.559692] [00:22:29.564081] log_dir: ./exp/debug/cifar100-LT/debug
[00:22:31.296550] Epoch: [378]  [  0/195]  eta: 0:05:37  lr: 0.001407  loss: 0.3311 (0.3311)  time: 1.7313  data: 1.2226  max mem: 9341
[00:22:41.514748] Epoch: [378]  [ 20/195]  eta: 0:01:39  lr: 0.001407  loss: 0.3299 (0.3265)  time: 0.5108  data: 0.0003  max mem: 9341
[00:22:51.741680] Epoch: [378]  [ 40/195]  eta: 0:01:23  lr: 0.001406  loss: 0.3218 (0.3254)  time: 0.5113  data: 0.0002  max mem: 9341
[00:23:01.958296] Epoch: [378]  [ 60/195]  eta: 0:01:11  lr: 0.001406  loss: 0.3287 (0.3277)  time: 0.5108  data: 0.0003  max mem: 9341
[00:23:12.222769] Epoch: [378]  [ 80/195]  eta: 0:01:00  lr: 0.001405  loss: 0.3275 (0.3267)  time: 0.5132  data: 0.0003  max mem: 9341
[00:23:22.438031] Epoch: [378]  [100/195]  eta: 0:00:49  lr: 0.001405  loss: 0.3297 (0.3267)  time: 0.5107  data: 0.0003  max mem: 9341
[00:23:32.648399] Epoch: [378]  [120/195]  eta: 0:00:39  lr: 0.001404  loss: 0.3212 (0.3253)  time: 0.5105  data: 0.0003  max mem: 9341
[00:23:42.862447] Epoch: [378]  [140/195]  eta: 0:00:28  lr: 0.001404  loss: 0.3195 (0.3245)  time: 0.5106  data: 0.0002  max mem: 9341
[00:23:53.119021] Epoch: [378]  [160/195]  eta: 0:00:18  lr: 0.001403  loss: 0.3206 (0.3239)  time: 0.5128  data: 0.0002  max mem: 9341
[00:24:03.289035] Epoch: [378]  [180/195]  eta: 0:00:07  lr: 0.001403  loss: 0.3110 (0.3227)  time: 0.5085  data: 0.0001  max mem: 9341
[00:24:10.418670] Epoch: [378]  [194/195]  eta: 0:00:00  lr: 0.001402  loss: 0.3253 (0.3227)  time: 0.5104  data: 0.0001  max mem: 9341
[00:24:10.599149] Epoch: [378] Total time: 0:01:41 (0.5181 s / it)
[00:24:10.610206] Averaged stats: lr: 0.001402  loss: 0.3253 (0.3244)
[00:24:15.297203] {"train_lr": 0.0014050466600233308, "train_loss": 0.32439630111822715, "epoch": 378}
[00:24:15.297473] [00:24:15.297579] Training epoch 378 for 0:01:45
[00:24:15.297633] [00:24:15.302095] log_dir: ./exp/debug/cifar100-LT/debug
[00:24:17.013716] Epoch: [379]  [  0/195]  eta: 0:05:33  lr: 0.001402  loss: 0.3110 (0.3110)  time: 1.7106  data: 1.1987  max mem: 9341
[00:24:27.229047] Epoch: [379]  [ 20/195]  eta: 0:01:39  lr: 0.001402  loss: 0.3334 (0.3328)  time: 0.5107  data: 0.0002  max mem: 9341
[00:24:37.451512] Epoch: [379]  [ 40/195]  eta: 0:01:23  lr: 0.001402  loss: 0.3205 (0.3292)  time: 0.5111  data: 0.0002  max mem: 9341
[00:24:47.663134] Epoch: [379]  [ 60/195]  eta: 0:01:11  lr: 0.001401  loss: 0.3263 (0.3287)  time: 0.5105  data: 0.0002  max mem: 9341
[00:24:57.920956] Epoch: [379]  [ 80/195]  eta: 0:01:00  lr: 0.001400  loss: 0.3351 (0.3295)  time: 0.5128  data: 0.0002  max mem: 9341
[00:25:08.136716] Epoch: [379]  [100/195]  eta: 0:00:49  lr: 0.001400  loss: 0.3311 (0.3287)  time: 0.5107  data: 0.0002  max mem: 9341
[00:25:18.348466] Epoch: [379]  [120/195]  eta: 0:00:39  lr: 0.001400  loss: 0.3274 (0.3287)  time: 0.5105  data: 0.0002  max mem: 9341
[00:25:28.560247] Epoch: [379]  [140/195]  eta: 0:00:28  lr: 0.001399  loss: 0.3180 (0.3270)  time: 0.5105  data: 0.0002  max mem: 9341
[00:25:38.824377] Epoch: [379]  [160/195]  eta: 0:00:18  lr: 0.001398  loss: 0.3148 (0.3257)  time: 0.5131  data: 0.0002  max mem: 9341
[00:25:48.998354] Epoch: [379]  [180/195]  eta: 0:00:07  lr: 0.001398  loss: 0.3185 (0.3251)  time: 0.5086  data: 0.0001  max mem: 9341
[00:25:56.127296] Epoch: [379]  [194/195]  eta: 0:00:00  lr: 0.001398  loss: 0.3277 (0.3256)  time: 0.5105  data: 0.0001  max mem: 9341
[00:25:56.282172] Epoch: [379] Total time: 0:01:40 (0.5178 s / it)
[00:25:56.329333] Averaged stats: lr: 0.001398  loss: 0.3277 (0.3242)
[00:26:01.038351] {"train_lr": 0.001400157467992179, "train_loss": 0.32423540510428256, "epoch": 379}
[00:26:01.038625] [00:26:01.038725] Training epoch 379 for 0:01:45
[00:26:01.038778] [00:26:01.043326] log_dir: ./exp/debug/cifar100-LT/debug
[00:26:02.825631] Epoch: [380]  [  0/195]  eta: 0:05:47  lr: 0.001398  loss: 0.3388 (0.3388)  time: 1.7815  data: 1.2643  max mem: 9341
[00:26:13.035336] Epoch: [380]  [ 20/195]  eta: 0:01:39  lr: 0.001397  loss: 0.3246 (0.3224)  time: 0.5104  data: 0.0002  max mem: 9341
[00:26:23.251994] Epoch: [380]  [ 40/195]  eta: 0:01:23  lr: 0.001397  loss: 0.3275 (0.3238)  time: 0.5108  data: 0.0002  max mem: 9341
[00:26:33.469939] Epoch: [380]  [ 60/195]  eta: 0:01:11  lr: 0.001396  loss: 0.3243 (0.3247)  time: 0.5108  data: 0.0002  max mem: 9341
[00:26:43.738226] Epoch: [380]  [ 80/195]  eta: 0:01:00  lr: 0.001396  loss: 0.3228 (0.3242)  time: 0.5134  data: 0.0002  max mem: 9341
[00:26:53.949776] Epoch: [380]  [100/195]  eta: 0:00:49  lr: 0.001395  loss: 0.3431 (0.3277)  time: 0.5105  data: 0.0002  max mem: 9341
[00:27:04.170255] Epoch: [380]  [120/195]  eta: 0:00:39  lr: 0.001395  loss: 0.3199 (0.3271)  time: 0.5110  data: 0.0002  max mem: 9341
[00:27:14.389422] Epoch: [380]  [140/195]  eta: 0:00:28  lr: 0.001394  loss: 0.3317 (0.3271)  time: 0.5109  data: 0.0002  max mem: 9341
[00:27:24.642730] Epoch: [380]  [160/195]  eta: 0:00:18  lr: 0.001393  loss: 0.3382 (0.3286)  time: 0.5126  data: 0.0002  max mem: 9341
[00:27:34.805962] Epoch: [380]  [180/195]  eta: 0:00:07  lr: 0.001393  loss: 0.3331 (0.3293)  time: 0.5081  data: 0.0001  max mem: 9341
[00:27:41.931305] Epoch: [380]  [194/195]  eta: 0:00:00  lr: 0.001393  loss: 0.3278 (0.3293)  time: 0.5102  data: 0.0001  max mem: 9341
[00:27:42.114615] Epoch: [380] Total time: 0:01:41 (0.5183 s / it)
[00:27:42.125396] Averaged stats: lr: 0.001393  loss: 0.3278 (0.3291)
[00:27:46.806736] {"train_lr": 0.0013952648558221547, "train_loss": 0.32906698611302254, "epoch": 380}
[00:27:46.807087] [00:27:46.807179] Training epoch 380 for 0:01:45
[00:27:46.807234] [00:27:46.812268] log_dir: ./exp/debug/cifar100-LT/debug
[00:27:48.527565] Epoch: [381]  [  0/195]  eta: 0:05:34  lr: 0.001393  loss: 0.3422 (0.3422)  time: 1.7139  data: 1.2056  max mem: 9341
[00:27:58.740000] Epoch: [381]  [ 20/195]  eta: 0:01:39  lr: 0.001392  loss: 0.3174 (0.3217)  time: 0.5106  data: 0.0002  max mem: 9341
[00:28:08.972490] Epoch: [381]  [ 40/195]  eta: 0:01:23  lr: 0.001392  loss: 0.3244 (0.3229)  time: 0.5116  data: 0.0002  max mem: 9341
[00:28:19.214435] Epoch: [381]  [ 60/195]  eta: 0:01:11  lr: 0.001391  loss: 0.3406 (0.3293)  time: 0.5120  data: 0.0002  max mem: 9341
[00:28:29.525329] Epoch: [381]  [ 80/195]  eta: 0:01:00  lr: 0.001391  loss: 0.3216 (0.3293)  time: 0.5155  data: 0.0002  max mem: 9341
[00:28:39.762024] Epoch: [381]  [100/195]  eta: 0:00:49  lr: 0.001390  loss: 0.3443 (0.3322)  time: 0.5118  data: 0.0002  max mem: 9341
[00:28:49.994961] Epoch: [381]  [120/195]  eta: 0:00:39  lr: 0.001390  loss: 0.3350 (0.3338)  time: 0.5116  data: 0.0002  max mem: 9341
[00:29:00.231430] Epoch: [381]  [140/195]  eta: 0:00:28  lr: 0.001389  loss: 0.3262 (0.3327)  time: 0.5118  data: 0.0002  max mem: 9341
[00:29:10.533081] Epoch: [381]  [160/195]  eta: 0:00:18  lr: 0.001389  loss: 0.3294 (0.3323)  time: 0.5150  data: 0.0002  max mem: 9341
[00:29:20.732317] Epoch: [381]  [180/195]  eta: 0:00:07  lr: 0.001388  loss: 0.3315 (0.3328)  time: 0.5099  data: 0.0001  max mem: 9341
[00:29:27.889391] Epoch: [381]  [194/195]  eta: 0:00:00  lr: 0.001388  loss: 0.3365 (0.3330)  time: 0.5130  data: 0.0001  max mem: 9341
[00:29:28.059607] Epoch: [381] Total time: 0:01:41 (0.5192 s / it)
[00:29:28.071175] Averaged stats: lr: 0.001388  loss: 0.3365 (0.3341)
[00:29:32.758453] {"train_lr": 0.001390368907114497, "train_loss": 0.3340802251910552, "epoch": 381}
[00:29:32.758810] [00:29:32.758903] Training epoch 381 for 0:01:45
[00:29:32.758957] [00:29:32.763409] log_dir: ./exp/debug/cifar100-LT/debug
[00:29:34.728740] Epoch: [382]  [  0/195]  eta: 0:06:23  lr: 0.001388  loss: 0.3261 (0.3261)  time: 1.9642  data: 1.4623  max mem: 9341
[00:29:44.940668] Epoch: [382]  [ 20/195]  eta: 0:01:41  lr: 0.001387  loss: 0.3224 (0.3292)  time: 0.5105  data: 0.0002  max mem: 9341
[00:29:55.159611] Epoch: [382]  [ 40/195]  eta: 0:01:24  lr: 0.001387  loss: 0.3256 (0.3265)  time: 0.5109  data: 0.0002  max mem: 9341
[00:30:05.376445] Epoch: [382]  [ 60/195]  eta: 0:01:12  lr: 0.001387  loss: 0.3207 (0.3251)  time: 0.5108  data: 0.0002  max mem: 9341
[00:30:15.634873] Epoch: [382]  [ 80/195]  eta: 0:01:00  lr: 0.001386  loss: 0.3183 (0.3234)  time: 0.5129  data: 0.0002  max mem: 9341
[00:30:25.846934] Epoch: [382]  [100/195]  eta: 0:00:49  lr: 0.001385  loss: 0.3335 (0.3254)  time: 0.5105  data: 0.0002  max mem: 9341
[00:30:36.055667] Epoch: [382]  [120/195]  eta: 0:00:39  lr: 0.001385  loss: 0.3161 (0.3238)  time: 0.5104  data: 0.0002  max mem: 9341
[00:30:46.262684] Epoch: [382]  [140/195]  eta: 0:00:28  lr: 0.001385  loss: 0.3223 (0.3239)  time: 0.5103  data: 0.0002  max mem: 9341
[00:30:56.521459] Epoch: [382]  [160/195]  eta: 0:00:18  lr: 0.001384  loss: 0.3256 (0.3242)  time: 0.5129  data: 0.0002  max mem: 9341
[00:31:06.686324] Epoch: [382]  [180/195]  eta: 0:00:07  lr: 0.001383  loss: 0.3175 (0.3242)  time: 0.5082  data: 0.0001  max mem: 9341
[00:31:13.813833] Epoch: [382]  [194/195]  eta: 0:00:00  lr: 0.001383  loss: 0.3251 (0.3246)  time: 0.5103  data: 0.0001  max mem: 9341
[00:31:13.989890] Epoch: [382] Total time: 0:01:41 (0.5191 s / it)
[00:31:13.997633] Averaged stats: lr: 0.001383  loss: 0.3251 (0.3274)
[00:31:18.646027] {"train_lr": 0.001385469705527462, "train_loss": 0.32737903845233796, "epoch": 382}
[00:31:18.646295] [00:31:18.646381] Training epoch 382 for 0:01:45
[00:31:18.646434] [00:31:18.650964] log_dir: ./exp/debug/cifar100-LT/debug
[00:31:20.446888] Epoch: [383]  [  0/195]  eta: 0:05:49  lr: 0.001383  loss: 0.3736 (0.3736)  time: 1.7940  data: 1.2844  max mem: 9341
[00:31:30.676386] Epoch: [383]  [ 20/195]  eta: 0:01:40  lr: 0.001382  loss: 0.3325 (0.3316)  time: 0.5114  data: 0.0003  max mem: 9341
[00:31:40.890541] Epoch: [383]  [ 40/195]  eta: 0:01:24  lr: 0.001382  loss: 0.3097 (0.3241)  time: 0.5106  data: 0.0002  max mem: 9341
[00:31:51.101635] Epoch: [383]  [ 60/195]  eta: 0:01:11  lr: 0.001382  loss: 0.3227 (0.3237)  time: 0.5105  data: 0.0002  max mem: 9341
[00:32:01.361249] Epoch: [383]  [ 80/195]  eta: 0:01:00  lr: 0.001381  loss: 0.3239 (0.3247)  time: 0.5129  data: 0.0002  max mem: 9341
[00:32:11.571895] Epoch: [383]  [100/195]  eta: 0:00:49  lr: 0.001380  loss: 0.3219 (0.3242)  time: 0.5105  data: 0.0002  max mem: 9341
[00:32:21.782354] Epoch: [383]  [120/195]  eta: 0:00:39  lr: 0.001380  loss: 0.3301 (0.3249)  time: 0.5105  data: 0.0002  max mem: 9341
[00:32:31.990613] Epoch: [383]  [140/195]  eta: 0:00:28  lr: 0.001380  loss: 0.3315 (0.3259)  time: 0.5103  data: 0.0002  max mem: 9341
[00:32:42.245454] Epoch: [383]  [160/195]  eta: 0:00:18  lr: 0.001379  loss: 0.3165 (0.3248)  time: 0.5127  data: 0.0002  max mem: 9341
[00:32:52.408251] Epoch: [383]  [180/195]  eta: 0:00:07  lr: 0.001378  loss: 0.3189 (0.3240)  time: 0.5081  data: 0.0001  max mem: 9341
[00:32:59.529679] Epoch: [383]  [194/195]  eta: 0:00:00  lr: 0.001378  loss: 0.3177 (0.3234)  time: 0.5099  data: 0.0001  max mem: 9341
[00:32:59.737880] Epoch: [383] Total time: 0:01:41 (0.5184 s / it)
[00:32:59.742909] Averaged stats: lr: 0.001378  loss: 0.3177 (0.3238)
[00:33:04.558877] {"train_lr": 0.0013805673347748926, "train_loss": 0.32377603971041163, "epoch": 383}
[00:33:04.559167] [00:33:04.559271] Training epoch 383 for 0:01:45
[00:33:04.559326] [00:33:04.563820] log_dir: ./exp/debug/cifar100-LT/debug
[00:33:06.216617] Epoch: [384]  [  0/195]  eta: 0:05:22  lr: 0.001378  loss: 0.3112 (0.3112)  time: 1.6517  data: 1.1415  max mem: 9341
[00:33:16.426850] Epoch: [384]  [ 20/195]  eta: 0:01:38  lr: 0.001378  loss: 0.3192 (0.3215)  time: 0.5105  data: 0.0002  max mem: 9341
[00:33:26.638761] Epoch: [384]  [ 40/195]  eta: 0:01:23  lr: 0.001377  loss: 0.3175 (0.3191)  time: 0.5105  data: 0.0002  max mem: 9341
[00:33:36.867594] Epoch: [384]  [ 60/195]  eta: 0:01:11  lr: 0.001377  loss: 0.3332 (0.3205)  time: 0.5114  data: 0.0002  max mem: 9341
[00:33:47.126528] Epoch: [384]  [ 80/195]  eta: 0:01:00  lr: 0.001376  loss: 0.3194 (0.3203)  time: 0.5129  data: 0.0003  max mem: 9341
[00:33:57.332566] Epoch: [384]  [100/195]  eta: 0:00:49  lr: 0.001376  loss: 0.3300 (0.3226)  time: 0.5102  data: 0.0002  max mem: 9341
[00:34:07.545269] Epoch: [384]  [120/195]  eta: 0:00:39  lr: 0.001375  loss: 0.3149 (0.3214)  time: 0.5106  data: 0.0002  max mem: 9341
[00:34:17.753870] Epoch: [384]  [140/195]  eta: 0:00:28  lr: 0.001375  loss: 0.3145 (0.3207)  time: 0.5104  data: 0.0002  max mem: 9341
[00:34:28.006164] Epoch: [384]  [160/195]  eta: 0:00:18  lr: 0.001374  loss: 0.3227 (0.3207)  time: 0.5126  data: 0.0002  max mem: 9341
[00:34:38.166300] Epoch: [384]  [180/195]  eta: 0:00:07  lr: 0.001373  loss: 0.3149 (0.3210)  time: 0.5080  data: 0.0001  max mem: 9341
[00:34:45.293123] Epoch: [384]  [194/195]  eta: 0:00:00  lr: 0.001373  loss: 0.3128 (0.3210)  time: 0.5102  data: 0.0001  max mem: 9341
[00:34:45.473274] Epoch: [384] Total time: 0:01:40 (0.5175 s / it)
[00:34:45.480704] Averaged stats: lr: 0.001373  loss: 0.3128 (0.3219)
[00:34:50.173273] {"train_lr": 0.0013756618786247695, "train_loss": 0.32191543615399265, "epoch": 384}
[00:34:50.173544] [00:34:50.173652] Training epoch 384 for 0:01:45
[00:34:50.173707] [00:34:50.178166] log_dir: ./exp/debug/cifar100-LT/debug
[00:34:52.009513] Epoch: [385]  [  0/195]  eta: 0:05:56  lr: 0.001373  loss: 0.3537 (0.3537)  time: 1.8303  data: 1.3166  max mem: 9341
[00:35:02.225606] Epoch: [385]  [ 20/195]  eta: 0:01:40  lr: 0.001373  loss: 0.3254 (0.3277)  time: 0.5107  data: 0.0002  max mem: 9341
[00:35:12.437652] Epoch: [385]  [ 40/195]  eta: 0:01:24  lr: 0.001372  loss: 0.3201 (0.3228)  time: 0.5105  data: 0.0002  max mem: 9341
[00:35:22.646926] Epoch: [385]  [ 60/195]  eta: 0:01:11  lr: 0.001372  loss: 0.3228 (0.3245)  time: 0.5104  data: 0.0004  max mem: 9341
[00:35:32.901014] Epoch: [385]  [ 80/195]  eta: 0:01:00  lr: 0.001371  loss: 0.3125 (0.3219)  time: 0.5126  data: 0.0003  max mem: 9341
[00:35:43.110143] Epoch: [385]  [100/195]  eta: 0:00:49  lr: 0.001371  loss: 0.3241 (0.3232)  time: 0.5104  data: 0.0003  max mem: 9341
[00:35:53.322261] Epoch: [385]  [120/195]  eta: 0:00:39  lr: 0.001370  loss: 0.3155 (0.3228)  time: 0.5106  data: 0.0003  max mem: 9341
[00:36:03.531820] Epoch: [385]  [140/195]  eta: 0:00:28  lr: 0.001370  loss: 0.3259 (0.3236)  time: 0.5104  data: 0.0003  max mem: 9341
[00:36:13.820907] Epoch: [385]  [160/195]  eta: 0:00:18  lr: 0.001369  loss: 0.3114 (0.3226)  time: 0.5144  data: 0.0002  max mem: 9341
[00:36:24.005136] Epoch: [385]  [180/195]  eta: 0:00:07  lr: 0.001369  loss: 0.3210 (0.3228)  time: 0.5092  data: 0.0002  max mem: 9341
[00:36:31.149098] Epoch: [385]  [194/195]  eta: 0:00:00  lr: 0.001368  loss: 0.3116 (0.3222)  time: 0.5121  data: 0.0001  max mem: 9341
[00:36:31.328900] Epoch: [385] Total time: 0:01:41 (0.5187 s / it)
[00:36:31.336386] Averaged stats: lr: 0.001368  loss: 0.3116 (0.3224)
[00:36:36.203020] {"train_lr": 0.0013707534208978087, "train_loss": 0.3224430718100988, "epoch": 385}
[00:36:36.203396] [00:36:36.203502] Training epoch 385 for 0:01:46
[00:36:36.203557] [00:36:36.208060] log_dir: ./exp/debug/cifar100-LT/debug
[00:36:38.161444] Epoch: [386]  [  0/195]  eta: 0:06:20  lr: 0.001368  loss: 0.2953 (0.2953)  time: 1.9517  data: 1.4446  max mem: 9341
[00:36:48.374244] Epoch: [386]  [ 20/195]  eta: 0:01:41  lr: 0.001368  loss: 0.3194 (0.3214)  time: 0.5106  data: 0.0003  max mem: 9341
[00:36:58.593772] Epoch: [386]  [ 40/195]  eta: 0:01:24  lr: 0.001367  loss: 0.3173 (0.3213)  time: 0.5109  data: 0.0002  max mem: 9341
[00:37:08.814767] Epoch: [386]  [ 60/195]  eta: 0:01:12  lr: 0.001367  loss: 0.3314 (0.3232)  time: 0.5110  data: 0.0003  max mem: 9341
[00:37:19.073490] Epoch: [386]  [ 80/195]  eta: 0:01:00  lr: 0.001366  loss: 0.3198 (0.3232)  time: 0.5129  data: 0.0003  max mem: 9341
[00:37:29.291746] Epoch: [386]  [100/195]  eta: 0:00:49  lr: 0.001366  loss: 0.3210 (0.3241)  time: 0.5108  data: 0.0003  max mem: 9341
[00:37:39.504039] Epoch: [386]  [120/195]  eta: 0:00:39  lr: 0.001365  loss: 0.3194 (0.3236)  time: 0.5106  data: 0.0003  max mem: 9341
[00:37:49.718752] Epoch: [386]  [140/195]  eta: 0:00:28  lr: 0.001365  loss: 0.3227 (0.3237)  time: 0.5107  data: 0.0003  max mem: 9341
[00:37:59.976910] Epoch: [386]  [160/195]  eta: 0:00:18  lr: 0.001364  loss: 0.3248 (0.3241)  time: 0.5129  data: 0.0003  max mem: 9341
[00:38:10.139265] Epoch: [386]  [180/195]  eta: 0:00:07  lr: 0.001364  loss: 0.3207 (0.3236)  time: 0.5081  data: 0.0001  max mem: 9341
[00:38:17.270852] Epoch: [386]  [194/195]  eta: 0:00:00  lr: 0.001363  loss: 0.3279 (0.3239)  time: 0.5106  data: 0.0001  max mem: 9341
[00:38:17.440388] Epoch: [386] Total time: 0:01:41 (0.5191 s / it)
[00:38:17.446721] Averaged stats: lr: 0.001363  loss: 0.3279 (0.3247)
[00:38:22.227288] {"train_lr": 0.0013658420454660098, "train_loss": 0.32471372668559734, "epoch": 386}
[00:38:22.227557] [00:38:22.227668] Training epoch 386 for 0:01:46
[00:38:22.227724] [00:38:22.232197] log_dir: ./exp/debug/cifar100-LT/debug
[00:38:24.041175] Epoch: [387]  [  0/195]  eta: 0:05:52  lr: 0.001363  loss: 0.3211 (0.3211)  time: 1.8082  data: 1.3148  max mem: 9341
[00:38:34.260631] Epoch: [387]  [ 20/195]  eta: 0:01:40  lr: 0.001363  loss: 0.3223 (0.3198)  time: 0.5109  data: 0.0003  max mem: 9341
[00:38:44.473432] Epoch: [387]  [ 40/195]  eta: 0:01:24  lr: 0.001362  loss: 0.3164 (0.3209)  time: 0.5105  data: 0.0003  max mem: 9341
[00:38:54.693891] Epoch: [387]  [ 60/195]  eta: 0:01:11  lr: 0.001362  loss: 0.3162 (0.3214)  time: 0.5110  data: 0.0003  max mem: 9341
[00:39:04.955169] Epoch: [387]  [ 80/195]  eta: 0:01:00  lr: 0.001361  loss: 0.3239 (0.3226)  time: 0.5130  data: 0.0003  max mem: 9341
[00:39:15.177437] Epoch: [387]  [100/195]  eta: 0:00:49  lr: 0.001361  loss: 0.3153 (0.3223)  time: 0.5110  data: 0.0003  max mem: 9341
[00:39:25.392266] Epoch: [387]  [120/195]  eta: 0:00:39  lr: 0.001360  loss: 0.3173 (0.3223)  time: 0.5107  data: 0.0003  max mem: 9341
[00:39:35.609284] Epoch: [387]  [140/195]  eta: 0:00:28  lr: 0.001360  loss: 0.3192 (0.3227)  time: 0.5108  data: 0.0003  max mem: 9341
[00:39:45.893021] Epoch: [387]  [160/195]  eta: 0:00:18  lr: 0.001359  loss: 0.3186 (0.3227)  time: 0.5141  data: 0.0003  max mem: 9341
[00:39:56.074512] Epoch: [387]  [180/195]  eta: 0:00:07  lr: 0.001359  loss: 0.3236 (0.3231)  time: 0.5090  data: 0.0002  max mem: 9341
[00:40:03.205635] Epoch: [387]  [194/195]  eta: 0:00:00  lr: 0.001358  loss: 0.3330 (0.3236)  time: 0.5108  data: 0.0001  max mem: 9341
[00:40:03.374215] Epoch: [387] Total time: 0:01:41 (0.5187 s / it)
[00:40:03.401230] Averaged stats: lr: 0.001358  loss: 0.3330 (0.3234)
[00:40:08.232301] {"train_lr": 0.00136092783625123, "train_loss": 0.3233676886329284, "epoch": 387}
[00:40:08.232531] [00:40:08.232628] Training epoch 387 for 0:01:46
[00:40:08.232681] [00:40:08.237088] log_dir: ./exp/debug/cifar100-LT/debug
[00:40:10.091725] Epoch: [388]  [  0/195]  eta: 0:06:01  lr: 0.001358  loss: 0.3355 (0.3355)  time: 1.8539  data: 1.3617  max mem: 9341
[00:40:20.305692] Epoch: [388]  [ 20/195]  eta: 0:01:40  lr: 0.001358  loss: 0.3261 (0.3264)  time: 0.5106  data: 0.0002  max mem: 9341
[00:40:30.523255] Epoch: [388]  [ 40/195]  eta: 0:01:24  lr: 0.001357  loss: 0.3202 (0.3244)  time: 0.5108  data: 0.0003  max mem: 9341
[00:40:40.735427] Epoch: [388]  [ 60/195]  eta: 0:01:11  lr: 0.001357  loss: 0.3245 (0.3243)  time: 0.5105  data: 0.0003  max mem: 9341
[00:40:50.993843] Epoch: [388]  [ 80/195]  eta: 0:01:00  lr: 0.001356  loss: 0.3151 (0.3221)  time: 0.5129  data: 0.0003  max mem: 9341
[00:41:01.208343] Epoch: [388]  [100/195]  eta: 0:00:49  lr: 0.001356  loss: 0.3233 (0.3233)  time: 0.5107  data: 0.0002  max mem: 9341
[00:41:11.423262] Epoch: [388]  [120/195]  eta: 0:00:39  lr: 0.001355  loss: 0.3188 (0.3228)  time: 0.5107  data: 0.0003  max mem: 9341
[00:41:21.637159] Epoch: [388]  [140/195]  eta: 0:00:28  lr: 0.001355  loss: 0.3164 (0.3221)  time: 0.5106  data: 0.0003  max mem: 9341
[00:41:31.899445] Epoch: [388]  [160/195]  eta: 0:00:18  lr: 0.001354  loss: 0.3304 (0.3228)  time: 0.5131  data: 0.0003  max mem: 9341
[00:41:42.073487] Epoch: [388]  [180/195]  eta: 0:00:07  lr: 0.001354  loss: 0.3109 (0.3220)  time: 0.5087  data: 0.0002  max mem: 9341
[00:41:49.206770] Epoch: [388]  [194/195]  eta: 0:00:00  lr: 0.001353  loss: 0.3244 (0.3229)  time: 0.5108  data: 0.0001  max mem: 9341
[00:41:49.387408] Epoch: [388] Total time: 0:01:41 (0.5187 s / it)
[00:41:49.394314] Averaged stats: lr: 0.001353  loss: 0.3244 (0.3238)
[00:41:54.147211] {"train_lr": 0.0013560108772237413, "train_loss": 0.3237877437510552, "epoch": 388}
[00:41:54.147478] [00:41:54.147584] Training epoch 388 for 0:01:45
[00:41:54.147638] [00:41:54.152111] log_dir: ./exp/debug/cifar100-LT/debug
[00:41:55.950607] Epoch: [389]  [  0/195]  eta: 0:05:50  lr: 0.001353  loss: 0.3347 (0.3347)  time: 1.7975  data: 1.3019  max mem: 9341
[00:42:06.187784] Epoch: [389]  [ 20/195]  eta: 0:01:40  lr: 0.001353  loss: 0.3149 (0.3173)  time: 0.5118  data: 0.0003  max mem: 9341
[00:42:16.427270] Epoch: [389]  [ 40/195]  eta: 0:01:24  lr: 0.001353  loss: 0.3199 (0.3186)  time: 0.5119  data: 0.0002  max mem: 9341
[00:42:26.641694] Epoch: [389]  [ 60/195]  eta: 0:01:11  lr: 0.001352  loss: 0.3184 (0.3170)  time: 0.5107  data: 0.0002  max mem: 9341
[00:42:36.899005] Epoch: [389]  [ 80/195]  eta: 0:01:00  lr: 0.001351  loss: 0.3200 (0.3171)  time: 0.5128  data: 0.0003  max mem: 9341
[00:42:47.105513] Epoch: [389]  [100/195]  eta: 0:00:49  lr: 0.001351  loss: 0.3220 (0.3177)  time: 0.5103  data: 0.0002  max mem: 9341
[00:42:57.314033] Epoch: [389]  [120/195]  eta: 0:00:39  lr: 0.001351  loss: 0.3275 (0.3192)  time: 0.5104  data: 0.0002  max mem: 9341
[00:43:07.525392] Epoch: [389]  [140/195]  eta: 0:00:28  lr: 0.001350  loss: 0.3195 (0.3203)  time: 0.5105  data: 0.0002  max mem: 9341
[00:43:17.786803] Epoch: [389]  [160/195]  eta: 0:00:18  lr: 0.001349  loss: 0.3152 (0.3200)  time: 0.5130  data: 0.0002  max mem: 9341
[00:43:27.963971] Epoch: [389]  [180/195]  eta: 0:00:07  lr: 0.001349  loss: 0.3171 (0.3196)  time: 0.5088  data: 0.0001  max mem: 9341
[00:43:35.097146] Epoch: [389]  [194/195]  eta: 0:00:00  lr: 0.001349  loss: 0.3174 (0.3192)  time: 0.5108  data: 0.0001  max mem: 9341
[00:43:35.273533] Epoch: [389] Total time: 0:01:41 (0.5186 s / it)
[00:43:35.286460] Averaged stats: lr: 0.001349  loss: 0.3174 (0.3192)
[00:43:39.982178] {"train_lr": 0.0013510912524008116, "train_loss": 0.31916116449313287, "epoch": 389}
[00:43:39.982504] [00:43:39.982592] Training epoch 389 for 0:01:45
[00:43:39.982647] [00:43:39.987616] log_dir: ./exp/debug/cifar100-LT/debug
[00:43:41.820756] Epoch: [390]  [  0/195]  eta: 0:05:57  lr: 0.001348  loss: 0.3013 (0.3013)  time: 1.8314  data: 1.3267  max mem: 9341
[00:43:52.043997] Epoch: [390]  [ 20/195]  eta: 0:01:40  lr: 0.001348  loss: 0.3274 (0.3257)  time: 0.5111  data: 0.0003  max mem: 9341
[00:44:02.276173] Epoch: [390]  [ 40/195]  eta: 0:01:24  lr: 0.001348  loss: 0.3243 (0.3235)  time: 0.5115  data: 0.0002  max mem: 9341
[00:44:12.497185] Epoch: [390]  [ 60/195]  eta: 0:01:11  lr: 0.001347  loss: 0.3156 (0.3216)  time: 0.5109  data: 0.0002  max mem: 9341
[00:44:22.751596] Epoch: [390]  [ 80/195]  eta: 0:01:00  lr: 0.001346  loss: 0.3174 (0.3210)  time: 0.5127  data: 0.0002  max mem: 9341
[00:44:32.968116] Epoch: [390]  [100/195]  eta: 0:00:49  lr: 0.001346  loss: 0.3173 (0.3211)  time: 0.5108  data: 0.0002  max mem: 9341
[00:44:43.185569] Epoch: [390]  [120/195]  eta: 0:00:39  lr: 0.001346  loss: 0.3103 (0.3201)  time: 0.5108  data: 0.0002  max mem: 9341
[00:44:53.403519] Epoch: [390]  [140/195]  eta: 0:00:28  lr: 0.001345  loss: 0.3210 (0.3206)  time: 0.5108  data: 0.0002  max mem: 9341
[00:45:03.661017] Epoch: [390]  [160/195]  eta: 0:00:18  lr: 0.001344  loss: 0.3216 (0.3210)  time: 0.5128  data: 0.0002  max mem: 9341
[00:45:13.829040] Epoch: [390]  [180/195]  eta: 0:00:07  lr: 0.001344  loss: 0.3132 (0.3206)  time: 0.5084  data: 0.0002  max mem: 9341
[00:45:20.955135] Epoch: [390]  [194/195]  eta: 0:00:00  lr: 0.001344  loss: 0.3077 (0.3198)  time: 0.5104  data: 0.0001  max mem: 9341
[00:45:21.136006] Epoch: [390] Total time: 0:01:41 (0.5187 s / it)
[00:45:21.149114] Averaged stats: lr: 0.001344  loss: 0.3077 (0.3218)
[00:45:25.898354] {"train_lr": 0.0013461690458452485, "train_loss": 0.3217792180868296, "epoch": 390}
[00:45:25.898701] [00:45:25.898802] Training epoch 390 for 0:01:45
[00:45:25.898856] [00:45:25.903477] log_dir: ./exp/debug/cifar100-LT/debug
[00:45:27.732832] Epoch: [391]  [  0/195]  eta: 0:05:56  lr: 0.001344  loss: 0.3230 (0.3230)  time: 1.8284  data: 1.3178  max mem: 9341
[00:45:37.960208] Epoch: [391]  [ 20/195]  eta: 0:01:40  lr: 0.001343  loss: 0.3132 (0.3149)  time: 0.5113  data: 0.0002  max mem: 9341
[00:45:48.178315] Epoch: [391]  [ 40/195]  eta: 0:01:24  lr: 0.001343  loss: 0.3251 (0.3169)  time: 0.5108  data: 0.0002  max mem: 9341
[00:45:58.390021] Epoch: [391]  [ 60/195]  eta: 0:01:11  lr: 0.001342  loss: 0.3137 (0.3179)  time: 0.5105  data: 0.0002  max mem: 9341
[00:46:08.648230] Epoch: [391]  [ 80/195]  eta: 0:01:00  lr: 0.001341  loss: 0.3237 (0.3193)  time: 0.5128  data: 0.0003  max mem: 9341
[00:46:18.850927] Epoch: [391]  [100/195]  eta: 0:00:49  lr: 0.001341  loss: 0.3206 (0.3204)  time: 0.5101  data: 0.0002  max mem: 9341
[00:46:29.058862] Epoch: [391]  [120/195]  eta: 0:00:39  lr: 0.001341  loss: 0.3254 (0.3208)  time: 0.5103  data: 0.0003  max mem: 9341
[00:46:39.271678] Epoch: [391]  [140/195]  eta: 0:00:28  lr: 0.001340  loss: 0.3382 (0.3229)  time: 0.5106  data: 0.0003  max mem: 9341
[00:46:49.531629] Epoch: [391]  [160/195]  eta: 0:00:18  lr: 0.001339  loss: 0.3261 (0.3229)  time: 0.5129  data: 0.0002  max mem: 9341
[00:46:59.703951] Epoch: [391]  [180/195]  eta: 0:00:07  lr: 0.001339  loss: 0.3157 (0.3234)  time: 0.5086  data: 0.0002  max mem: 9341
[00:47:06.832704] Epoch: [391]  [194/195]  eta: 0:00:00  lr: 0.001339  loss: 0.3260 (0.3240)  time: 0.5104  data: 0.0001  max mem: 9341
[00:47:06.992784] Epoch: [391] Total time: 0:01:41 (0.5184 s / it)
[00:47:07.009194] Averaged stats: lr: 0.001339  loss: 0.3260 (0.3262)
[00:47:11.815676] {"train_lr": 0.0013412443416639915, "train_loss": 0.32615396544719355, "epoch": 391}
[00:47:11.815951] [00:47:11.816056] Training epoch 391 for 0:01:45
[00:47:11.816124] [00:47:11.820609] log_dir: ./exp/debug/cifar100-LT/debug
[00:47:13.698392] Epoch: [392]  [  0/195]  eta: 0:06:05  lr: 0.001339  loss: 0.3184 (0.3184)  time: 1.8768  data: 1.3645  max mem: 9341
[00:47:23.921127] Epoch: [392]  [ 20/195]  eta: 0:01:40  lr: 0.001338  loss: 0.3163 (0.3199)  time: 0.5111  data: 0.0002  max mem: 9341
[00:47:34.131785] Epoch: [392]  [ 40/195]  eta: 0:01:24  lr: 0.001338  loss: 0.3179 (0.3206)  time: 0.5105  data: 0.0003  max mem: 9341
[00:47:44.364112] Epoch: [392]  [ 60/195]  eta: 0:01:12  lr: 0.001337  loss: 0.3173 (0.3219)  time: 0.5116  data: 0.0002  max mem: 9341
[00:47:54.664191] Epoch: [392]  [ 80/195]  eta: 0:01:00  lr: 0.001337  loss: 0.3192 (0.3219)  time: 0.5149  data: 0.0003  max mem: 9341
[00:48:04.901099] Epoch: [392]  [100/195]  eta: 0:00:49  lr: 0.001336  loss: 0.3292 (0.3237)  time: 0.5118  data: 0.0003  max mem: 9341
[00:48:15.116389] Epoch: [392]  [120/195]  eta: 0:00:39  lr: 0.001336  loss: 0.3193 (0.3231)  time: 0.5107  data: 0.0003  max mem: 9341
[00:48:25.330014] Epoch: [392]  [140/195]  eta: 0:00:28  lr: 0.001335  loss: 0.3192 (0.3233)  time: 0.5106  data: 0.0002  max mem: 9341
[00:48:35.590198] Epoch: [392]  [160/195]  eta: 0:00:18  lr: 0.001335  loss: 0.3284 (0.3235)  time: 0.5130  data: 0.0002  max mem: 9341
[00:48:45.764247] Epoch: [392]  [180/195]  eta: 0:00:07  lr: 0.001334  loss: 0.3291 (0.3245)  time: 0.5086  data: 0.0002  max mem: 9341
[00:48:52.896421] Epoch: [392]  [194/195]  eta: 0:00:00  lr: 0.001334  loss: 0.3178 (0.3240)  time: 0.5106  data: 0.0001  max mem: 9341
[00:48:53.071065] Epoch: [392] Total time: 0:01:41 (0.5192 s / it)
[00:48:53.088102] Averaged stats: lr: 0.001334  loss: 0.3178 (0.3254)
[00:48:57.836456] {"train_lr": 0.0013363172240066397, "train_loss": 0.32535426461925876, "epoch": 392}
[00:48:57.836776] [00:48:57.836885] Training epoch 392 for 0:01:46
[00:48:57.836940] [00:48:57.842054] log_dir: ./exp/debug/cifar100-LT/debug
[00:48:59.738498] Epoch: [393]  [  0/195]  eta: 0:06:09  lr: 0.001334  loss: 0.2936 (0.2936)  time: 1.8955  data: 1.4019  max mem: 9341
[00:49:09.957506] Epoch: [393]  [ 20/195]  eta: 0:01:40  lr: 0.001333  loss: 0.3232 (0.3177)  time: 0.5109  data: 0.0003  max mem: 9341
[00:49:20.172323] Epoch: [393]  [ 40/195]  eta: 0:01:24  lr: 0.001333  loss: 0.3321 (0.3223)  time: 0.5107  data: 0.0002  max mem: 9341
[00:49:30.384122] Epoch: [393]  [ 60/195]  eta: 0:01:12  lr: 0.001332  loss: 0.3171 (0.3203)  time: 0.5105  data: 0.0003  max mem: 9341
[00:49:40.645808] Epoch: [393]  [ 80/195]  eta: 0:01:00  lr: 0.001332  loss: 0.3196 (0.3201)  time: 0.5130  data: 0.0003  max mem: 9341
[00:49:50.859419] Epoch: [393]  [100/195]  eta: 0:00:49  lr: 0.001331  loss: 0.3125 (0.3184)  time: 0.5106  data: 0.0003  max mem: 9341
[00:50:01.079677] Epoch: [393]  [120/195]  eta: 0:00:39  lr: 0.001331  loss: 0.3165 (0.3193)  time: 0.5109  data: 0.0003  max mem: 9341
[00:50:11.293670] Epoch: [393]  [140/195]  eta: 0:00:28  lr: 0.001330  loss: 0.3156 (0.3195)  time: 0.5106  data: 0.0003  max mem: 9341
[00:50:21.560662] Epoch: [393]  [160/195]  eta: 0:00:18  lr: 0.001330  loss: 0.3201 (0.3194)  time: 0.5133  data: 0.0003  max mem: 9341
[00:50:31.722684] Epoch: [393]  [180/195]  eta: 0:00:07  lr: 0.001329  loss: 0.3173 (0.3191)  time: 0.5081  data: 0.0002  max mem: 9341
[00:50:38.851750] Epoch: [393]  [194/195]  eta: 0:00:00  lr: 0.001329  loss: 0.3266 (0.3193)  time: 0.5103  data: 0.0001  max mem: 9341
[00:50:39.046621] Epoch: [393] Total time: 0:01:41 (0.5190 s / it)
[00:50:39.053032] Averaged stats: lr: 0.001329  loss: 0.3266 (0.3201)
[00:50:43.700448] {"train_lr": 0.0013313877770640406, "train_loss": 0.3201419541086906, "epoch": 393}
[00:50:43.700773] [00:50:43.700879] Training epoch 393 for 0:01:45
[00:50:43.700933] [00:50:43.705452] log_dir: ./exp/debug/cifar100-LT/debug
[00:50:45.379015] Epoch: [394]  [  0/195]  eta: 0:05:26  lr: 0.001329  loss: 0.2951 (0.2951)  time: 1.6720  data: 1.1829  max mem: 9341
[00:50:55.584636] Epoch: [394]  [ 20/195]  eta: 0:01:38  lr: 0.001328  loss: 0.3110 (0.3128)  time: 0.5102  data: 0.0003  max mem: 9341
[00:51:05.795562] Epoch: [394]  [ 40/195]  eta: 0:01:23  lr: 0.001328  loss: 0.3093 (0.3136)  time: 0.5105  data: 0.0003  max mem: 9341
[00:51:16.005525] Epoch: [394]  [ 60/195]  eta: 0:01:11  lr: 0.001328  loss: 0.3209 (0.3147)  time: 0.5104  data: 0.0002  max mem: 9341
[00:51:26.261771] Epoch: [394]  [ 80/195]  eta: 0:01:00  lr: 0.001327  loss: 0.3181 (0.3152)  time: 0.5128  data: 0.0002  max mem: 9341
[00:51:36.482940] Epoch: [394]  [100/195]  eta: 0:00:49  lr: 0.001326  loss: 0.3181 (0.3159)  time: 0.5110  data: 0.0002  max mem: 9341
[00:51:46.703727] Epoch: [394]  [120/195]  eta: 0:00:39  lr: 0.001326  loss: 0.3175 (0.3167)  time: 0.5110  data: 0.0002  max mem: 9341
[00:51:56.923297] Epoch: [394]  [140/195]  eta: 0:00:28  lr: 0.001325  loss: 0.3193 (0.3170)  time: 0.5109  data: 0.0002  max mem: 9341
[00:52:07.182779] Epoch: [394]  [160/195]  eta: 0:00:18  lr: 0.001325  loss: 0.3036 (0.3163)  time: 0.5129  data: 0.0002  max mem: 9341
[00:52:17.355423] Epoch: [394]  [180/195]  eta: 0:00:07  lr: 0.001324  loss: 0.3170 (0.3169)  time: 0.5086  data: 0.0001  max mem: 9341
[00:52:24.483097] Epoch: [394]  [194/195]  eta: 0:00:00  lr: 0.001324  loss: 0.3107 (0.3168)  time: 0.5104  data: 0.0001  max mem: 9341
[00:52:24.656198] Epoch: [394] Total time: 0:01:40 (0.5177 s / it)
[00:52:24.661946] Averaged stats: lr: 0.001324  loss: 0.3107 (0.3197)
[00:52:29.339435] {"train_lr": 0.001326456085066843, "train_loss": 0.31974692214757966, "epoch": 394}
[00:52:29.339728] [00:52:29.339830] Training epoch 394 for 0:01:45
[00:52:29.339884] [00:52:29.344793] log_dir: ./exp/debug/cifar100-LT/debug
[00:52:31.089258] Epoch: [395]  [  0/195]  eta: 0:05:40  lr: 0.001324  loss: 0.3497 (0.3497)  time: 1.7437  data: 1.2360  max mem: 9341
[00:52:41.305529] Epoch: [395]  [ 20/195]  eta: 0:01:39  lr: 0.001323  loss: 0.3239 (0.3276)  time: 0.5108  data: 0.0002  max mem: 9341
[00:52:51.523591] Epoch: [395]  [ 40/195]  eta: 0:01:23  lr: 0.001323  loss: 0.3163 (0.3221)  time: 0.5108  data: 0.0002  max mem: 9341
[00:53:01.744497] Epoch: [395]  [ 60/195]  eta: 0:01:11  lr: 0.001323  loss: 0.3203 (0.3224)  time: 0.5110  data: 0.0002  max mem: 9341
[00:53:12.020583] Epoch: [395]  [ 80/195]  eta: 0:01:00  lr: 0.001322  loss: 0.3181 (0.3225)  time: 0.5138  data: 0.0002  max mem: 9341
[00:53:22.228166] Epoch: [395]  [100/195]  eta: 0:00:49  lr: 0.001321  loss: 0.3142 (0.3214)  time: 0.5103  data: 0.0002  max mem: 9341
[00:53:32.436835] Epoch: [395]  [120/195]  eta: 0:00:39  lr: 0.001321  loss: 0.3108 (0.3200)  time: 0.5104  data: 0.0003  max mem: 9341
[00:53:42.649475] Epoch: [395]  [140/195]  eta: 0:00:28  lr: 0.001321  loss: 0.3218 (0.3195)  time: 0.5106  data: 0.0002  max mem: 9341
[00:53:52.904011] Epoch: [395]  [160/195]  eta: 0:00:18  lr: 0.001320  loss: 0.3245 (0.3201)  time: 0.5127  data: 0.0002  max mem: 9341
[00:54:03.068934] Epoch: [395]  [180/195]  eta: 0:00:07  lr: 0.001319  loss: 0.3300 (0.3206)  time: 0.5082  data: 0.0001  max mem: 9341
[00:54:10.203833] Epoch: [395]  [194/195]  eta: 0:00:00  lr: 0.001319  loss: 0.3161 (0.3200)  time: 0.5107  data: 0.0001  max mem: 9341
[00:54:10.387992] Epoch: [395] Total time: 0:01:41 (0.5182 s / it)
[00:54:10.406084] Averaged stats: lr: 0.001319  loss: 0.3161 (0.3203)
[00:54:15.089208] {"train_lr": 0.001321522232284062, "train_loss": 0.32031717812403654, "epoch": 395}
[00:54:15.089535] [00:54:15.089633] Training epoch 395 for 0:01:45
[00:54:15.089687] [00:54:15.094136] log_dir: ./exp/debug/cifar100-LT/debug
[00:54:16.903456] Epoch: [396]  [  0/195]  eta: 0:05:52  lr: 0.001319  loss: 0.3240 (0.3240)  time: 1.8079  data: 1.3133  max mem: 9341
[00:54:27.118962] Epoch: [396]  [ 20/195]  eta: 0:01:40  lr: 0.001318  loss: 0.3167 (0.3157)  time: 0.5107  data: 0.0002  max mem: 9341
[00:54:37.367750] Epoch: [396]  [ 40/195]  eta: 0:01:24  lr: 0.001318  loss: 0.3203 (0.3186)  time: 0.5124  data: 0.0002  max mem: 9341
[00:54:47.584562] Epoch: [396]  [ 60/195]  eta: 0:01:11  lr: 0.001318  loss: 0.3109 (0.3173)  time: 0.5108  data: 0.0002  max mem: 9341
[00:54:57.854356] Epoch: [396]  [ 80/195]  eta: 0:01:00  lr: 0.001317  loss: 0.3105 (0.3155)  time: 0.5134  data: 0.0002  max mem: 9341
[00:55:08.090337] Epoch: [396]  [100/195]  eta: 0:00:49  lr: 0.001316  loss: 0.3140 (0.3159)  time: 0.5117  data: 0.0002  max mem: 9341
[00:55:18.319180] Epoch: [396]  [120/195]  eta: 0:00:39  lr: 0.001316  loss: 0.3136 (0.3160)  time: 0.5114  data: 0.0002  max mem: 9341
[00:55:28.543859] Epoch: [396]  [140/195]  eta: 0:00:28  lr: 0.001316  loss: 0.3112 (0.3158)  time: 0.5112  data: 0.0002  max mem: 9341
[00:55:38.843973] Epoch: [396]  [160/195]  eta: 0:00:18  lr: 0.001315  loss: 0.3245 (0.3164)  time: 0.5149  data: 0.0002  max mem: 9341
[00:55:49.029713] Epoch: [396]  [180/195]  eta: 0:00:07  lr: 0.001314  loss: 0.3253 (0.3181)  time: 0.5092  data: 0.0001  max mem: 9341
[00:55:56.178868] Epoch: [396]  [194/195]  eta: 0:00:00  lr: 0.001314  loss: 0.3356 (0.3193)  time: 0.5125  data: 0.0001  max mem: 9341
[00:55:56.354882] Epoch: [396] Total time: 0:01:41 (0.5193 s / it)
[00:55:56.362885] Averaged stats: lr: 0.001314  loss: 0.3356 (0.3198)
[00:56:01.124982] {"train_lr": 0.0013165863030216117, "train_loss": 0.31984879487218, "epoch": 396}
[00:56:01.125317] [00:56:01.125402] Training epoch 396 for 0:01:46
[00:56:01.125456] [00:56:01.130070] log_dir: ./exp/debug/cifar100-LT/debug
[00:56:02.763675] Epoch: [397]  [  0/195]  eta: 0:05:18  lr: 0.001314  loss: 0.3171 (0.3171)  time: 1.6323  data: 1.1276  max mem: 9341
[00:56:12.998117] Epoch: [397]  [ 20/195]  eta: 0:01:38  lr: 0.001314  loss: 0.3222 (0.3169)  time: 0.5117  data: 0.0002  max mem: 9341
[00:56:23.229177] Epoch: [397]  [ 40/195]  eta: 0:01:23  lr: 0.001313  loss: 0.3168 (0.3172)  time: 0.5115  data: 0.0002  max mem: 9341
[00:56:33.461104] Epoch: [397]  [ 60/195]  eta: 0:01:11  lr: 0.001313  loss: 0.3145 (0.3172)  time: 0.5115  data: 0.0002  max mem: 9341
[00:56:43.760985] Epoch: [397]  [ 80/195]  eta: 0:01:00  lr: 0.001312  loss: 0.3292 (0.3179)  time: 0.5149  data: 0.0002  max mem: 9341
[00:56:54.001076] Epoch: [397]  [100/195]  eta: 0:00:49  lr: 0.001311  loss: 0.3133 (0.3171)  time: 0.5119  data: 0.0002  max mem: 9341
[00:57:04.242151] Epoch: [397]  [120/195]  eta: 0:00:39  lr: 0.001311  loss: 0.3109 (0.3167)  time: 0.5120  data: 0.0002  max mem: 9341
[00:57:14.484398] Epoch: [397]  [140/195]  eta: 0:00:28  lr: 0.001311  loss: 0.3197 (0.3172)  time: 0.5120  data: 0.0002  max mem: 9341
[00:57:24.783147] Epoch: [397]  [160/195]  eta: 0:00:18  lr: 0.001310  loss: 0.3141 (0.3176)  time: 0.5149  data: 0.0002  max mem: 9341
[00:57:34.980837] Epoch: [397]  [180/195]  eta: 0:00:07  lr: 0.001309  loss: 0.3239 (0.3178)  time: 0.5098  data: 0.0001  max mem: 9341
[00:57:42.137508] Epoch: [397]  [194/195]  eta: 0:00:00  lr: 0.001309  loss: 0.3183 (0.3179)  time: 0.5131  data: 0.0001  max mem: 9341
[00:57:42.319432] Epoch: [397] Total time: 0:01:41 (0.5189 s / it)
[00:57:42.337268] Averaged stats: lr: 0.001309  loss: 0.3183 (0.3189)
[00:57:47.062485] {"train_lr": 0.0013116483816209186, "train_loss": 0.31891836581321864, "epoch": 397}
[00:57:47.062749] [00:57:47.062832] Training epoch 397 for 0:01:45
[00:57:47.062885] [00:57:47.067335] log_dir: ./exp/debug/cifar100-LT/debug
[00:57:48.928762] Epoch: [398]  [  0/195]  eta: 0:06:02  lr: 0.001309  loss: 0.3439 (0.3439)  time: 1.8601  data: 1.3522  max mem: 9341
[00:57:59.139042] Epoch: [398]  [ 20/195]  eta: 0:01:40  lr: 0.001309  loss: 0.3086 (0.3134)  time: 0.5105  data: 0.0002  max mem: 9341
[00:58:09.349871] Epoch: [398]  [ 40/195]  eta: 0:01:24  lr: 0.001308  loss: 0.3216 (0.3173)  time: 0.5105  data: 0.0002  max mem: 9341
[00:58:19.567223] Epoch: [398]  [ 60/195]  eta: 0:01:11  lr: 0.001308  loss: 0.3176 (0.3186)  time: 0.5108  data: 0.0002  max mem: 9341
[00:58:29.829711] Epoch: [398]  [ 80/195]  eta: 0:01:00  lr: 0.001307  loss: 0.3177 (0.3190)  time: 0.5131  data: 0.0002  max mem: 9341
[00:58:40.049293] Epoch: [398]  [100/195]  eta: 0:00:49  lr: 0.001307  loss: 0.3169 (0.3190)  time: 0.5109  data: 0.0002  max mem: 9341
[00:58:50.270255] Epoch: [398]  [120/195]  eta: 0:00:39  lr: 0.001306  loss: 0.3223 (0.3192)  time: 0.5110  data: 0.0002  max mem: 9341
[00:59:00.486725] Epoch: [398]  [140/195]  eta: 0:00:28  lr: 0.001306  loss: 0.3286 (0.3210)  time: 0.5108  data: 0.0002  max mem: 9341
[00:59:10.742123] Epoch: [398]  [160/195]  eta: 0:00:18  lr: 0.001305  loss: 0.3273 (0.3215)  time: 0.5127  data: 0.0002  max mem: 9341
[00:59:20.909955] Epoch: [398]  [180/195]  eta: 0:00:07  lr: 0.001305  loss: 0.3083 (0.3208)  time: 0.5083  data: 0.0001  max mem: 9341
[00:59:28.039220] Epoch: [398]  [194/195]  eta: 0:00:00  lr: 0.001304  loss: 0.3191 (0.3208)  time: 0.5104  data: 0.0001  max mem: 9341
[00:59:28.223235] Epoch: [398] Total time: 0:01:41 (0.5187 s / it)
[00:59:28.223996] Averaged stats: lr: 0.001304  loss: 0.3191 (0.3191)
[00:59:33.050503] {"train_lr": 0.0013067085524574359, "train_loss": 0.3191296057441296, "epoch": 398}
[00:59:33.050761] [00:59:33.050844] Training epoch 398 for 0:01:45
[00:59:33.050897] [00:59:33.055306] log_dir: ./exp/debug/cifar100-LT/debug
[00:59:34.821495] Epoch: [399]  [  0/195]  eta: 0:05:44  lr: 0.001304  loss: 0.3103 (0.3103)  time: 1.7652  data: 1.2523  max mem: 9341
[00:59:45.040503] Epoch: [399]  [ 20/195]  eta: 0:01:39  lr: 0.001304  loss: 0.3174 (0.3172)  time: 0.5109  data: 0.0003  max mem: 9341
[00:59:55.288718] Epoch: [399]  [ 40/195]  eta: 0:01:24  lr: 0.001303  loss: 0.3314 (0.3239)  time: 0.5123  data: 0.0003  max mem: 9341
[01:00:05.502726] Epoch: [399]  [ 60/195]  eta: 0:01:11  lr: 0.001303  loss: 0.3225 (0.3230)  time: 0.5106  data: 0.0003  max mem: 9341
[01:00:15.755881] Epoch: [399]  [ 80/195]  eta: 0:01:00  lr: 0.001302  loss: 0.3136 (0.3222)  time: 0.5126  data: 0.0003  max mem: 9341
[01:00:25.996672] Epoch: [399]  [100/195]  eta: 0:00:49  lr: 0.001302  loss: 0.3105 (0.3211)  time: 0.5120  data: 0.0003  max mem: 9341
[01:00:36.235163] Epoch: [399]  [120/195]  eta: 0:00:39  lr: 0.001301  loss: 0.3246 (0.3219)  time: 0.5119  data: 0.0002  max mem: 9341
[01:00:46.471438] Epoch: [399]  [140/195]  eta: 0:00:28  lr: 0.001301  loss: 0.3123 (0.3216)  time: 0.5117  data: 0.0002  max mem: 9341
[01:00:56.775751] Epoch: [399]  [160/195]  eta: 0:00:18  lr: 0.001300  loss: 0.3236 (0.3218)  time: 0.5152  data: 0.0002  max mem: 9341
[01:01:06.971024] Epoch: [399]  [180/195]  eta: 0:00:07  lr: 0.001300  loss: 0.3161 (0.3213)  time: 0.5097  data: 0.0001  max mem: 9341
[01:01:14.123988] Epoch: [399]  [194/195]  eta: 0:00:00  lr: 0.001299  loss: 0.3170 (0.3211)  time: 0.5128  data: 0.0001  max mem: 9341
[01:01:14.298113] Epoch: [399] Total time: 0:01:41 (0.5192 s / it)
[01:01:14.314196] Averaged stats: lr: 0.001299  loss: 0.3170 (0.3195)
[01:01:19.003081] {"train_lr": 0.0013017668999392093, "train_loss": 0.3195274498600226, "epoch": 399}
[01:01:19.003427] [01:01:19.003530] Training epoch 399 for 0:01:45
[01:01:19.003585] [01:01:19.007989] log_dir: ./exp/debug/cifar100-LT/debug
[01:01:20.601804] Epoch: [400]  [  0/195]  eta: 0:05:10  lr: 0.001299  loss: 0.3456 (0.3456)  time: 1.5923  data: 1.0926  max mem: 9341
[01:01:30.817004] Epoch: [400]  [ 20/195]  eta: 0:01:38  lr: 0.001299  loss: 0.3224 (0.3194)  time: 0.5107  data: 0.0002  max mem: 9341
[01:01:41.036071] Epoch: [400]  [ 40/195]  eta: 0:01:23  lr: 0.001298  loss: 0.3172 (0.3212)  time: 0.5109  data: 0.0002  max mem: 9341
[01:01:51.252191] Epoch: [400]  [ 60/195]  eta: 0:01:11  lr: 0.001298  loss: 0.3177 (0.3210)  time: 0.5107  data: 0.0003  max mem: 9341
[01:02:01.508495] Epoch: [400]  [ 80/195]  eta: 0:01:00  lr: 0.001297  loss: 0.3154 (0.3204)  time: 0.5127  data: 0.0003  max mem: 9341
[01:02:11.721083] Epoch: [400]  [100/195]  eta: 0:00:49  lr: 0.001297  loss: 0.3272 (0.3199)  time: 0.5106  data: 0.0002  max mem: 9341
[01:02:21.933067] Epoch: [400]  [120/195]  eta: 0:00:38  lr: 0.001296  loss: 0.3152 (0.3200)  time: 0.5105  data: 0.0002  max mem: 9341
[01:02:32.143836] Epoch: [400]  [140/195]  eta: 0:00:28  lr: 0.001296  loss: 0.3096 (0.3188)  time: 0.5105  data: 0.0002  max mem: 9341
[01:02:42.403746] Epoch: [400]  [160/195]  eta: 0:00:18  lr: 0.001295  loss: 0.3102 (0.3190)  time: 0.5129  data: 0.0002  max mem: 9341
[01:02:52.577011] Epoch: [400]  [180/195]  eta: 0:00:07  lr: 0.001295  loss: 0.3214 (0.3192)  time: 0.5086  data: 0.0001  max mem: 9341
[01:02:59.710477] Epoch: [400]  [194/195]  eta: 0:00:00  lr: 0.001294  loss: 0.3218 (0.3193)  time: 0.5106  data: 0.0001  max mem: 9341
[01:02:59.895579] Epoch: [400] Total time: 0:01:40 (0.5174 s / it)
[01:02:59.907772] Averaged stats: lr: 0.001294  loss: 0.3218 (0.3201)
[01:03:04.581133] {"train_lr": 0.0012968235085054508, "train_loss": 0.32008194564244685, "epoch": 400}
[01:03:04.581450] [01:03:04.581552] Training epoch 400 for 0:01:45
[01:03:04.581631] [01:03:04.586026] log_dir: ./exp/debug/cifar100-LT/debug
[01:03:06.363378] Epoch: [401]  [  0/195]  eta: 0:05:46  lr: 0.001294  loss: 0.2744 (0.2744)  time: 1.7760  data: 1.2592  max mem: 9341
[01:03:16.581081] Epoch: [401]  [ 20/195]  eta: 0:01:39  lr: 0.001294  loss: 0.3210 (0.3219)  time: 0.5108  data: 0.0003  max mem: 9341
[01:03:26.789750] Epoch: [401]  [ 40/195]  eta: 0:01:23  lr: 0.001293  loss: 0.3181 (0.3218)  time: 0.5104  data: 0.0003  max mem: 9341
[01:03:37.008009] Epoch: [401]  [ 60/195]  eta: 0:01:11  lr: 0.001293  loss: 0.3183 (0.3231)  time: 0.5108  data: 0.0003  max mem: 9341
[01:03:47.266092] Epoch: [401]  [ 80/195]  eta: 0:01:00  lr: 0.001292  loss: 0.3205 (0.3217)  time: 0.5128  data: 0.0003  max mem: 9341
[01:03:57.483942] Epoch: [401]  [100/195]  eta: 0:00:49  lr: 0.001292  loss: 0.3208 (0.3215)  time: 0.5108  data: 0.0003  max mem: 9341
[01:04:07.689663] Epoch: [401]  [120/195]  eta: 0:00:39  lr: 0.001291  loss: 0.3103 (0.3202)  time: 0.5102  data: 0.0003  max mem: 9341
[01:04:17.901700] Epoch: [401]  [140/195]  eta: 0:00:28  lr: 0.001291  loss: 0.3148 (0.3200)  time: 0.5105  data: 0.0002  max mem: 9341
[01:04:28.160047] Epoch: [401]  [160/195]  eta: 0:00:18  lr: 0.001290  loss: 0.3120 (0.3191)  time: 0.5129  data: 0.0003  max mem: 9341
[01:04:38.330909] Epoch: [401]  [180/195]  eta: 0:00:07  lr: 0.001290  loss: 0.3271 (0.3190)  time: 0.5085  data: 0.0002  max mem: 9341
[01:04:45.454635] Epoch: [401]  [194/195]  eta: 0:00:00  lr: 0.001289  loss: 0.3181 (0.3187)  time: 0.5103  data: 0.0001  max mem: 9341
[01:04:45.620770] Epoch: [401] Total time: 0:01:41 (0.5181 s / it)
[01:04:45.634880] Averaged stats: lr: 0.001289  loss: 0.3181 (0.3188)
[01:04:50.325002] {"train_lr": 0.0012918784626250704, "train_loss": 0.31876221648775616, "epoch": 401}
[01:04:50.325332] [01:04:50.325438] Training epoch 401 for 0:01:45
[01:04:50.325493] [01:04:50.329920] log_dir: ./exp/debug/cifar100-LT/debug
[01:04:51.873649] Epoch: [402]  [  0/195]  eta: 0:05:00  lr: 0.001289  loss: 0.3224 (0.3224)  time: 1.5419  data: 1.0338  max mem: 9341
[01:05:02.112479] Epoch: [402]  [ 20/195]  eta: 0:01:38  lr: 0.001289  loss: 0.3109 (0.3159)  time: 0.5119  data: 0.0002  max mem: 9341
[01:05:12.346105] Epoch: [402]  [ 40/195]  eta: 0:01:23  lr: 0.001288  loss: 0.3176 (0.3175)  time: 0.5116  data: 0.0002  max mem: 9341
[01:05:22.584051] Epoch: [402]  [ 60/195]  eta: 0:01:11  lr: 0.001288  loss: 0.3112 (0.3190)  time: 0.5118  data: 0.0002  max mem: 9341
[01:05:32.895605] Epoch: [402]  [ 80/195]  eta: 0:01:00  lr: 0.001287  loss: 0.3204 (0.3194)  time: 0.5155  data: 0.0002  max mem: 9341
[01:05:43.145069] Epoch: [402]  [100/195]  eta: 0:00:49  lr: 0.001287  loss: 0.3123 (0.3185)  time: 0.5124  data: 0.0002  max mem: 9341
[01:05:53.384240] Epoch: [402]  [120/195]  eta: 0:00:39  lr: 0.001286  loss: 0.3240 (0.3200)  time: 0.5119  data: 0.0002  max mem: 9341
[01:06:03.630849] Epoch: [402]  [140/195]  eta: 0:00:28  lr: 0.001286  loss: 0.3202 (0.3203)  time: 0.5123  data: 0.0002  max mem: 9341
[01:06:13.934942] Epoch: [402]  [160/195]  eta: 0:00:18  lr: 0.001285  loss: 0.3178 (0.3203)  time: 0.5151  data: 0.0003  max mem: 9341
[01:06:24.131396] Epoch: [402]  [180/195]  eta: 0:00:07  lr: 0.001285  loss: 0.3186 (0.3203)  time: 0.5098  data: 0.0002  max mem: 9341
[01:06:31.284819] Epoch: [402]  [194/195]  eta: 0:00:00  lr: 0.001284  loss: 0.3250 (0.3209)  time: 0.5129  data: 0.0001  max mem: 9341
[01:06:31.471190] Epoch: [402] Total time: 0:01:41 (0.5187 s / it)
[01:06:31.475468] Averaged stats: lr: 0.001284  loss: 0.3250 (0.3197)
[01:06:36.196413] {"train_lr": 0.001286931846795268, "train_loss": 0.3196726546837733, "epoch": 402}
[01:06:36.196801] [01:06:36.196907] Training epoch 402 for 0:01:45
[01:06:36.196963] [01:06:36.201485] log_dir: ./exp/debug/cifar100-LT/debug
[01:06:37.927418] Epoch: [403]  [  0/195]  eta: 0:05:36  lr: 0.001284  loss: 0.3469 (0.3469)  time: 1.7246  data: 1.2195  max mem: 9341
[01:06:48.147440] Epoch: [403]  [ 20/195]  eta: 0:01:39  lr: 0.001284  loss: 0.3086 (0.3140)  time: 0.5109  data: 0.0002  max mem: 9341
[01:06:58.361499] Epoch: [403]  [ 40/195]  eta: 0:01:23  lr: 0.001283  loss: 0.3205 (0.3135)  time: 0.5106  data: 0.0003  max mem: 9341
[01:07:08.580652] Epoch: [403]  [ 60/195]  eta: 0:01:11  lr: 0.001283  loss: 0.3139 (0.3147)  time: 0.5109  data: 0.0002  max mem: 9341
[01:07:18.880424] Epoch: [403]  [ 80/195]  eta: 0:01:00  lr: 0.001282  loss: 0.3047 (0.3153)  time: 0.5149  data: 0.0003  max mem: 9341
[01:07:29.127423] Epoch: [403]  [100/195]  eta: 0:00:49  lr: 0.001282  loss: 0.3216 (0.3163)  time: 0.5123  data: 0.0002  max mem: 9341
[01:07:39.350221] Epoch: [403]  [120/195]  eta: 0:00:39  lr: 0.001281  loss: 0.3146 (0.3173)  time: 0.5111  data: 0.0002  max mem: 9341
[01:07:49.562730] Epoch: [403]  [140/195]  eta: 0:00:28  lr: 0.001281  loss: 0.3119 (0.3170)  time: 0.5106  data: 0.0002  max mem: 9341
[01:07:59.826710] Epoch: [403]  [160/195]  eta: 0:00:18  lr: 0.001280  loss: 0.3037 (0.3165)  time: 0.5131  data: 0.0002  max mem: 9341
[01:08:09.999802] Epoch: [403]  [180/195]  eta: 0:00:07  lr: 0.001280  loss: 0.3235 (0.3176)  time: 0.5086  data: 0.0001  max mem: 9341
[01:08:17.138104] Epoch: [403]  [194/195]  eta: 0:00:00  lr: 0.001279  loss: 0.3246 (0.3178)  time: 0.5110  data: 0.0001  max mem: 9341
[01:08:17.307469] Epoch: [403] Total time: 0:01:41 (0.5185 s / it)
[01:08:17.322956] Averaged stats: lr: 0.001279  loss: 0.3246 (0.3187)
[01:08:22.039394] {"train_lr": 0.0012819837455400592, "train_loss": 0.31874890529956573, "epoch": 403}
[01:08:22.039662] [01:08:22.039775] Training epoch 403 for 0:01:45
[01:08:22.039829] [01:08:22.044336] log_dir: ./exp/debug/cifar100-LT/debug
[01:08:23.754875] Epoch: [404]  [  0/195]  eta: 0:05:33  lr: 0.001279  loss: 0.3193 (0.3193)  time: 1.7090  data: 1.1955  max mem: 9341
[01:08:34.010959] Epoch: [404]  [ 20/195]  eta: 0:01:39  lr: 0.001279  loss: 0.3195 (0.3216)  time: 0.5127  data: 0.0003  max mem: 9341
[01:08:44.224496] Epoch: [404]  [ 40/195]  eta: 0:01:23  lr: 0.001278  loss: 0.3241 (0.3239)  time: 0.5106  data: 0.0002  max mem: 9341
[01:08:54.436395] Epoch: [404]  [ 60/195]  eta: 0:01:11  lr: 0.001278  loss: 0.3138 (0.3209)  time: 0.5105  data: 0.0002  max mem: 9341
[01:09:04.694671] Epoch: [404]  [ 80/195]  eta: 0:01:00  lr: 0.001277  loss: 0.3125 (0.3185)  time: 0.5129  data: 0.0003  max mem: 9341
[01:09:14.906787] Epoch: [404]  [100/195]  eta: 0:00:49  lr: 0.001277  loss: 0.3166 (0.3187)  time: 0.5105  data: 0.0003  max mem: 9341
[01:09:25.118095] Epoch: [404]  [120/195]  eta: 0:00:39  lr: 0.001276  loss: 0.3163 (0.3190)  time: 0.5105  data: 0.0003  max mem: 9341
[01:09:35.330238] Epoch: [404]  [140/195]  eta: 0:00:28  lr: 0.001276  loss: 0.3120 (0.3189)  time: 0.5105  data: 0.0002  max mem: 9341
[01:09:45.593188] Epoch: [404]  [160/195]  eta: 0:00:18  lr: 0.001275  loss: 0.3159 (0.3187)  time: 0.5131  data: 0.0002  max mem: 9341
[01:09:55.764898] Epoch: [404]  [180/195]  eta: 0:00:07  lr: 0.001275  loss: 0.3224 (0.3192)  time: 0.5085  data: 0.0002  max mem: 9341
[01:10:02.895052] Epoch: [404]  [194/195]  eta: 0:00:00  lr: 0.001274  loss: 0.3198 (0.3191)  time: 0.5104  data: 0.0001  max mem: 9341
[01:10:03.069058] Epoch: [404] Total time: 0:01:41 (0.5181 s / it)
[01:10:03.080703] Averaged stats: lr: 0.001274  loss: 0.3198 (0.3194)
[01:10:07.777278] {"train_lr": 0.0012770342434088346, "train_loss": 0.3193893605317825, "epoch": 404}
[01:10:07.777541] [01:10:07.777639] Training epoch 404 for 0:01:45
[01:10:07.777694] [01:10:07.782146] log_dir: ./exp/debug/cifar100-LT/debug
[01:10:09.580654] Epoch: [405]  [  0/195]  eta: 0:05:50  lr: 0.001274  loss: 0.2993 (0.2993)  time: 1.7975  data: 1.2799  max mem: 9341
[01:10:19.818105] Epoch: [405]  [ 20/195]  eta: 0:01:40  lr: 0.001274  loss: 0.3217 (0.3166)  time: 0.5118  data: 0.0002  max mem: 9341
[01:10:30.061369] Epoch: [405]  [ 40/195]  eta: 0:01:24  lr: 0.001274  loss: 0.3125 (0.3178)  time: 0.5121  data: 0.0002  max mem: 9341
[01:10:40.306939] Epoch: [405]  [ 60/195]  eta: 0:01:11  lr: 0.001273  loss: 0.3050 (0.3148)  time: 0.5122  data: 0.0002  max mem: 9341
[01:10:50.616509] Epoch: [405]  [ 80/195]  eta: 0:01:00  lr: 0.001272  loss: 0.3076 (0.3144)  time: 0.5154  data: 0.0003  max mem: 9341
[01:11:00.837647] Epoch: [405]  [100/195]  eta: 0:00:49  lr: 0.001272  loss: 0.3138 (0.3151)  time: 0.5110  data: 0.0002  max mem: 9341
[01:11:11.054160] Epoch: [405]  [120/195]  eta: 0:00:39  lr: 0.001272  loss: 0.3160 (0.3153)  time: 0.5108  data: 0.0003  max mem: 9341
[01:11:21.274327] Epoch: [405]  [140/195]  eta: 0:00:28  lr: 0.001271  loss: 0.3163 (0.3156)  time: 0.5110  data: 0.0002  max mem: 9341
[01:11:31.540211] Epoch: [405]  [160/195]  eta: 0:00:18  lr: 0.001270  loss: 0.3143 (0.3156)  time: 0.5132  data: 0.0002  max mem: 9341
[01:11:41.717474] Epoch: [405]  [180/195]  eta: 0:00:07  lr: 0.001270  loss: 0.3190 (0.3162)  time: 0.5088  data: 0.0001  max mem: 9341
[01:11:48.854082] Epoch: [405]  [194/195]  eta: 0:00:00  lr: 0.001269  loss: 0.3098 (0.3162)  time: 0.5109  data: 0.0001  max mem: 9341
[01:11:49.038896] Epoch: [405] Total time: 0:01:41 (0.5193 s / it)
[01:11:49.067435] Averaged stats: lr: 0.001269  loss: 0.3098 (0.3170)
[01:11:53.775401] {"train_lr": 0.0012720834249749385, "train_loss": 0.3170322096500641, "epoch": 405}
[01:11:53.775674] [01:11:53.775777] Training epoch 405 for 0:01:45
[01:11:53.775854] [01:11:53.781025] log_dir: ./exp/debug/cifar100-LT/debug
[01:11:55.551713] Epoch: [406]  [  0/195]  eta: 0:05:45  lr: 0.001269  loss: 0.3355 (0.3355)  time: 1.7696  data: 1.2591  max mem: 9341
[01:12:05.764007] Epoch: [406]  [ 20/195]  eta: 0:01:39  lr: 0.001269  loss: 0.3171 (0.3151)  time: 0.5106  data: 0.0002  max mem: 9341
[01:12:15.998460] Epoch: [406]  [ 40/195]  eta: 0:01:23  lr: 0.001269  loss: 0.3153 (0.3190)  time: 0.5116  data: 0.0002  max mem: 9341
[01:12:26.214018] Epoch: [406]  [ 60/195]  eta: 0:01:11  lr: 0.001268  loss: 0.3275 (0.3195)  time: 0.5107  data: 0.0002  max mem: 9341
[01:12:36.475590] Epoch: [406]  [ 80/195]  eta: 0:01:00  lr: 0.001267  loss: 0.3141 (0.3188)  time: 0.5130  data: 0.0002  max mem: 9341
[01:12:46.690205] Epoch: [406]  [100/195]  eta: 0:00:49  lr: 0.001267  loss: 0.3248 (0.3200)  time: 0.5107  data: 0.0002  max mem: 9341
[01:12:56.903243] Epoch: [406]  [120/195]  eta: 0:00:39  lr: 0.001267  loss: 0.3111 (0.3190)  time: 0.5106  data: 0.0002  max mem: 9341
[01:13:07.117588] Epoch: [406]  [140/195]  eta: 0:00:28  lr: 0.001266  loss: 0.3128 (0.3185)  time: 0.5107  data: 0.0002  max mem: 9341
[01:13:17.379006] Epoch: [406]  [160/195]  eta: 0:00:18  lr: 0.001265  loss: 0.3118 (0.3183)  time: 0.5130  data: 0.0002  max mem: 9341
[01:13:27.546604] Epoch: [406]  [180/195]  eta: 0:00:07  lr: 0.001265  loss: 0.3231 (0.3183)  time: 0.5083  data: 0.0001  max mem: 9341
[01:13:34.677376] Epoch: [406]  [194/195]  eta: 0:00:00  lr: 0.001265  loss: 0.3206 (0.3185)  time: 0.5104  data: 0.0001  max mem: 9341
[01:13:34.853124] Epoch: [406] Total time: 0:01:41 (0.5183 s / it)
[01:13:34.871792] Averaged stats: lr: 0.001265  loss: 0.3206 (0.3164)
[01:13:39.638859] {"train_lr": 0.0012671313748341914, "train_loss": 0.3163942259855759, "epoch": 406}
[01:13:39.639187] [01:13:39.639270] Training epoch 406 for 0:01:45
[01:13:39.639324] [01:13:39.643799] log_dir: ./exp/debug/cifar100-LT/debug
[01:13:41.371154] Epoch: [407]  [  0/195]  eta: 0:05:36  lr: 0.001264  loss: 0.2905 (0.2905)  time: 1.7259  data: 1.2199  max mem: 9341
[01:13:51.587294] Epoch: [407]  [ 20/195]  eta: 0:01:39  lr: 0.001264  loss: 0.3122 (0.3136)  time: 0.5107  data: 0.0002  max mem: 9341
[01:14:01.815023] Epoch: [407]  [ 40/195]  eta: 0:01:23  lr: 0.001264  loss: 0.3102 (0.3137)  time: 0.5113  data: 0.0002  max mem: 9341
[01:14:12.030805] Epoch: [407]  [ 60/195]  eta: 0:01:11  lr: 0.001263  loss: 0.3059 (0.3125)  time: 0.5107  data: 0.0002  max mem: 9341
[01:14:22.297014] Epoch: [407]  [ 80/195]  eta: 0:01:00  lr: 0.001262  loss: 0.3195 (0.3153)  time: 0.5133  data: 0.0002  max mem: 9341
[01:14:32.515101] Epoch: [407]  [100/195]  eta: 0:00:49  lr: 0.001262  loss: 0.3126 (0.3150)  time: 0.5109  data: 0.0002  max mem: 9341
[01:14:42.732182] Epoch: [407]  [120/195]  eta: 0:00:39  lr: 0.001262  loss: 0.3162 (0.3157)  time: 0.5108  data: 0.0002  max mem: 9341
[01:14:52.948770] Epoch: [407]  [140/195]  eta: 0:00:28  lr: 0.001261  loss: 0.3095 (0.3150)  time: 0.5108  data: 0.0002  max mem: 9341
[01:15:03.211079] Epoch: [407]  [160/195]  eta: 0:00:18  lr: 0.001260  loss: 0.3176 (0.3150)  time: 0.5131  data: 0.0002  max mem: 9341
[01:15:13.382267] Epoch: [407]  [180/195]  eta: 0:00:07  lr: 0.001260  loss: 0.3133 (0.3155)  time: 0.5085  data: 0.0002  max mem: 9341
[01:15:20.517672] Epoch: [407]  [194/195]  eta: 0:00:00  lr: 0.001260  loss: 0.3197 (0.3158)  time: 0.5109  data: 0.0001  max mem: 9341
[01:15:20.696891] Epoch: [407] Total time: 0:01:41 (0.5182 s / it)
[01:15:20.704769] Averaged stats: lr: 0.001260  loss: 0.3197 (0.3158)
[01:15:25.388836] {"train_lr": 0.0012621781776034732, "train_loss": 0.31575732920796445, "epoch": 407}
[01:15:25.389190] [01:15:25.389292] Training epoch 407 for 0:01:45
[01:15:25.389347] [01:15:25.393788] log_dir: ./exp/debug/cifar100-LT/debug
[01:15:27.223314] Epoch: [408]  [  0/195]  eta: 0:05:56  lr: 0.001260  loss: 0.3269 (0.3269)  time: 1.8282  data: 1.3403  max mem: 9341
[01:15:37.444436] Epoch: [408]  [ 20/195]  eta: 0:01:40  lr: 0.001259  loss: 0.3084 (0.3096)  time: 0.5110  data: 0.0003  max mem: 9341
[01:15:47.663333] Epoch: [408]  [ 40/195]  eta: 0:01:24  lr: 0.001259  loss: 0.3173 (0.3155)  time: 0.5109  data: 0.0002  max mem: 9341
[01:15:57.886636] Epoch: [408]  [ 60/195]  eta: 0:01:11  lr: 0.001258  loss: 0.3176 (0.3175)  time: 0.5111  data: 0.0002  max mem: 9341
[01:16:08.143096] Epoch: [408]  [ 80/195]  eta: 0:01:00  lr: 0.001257  loss: 0.3208 (0.3178)  time: 0.5128  data: 0.0002  max mem: 9341
[01:16:18.363754] Epoch: [408]  [100/195]  eta: 0:00:49  lr: 0.001257  loss: 0.3086 (0.3169)  time: 0.5110  data: 0.0002  max mem: 9341
[01:16:28.584861] Epoch: [408]  [120/195]  eta: 0:00:39  lr: 0.001257  loss: 0.3159 (0.3177)  time: 0.5110  data: 0.0002  max mem: 9341
[01:16:38.800007] Epoch: [408]  [140/195]  eta: 0:00:28  lr: 0.001256  loss: 0.3092 (0.3168)  time: 0.5107  data: 0.0002  max mem: 9341
[01:16:49.060252] Epoch: [408]  [160/195]  eta: 0:00:18  lr: 0.001255  loss: 0.3088 (0.3166)  time: 0.5130  data: 0.0002  max mem: 9341
[01:16:59.240971] Epoch: [408]  [180/195]  eta: 0:00:07  lr: 0.001255  loss: 0.3190 (0.3166)  time: 0.5090  data: 0.0001  max mem: 9341
[01:17:06.369947] Epoch: [408]  [194/195]  eta: 0:00:00  lr: 0.001255  loss: 0.3130 (0.3159)  time: 0.5106  data: 0.0001  max mem: 9341
[01:17:06.565307] Epoch: [408] Total time: 0:01:41 (0.5188 s / it)
[01:17:06.566191] Averaged stats: lr: 0.001255  loss: 0.3130 (0.3190)
[01:17:11.362719] {"train_lr": 0.0012572239179192554, "train_loss": 0.31903079117719946, "epoch": 408}
[01:17:11.363002] [01:17:11.363099] Training epoch 408 for 0:01:45
[01:17:11.363154] [01:17:11.367675] log_dir: ./exp/debug/cifar100-LT/debug
[01:17:13.339944] Epoch: [409]  [  0/195]  eta: 0:06:24  lr: 0.001255  loss: 0.2963 (0.2963)  time: 1.9714  data: 1.4581  max mem: 9341
[01:17:23.587368] Epoch: [409]  [ 20/195]  eta: 0:01:41  lr: 0.001254  loss: 0.3225 (0.3219)  time: 0.5123  data: 0.0003  max mem: 9341
[01:17:33.832201] Epoch: [409]  [ 40/195]  eta: 0:01:24  lr: 0.001254  loss: 0.3204 (0.3198)  time: 0.5122  data: 0.0002  max mem: 9341
[01:17:44.052765] Epoch: [409]  [ 60/195]  eta: 0:01:12  lr: 0.001253  loss: 0.3221 (0.3202)  time: 0.5110  data: 0.0003  max mem: 9341
[01:17:54.309223] Epoch: [409]  [ 80/195]  eta: 0:01:00  lr: 0.001253  loss: 0.3163 (0.3185)  time: 0.5128  data: 0.0003  max mem: 9341
[01:18:04.519043] Epoch: [409]  [100/195]  eta: 0:00:49  lr: 0.001252  loss: 0.3204 (0.3192)  time: 0.5104  data: 0.0003  max mem: 9341
[01:18:14.732511] Epoch: [409]  [120/195]  eta: 0:00:39  lr: 0.001252  loss: 0.3180 (0.3187)  time: 0.5106  data: 0.0003  max mem: 9341
[01:18:24.941804] Epoch: [409]  [140/195]  eta: 0:00:28  lr: 0.001251  loss: 0.3156 (0.3185)  time: 0.5104  data: 0.0003  max mem: 9341
[01:18:35.198094] Epoch: [409]  [160/195]  eta: 0:00:18  lr: 0.001250  loss: 0.3166 (0.3183)  time: 0.5128  data: 0.0003  max mem: 9341
[01:18:45.368481] Epoch: [409]  [180/195]  eta: 0:00:07  lr: 0.001250  loss: 0.3061 (0.3170)  time: 0.5085  data: 0.0002  max mem: 9341
[01:18:52.498935] Epoch: [409]  [194/195]  eta: 0:00:00  lr: 0.001250  loss: 0.3119 (0.3169)  time: 0.5104  data: 0.0001  max mem: 9341
[01:18:52.683539] Epoch: [409] Total time: 0:01:41 (0.5196 s / it)
[01:18:52.697050] Averaged stats: lr: 0.001250  loss: 0.3119 (0.3180)
[01:18:57.373465] {"train_lr": 0.0012522686804361638, "train_loss": 0.31800447763540807, "epoch": 409}
[01:18:57.373739] [01:18:57.373852] Training epoch 409 for 0:01:46
[01:18:57.373908] [01:18:57.378921] log_dir: ./exp/debug/cifar100-LT/debug
[01:18:59.221265] Epoch: [410]  [  0/195]  eta: 0:05:58  lr: 0.001250  loss: 0.3259 (0.3259)  time: 1.8408  data: 1.3545  max mem: 9341
[01:19:09.450180] Epoch: [410]  [ 20/195]  eta: 0:01:40  lr: 0.001249  loss: 0.3159 (0.3164)  time: 0.5114  data: 0.0002  max mem: 9341
[01:19:19.666640] Epoch: [410]  [ 40/195]  eta: 0:01:24  lr: 0.001249  loss: 0.3136 (0.3163)  time: 0.5108  data: 0.0002  max mem: 9341
[01:19:29.879958] Epoch: [410]  [ 60/195]  eta: 0:01:11  lr: 0.001248  loss: 0.3094 (0.3162)  time: 0.5106  data: 0.0002  max mem: 9341
[01:19:40.179692] Epoch: [410]  [ 80/195]  eta: 0:01:00  lr: 0.001248  loss: 0.3253 (0.3178)  time: 0.5149  data: 0.0002  max mem: 9341
[01:19:50.407015] Epoch: [410]  [100/195]  eta: 0:00:49  lr: 0.001247  loss: 0.3223 (0.3190)  time: 0.5113  data: 0.0002  max mem: 9341
[01:20:00.646012] Epoch: [410]  [120/195]  eta: 0:00:39  lr: 0.001247  loss: 0.3113 (0.3182)  time: 0.5119  data: 0.0002  max mem: 9341
[01:20:10.887238] Epoch: [410]  [140/195]  eta: 0:00:28  lr: 0.001246  loss: 0.3237 (0.3190)  time: 0.5120  data: 0.0002  max mem: 9341
[01:20:21.188020] Epoch: [410]  [160/195]  eta: 0:00:18  lr: 0.001246  loss: 0.3185 (0.3190)  time: 0.5150  data: 0.0002  max mem: 9341
[01:20:31.377447] Epoch: [410]  [180/195]  eta: 0:00:07  lr: 0.001245  loss: 0.3134 (0.3186)  time: 0.5094  data: 0.0001  max mem: 9341
[01:20:38.528871] Epoch: [410]  [194/195]  eta: 0:00:00  lr: 0.001245  loss: 0.3115 (0.3184)  time: 0.5127  data: 0.0001  max mem: 9341
[01:20:38.694068] Epoch: [410] Total time: 0:01:41 (0.5196 s / it)
[01:20:38.711318] Averaged stats: lr: 0.001245  loss: 0.3115 (0.3162)
[01:20:43.396863] {"train_lr": 0.0012473125498255397, "train_loss": 0.31616223343671895, "epoch": 410}
[01:20:43.397134] [01:20:43.397233] Training epoch 410 for 0:01:46
[01:20:43.397288] [01:20:43.402228] log_dir: ./exp/debug/cifar100-LT/debug
[01:20:45.021587] Epoch: [411]  [  0/195]  eta: 0:05:15  lr: 0.001245  loss: 0.3151 (0.3151)  time: 1.6183  data: 1.1169  max mem: 9341
[01:20:55.239607] Epoch: [411]  [ 20/195]  eta: 0:01:38  lr: 0.001244  loss: 0.3092 (0.3107)  time: 0.5108  data: 0.0002  max mem: 9341
[01:21:05.465099] Epoch: [411]  [ 40/195]  eta: 0:01:23  lr: 0.001244  loss: 0.3072 (0.3089)  time: 0.5112  data: 0.0002  max mem: 9341
[01:21:15.689174] Epoch: [411]  [ 60/195]  eta: 0:01:11  lr: 0.001243  loss: 0.3076 (0.3081)  time: 0.5111  data: 0.0002  max mem: 9341
[01:21:25.952857] Epoch: [411]  [ 80/195]  eta: 0:01:00  lr: 0.001243  loss: 0.3196 (0.3112)  time: 0.5131  data: 0.0002  max mem: 9341
[01:21:36.174120] Epoch: [411]  [100/195]  eta: 0:00:49  lr: 0.001242  loss: 0.3203 (0.3132)  time: 0.5110  data: 0.0002  max mem: 9341
[01:21:46.397733] Epoch: [411]  [120/195]  eta: 0:00:39  lr: 0.001242  loss: 0.3267 (0.3148)  time: 0.5111  data: 0.0002  max mem: 9341
[01:21:56.615419] Epoch: [411]  [140/195]  eta: 0:00:28  lr: 0.001241  loss: 0.3104 (0.3145)  time: 0.5108  data: 0.0002  max mem: 9341
[01:22:06.888468] Epoch: [411]  [160/195]  eta: 0:00:18  lr: 0.001241  loss: 0.3164 (0.3152)  time: 0.5136  data: 0.0002  max mem: 9341
[01:22:17.066998] Epoch: [411]  [180/195]  eta: 0:00:07  lr: 0.001240  loss: 0.3187 (0.3156)  time: 0.5089  data: 0.0001  max mem: 9341
[01:22:24.214403] Epoch: [411]  [194/195]  eta: 0:00:00  lr: 0.001240  loss: 0.3117 (0.3155)  time: 0.5113  data: 0.0001  max mem: 9341
[01:22:24.376164] Epoch: [411] Total time: 0:01:40 (0.5178 s / it)
[01:22:24.391573] Averaged stats: lr: 0.001240  loss: 0.3117 (0.3169)
[01:22:28.956627] {"train_lr": 0.0012423556107739803, "train_loss": 0.31687616631388665, "epoch": 411}
[01:22:28.956898] [01:22:28.956983] Training epoch 411 for 0:01:45
[01:22:28.957037] [01:22:28.961575] log_dir: ./exp/debug/cifar100-LT/debug
[01:22:30.729918] Epoch: [412]  [  0/195]  eta: 0:05:44  lr: 0.001240  loss: 0.3143 (0.3143)  time: 1.7675  data: 1.2636  max mem: 9341
[01:22:40.939733] Epoch: [412]  [ 20/195]  eta: 0:01:39  lr: 0.001239  loss: 0.3202 (0.3212)  time: 0.5104  data: 0.0002  max mem: 9341
[01:22:51.147963] Epoch: [412]  [ 40/195]  eta: 0:01:23  lr: 0.001239  loss: 0.3127 (0.3178)  time: 0.5104  data: 0.0002  max mem: 9341
[01:23:01.362818] Epoch: [412]  [ 60/195]  eta: 0:01:11  lr: 0.001238  loss: 0.3219 (0.3202)  time: 0.5107  data: 0.0002  max mem: 9341
[01:23:11.672326] Epoch: [412]  [ 80/195]  eta: 0:01:00  lr: 0.001238  loss: 0.3199 (0.3218)  time: 0.5154  data: 0.0002  max mem: 9341
[01:23:21.888016] Epoch: [412]  [100/195]  eta: 0:00:49  lr: 0.001237  loss: 0.3209 (0.3222)  time: 0.5107  data: 0.0004  max mem: 9341
[01:23:32.095762] Epoch: [412]  [120/195]  eta: 0:00:39  lr: 0.001237  loss: 0.3150 (0.3201)  time: 0.5103  data: 0.0003  max mem: 9341
[01:23:42.308156] Epoch: [412]  [140/195]  eta: 0:00:28  lr: 0.001236  loss: 0.3088 (0.3184)  time: 0.5106  data: 0.0003  max mem: 9341
[01:23:52.561553] Epoch: [412]  [160/195]  eta: 0:00:18  lr: 0.001236  loss: 0.3141 (0.3191)  time: 0.5126  data: 0.0002  max mem: 9341
[01:24:02.732326] Epoch: [412]  [180/195]  eta: 0:00:07  lr: 0.001235  loss: 0.3231 (0.3189)  time: 0.5085  data: 0.0002  max mem: 9341
[01:24:09.862005] Epoch: [412]  [194/195]  eta: 0:00:00  lr: 0.001235  loss: 0.3189 (0.3185)  time: 0.5106  data: 0.0001  max mem: 9341
[01:24:10.046084] Epoch: [412] Total time: 0:01:41 (0.5184 s / it)
[01:24:10.048724] Averaged stats: lr: 0.001235  loss: 0.3189 (0.3177)
[01:24:14.648600] {"train_lr": 0.0012373979479818925, "train_loss": 0.31773684605574, "epoch": 412}
[01:24:14.648929] [01:24:14.649036] Training epoch 412 for 0:01:45
[01:24:14.649089] [01:24:14.653523] log_dir: ./exp/debug/cifar100-LT/debug
[01:24:16.377732] Epoch: [413]  [  0/195]  eta: 0:05:35  lr: 0.001235  loss: 0.3364 (0.3364)  time: 1.7227  data: 1.2246  max mem: 9341
[01:24:26.588633] Epoch: [413]  [ 20/195]  eta: 0:01:39  lr: 0.001234  loss: 0.3015 (0.3030)  time: 0.5105  data: 0.0002  max mem: 9341
[01:24:36.805091] Epoch: [413]  [ 40/195]  eta: 0:01:23  lr: 0.001234  loss: 0.3171 (0.3124)  time: 0.5107  data: 0.0003  max mem: 9341
[01:24:47.018027] Epoch: [413]  [ 60/195]  eta: 0:01:11  lr: 0.001233  loss: 0.3111 (0.3135)  time: 0.5106  data: 0.0003  max mem: 9341
[01:24:57.271945] Epoch: [413]  [ 80/195]  eta: 0:01:00  lr: 0.001233  loss: 0.3143 (0.3140)  time: 0.5126  data: 0.0003  max mem: 9341
[01:25:07.485108] Epoch: [413]  [100/195]  eta: 0:00:49  lr: 0.001232  loss: 0.3120 (0.3143)  time: 0.5106  data: 0.0003  max mem: 9341
[01:25:17.691059] Epoch: [413]  [120/195]  eta: 0:00:39  lr: 0.001232  loss: 0.3134 (0.3147)  time: 0.5102  data: 0.0002  max mem: 9341
[01:25:27.903839] Epoch: [413]  [140/195]  eta: 0:00:28  lr: 0.001231  loss: 0.3150 (0.3144)  time: 0.5106  data: 0.0002  max mem: 9341
[01:25:38.158723] Epoch: [413]  [160/195]  eta: 0:00:18  lr: 0.001231  loss: 0.3146 (0.3148)  time: 0.5127  data: 0.0002  max mem: 9341
[01:25:48.328889] Epoch: [413]  [180/195]  eta: 0:00:07  lr: 0.001230  loss: 0.3234 (0.3153)  time: 0.5085  data: 0.0002  max mem: 9341
[01:25:55.452182] Epoch: [413]  [194/195]  eta: 0:00:00  lr: 0.001230  loss: 0.3092 (0.3151)  time: 0.5102  data: 0.0001  max mem: 9341
[01:25:55.620301] Epoch: [413] Total time: 0:01:40 (0.5178 s / it)
[01:25:55.636637] Averaged stats: lr: 0.001230  loss: 0.3092 (0.3155)
[01:26:00.327593] {"train_lr": 0.0012324396461620602, "train_loss": 0.3154979952634909, "epoch": 413}
[01:26:00.327860] [01:26:00.327964] Training epoch 413 for 0:01:45
[01:26:00.328018] [01:26:00.332495] log_dir: ./exp/debug/cifar100-LT/debug
[01:26:02.216505] Epoch: [414]  [  0/195]  eta: 0:06:07  lr: 0.001230  loss: 0.3172 (0.3172)  time: 1.8830  data: 1.3694  max mem: 9341
[01:26:12.446490] Epoch: [414]  [ 20/195]  eta: 0:01:40  lr: 0.001229  loss: 0.3090 (0.3159)  time: 0.5114  data: 0.0002  max mem: 9341
[01:26:22.661551] Epoch: [414]  [ 40/195]  eta: 0:01:24  lr: 0.001229  loss: 0.3044 (0.3136)  time: 0.5107  data: 0.0003  max mem: 9341
[01:26:32.878335] Epoch: [414]  [ 60/195]  eta: 0:01:12  lr: 0.001229  loss: 0.3110 (0.3140)  time: 0.5108  data: 0.0002  max mem: 9341
[01:26:43.132553] Epoch: [414]  [ 80/195]  eta: 0:01:00  lr: 0.001228  loss: 0.3125 (0.3138)  time: 0.5127  data: 0.0002  max mem: 9341
[01:26:53.340357] Epoch: [414]  [100/195]  eta: 0:00:49  lr: 0.001227  loss: 0.3088 (0.3144)  time: 0.5103  data: 0.0002  max mem: 9341
[01:27:03.555012] Epoch: [414]  [120/195]  eta: 0:00:39  lr: 0.001227  loss: 0.3104 (0.3144)  time: 0.5107  data: 0.0002  max mem: 9341
[01:27:13.762585] Epoch: [414]  [140/195]  eta: 0:00:28  lr: 0.001227  loss: 0.3041 (0.3138)  time: 0.5103  data: 0.0002  max mem: 9341
[01:27:24.013151] Epoch: [414]  [160/195]  eta: 0:00:18  lr: 0.001226  loss: 0.3207 (0.3146)  time: 0.5125  data: 0.0002  max mem: 9341
[01:27:34.181433] Epoch: [414]  [180/195]  eta: 0:00:07  lr: 0.001225  loss: 0.3097 (0.3150)  time: 0.5084  data: 0.0001  max mem: 9341
[01:27:41.306576] Epoch: [414]  [194/195]  eta: 0:00:00  lr: 0.001225  loss: 0.3097 (0.3143)  time: 0.5101  data: 0.0001  max mem: 9341
[01:27:41.479968] Epoch: [414] Total time: 0:01:41 (0.5187 s / it)
[01:27:41.495299] Averaged stats: lr: 0.001225  loss: 0.3097 (0.3152)
[01:27:46.298225] {"train_lr": 0.0012274807900381755, "train_loss": 0.31523646314938863, "epoch": 414}
[01:27:46.298487] [01:27:46.298611] Training epoch 414 for 0:01:45
[01:27:46.298665] [01:27:46.303191] log_dir: ./exp/debug/cifar100-LT/debug
[01:27:47.940687] Epoch: [415]  [  0/195]  eta: 0:05:18  lr: 0.001225  loss: 0.3081 (0.3081)  time: 1.6351  data: 1.1326  max mem: 9341
[01:27:58.160217] Epoch: [415]  [ 20/195]  eta: 0:01:38  lr: 0.001224  loss: 0.3032 (0.3096)  time: 0.5109  data: 0.0003  max mem: 9341
[01:28:08.377865] Epoch: [415]  [ 40/195]  eta: 0:01:23  lr: 0.001224  loss: 0.3205 (0.3153)  time: 0.5108  data: 0.0003  max mem: 9341
[01:28:18.589154] Epoch: [415]  [ 60/195]  eta: 0:01:11  lr: 0.001224  loss: 0.3074 (0.3146)  time: 0.5105  data: 0.0003  max mem: 9341
[01:28:28.845124] Epoch: [415]  [ 80/195]  eta: 0:01:00  lr: 0.001223  loss: 0.3258 (0.3173)  time: 0.5127  data: 0.0003  max mem: 9341
[01:28:39.056718] Epoch: [415]  [100/195]  eta: 0:00:49  lr: 0.001222  loss: 0.3146 (0.3168)  time: 0.5105  data: 0.0003  max mem: 9341
[01:28:49.274438] Epoch: [415]  [120/195]  eta: 0:00:39  lr: 0.001222  loss: 0.3119 (0.3160)  time: 0.5108  data: 0.0003  max mem: 9341
[01:28:59.482993] Epoch: [415]  [140/195]  eta: 0:00:28  lr: 0.001222  loss: 0.3209 (0.3164)  time: 0.5104  data: 0.0002  max mem: 9341
[01:29:09.739387] Epoch: [415]  [160/195]  eta: 0:00:18  lr: 0.001221  loss: 0.3196 (0.3172)  time: 0.5128  data: 0.0003  max mem: 9341
[01:29:19.911931] Epoch: [415]  [180/195]  eta: 0:00:07  lr: 0.001220  loss: 0.3243 (0.3176)  time: 0.5086  data: 0.0002  max mem: 9341
[01:29:27.043686] Epoch: [415]  [194/195]  eta: 0:00:00  lr: 0.001220  loss: 0.3156 (0.3171)  time: 0.5105  data: 0.0001  max mem: 9341
[01:29:27.240023] Epoch: [415] Total time: 0:01:40 (0.5176 s / it)
[01:29:27.245331] Averaged stats: lr: 0.001220  loss: 0.3156 (0.3159)
[01:29:31.950458] {"train_lr": 0.0012225214643434164, "train_loss": 0.31585702649675884, "epoch": 415}
[01:29:31.950722] [01:29:31.950830] Training epoch 415 for 0:01:45
[01:29:31.950884] [01:29:31.955378] log_dir: ./exp/debug/cifar100-LT/debug
[01:29:33.736108] Epoch: [416]  [  0/195]  eta: 0:05:47  lr: 0.001220  loss: 0.3512 (0.3512)  time: 1.7797  data: 1.2814  max mem: 9341
[01:29:43.955625] Epoch: [416]  [ 20/195]  eta: 0:01:39  lr: 0.001219  loss: 0.3190 (0.3196)  time: 0.5109  data: 0.0002  max mem: 9341
[01:29:54.168017] Epoch: [416]  [ 40/195]  eta: 0:01:23  lr: 0.001219  loss: 0.3212 (0.3208)  time: 0.5106  data: 0.0002  max mem: 9341
[01:30:04.385449] Epoch: [416]  [ 60/195]  eta: 0:01:11  lr: 0.001219  loss: 0.3254 (0.3210)  time: 0.5108  data: 0.0002  max mem: 9341
[01:30:14.646603] Epoch: [416]  [ 80/195]  eta: 0:01:00  lr: 0.001218  loss: 0.3222 (0.3204)  time: 0.5130  data: 0.0002  max mem: 9341
[01:30:24.864624] Epoch: [416]  [100/195]  eta: 0:00:49  lr: 0.001217  loss: 0.3148 (0.3216)  time: 0.5108  data: 0.0002  max mem: 9341
[01:30:35.077024] Epoch: [416]  [120/195]  eta: 0:00:39  lr: 0.001217  loss: 0.3027 (0.3197)  time: 0.5106  data: 0.0002  max mem: 9341
[01:30:45.289040] Epoch: [416]  [140/195]  eta: 0:00:28  lr: 0.001217  loss: 0.3097 (0.3187)  time: 0.5105  data: 0.0002  max mem: 9341
[01:30:55.545698] Epoch: [416]  [160/195]  eta: 0:00:18  lr: 0.001216  loss: 0.3127 (0.3178)  time: 0.5128  data: 0.0002  max mem: 9341
[01:31:05.729954] Epoch: [416]  [180/195]  eta: 0:00:07  lr: 0.001215  loss: 0.3205 (0.3177)  time: 0.5092  data: 0.0001  max mem: 9341
[01:31:12.856468] Epoch: [416]  [194/195]  eta: 0:00:00  lr: 0.001215  loss: 0.3210 (0.3176)  time: 0.5106  data: 0.0001  max mem: 9341
[01:31:13.036112] Epoch: [416] Total time: 0:01:41 (0.5184 s / it)
[01:31:13.053163] Averaged stats: lr: 0.001215  loss: 0.3210 (0.3163)
[01:31:17.943485] {"train_lr": 0.0012175617538189687, "train_loss": 0.3162851844269496, "epoch": 416}
[01:31:17.943719] [01:31:17.943801] Training epoch 416 for 0:01:45
[01:31:17.943854] [01:31:17.948296] log_dir: ./exp/debug/cifar100-LT/debug
[01:31:19.717374] Epoch: [417]  [  0/195]  eta: 0:05:44  lr: 0.001215  loss: 0.3365 (0.3365)  time: 1.7681  data: 1.2798  max mem: 9341
[01:31:29.940358] Epoch: [417]  [ 20/195]  eta: 0:01:39  lr: 0.001214  loss: 0.3228 (0.3237)  time: 0.5111  data: 0.0002  max mem: 9341
[01:31:40.155540] Epoch: [417]  [ 40/195]  eta: 0:01:23  lr: 0.001214  loss: 0.3019 (0.3153)  time: 0.5107  data: 0.0002  max mem: 9341
[01:31:50.370045] Epoch: [417]  [ 60/195]  eta: 0:01:11  lr: 0.001214  loss: 0.3093 (0.3122)  time: 0.5106  data: 0.0003  max mem: 9341
[01:32:00.624814] Epoch: [417]  [ 80/195]  eta: 0:01:00  lr: 0.001213  loss: 0.3121 (0.3125)  time: 0.5127  data: 0.0003  max mem: 9341
[01:32:10.837201] Epoch: [417]  [100/195]  eta: 0:00:49  lr: 0.001212  loss: 0.3115 (0.3126)  time: 0.5105  data: 0.0002  max mem: 9341
[01:32:21.049695] Epoch: [417]  [120/195]  eta: 0:00:39  lr: 0.001212  loss: 0.3067 (0.3113)  time: 0.5106  data: 0.0003  max mem: 9341
[01:32:31.262770] Epoch: [417]  [140/195]  eta: 0:00:28  lr: 0.001212  loss: 0.3277 (0.3136)  time: 0.5106  data: 0.0002  max mem: 9341
[01:32:41.521826] Epoch: [417]  [160/195]  eta: 0:00:18  lr: 0.001211  loss: 0.2958 (0.3114)  time: 0.5129  data: 0.0002  max mem: 9341
[01:32:51.697708] Epoch: [417]  [180/195]  eta: 0:00:07  lr: 0.001210  loss: 0.3166 (0.3114)  time: 0.5087  data: 0.0001  max mem: 9341
[01:32:58.828624] Epoch: [417]  [194/195]  eta: 0:00:00  lr: 0.001210  loss: 0.3047 (0.3115)  time: 0.5106  data: 0.0001  max mem: 9341
[01:32:59.020286] Epoch: [417] Total time: 0:01:41 (0.5183 s / it)
[01:32:59.025131] Averaged stats: lr: 0.001210  loss: 0.3047 (0.3153)
[01:33:03.730705] {"train_lr": 0.0012126017432126, "train_loss": 0.31531271464549576, "epoch": 417}
[01:33:03.731107] [01:33:03.731219] Training epoch 417 for 0:01:45
[01:33:03.731274] [01:33:03.735822] log_dir: ./exp/debug/cifar100-LT/debug
[01:33:05.506968] Epoch: [418]  [  0/195]  eta: 0:05:45  lr: 0.001210  loss: 0.3244 (0.3244)  time: 1.7696  data: 1.2647  max mem: 9341
[01:33:15.741186] Epoch: [418]  [ 20/195]  eta: 0:01:40  lr: 0.001210  loss: 0.3035 (0.3114)  time: 0.5116  data: 0.0002  max mem: 9341
[01:33:25.962059] Epoch: [418]  [ 40/195]  eta: 0:01:24  lr: 0.001209  loss: 0.3142 (0.3152)  time: 0.5110  data: 0.0003  max mem: 9341
[01:33:36.175069] Epoch: [418]  [ 60/195]  eta: 0:01:11  lr: 0.001209  loss: 0.3248 (0.3181)  time: 0.5106  data: 0.0003  max mem: 9341
[01:33:46.453236] Epoch: [418]  [ 80/195]  eta: 0:01:00  lr: 0.001208  loss: 0.3153 (0.3179)  time: 0.5138  data: 0.0003  max mem: 9341
[01:33:56.670119] Epoch: [418]  [100/195]  eta: 0:00:49  lr: 0.001207  loss: 0.3278 (0.3196)  time: 0.5108  data: 0.0002  max mem: 9341
[01:34:06.886425] Epoch: [418]  [120/195]  eta: 0:00:39  lr: 0.001207  loss: 0.3122 (0.3191)  time: 0.5108  data: 0.0003  max mem: 9341
[01:34:17.100368] Epoch: [418]  [140/195]  eta: 0:00:28  lr: 0.001207  loss: 0.3021 (0.3170)  time: 0.5106  data: 0.0003  max mem: 9341
[01:34:27.352644] Epoch: [418]  [160/195]  eta: 0:00:18  lr: 0.001206  loss: 0.3175 (0.3170)  time: 0.5126  data: 0.0003  max mem: 9341
[01:34:37.518796] Epoch: [418]  [180/195]  eta: 0:00:07  lr: 0.001205  loss: 0.3104 (0.3171)  time: 0.5083  data: 0.0001  max mem: 9341
[01:34:44.651676] Epoch: [418]  [194/195]  eta: 0:00:00  lr: 0.001205  loss: 0.3087 (0.3163)  time: 0.5105  data: 0.0001  max mem: 9341
[01:34:44.830875] Epoch: [418] Total time: 0:01:41 (0.5184 s / it)
[01:34:44.841459] Averaged stats: lr: 0.001205  loss: 0.3087 (0.3143)
[01:34:49.589886] {"train_lr": 0.0012076415172772123, "train_loss": 0.3142825876482022, "epoch": 418}
[01:34:49.598314] [01:34:49.598425] Training epoch 418 for 0:01:45
[01:34:49.598480] [01:34:49.602853] log_dir: ./exp/debug/cifar100-LT/debug
[01:34:51.529844] Epoch: [419]  [  0/195]  eta: 0:06:15  lr: 0.001205  loss: 0.2960 (0.2960)  time: 1.9260  data: 1.4387  max mem: 9341
[01:35:01.736659] Epoch: [419]  [ 20/195]  eta: 0:01:41  lr: 0.001205  loss: 0.3096 (0.3131)  time: 0.5103  data: 0.0003  max mem: 9341
[01:35:11.949725] Epoch: [419]  [ 40/195]  eta: 0:01:24  lr: 0.001204  loss: 0.3181 (0.3134)  time: 0.5106  data: 0.0002  max mem: 9341
[01:35:22.170236] Epoch: [419]  [ 60/195]  eta: 0:01:12  lr: 0.001204  loss: 0.3058 (0.3110)  time: 0.5110  data: 0.0003  max mem: 9341
[01:35:32.425702] Epoch: [419]  [ 80/195]  eta: 0:01:00  lr: 0.001203  loss: 0.3016 (0.3110)  time: 0.5127  data: 0.0003  max mem: 9341
[01:35:42.642774] Epoch: [419]  [100/195]  eta: 0:00:49  lr: 0.001203  loss: 0.3074 (0.3117)  time: 0.5108  data: 0.0002  max mem: 9341
[01:35:52.882047] Epoch: [419]  [120/195]  eta: 0:00:39  lr: 0.001202  loss: 0.3151 (0.3122)  time: 0.5119  data: 0.0003  max mem: 9341
[01:36:03.121483] Epoch: [419]  [140/195]  eta: 0:00:28  lr: 0.001202  loss: 0.3121 (0.3114)  time: 0.5119  data: 0.0002  max mem: 9341
[01:36:13.429839] Epoch: [419]  [160/195]  eta: 0:00:18  lr: 0.001201  loss: 0.3155 (0.3121)  time: 0.5154  data: 0.0002  max mem: 9341
[01:36:23.630239] Epoch: [419]  [180/195]  eta: 0:00:07  lr: 0.001200  loss: 0.3140 (0.3123)  time: 0.5100  data: 0.0002  max mem: 9341
[01:36:30.788617] Epoch: [419]  [194/195]  eta: 0:00:00  lr: 0.001200  loss: 0.3185 (0.3128)  time: 0.5130  data: 0.0001  max mem: 9341
[01:36:30.966481] Epoch: [419] Total time: 0:01:41 (0.5198 s / it)
[01:36:30.998356] Averaged stats: lr: 0.001200  loss: 0.3185 (0.3147)
[01:36:35.750144] {"train_lr": 0.0012026811607693797, "train_loss": 0.31468812985680045, "epoch": 419}
[01:36:35.750536] [01:36:35.750642] Training epoch 419 for 0:01:46
[01:36:35.750695] [01:36:35.755848] log_dir: ./exp/debug/cifar100-LT/debug
[01:36:37.515870] Epoch: [420]  [  0/195]  eta: 0:05:42  lr: 0.001200  loss: 0.3312 (0.3312)  time: 1.7574  data: 1.2568  max mem: 9341
[01:36:47.720899] Epoch: [420]  [ 20/195]  eta: 0:01:39  lr: 0.001200  loss: 0.3220 (0.3177)  time: 0.5102  data: 0.0002  max mem: 9341
[01:36:57.956396] Epoch: [420]  [ 40/195]  eta: 0:01:23  lr: 0.001199  loss: 0.3136 (0.3186)  time: 0.5117  data: 0.0003  max mem: 9341
[01:37:08.173373] Epoch: [420]  [ 60/195]  eta: 0:01:11  lr: 0.001199  loss: 0.3134 (0.3177)  time: 0.5108  data: 0.0002  max mem: 9341
[01:37:18.430231] Epoch: [420]  [ 80/195]  eta: 0:01:00  lr: 0.001198  loss: 0.3085 (0.3160)  time: 0.5128  data: 0.0003  max mem: 9341
[01:37:28.646349] Epoch: [420]  [100/195]  eta: 0:00:49  lr: 0.001198  loss: 0.3109 (0.3147)  time: 0.5108  data: 0.0002  max mem: 9341
[01:37:38.864403] Epoch: [420]  [120/195]  eta: 0:00:39  lr: 0.001197  loss: 0.3308 (0.3173)  time: 0.5108  data: 0.0003  max mem: 9341
[01:37:49.075145] Epoch: [420]  [140/195]  eta: 0:00:28  lr: 0.001197  loss: 0.3160 (0.3170)  time: 0.5105  data: 0.0002  max mem: 9341
[01:37:59.332629] Epoch: [420]  [160/195]  eta: 0:00:18  lr: 0.001196  loss: 0.3080 (0.3167)  time: 0.5128  data: 0.0002  max mem: 9341
[01:38:09.502209] Epoch: [420]  [180/195]  eta: 0:00:07  lr: 0.001196  loss: 0.3181 (0.3171)  time: 0.5084  data: 0.0002  max mem: 9341
[01:38:16.638887] Epoch: [420]  [194/195]  eta: 0:00:00  lr: 0.001195  loss: 0.3098 (0.3168)  time: 0.5108  data: 0.0001  max mem: 9341
[01:38:16.809821] Epoch: [420] Total time: 0:01:41 (0.5182 s / it)
[01:38:16.827425] Averaged stats: lr: 0.001195  loss: 0.3098 (0.3157)
[01:38:21.540247] {"train_lr": 0.001197720758447903, "train_loss": 0.31570498211643633, "epoch": 420}
[01:38:21.540510] [01:38:21.540608] Training epoch 420 for 0:01:45
[01:38:21.540663] [01:38:21.545091] log_dir: ./exp/debug/cifar100-LT/debug
[01:38:23.414403] Epoch: [421]  [  0/195]  eta: 0:06:04  lr: 0.001195  loss: 0.3374 (0.3374)  time: 1.8681  data: 1.3699  max mem: 9341
[01:38:33.649344] Epoch: [421]  [ 20/195]  eta: 0:01:40  lr: 0.001195  loss: 0.3150 (0.3194)  time: 0.5117  data: 0.0002  max mem: 9341
[01:38:43.859940] Epoch: [421]  [ 40/195]  eta: 0:01:24  lr: 0.001194  loss: 0.3206 (0.3194)  time: 0.5105  data: 0.0002  max mem: 9341
[01:38:54.083545] Epoch: [421]  [ 60/195]  eta: 0:01:12  lr: 0.001194  loss: 0.3182 (0.3189)  time: 0.5111  data: 0.0002  max mem: 9341
[01:39:04.353364] Epoch: [421]  [ 80/195]  eta: 0:01:00  lr: 0.001193  loss: 0.3182 (0.3193)  time: 0.5134  data: 0.0002  max mem: 9341
[01:39:14.578012] Epoch: [421]  [100/195]  eta: 0:00:49  lr: 0.001193  loss: 0.3102 (0.3185)  time: 0.5112  data: 0.0002  max mem: 9341
[01:39:24.801090] Epoch: [421]  [120/195]  eta: 0:00:39  lr: 0.001192  loss: 0.3016 (0.3168)  time: 0.5111  data: 0.0002  max mem: 9341
[01:39:35.018092] Epoch: [421]  [140/195]  eta: 0:00:28  lr: 0.001192  loss: 0.3164 (0.3162)  time: 0.5108  data: 0.0002  max mem: 9341
[01:39:45.281364] Epoch: [421]  [160/195]  eta: 0:00:18  lr: 0.001191  loss: 0.2999 (0.3157)  time: 0.5131  data: 0.0002  max mem: 9341
[01:39:55.455288] Epoch: [421]  [180/195]  eta: 0:00:07  lr: 0.001191  loss: 0.3219 (0.3162)  time: 0.5086  data: 0.0001  max mem: 9341
[01:40:02.583937] Epoch: [421]  [194/195]  eta: 0:00:00  lr: 0.001190  loss: 0.3261 (0.3167)  time: 0.5102  data: 0.0001  max mem: 9341
[01:40:02.751700] Epoch: [421] Total time: 0:01:41 (0.5190 s / it)
[01:40:02.768463] Averaged stats: lr: 0.001190  loss: 0.3261 (0.3155)
[01:40:07.440616] {"train_lr": 0.0011927603950723767, "train_loss": 0.3155178221372458, "epoch": 421}
[01:40:07.440884] [01:40:07.440977] Training epoch 421 for 0:01:45
[01:40:07.441032] [01:40:07.446032] log_dir: ./exp/debug/cifar100-LT/debug
[01:40:09.038578] Epoch: [422]  [  0/195]  eta: 0:05:10  lr: 0.001190  loss: 0.3196 (0.3196)  time: 1.5914  data: 1.0758  max mem: 9341
[01:40:19.274353] Epoch: [422]  [ 20/195]  eta: 0:01:38  lr: 0.001190  loss: 0.3153 (0.3149)  time: 0.5117  data: 0.0002  max mem: 9341
[01:40:29.492317] Epoch: [422]  [ 40/195]  eta: 0:01:23  lr: 0.001189  loss: 0.3160 (0.3173)  time: 0.5108  data: 0.0002  max mem: 9341
[01:40:39.711601] Epoch: [422]  [ 60/195]  eta: 0:01:11  lr: 0.001189  loss: 0.3126 (0.3160)  time: 0.5109  data: 0.0002  max mem: 9341
[01:40:49.973389] Epoch: [422]  [ 80/195]  eta: 0:01:00  lr: 0.001188  loss: 0.3116 (0.3157)  time: 0.5130  data: 0.0002  max mem: 9341
[01:41:00.185897] Epoch: [422]  [100/195]  eta: 0:00:49  lr: 0.001188  loss: 0.3004 (0.3149)  time: 0.5106  data: 0.0002  max mem: 9341
[01:41:10.397579] Epoch: [422]  [120/195]  eta: 0:00:39  lr: 0.001187  loss: 0.3178 (0.3160)  time: 0.5105  data: 0.0002  max mem: 9341
[01:41:20.614589] Epoch: [422]  [140/195]  eta: 0:00:28  lr: 0.001187  loss: 0.3036 (0.3154)  time: 0.5108  data: 0.0002  max mem: 9341
[01:41:30.917446] Epoch: [422]  [160/195]  eta: 0:00:18  lr: 0.001186  loss: 0.3136 (0.3149)  time: 0.5151  data: 0.0003  max mem: 9341
[01:41:41.108729] Epoch: [422]  [180/195]  eta: 0:00:07  lr: 0.001186  loss: 0.3170 (0.3154)  time: 0.5095  data: 0.0002  max mem: 9341
[01:41:48.260838] Epoch: [422]  [194/195]  eta: 0:00:00  lr: 0.001185  loss: 0.3148 (0.3159)  time: 0.5126  data: 0.0001  max mem: 9341
[01:41:48.435130] Epoch: [422] Total time: 0:01:40 (0.5179 s / it)
[01:41:48.447475] Averaged stats: lr: 0.001185  loss: 0.3148 (0.3163)
[01:41:53.171382] {"train_lr": 0.0011878001554017257, "train_loss": 0.3163148018412101, "epoch": 422}
[01:41:53.171713] [01:41:53.171817] Training epoch 422 for 0:01:45
[01:41:53.171873] [01:41:53.176399] log_dir: ./exp/debug/cifar100-LT/debug
[01:41:55.061664] Epoch: [423]  [  0/195]  eta: 0:06:07  lr: 0.001185  loss: 0.3309 (0.3309)  time: 1.8837  data: 1.3850  max mem: 9341
[01:42:05.291564] Epoch: [423]  [ 20/195]  eta: 0:01:40  lr: 0.001185  loss: 0.3078 (0.3111)  time: 0.5114  data: 0.0002  max mem: 9341
[01:42:15.505516] Epoch: [423]  [ 40/195]  eta: 0:01:24  lr: 0.001184  loss: 0.3195 (0.3154)  time: 0.5106  data: 0.0002  max mem: 9341
[01:42:25.720734] Epoch: [423]  [ 60/195]  eta: 0:01:12  lr: 0.001184  loss: 0.3165 (0.3172)  time: 0.5107  data: 0.0002  max mem: 9341
[01:42:35.975125] Epoch: [423]  [ 80/195]  eta: 0:01:00  lr: 0.001183  loss: 0.3037 (0.3153)  time: 0.5127  data: 0.0002  max mem: 9341
[01:42:46.188499] Epoch: [423]  [100/195]  eta: 0:00:49  lr: 0.001183  loss: 0.3195 (0.3167)  time: 0.5106  data: 0.0002  max mem: 9341
[01:42:56.403348] Epoch: [423]  [120/195]  eta: 0:00:39  lr: 0.001182  loss: 0.3144 (0.3164)  time: 0.5107  data: 0.0002  max mem: 9341
[01:43:06.619916] Epoch: [423]  [140/195]  eta: 0:00:28  lr: 0.001182  loss: 0.3075 (0.3160)  time: 0.5108  data: 0.0002  max mem: 9341
[01:43:16.877070] Epoch: [423]  [160/195]  eta: 0:00:18  lr: 0.001181  loss: 0.3183 (0.3163)  time: 0.5128  data: 0.0002  max mem: 9341
[01:43:27.048031] Epoch: [423]  [180/195]  eta: 0:00:07  lr: 0.001181  loss: 0.3156 (0.3163)  time: 0.5085  data: 0.0001  max mem: 9341
[01:43:34.179898] Epoch: [423]  [194/195]  eta: 0:00:00  lr: 0.001180  loss: 0.3111 (0.3157)  time: 0.5106  data: 0.0001  max mem: 9341
[01:43:34.354479] Epoch: [423] Total time: 0:01:41 (0.5189 s / it)
[01:43:34.372672] Averaged stats: lr: 0.001180  loss: 0.3111 (0.3143)
[01:43:39.088974] {"train_lr": 0.0011828401241927593, "train_loss": 0.31431411968973966, "epoch": 423}
[01:43:39.089248] [01:43:39.089338] Training epoch 423 for 0:01:45
[01:43:39.089400] [01:43:39.094443] log_dir: ./exp/debug/cifar100-LT/debug
[01:43:40.883652] Epoch: [424]  [  0/195]  eta: 0:05:48  lr: 0.001180  loss: 0.3171 (0.3171)  time: 1.7882  data: 1.2831  max mem: 9341
[01:43:51.116474] Epoch: [424]  [ 20/195]  eta: 0:01:40  lr: 0.001180  loss: 0.3146 (0.3188)  time: 0.5116  data: 0.0002  max mem: 9341
[01:44:01.327596] Epoch: [424]  [ 40/195]  eta: 0:01:24  lr: 0.001179  loss: 0.3103 (0.3139)  time: 0.5105  data: 0.0002  max mem: 9341
[01:44:11.543799] Epoch: [424]  [ 60/195]  eta: 0:01:11  lr: 0.001179  loss: 0.3117 (0.3137)  time: 0.5108  data: 0.0002  max mem: 9341
[01:44:21.807990] Epoch: [424]  [ 80/195]  eta: 0:01:00  lr: 0.001178  loss: 0.3136 (0.3140)  time: 0.5131  data: 0.0002  max mem: 9341
[01:44:32.036296] Epoch: [424]  [100/195]  eta: 0:00:49  lr: 0.001178  loss: 0.3176 (0.3148)  time: 0.5114  data: 0.0002  max mem: 9341
[01:44:42.246337] Epoch: [424]  [120/195]  eta: 0:00:39  lr: 0.001177  loss: 0.3238 (0.3163)  time: 0.5104  data: 0.0002  max mem: 9341
[01:44:52.458381] Epoch: [424]  [140/195]  eta: 0:00:28  lr: 0.001177  loss: 0.3114 (0.3160)  time: 0.5105  data: 0.0002  max mem: 9341
[01:45:02.713312] Epoch: [424]  [160/195]  eta: 0:00:18  lr: 0.001176  loss: 0.3061 (0.3153)  time: 0.5127  data: 0.0002  max mem: 9341
[01:45:12.887608] Epoch: [424]  [180/195]  eta: 0:00:07  lr: 0.001176  loss: 0.3136 (0.3153)  time: 0.5087  data: 0.0001  max mem: 9341
[01:45:20.018124] Epoch: [424]  [194/195]  eta: 0:00:00  lr: 0.001175  loss: 0.3129 (0.3151)  time: 0.5106  data: 0.0001  max mem: 9341
[01:45:20.178761] Epoch: [424] Total time: 0:01:41 (0.5184 s / it)
[01:45:20.199043] Averaged stats: lr: 0.001175  loss: 0.3129 (0.3147)
[01:45:24.895068] {"train_lr": 0.0011778803861987209, "train_loss": 0.31469968381600505, "epoch": 424}
[01:45:24.895330] [01:45:24.895413] Training epoch 424 for 0:01:45
[01:45:24.895467] [01:45:24.899925] log_dir: ./exp/debug/cifar100-LT/debug
[01:45:26.567077] Epoch: [425]  [  0/195]  eta: 0:05:24  lr: 0.001175  loss: 0.3250 (0.3250)  time: 1.6660  data: 1.1527  max mem: 9341
[01:45:36.798530] Epoch: [425]  [ 20/195]  eta: 0:01:39  lr: 0.001175  loss: 0.3132 (0.3141)  time: 0.5115  data: 0.0002  max mem: 9341
[01:45:47.011309] Epoch: [425]  [ 40/195]  eta: 0:01:23  lr: 0.001174  loss: 0.3122 (0.3152)  time: 0.5106  data: 0.0002  max mem: 9341
[01:45:57.227339] Epoch: [425]  [ 60/195]  eta: 0:01:11  lr: 0.001174  loss: 0.3123 (0.3139)  time: 0.5107  data: 0.0003  max mem: 9341
[01:46:07.483139] Epoch: [425]  [ 80/195]  eta: 0:01:00  lr: 0.001173  loss: 0.3018 (0.3127)  time: 0.5127  data: 0.0003  max mem: 9341
[01:46:17.700221] Epoch: [425]  [100/195]  eta: 0:00:49  lr: 0.001173  loss: 0.3079 (0.3122)  time: 0.5108  data: 0.0002  max mem: 9341
[01:46:27.937606] Epoch: [425]  [120/195]  eta: 0:00:39  lr: 0.001172  loss: 0.3104 (0.3123)  time: 0.5118  data: 0.0002  max mem: 9341
[01:46:38.172790] Epoch: [425]  [140/195]  eta: 0:00:28  lr: 0.001172  loss: 0.3075 (0.3121)  time: 0.5117  data: 0.0003  max mem: 9341
[01:46:48.461521] Epoch: [425]  [160/195]  eta: 0:00:18  lr: 0.001171  loss: 0.3005 (0.3116)  time: 0.5144  data: 0.0002  max mem: 9341
[01:46:58.629725] Epoch: [425]  [180/195]  eta: 0:00:07  lr: 0.001171  loss: 0.3068 (0.3111)  time: 0.5084  data: 0.0001  max mem: 9341
[01:47:05.759436] Epoch: [425]  [194/195]  eta: 0:00:00  lr: 0.001170  loss: 0.3163 (0.3119)  time: 0.5102  data: 0.0001  max mem: 9341
[01:47:05.929681] Epoch: [425] Total time: 0:01:41 (0.5181 s / it)
[01:47:05.939254] Averaged stats: lr: 0.001170  loss: 0.3163 (0.3131)
[01:47:10.658702] {"train_lr": 0.0011729210261678565, "train_loss": 0.31305128414279376, "epoch": 425}
[01:47:10.659054] [01:47:10.659164] Training epoch 425 for 0:01:45
[01:47:10.659218] [01:47:10.663633] log_dir: ./exp/debug/cifar100-LT/debug
[01:47:12.346981] Epoch: [426]  [  0/195]  eta: 0:05:27  lr: 0.001170  loss: 0.3178 (0.3178)  time: 1.6813  data: 1.1546  max mem: 9341
[01:47:22.564023] Epoch: [426]  [ 20/195]  eta: 0:01:39  lr: 0.001170  loss: 0.3220 (0.3236)  time: 0.5108  data: 0.0003  max mem: 9341
[01:47:32.774242] Epoch: [426]  [ 40/195]  eta: 0:01:23  lr: 0.001169  loss: 0.3175 (0.3207)  time: 0.5104  data: 0.0002  max mem: 9341
[01:47:42.988386] Epoch: [426]  [ 60/195]  eta: 0:01:11  lr: 0.001169  loss: 0.3234 (0.3236)  time: 0.5106  data: 0.0002  max mem: 9341
[01:47:53.248857] Epoch: [426]  [ 80/195]  eta: 0:01:00  lr: 0.001168  loss: 0.3187 (0.3227)  time: 0.5130  data: 0.0003  max mem: 9341
[01:48:03.467152] Epoch: [426]  [100/195]  eta: 0:00:49  lr: 0.001168  loss: 0.3204 (0.3231)  time: 0.5109  data: 0.0002  max mem: 9341
[01:48:13.679968] Epoch: [426]  [120/195]  eta: 0:00:39  lr: 0.001167  loss: 0.3211 (0.3228)  time: 0.5106  data: 0.0002  max mem: 9341
[01:48:23.895946] Epoch: [426]  [140/195]  eta: 0:00:28  lr: 0.001167  loss: 0.3151 (0.3224)  time: 0.5107  data: 0.0003  max mem: 9341
[01:48:34.147094] Epoch: [426]  [160/195]  eta: 0:00:18  lr: 0.001166  loss: 0.3116 (0.3221)  time: 0.5125  data: 0.0003  max mem: 9341
[01:48:44.314013] Epoch: [426]  [180/195]  eta: 0:00:07  lr: 0.001166  loss: 0.3163 (0.3212)  time: 0.5083  data: 0.0002  max mem: 9341
[01:48:51.445266] Epoch: [426]  [194/195]  eta: 0:00:00  lr: 0.001165  loss: 0.2973 (0.3203)  time: 0.5105  data: 0.0001  max mem: 9341
[01:48:51.625658] Epoch: [426] Total time: 0:01:40 (0.5178 s / it)
[01:48:51.630821] Averaged stats: lr: 0.001165  loss: 0.2973 (0.3191)
[01:48:56.265248] {"train_lr": 0.001167962128841936, "train_loss": 0.31912283251682916, "epoch": 426}
[01:48:56.265563] [01:48:56.265665] Training epoch 426 for 0:01:45
[01:48:56.265720] [01:48:56.270347] log_dir: ./exp/debug/cifar100-LT/debug
[01:48:58.021358] Epoch: [427]  [  0/195]  eta: 0:05:41  lr: 0.001165  loss: 0.2926 (0.2926)  time: 1.7495  data: 1.2452  max mem: 9341
[01:49:08.341698] Epoch: [427]  [ 20/195]  eta: 0:01:40  lr: 0.001165  loss: 0.3144 (0.3105)  time: 0.5160  data: 0.0002  max mem: 9341
[01:49:18.560175] Epoch: [427]  [ 40/195]  eta: 0:01:24  lr: 0.001164  loss: 0.3155 (0.3138)  time: 0.5109  data: 0.0002  max mem: 9341
[01:49:28.781881] Epoch: [427]  [ 60/195]  eta: 0:01:11  lr: 0.001164  loss: 0.3203 (0.3170)  time: 0.5110  data: 0.0002  max mem: 9341
[01:49:39.041777] Epoch: [427]  [ 80/195]  eta: 0:01:00  lr: 0.001163  loss: 0.3145 (0.3174)  time: 0.5129  data: 0.0003  max mem: 9341
[01:49:49.252382] Epoch: [427]  [100/195]  eta: 0:00:49  lr: 0.001163  loss: 0.3094 (0.3164)  time: 0.5105  data: 0.0002  max mem: 9341
[01:49:59.467150] Epoch: [427]  [120/195]  eta: 0:00:39  lr: 0.001162  loss: 0.3129 (0.3163)  time: 0.5107  data: 0.0003  max mem: 9341
[01:50:09.679180] Epoch: [427]  [140/195]  eta: 0:00:28  lr: 0.001162  loss: 0.3123 (0.3156)  time: 0.5105  data: 0.0003  max mem: 9341
[01:50:19.933152] Epoch: [427]  [160/195]  eta: 0:00:18  lr: 0.001161  loss: 0.3046 (0.3155)  time: 0.5126  data: 0.0002  max mem: 9341
[01:50:30.100834] Epoch: [427]  [180/195]  eta: 0:00:07  lr: 0.001161  loss: 0.3064 (0.3149)  time: 0.5083  data: 0.0002  max mem: 9341
[01:50:37.230847] Epoch: [427]  [194/195]  eta: 0:00:00  lr: 0.001160  loss: 0.3120 (0.3149)  time: 0.5102  data: 0.0001  max mem: 9341
[01:50:37.415162] Epoch: [427] Total time: 0:01:41 (0.5187 s / it)
[01:50:37.416016] Averaged stats: lr: 0.001160  loss: 0.3120 (0.3144)
[01:50:42.117405] {"train_lr": 0.0011630037789548393, "train_loss": 0.3144344273094948, "epoch": 427}
[01:50:42.117668] [01:50:42.117781] Training epoch 427 for 0:01:45
[01:50:42.117834] [01:50:42.122251] log_dir: ./exp/debug/cifar100-LT/debug
[01:50:43.941102] Epoch: [428]  [  0/195]  eta: 0:05:54  lr: 0.001160  loss: 0.3200 (0.3200)  time: 1.8173  data: 1.3227  max mem: 9341
[01:50:54.167645] Epoch: [428]  [ 20/195]  eta: 0:01:40  lr: 0.001160  loss: 0.3055 (0.3124)  time: 0.5113  data: 0.0002  max mem: 9341
[01:51:04.385627] Epoch: [428]  [ 40/195]  eta: 0:01:24  lr: 0.001160  loss: 0.3185 (0.3160)  time: 0.5108  data: 0.0002  max mem: 9341
[01:51:14.601767] Epoch: [428]  [ 60/195]  eta: 0:01:11  lr: 0.001159  loss: 0.3080 (0.3136)  time: 0.5107  data: 0.0003  max mem: 9341
[01:51:24.861264] Epoch: [428]  [ 80/195]  eta: 0:01:00  lr: 0.001158  loss: 0.3140 (0.3132)  time: 0.5129  data: 0.0002  max mem: 9341
[01:51:35.070908] Epoch: [428]  [100/195]  eta: 0:00:49  lr: 0.001158  loss: 0.3095 (0.3131)  time: 0.5104  data: 0.0002  max mem: 9341
[01:51:45.281019] Epoch: [428]  [120/195]  eta: 0:00:39  lr: 0.001157  loss: 0.3078 (0.3125)  time: 0.5105  data: 0.0002  max mem: 9341
[01:51:55.495299] Epoch: [428]  [140/195]  eta: 0:00:28  lr: 0.001157  loss: 0.3043 (0.3112)  time: 0.5107  data: 0.0002  max mem: 9341
[01:52:05.756998] Epoch: [428]  [160/195]  eta: 0:00:18  lr: 0.001156  loss: 0.3001 (0.3100)  time: 0.5130  data: 0.0002  max mem: 9341
[01:52:15.928108] Epoch: [428]  [180/195]  eta: 0:00:07  lr: 0.001156  loss: 0.3003 (0.3100)  time: 0.5085  data: 0.0002  max mem: 9341
[01:52:23.058315] Epoch: [428]  [194/195]  eta: 0:00:00  lr: 0.001155  loss: 0.3139 (0.3105)  time: 0.5103  data: 0.0001  max mem: 9341
[01:52:23.233111] Epoch: [428] Total time: 0:01:41 (0.5185 s / it)
[01:52:23.241821] Averaged stats: lr: 0.001155  loss: 0.3139 (0.3113)
[01:52:27.922786] {"train_lr": 0.0011580460612310864, "train_loss": 0.3112580453356107, "epoch": 428}
[01:52:27.923059] [01:52:27.923168] Training epoch 428 for 0:01:45
[01:52:27.923229] [01:52:27.927725] log_dir: ./exp/debug/cifar100-LT/debug
[01:52:29.612624] Epoch: [429]  [  0/195]  eta: 0:05:28  lr: 0.001155  loss: 0.3134 (0.3134)  time: 1.6840  data: 1.1868  max mem: 9341
[01:52:39.830029] Epoch: [429]  [ 20/195]  eta: 0:01:39  lr: 0.001155  loss: 0.3219 (0.3181)  time: 0.5108  data: 0.0002  max mem: 9341
[01:52:50.050337] Epoch: [429]  [ 40/195]  eta: 0:01:23  lr: 0.001155  loss: 0.3101 (0.3150)  time: 0.5110  data: 0.0002  max mem: 9341
[01:53:00.266776] Epoch: [429]  [ 60/195]  eta: 0:01:11  lr: 0.001154  loss: 0.3110 (0.3162)  time: 0.5108  data: 0.0002  max mem: 9341
[01:53:10.526537] Epoch: [429]  [ 80/195]  eta: 0:01:00  lr: 0.001153  loss: 0.3113 (0.3143)  time: 0.5129  data: 0.0002  max mem: 9341
[01:53:20.742314] Epoch: [429]  [100/195]  eta: 0:00:49  lr: 0.001153  loss: 0.3022 (0.3133)  time: 0.5107  data: 0.0003  max mem: 9341
[01:53:30.958874] Epoch: [429]  [120/195]  eta: 0:00:39  lr: 0.001153  loss: 0.3063 (0.3131)  time: 0.5108  data: 0.0002  max mem: 9341
[01:53:41.171111] Epoch: [429]  [140/195]  eta: 0:00:28  lr: 0.001152  loss: 0.3088 (0.3130)  time: 0.5106  data: 0.0003  max mem: 9341
[01:53:51.426672] Epoch: [429]  [160/195]  eta: 0:00:18  lr: 0.001151  loss: 0.3078 (0.3129)  time: 0.5127  data: 0.0002  max mem: 9341
[01:54:01.598430] Epoch: [429]  [180/195]  eta: 0:00:07  lr: 0.001151  loss: 0.3068 (0.3124)  time: 0.5085  data: 0.0001  max mem: 9341
[01:54:08.725409] Epoch: [429]  [194/195]  eta: 0:00:00  lr: 0.001150  loss: 0.3104 (0.3126)  time: 0.5103  data: 0.0001  max mem: 9341
[01:54:08.915608] Epoch: [429] Total time: 0:01:40 (0.5179 s / it)
[01:54:08.919790] Averaged stats: lr: 0.001150  loss: 0.3104 (0.3127)
[01:54:13.649405] {"train_lr": 0.0011530890603843951, "train_loss": 0.31272738021917834, "epoch": 429}
[01:54:13.649675] [01:54:13.649760] Training epoch 429 for 0:01:45
[01:54:13.649813] [01:54:13.654285] log_dir: ./exp/debug/cifar100-LT/debug
[01:54:15.515705] Epoch: [430]  [  0/195]  eta: 0:06:02  lr: 0.001150  loss: 0.3257 (0.3257)  time: 1.8605  data: 1.3544  max mem: 9341
[01:54:25.744681] Epoch: [430]  [ 20/195]  eta: 0:01:40  lr: 0.001150  loss: 0.3154 (0.3159)  time: 0.5114  data: 0.0002  max mem: 9341
[01:54:35.991397] Epoch: [430]  [ 40/195]  eta: 0:01:24  lr: 0.001150  loss: 0.3157 (0.3166)  time: 0.5122  data: 0.0002  max mem: 9341
[01:54:46.205119] Epoch: [430]  [ 60/195]  eta: 0:01:12  lr: 0.001149  loss: 0.3056 (0.3132)  time: 0.5106  data: 0.0002  max mem: 9341
[01:54:56.461088] Epoch: [430]  [ 80/195]  eta: 0:01:00  lr: 0.001148  loss: 0.3190 (0.3140)  time: 0.5127  data: 0.0003  max mem: 9341
[01:55:06.672380] Epoch: [430]  [100/195]  eta: 0:00:49  lr: 0.001148  loss: 0.3135 (0.3146)  time: 0.5105  data: 0.0002  max mem: 9341
[01:55:16.882896] Epoch: [430]  [120/195]  eta: 0:00:39  lr: 0.001148  loss: 0.3126 (0.3142)  time: 0.5105  data: 0.0003  max mem: 9341
[01:55:27.097300] Epoch: [430]  [140/195]  eta: 0:00:28  lr: 0.001147  loss: 0.3053 (0.3130)  time: 0.5106  data: 0.0002  max mem: 9341
[01:55:37.359851] Epoch: [430]  [160/195]  eta: 0:00:18  lr: 0.001146  loss: 0.3064 (0.3129)  time: 0.5130  data: 0.0003  max mem: 9341
[01:55:47.542861] Epoch: [430]  [180/195]  eta: 0:00:07  lr: 0.001146  loss: 0.3038 (0.3122)  time: 0.5091  data: 0.0002  max mem: 9341
[01:55:54.680195] Epoch: [430]  [194/195]  eta: 0:00:00  lr: 0.001146  loss: 0.3116 (0.3125)  time: 0.5114  data: 0.0001  max mem: 9341
[01:55:54.841756] Epoch: [430] Total time: 0:01:41 (0.5189 s / it)
[01:55:54.860352] Averaged stats: lr: 0.001146  loss: 0.3116 (0.3134)
[01:55:59.537078] {"train_lr": 0.0011481328611162261, "train_loss": 0.31344156412359997, "epoch": 430}
[01:55:59.537338] [01:55:59.537442] Training epoch 430 for 0:01:45
[01:55:59.537496] [01:55:59.541898] log_dir: ./exp/debug/cifar100-LT/debug
[01:56:01.217118] Epoch: [431]  [  0/195]  eta: 0:05:26  lr: 0.001145  loss: 0.3069 (0.3069)  time: 1.6741  data: 1.1704  max mem: 9341
[01:56:11.434055] Epoch: [431]  [ 20/195]  eta: 0:01:39  lr: 0.001145  loss: 0.3222 (0.3241)  time: 0.5107  data: 0.0003  max mem: 9341
[01:56:21.651332] Epoch: [431]  [ 40/195]  eta: 0:01:23  lr: 0.001145  loss: 0.3167 (0.3218)  time: 0.5108  data: 0.0003  max mem: 9341
[01:56:31.869031] Epoch: [431]  [ 60/195]  eta: 0:01:11  lr: 0.001144  loss: 0.3037 (0.3167)  time: 0.5108  data: 0.0003  max mem: 9341
[01:56:42.127705] Epoch: [431]  [ 80/195]  eta: 0:01:00  lr: 0.001143  loss: 0.3117 (0.3151)  time: 0.5129  data: 0.0003  max mem: 9341
[01:56:52.338542] Epoch: [431]  [100/195]  eta: 0:00:49  lr: 0.001143  loss: 0.3000 (0.3138)  time: 0.5105  data: 0.0003  max mem: 9341
[01:57:02.552862] Epoch: [431]  [120/195]  eta: 0:00:39  lr: 0.001143  loss: 0.3126 (0.3138)  time: 0.5107  data: 0.0003  max mem: 9341
[01:57:12.774224] Epoch: [431]  [140/195]  eta: 0:00:28  lr: 0.001142  loss: 0.3115 (0.3133)  time: 0.5110  data: 0.0002  max mem: 9341
[01:57:23.038958] Epoch: [431]  [160/195]  eta: 0:00:18  lr: 0.001141  loss: 0.3195 (0.3143)  time: 0.5132  data: 0.0002  max mem: 9341
[01:57:33.213537] Epoch: [431]  [180/195]  eta: 0:00:07  lr: 0.001141  loss: 0.3139 (0.3151)  time: 0.5087  data: 0.0001  max mem: 9341
[01:57:40.345452] Epoch: [431]  [194/195]  eta: 0:00:00  lr: 0.001141  loss: 0.3149 (0.3145)  time: 0.5107  data: 0.0001  max mem: 9341
[01:57:40.517050] Epoch: [431] Total time: 0:01:40 (0.5178 s / it)
[01:57:40.536601] Averaged stats: lr: 0.001141  loss: 0.3149 (0.3153)
[01:57:45.335951] {"train_lr": 0.0011431775481143534, "train_loss": 0.3153450506619918, "epoch": 431}
[01:57:45.336281] [01:57:45.336382] Training epoch 431 for 0:01:45
[01:57:45.336438] [01:57:45.340803] log_dir: ./exp/debug/cifar100-LT/debug
[01:57:46.948682] Epoch: [432]  [  0/195]  eta: 0:05:12  lr: 0.001140  loss: 0.3347 (0.3347)  time: 1.6045  data: 1.0974  max mem: 9341
[01:57:57.171470] Epoch: [432]  [ 20/195]  eta: 0:01:38  lr: 0.001140  loss: 0.3056 (0.3114)  time: 0.5110  data: 0.0003  max mem: 9341
[01:58:07.387783] Epoch: [432]  [ 40/195]  eta: 0:01:23  lr: 0.001140  loss: 0.3238 (0.3159)  time: 0.5107  data: 0.0003  max mem: 9341
[01:58:17.598288] Epoch: [432]  [ 60/195]  eta: 0:01:11  lr: 0.001139  loss: 0.3253 (0.3188)  time: 0.5105  data: 0.0003  max mem: 9341
[01:58:27.849095] Epoch: [432]  [ 80/195]  eta: 0:01:00  lr: 0.001138  loss: 0.3167 (0.3186)  time: 0.5125  data: 0.0003  max mem: 9341
[01:58:38.067943] Epoch: [432]  [100/195]  eta: 0:00:49  lr: 0.001138  loss: 0.3262 (0.3194)  time: 0.5109  data: 0.0003  max mem: 9341
[01:58:48.277580] Epoch: [432]  [120/195]  eta: 0:00:39  lr: 0.001138  loss: 0.3173 (0.3189)  time: 0.5104  data: 0.0002  max mem: 9341
[01:58:58.496244] Epoch: [432]  [140/195]  eta: 0:00:28  lr: 0.001137  loss: 0.3169 (0.3179)  time: 0.5109  data: 0.0002  max mem: 9341
[01:59:08.755331] Epoch: [432]  [160/195]  eta: 0:00:18  lr: 0.001136  loss: 0.3174 (0.3186)  time: 0.5129  data: 0.0002  max mem: 9341
[01:59:18.921820] Epoch: [432]  [180/195]  eta: 0:00:07  lr: 0.001136  loss: 0.3215 (0.3187)  time: 0.5083  data: 0.0002  max mem: 9341
[01:59:26.044545] Epoch: [432]  [194/195]  eta: 0:00:00  lr: 0.001136  loss: 0.3300 (0.3189)  time: 0.5100  data: 0.0001  max mem: 9341
[01:59:26.226562] Epoch: [432] Total time: 0:01:40 (0.5174 s / it)
[01:59:26.241798] Averaged stats: lr: 0.001136  loss: 0.3300 (0.3175)
[01:59:30.916704] {"train_lr": 0.0011382232060514088, "train_loss": 0.3174635997376381, "epoch": 432}
[01:59:30.916974] [01:59:30.917079] Training epoch 432 for 0:01:45
[01:59:30.917134] [01:59:30.921565] log_dir: ./exp/debug/cifar100-LT/debug
[01:59:32.683348] Epoch: [433]  [  0/195]  eta: 0:05:43  lr: 0.001136  loss: 0.2980 (0.2980)  time: 1.7603  data: 1.2400  max mem: 9341
[01:59:42.930812] Epoch: [433]  [ 20/195]  eta: 0:01:40  lr: 0.001135  loss: 0.3077 (0.3042)  time: 0.5123  data: 0.0002  max mem: 9341
[01:59:53.152420] Epoch: [433]  [ 40/195]  eta: 0:01:24  lr: 0.001135  loss: 0.3087 (0.3063)  time: 0.5110  data: 0.0003  max mem: 9341
[02:00:03.379416] Epoch: [433]  [ 60/195]  eta: 0:01:11  lr: 0.001134  loss: 0.3043 (0.3076)  time: 0.5113  data: 0.0003  max mem: 9341
[02:00:13.640211] Epoch: [433]  [ 80/195]  eta: 0:01:00  lr: 0.001134  loss: 0.3084 (0.3072)  time: 0.5130  data: 0.0003  max mem: 9341
[02:00:23.854351] Epoch: [433]  [100/195]  eta: 0:00:49  lr: 0.001133  loss: 0.3038 (0.3062)  time: 0.5107  data: 0.0003  max mem: 9341
[02:00:34.072175] Epoch: [433]  [120/195]  eta: 0:00:39  lr: 0.001133  loss: 0.3115 (0.3079)  time: 0.5108  data: 0.0003  max mem: 9341
[02:00:44.305083] Epoch: [433]  [140/195]  eta: 0:00:28  lr: 0.001132  loss: 0.3063 (0.3086)  time: 0.5116  data: 0.0002  max mem: 9341
[02:00:54.557527] Epoch: [433]  [160/195]  eta: 0:00:18  lr: 0.001131  loss: 0.3030 (0.3084)  time: 0.5126  data: 0.0002  max mem: 9341
[02:01:04.726733] Epoch: [433]  [180/195]  eta: 0:00:07  lr: 0.001131  loss: 0.2951 (0.3077)  time: 0.5084  data: 0.0002  max mem: 9341
[02:01:11.856336] Epoch: [433]  [194/195]  eta: 0:00:00  lr: 0.001131  loss: 0.3173 (0.3083)  time: 0.5103  data: 0.0001  max mem: 9341
[02:01:12.037056] Epoch: [433] Total time: 0:01:41 (0.5185 s / it)
[02:01:12.052380] Averaged stats: lr: 0.001131  loss: 0.3173 (0.3094)
[02:01:16.848067] {"train_lr": 0.0011332699195834197, "train_loss": 0.3094046298318949, "epoch": 433}
[02:01:16.848410] [02:01:16.848514] Training epoch 433 for 0:01:45
[02:01:16.848570] [02:01:16.853137] log_dir: ./exp/debug/cifar100-LT/debug
[02:01:18.501476] Epoch: [434]  [  0/195]  eta: 0:05:21  lr: 0.001131  loss: 0.3117 (0.3117)  time: 1.6467  data: 1.1520  max mem: 9341
[02:01:28.792235] Epoch: [434]  [ 20/195]  eta: 0:01:39  lr: 0.001130  loss: 0.3149 (0.3119)  time: 0.5145  data: 0.0002  max mem: 9341
[02:01:39.034326] Epoch: [434]  [ 40/195]  eta: 0:01:23  lr: 0.001130  loss: 0.3038 (0.3063)  time: 0.5120  data: 0.0003  max mem: 9341
[02:01:49.279115] Epoch: [434]  [ 60/195]  eta: 0:01:11  lr: 0.001129  loss: 0.3061 (0.3066)  time: 0.5122  data: 0.0002  max mem: 9341
[02:01:59.585448] Epoch: [434]  [ 80/195]  eta: 0:01:00  lr: 0.001129  loss: 0.3074 (0.3070)  time: 0.5153  data: 0.0003  max mem: 9341
[02:02:09.827460] Epoch: [434]  [100/195]  eta: 0:00:49  lr: 0.001128  loss: 0.2980 (0.3060)  time: 0.5120  data: 0.0002  max mem: 9341
[02:02:20.063795] Epoch: [434]  [120/195]  eta: 0:00:39  lr: 0.001128  loss: 0.3063 (0.3069)  time: 0.5118  data: 0.0003  max mem: 9341
[02:02:30.312754] Epoch: [434]  [140/195]  eta: 0:00:28  lr: 0.001127  loss: 0.3061 (0.3069)  time: 0.5124  data: 0.0002  max mem: 9341
[02:02:40.575137] Epoch: [434]  [160/195]  eta: 0:00:18  lr: 0.001127  loss: 0.3042 (0.3070)  time: 0.5131  data: 0.0002  max mem: 9341
[02:02:50.744969] Epoch: [434]  [180/195]  eta: 0:00:07  lr: 0.001126  loss: 0.3126 (0.3081)  time: 0.5084  data: 0.0002  max mem: 9341
[02:02:57.879968] Epoch: [434]  [194/195]  eta: 0:00:00  lr: 0.001126  loss: 0.3082 (0.3084)  time: 0.5108  data: 0.0001  max mem: 9341
[02:02:58.051222] Epoch: [434] Total time: 0:01:41 (0.5190 s / it)
[02:02:58.065840] Averaged stats: lr: 0.001126  loss: 0.3082 (0.3093)
[02:03:02.771322] {"train_lr": 0.0011283177733483955, "train_loss": 0.3093134906811592, "epoch": 434}
[02:03:02.771589] [02:03:02.771696] Training epoch 434 for 0:01:45
[02:03:02.771751] [02:03:02.776167] log_dir: ./exp/debug/cifar100-LT/debug
[02:03:04.354493] Epoch: [435]  [  0/195]  eta: 0:05:07  lr: 0.001126  loss: 0.2841 (0.2841)  time: 1.5774  data: 1.0665  max mem: 9341
[02:03:14.581473] Epoch: [435]  [ 20/195]  eta: 0:01:38  lr: 0.001125  loss: 0.3105 (0.3153)  time: 0.5113  data: 0.0002  max mem: 9341
[02:03:24.800250] Epoch: [435]  [ 40/195]  eta: 0:01:23  lr: 0.001125  loss: 0.3031 (0.3113)  time: 0.5109  data: 0.0002  max mem: 9341
[02:03:35.017959] Epoch: [435]  [ 60/195]  eta: 0:01:11  lr: 0.001124  loss: 0.3073 (0.3098)  time: 0.5108  data: 0.0002  max mem: 9341
[02:03:45.277266] Epoch: [435]  [ 80/195]  eta: 0:01:00  lr: 0.001124  loss: 0.3183 (0.3119)  time: 0.5129  data: 0.0002  max mem: 9341
[02:03:55.492450] Epoch: [435]  [100/195]  eta: 0:00:49  lr: 0.001123  loss: 0.3027 (0.3109)  time: 0.5107  data: 0.0002  max mem: 9341
[02:04:05.710311] Epoch: [435]  [120/195]  eta: 0:00:39  lr: 0.001123  loss: 0.3112 (0.3108)  time: 0.5108  data: 0.0002  max mem: 9341
[02:04:15.924415] Epoch: [435]  [140/195]  eta: 0:00:28  lr: 0.001122  loss: 0.3123 (0.3106)  time: 0.5106  data: 0.0002  max mem: 9341
[02:04:26.183351] Epoch: [435]  [160/195]  eta: 0:00:18  lr: 0.001122  loss: 0.3122 (0.3118)  time: 0.5129  data: 0.0002  max mem: 9341
[02:04:36.350931] Epoch: [435]  [180/195]  eta: 0:00:07  lr: 0.001121  loss: 0.3191 (0.3125)  time: 0.5083  data: 0.0002  max mem: 9341
[02:04:43.482110] Epoch: [435]  [194/195]  eta: 0:00:00  lr: 0.001121  loss: 0.3076 (0.3119)  time: 0.5105  data: 0.0001  max mem: 9341
[02:04:43.650515] Epoch: [435] Total time: 0:01:40 (0.5173 s / it)
[02:04:43.667199] Averaged stats: lr: 0.001121  loss: 0.3076 (0.3112)
[02:04:48.268231] {"train_lr": 0.0011233668519648428, "train_loss": 0.31120102510620384, "epoch": 435}
[02:04:48.268497] [02:04:48.268585] Training epoch 435 for 0:01:45
[02:04:48.268640] [02:04:48.273144] log_dir: ./exp/debug/cifar100-LT/debug
[02:04:49.819357] Epoch: [436]  [  0/195]  eta: 0:05:01  lr: 0.001121  loss: 0.3096 (0.3096)  time: 1.5444  data: 1.0301  max mem: 9341
[02:05:00.027084] Epoch: [436]  [ 20/195]  eta: 0:01:37  lr: 0.001120  loss: 0.3058 (0.3060)  time: 0.5103  data: 0.0002  max mem: 9341
[02:05:10.239374] Epoch: [436]  [ 40/195]  eta: 0:01:23  lr: 0.001120  loss: 0.3150 (0.3095)  time: 0.5106  data: 0.0002  max mem: 9341
[02:05:20.450701] Epoch: [436]  [ 60/195]  eta: 0:01:11  lr: 0.001119  loss: 0.3065 (0.3087)  time: 0.5105  data: 0.0002  max mem: 9341
[02:05:30.708534] Epoch: [436]  [ 80/195]  eta: 0:01:00  lr: 0.001119  loss: 0.3020 (0.3094)  time: 0.5128  data: 0.0002  max mem: 9341
[02:05:40.921238] Epoch: [436]  [100/195]  eta: 0:00:49  lr: 0.001118  loss: 0.3023 (0.3088)  time: 0.5106  data: 0.0002  max mem: 9341
[02:05:51.135161] Epoch: [436]  [120/195]  eta: 0:00:38  lr: 0.001118  loss: 0.3084 (0.3080)  time: 0.5106  data: 0.0002  max mem: 9341
[02:06:01.352591] Epoch: [436]  [140/195]  eta: 0:00:28  lr: 0.001117  loss: 0.3074 (0.3080)  time: 0.5108  data: 0.0002  max mem: 9341
[02:06:11.610037] Epoch: [436]  [160/195]  eta: 0:00:18  lr: 0.001117  loss: 0.3084 (0.3080)  time: 0.5128  data: 0.0002  max mem: 9341
[02:06:21.779345] Epoch: [436]  [180/195]  eta: 0:00:07  lr: 0.001116  loss: 0.3181 (0.3094)  time: 0.5084  data: 0.0001  max mem: 9341
[02:06:28.905092] Epoch: [436]  [194/195]  eta: 0:00:00  lr: 0.001116  loss: 0.3225 (0.3099)  time: 0.5103  data: 0.0001  max mem: 9341
[02:06:29.082696] Epoch: [436] Total time: 0:01:40 (0.5170 s / it)
[02:06:29.094400] Averaged stats: lr: 0.001116  loss: 0.3225 (0.3112)
[02:06:33.841657] {"train_lr": 0.0011184172400303537, "train_loss": 0.3112018997661578, "epoch": 436}
[02:06:33.841992] [02:06:33.842084] Training epoch 436 for 0:01:45
[02:06:33.842138] [02:06:33.847477] log_dir: ./exp/debug/cifar100-LT/debug
[02:06:35.466846] Epoch: [437]  [  0/195]  eta: 0:05:15  lr: 0.001116  loss: 0.3438 (0.3438)  time: 1.6185  data: 1.1184  max mem: 9341
[02:06:45.687289] Epoch: [437]  [ 20/195]  eta: 0:01:38  lr: 0.001115  loss: 0.3076 (0.3169)  time: 0.5109  data: 0.0002  max mem: 9341
[02:06:55.935051] Epoch: [437]  [ 40/195]  eta: 0:01:23  lr: 0.001115  loss: 0.3076 (0.3127)  time: 0.5123  data: 0.0003  max mem: 9341
[02:07:06.173045] Epoch: [437]  [ 60/195]  eta: 0:01:11  lr: 0.001115  loss: 0.3132 (0.3129)  time: 0.5118  data: 0.0002  max mem: 9341
[02:07:16.480792] Epoch: [437]  [ 80/195]  eta: 0:01:00  lr: 0.001114  loss: 0.3085 (0.3130)  time: 0.5153  data: 0.0003  max mem: 9341
[02:07:26.716459] Epoch: [437]  [100/195]  eta: 0:00:49  lr: 0.001113  loss: 0.3107 (0.3122)  time: 0.5117  data: 0.0002  max mem: 9341
[02:07:36.950300] Epoch: [437]  [120/195]  eta: 0:00:39  lr: 0.001113  loss: 0.3060 (0.3119)  time: 0.5116  data: 0.0003  max mem: 9341
[02:07:47.183646] Epoch: [437]  [140/195]  eta: 0:00:28  lr: 0.001112  loss: 0.3129 (0.3114)  time: 0.5116  data: 0.0002  max mem: 9341
[02:07:57.484904] Epoch: [437]  [160/195]  eta: 0:00:18  lr: 0.001112  loss: 0.3070 (0.3113)  time: 0.5150  data: 0.0002  max mem: 9341
[02:08:07.674830] Epoch: [437]  [180/195]  eta: 0:00:07  lr: 0.001111  loss: 0.3138 (0.3111)  time: 0.5094  data: 0.0002  max mem: 9341
[02:08:14.821379] Epoch: [437]  [194/195]  eta: 0:00:00  lr: 0.001111  loss: 0.3138 (0.3106)  time: 0.5123  data: 0.0001  max mem: 9341
[02:08:15.014313] Epoch: [437] Total time: 0:01:41 (0.5188 s / it)
[02:08:15.022659] Averaged stats: lr: 0.001111  loss: 0.3138 (0.3118)
[02:08:19.821705] {"train_lr": 0.0011134690221201399, "train_loss": 0.3118441707430742, "epoch": 437}
[02:08:19.822079] [02:08:19.822189] Training epoch 437 for 0:01:45
[02:08:19.822245] [02:08:19.826870] log_dir: ./exp/debug/cifar100-LT/debug
[02:08:21.665910] Epoch: [438]  [  0/195]  eta: 0:05:58  lr: 0.001111  loss: 0.3316 (0.3316)  time: 1.8379  data: 1.3433  max mem: 9341
[02:08:31.903948] Epoch: [438]  [ 20/195]  eta: 0:01:40  lr: 0.001110  loss: 0.3148 (0.3139)  time: 0.5118  data: 0.0002  max mem: 9341
[02:08:42.143261] Epoch: [438]  [ 40/195]  eta: 0:01:24  lr: 0.001110  loss: 0.3128 (0.3137)  time: 0.5119  data: 0.0002  max mem: 9341
[02:08:52.354954] Epoch: [438]  [ 60/195]  eta: 0:01:11  lr: 0.001110  loss: 0.3076 (0.3123)  time: 0.5105  data: 0.0002  max mem: 9341
[02:09:02.608047] Epoch: [438]  [ 80/195]  eta: 0:01:00  lr: 0.001109  loss: 0.3103 (0.3128)  time: 0.5126  data: 0.0002  max mem: 9341
[02:09:12.823997] Epoch: [438]  [100/195]  eta: 0:00:49  lr: 0.001108  loss: 0.3199 (0.3147)  time: 0.5107  data: 0.0002  max mem: 9341
[02:09:23.036042] Epoch: [438]  [120/195]  eta: 0:00:39  lr: 0.001108  loss: 0.3105 (0.3139)  time: 0.5105  data: 0.0002  max mem: 9341
[02:09:33.251340] Epoch: [438]  [140/195]  eta: 0:00:28  lr: 0.001108  loss: 0.3065 (0.3134)  time: 0.5107  data: 0.0002  max mem: 9341
[02:09:43.509227] Epoch: [438]  [160/195]  eta: 0:00:18  lr: 0.001107  loss: 0.3084 (0.3127)  time: 0.5128  data: 0.0002  max mem: 9341
[02:09:53.680918] Epoch: [438]  [180/195]  eta: 0:00:07  lr: 0.001106  loss: 0.2904 (0.3108)  time: 0.5085  data: 0.0001  max mem: 9341
[02:10:00.812869] Epoch: [438]  [194/195]  eta: 0:00:00  lr: 0.001106  loss: 0.3108 (0.3119)  time: 0.5106  data: 0.0001  max mem: 9341
[02:10:00.980222] Epoch: [438] Total time: 0:01:41 (0.5187 s / it)
[02:10:00.993368] Averaged stats: lr: 0.001106  loss: 0.3108 (0.3122)
[02:10:05.775455] {"train_lr": 0.001108522282785585, "train_loss": 0.31223324531546004, "epoch": 438}
[02:10:05.775785] [02:10:05.775870] Training epoch 438 for 0:01:45
[02:10:05.775922] [02:10:05.780393] log_dir: ./exp/debug/cifar100-LT/debug
[02:10:07.520237] Epoch: [439]  [  0/195]  eta: 0:05:38  lr: 0.001106  loss: 0.2975 (0.2975)  time: 1.7381  data: 1.2309  max mem: 9341
[02:10:17.765932] Epoch: [439]  [ 20/195]  eta: 0:01:39  lr: 0.001105  loss: 0.3061 (0.3075)  time: 0.5122  data: 0.0002  max mem: 9341
[02:10:27.979641] Epoch: [439]  [ 40/195]  eta: 0:01:23  lr: 0.001105  loss: 0.3103 (0.3109)  time: 0.5106  data: 0.0002  max mem: 9341
[02:10:38.195646] Epoch: [439]  [ 60/195]  eta: 0:01:11  lr: 0.001105  loss: 0.3066 (0.3103)  time: 0.5107  data: 0.0002  max mem: 9341
[02:10:48.456956] Epoch: [439]  [ 80/195]  eta: 0:01:00  lr: 0.001104  loss: 0.3036 (0.3101)  time: 0.5130  data: 0.0002  max mem: 9341
[02:10:58.700322] Epoch: [439]  [100/195]  eta: 0:00:49  lr: 0.001103  loss: 0.3208 (0.3112)  time: 0.5121  data: 0.0002  max mem: 9341
[02:11:08.942244] Epoch: [439]  [120/195]  eta: 0:00:39  lr: 0.001103  loss: 0.3125 (0.3121)  time: 0.5120  data: 0.0002  max mem: 9341
[02:11:19.181459] Epoch: [439]  [140/195]  eta: 0:00:28  lr: 0.001103  loss: 0.3049 (0.3117)  time: 0.5119  data: 0.0002  max mem: 9341
[02:11:29.466857] Epoch: [439]  [160/195]  eta: 0:00:18  lr: 0.001102  loss: 0.3133 (0.3115)  time: 0.5142  data: 0.0002  max mem: 9341
[02:11:39.639606] Epoch: [439]  [180/195]  eta: 0:00:07  lr: 0.001101  loss: 0.3009 (0.3107)  time: 0.5086  data: 0.0001  max mem: 9341
[02:11:46.777068] Epoch: [439]  [194/195]  eta: 0:00:00  lr: 0.001101  loss: 0.3009 (0.3108)  time: 0.5108  data: 0.0001  max mem: 9341
[02:11:46.957954] Epoch: [439] Total time: 0:01:41 (0.5189 s / it)
[02:11:46.963577] Averaged stats: lr: 0.001101  loss: 0.3009 (0.3112)
[02:11:51.668049] {"train_lr": 0.0011035771065528287, "train_loss": 0.3111618735469305, "epoch": 439}
[02:11:51.668402] [02:11:51.668486] Training epoch 439 for 0:01:45
[02:11:51.668540] [02:11:51.673029] log_dir: ./exp/debug/cifar100-LT/debug
[02:11:53.239488] Epoch: [440]  [  0/195]  eta: 0:05:05  lr: 0.001101  loss: 0.3235 (0.3235)  time: 1.5650  data: 1.0556  max mem: 9341
[02:12:03.460298] Epoch: [440]  [ 20/195]  eta: 0:01:38  lr: 0.001100  loss: 0.3001 (0.3025)  time: 0.5109  data: 0.0003  max mem: 9341
[02:12:13.671409] Epoch: [440]  [ 40/195]  eta: 0:01:23  lr: 0.001100  loss: 0.3100 (0.3080)  time: 0.5105  data: 0.0003  max mem: 9341
[02:12:23.893454] Epoch: [440]  [ 60/195]  eta: 0:01:11  lr: 0.001100  loss: 0.3012 (0.3068)  time: 0.5110  data: 0.0002  max mem: 9341
[02:12:34.154851] Epoch: [440]  [ 80/195]  eta: 0:01:00  lr: 0.001099  loss: 0.3086 (0.3076)  time: 0.5130  data: 0.0003  max mem: 9341
[02:12:44.376267] Epoch: [440]  [100/195]  eta: 0:00:49  lr: 0.001098  loss: 0.3084 (0.3087)  time: 0.5110  data: 0.0003  max mem: 9341
[02:12:54.591382] Epoch: [440]  [120/195]  eta: 0:00:38  lr: 0.001098  loss: 0.3141 (0.3096)  time: 0.5107  data: 0.0003  max mem: 9341
[02:13:04.803821] Epoch: [440]  [140/195]  eta: 0:00:28  lr: 0.001098  loss: 0.3091 (0.3092)  time: 0.5106  data: 0.0002  max mem: 9341
[02:13:15.066866] Epoch: [440]  [160/195]  eta: 0:00:18  lr: 0.001097  loss: 0.3116 (0.3091)  time: 0.5131  data: 0.0002  max mem: 9341
[02:13:25.245231] Epoch: [440]  [180/195]  eta: 0:00:07  lr: 0.001096  loss: 0.3120 (0.3092)  time: 0.5089  data: 0.0002  max mem: 9341
[02:13:32.377797] Epoch: [440]  [194/195]  eta: 0:00:00  lr: 0.001096  loss: 0.3074 (0.3093)  time: 0.5108  data: 0.0001  max mem: 9341
[02:13:32.554172] Epoch: [440] Total time: 0:01:40 (0.5173 s / it)
[02:13:32.566760] Averaged stats: lr: 0.001096  loss: 0.3074 (0.3097)
[02:13:37.268573] {"train_lr": 0.001098633577921276, "train_loss": 0.3097261657890601, "epoch": 440}
[02:13:37.268906] [02:13:37.269010] Training epoch 440 for 0:01:45
[02:13:37.269065] [02:13:37.273468] log_dir: ./exp/debug/cifar100-LT/debug
[02:13:38.856918] Epoch: [441]  [  0/195]  eta: 0:05:08  lr: 0.001096  loss: 0.2915 (0.2915)  time: 1.5820  data: 1.0684  max mem: 9341
[02:13:49.081698] Epoch: [441]  [ 20/195]  eta: 0:01:38  lr: 0.001096  loss: 0.3140 (0.3179)  time: 0.5112  data: 0.0002  max mem: 9341
[02:13:59.296897] Epoch: [441]  [ 40/195]  eta: 0:01:23  lr: 0.001095  loss: 0.3134 (0.3150)  time: 0.5107  data: 0.0002  max mem: 9341
[02:14:09.513121] Epoch: [441]  [ 60/195]  eta: 0:01:11  lr: 0.001095  loss: 0.3165 (0.3139)  time: 0.5108  data: 0.0002  max mem: 9341
[02:14:19.812466] Epoch: [441]  [ 80/195]  eta: 0:01:00  lr: 0.001094  loss: 0.2976 (0.3111)  time: 0.5149  data: 0.0002  max mem: 9341
[02:14:30.055916] Epoch: [441]  [100/195]  eta: 0:00:49  lr: 0.001094  loss: 0.3017 (0.3095)  time: 0.5121  data: 0.0002  max mem: 9341
[02:14:40.300287] Epoch: [441]  [120/195]  eta: 0:00:39  lr: 0.001093  loss: 0.3028 (0.3090)  time: 0.5122  data: 0.0002  max mem: 9341
[02:14:50.546135] Epoch: [441]  [140/195]  eta: 0:00:28  lr: 0.001093  loss: 0.2956 (0.3086)  time: 0.5122  data: 0.0002  max mem: 9341
[02:15:00.849568] Epoch: [441]  [160/195]  eta: 0:00:18  lr: 0.001092  loss: 0.3111 (0.3088)  time: 0.5151  data: 0.0002  max mem: 9341
[02:15:11.049194] Epoch: [441]  [180/195]  eta: 0:00:07  lr: 0.001092  loss: 0.3148 (0.3097)  time: 0.5099  data: 0.0001  max mem: 9341
[02:15:18.204912] Epoch: [441]  [194/195]  eta: 0:00:00  lr: 0.001091  loss: 0.3136 (0.3097)  time: 0.5129  data: 0.0001  max mem: 9341
[02:15:18.383160] Epoch: [441] Total time: 0:01:41 (0.5185 s / it)
[02:15:18.400748] Averaged stats: lr: 0.001091  loss: 0.3136 (0.3110)
[02:15:23.131477] {"train_lr": 0.0010936917813622034, "train_loss": 0.31095910196503, "epoch": 441}
[02:15:23.131746] [02:15:23.131841] Training epoch 441 for 0:01:45
[02:15:23.131896] [02:15:23.136490] log_dir: ./exp/debug/cifar100-LT/debug
[02:15:24.939913] Epoch: [442]  [  0/195]  eta: 0:05:51  lr: 0.001091  loss: 0.3302 (0.3302)  time: 1.8019  data: 1.2949  max mem: 9341
[02:15:35.151852] Epoch: [442]  [ 20/195]  eta: 0:01:40  lr: 0.001091  loss: 0.3232 (0.3230)  time: 0.5105  data: 0.0003  max mem: 9341
[02:15:45.369184] Epoch: [442]  [ 40/195]  eta: 0:01:24  lr: 0.001090  loss: 0.3086 (0.3169)  time: 0.5108  data: 0.0002  max mem: 9341
[02:15:55.587041] Epoch: [442]  [ 60/195]  eta: 0:01:11  lr: 0.001090  loss: 0.2979 (0.3121)  time: 0.5108  data: 0.0003  max mem: 9341
[02:16:05.873837] Epoch: [442]  [ 80/195]  eta: 0:01:00  lr: 0.001089  loss: 0.3138 (0.3119)  time: 0.5143  data: 0.0003  max mem: 9341
[02:16:16.096516] Epoch: [442]  [100/195]  eta: 0:00:49  lr: 0.001089  loss: 0.3046 (0.3118)  time: 0.5111  data: 0.0003  max mem: 9341
[02:16:26.311751] Epoch: [442]  [120/195]  eta: 0:00:39  lr: 0.001088  loss: 0.3089 (0.3125)  time: 0.5107  data: 0.0003  max mem: 9341
[02:16:36.529509] Epoch: [442]  [140/195]  eta: 0:00:28  lr: 0.001088  loss: 0.3086 (0.3123)  time: 0.5108  data: 0.0003  max mem: 9341
[02:16:46.785798] Epoch: [442]  [160/195]  eta: 0:00:18  lr: 0.001087  loss: 0.3132 (0.3128)  time: 0.5128  data: 0.0003  max mem: 9341
[02:16:56.960491] Epoch: [442]  [180/195]  eta: 0:00:07  lr: 0.001087  loss: 0.3106 (0.3125)  time: 0.5087  data: 0.0002  max mem: 9341
[02:17:04.093619] Epoch: [442]  [194/195]  eta: 0:00:00  lr: 0.001086  loss: 0.3077 (0.3120)  time: 0.5107  data: 0.0001  max mem: 9341
[02:17:04.286688] Epoch: [442] Total time: 0:01:41 (0.5187 s / it)
[02:17:04.291155] Averaged stats: lr: 0.001086  loss: 0.3077 (0.3116)
[02:17:08.990721] {"train_lr": 0.0010887518013172725, "train_loss": 0.3116487577175483, "epoch": 442}
[02:17:08.990988] [02:17:08.991092] Training epoch 442 for 0:01:45
[02:17:08.991146] [02:17:08.995573] log_dir: ./exp/debug/cifar100-LT/debug
[02:17:10.697049] Epoch: [443]  [  0/195]  eta: 0:05:31  lr: 0.001086  loss: 0.2841 (0.2841)  time: 1.7000  data: 1.2085  max mem: 9341
[02:17:20.926751] Epoch: [443]  [ 20/195]  eta: 0:01:39  lr: 0.001086  loss: 0.3167 (0.3152)  time: 0.5114  data: 0.0002  max mem: 9341
[02:17:31.148808] Epoch: [443]  [ 40/195]  eta: 0:01:23  lr: 0.001085  loss: 0.3109 (0.3134)  time: 0.5110  data: 0.0002  max mem: 9341
[02:17:41.367958] Epoch: [443]  [ 60/195]  eta: 0:01:11  lr: 0.001085  loss: 0.3063 (0.3125)  time: 0.5109  data: 0.0002  max mem: 9341
[02:17:51.630633] Epoch: [443]  [ 80/195]  eta: 0:01:00  lr: 0.001084  loss: 0.2999 (0.3095)  time: 0.5131  data: 0.0003  max mem: 9341
[02:18:01.855023] Epoch: [443]  [100/195]  eta: 0:00:49  lr: 0.001084  loss: 0.3047 (0.3090)  time: 0.5112  data: 0.0002  max mem: 9341
[02:18:12.084197] Epoch: [443]  [120/195]  eta: 0:00:39  lr: 0.001083  loss: 0.3083 (0.3090)  time: 0.5114  data: 0.0002  max mem: 9341
[02:18:22.306290] Epoch: [443]  [140/195]  eta: 0:00:28  lr: 0.001083  loss: 0.3083 (0.3082)  time: 0.5110  data: 0.0002  max mem: 9341
[02:18:32.563999] Epoch: [443]  [160/195]  eta: 0:00:18  lr: 0.001082  loss: 0.2966 (0.3074)  time: 0.5128  data: 0.0002  max mem: 9341
[02:18:42.742456] Epoch: [443]  [180/195]  eta: 0:00:07  lr: 0.001082  loss: 0.3181 (0.3084)  time: 0.5089  data: 0.0002  max mem: 9341
[02:18:49.876383] Epoch: [443]  [194/195]  eta: 0:00:00  lr: 0.001081  loss: 0.2952 (0.3071)  time: 0.5107  data: 0.0001  max mem: 9341
[02:18:50.056058] Epoch: [443] Total time: 0:01:41 (0.5183 s / it)
[02:18:50.071141] Averaged stats: lr: 0.001081  loss: 0.2952 (0.3092)
[02:18:54.833581] {"train_lr": 0.001083813722197108, "train_loss": 0.30920025136990426, "epoch": 443}
[02:18:54.833867] [02:18:54.833972] Training epoch 443 for 0:01:45
[02:18:54.834028] [02:18:54.838537] log_dir: ./exp/debug/cifar100-LT/debug
[02:18:56.572015] Epoch: [444]  [  0/195]  eta: 0:05:37  lr: 0.001081  loss: 0.3009 (0.3009)  time: 1.7320  data: 1.2311  max mem: 9341
[02:19:06.791516] Epoch: [444]  [ 20/195]  eta: 0:01:39  lr: 0.001081  loss: 0.3038 (0.3092)  time: 0.5109  data: 0.0002  max mem: 9341
[02:19:17.013935] Epoch: [444]  [ 40/195]  eta: 0:01:23  lr: 0.001080  loss: 0.3018 (0.3059)  time: 0.5110  data: 0.0003  max mem: 9341
[02:19:27.250101] Epoch: [444]  [ 60/195]  eta: 0:01:11  lr: 0.001080  loss: 0.3139 (0.3091)  time: 0.5117  data: 0.0003  max mem: 9341
[02:19:37.504418] Epoch: [444]  [ 80/195]  eta: 0:01:00  lr: 0.001079  loss: 0.3043 (0.3072)  time: 0.5127  data: 0.0002  max mem: 9341
[02:19:47.716624] Epoch: [444]  [100/195]  eta: 0:00:49  lr: 0.001079  loss: 0.3092 (0.3085)  time: 0.5105  data: 0.0002  max mem: 9341
[02:19:57.931628] Epoch: [444]  [120/195]  eta: 0:00:39  lr: 0.001078  loss: 0.3178 (0.3085)  time: 0.5107  data: 0.0003  max mem: 9341
[02:20:08.146889] Epoch: [444]  [140/195]  eta: 0:00:28  lr: 0.001078  loss: 0.3079 (0.3089)  time: 0.5107  data: 0.0002  max mem: 9341
[02:20:18.406815] Epoch: [444]  [160/195]  eta: 0:00:18  lr: 0.001077  loss: 0.3055 (0.3086)  time: 0.5129  data: 0.0002  max mem: 9341
[02:20:28.575894] Epoch: [444]  [180/195]  eta: 0:00:07  lr: 0.001077  loss: 0.3070 (0.3088)  time: 0.5084  data: 0.0002  max mem: 9341
[02:20:35.708327] Epoch: [444]  [194/195]  eta: 0:00:00  lr: 0.001076  loss: 0.3059 (0.3083)  time: 0.5107  data: 0.0001  max mem: 9341
[02:20:35.897501] Epoch: [444] Total time: 0:01:41 (0.5183 s / it)
[02:20:35.911837] Averaged stats: lr: 0.001076  loss: 0.3059 (0.3085)
[02:20:40.751127] {"train_lr": 0.0010788776283798638, "train_loss": 0.3085202296001789, "epoch": 444}
[02:20:40.751493] [02:20:40.751597] Training epoch 444 for 0:01:45
[02:20:40.751652] [02:20:40.756086] log_dir: ./exp/debug/cifar100-LT/debug
[02:20:42.652963] Epoch: [445]  [  0/195]  eta: 0:06:09  lr: 0.001076  loss: 0.3767 (0.3767)  time: 1.8955  data: 1.3825  max mem: 9341
[02:20:52.894251] Epoch: [445]  [ 20/195]  eta: 0:01:41  lr: 0.001076  loss: 0.3147 (0.3142)  time: 0.5120  data: 0.0003  max mem: 9341
[02:21:03.124323] Epoch: [445]  [ 40/195]  eta: 0:01:24  lr: 0.001075  loss: 0.3054 (0.3102)  time: 0.5114  data: 0.0003  max mem: 9341
[02:21:13.347888] Epoch: [445]  [ 60/195]  eta: 0:01:12  lr: 0.001075  loss: 0.3025 (0.3108)  time: 0.5111  data: 0.0002  max mem: 9341
[02:21:23.605905] Epoch: [445]  [ 80/195]  eta: 0:01:00  lr: 0.001074  loss: 0.3120 (0.3115)  time: 0.5128  data: 0.0003  max mem: 9341
[02:21:33.825762] Epoch: [445]  [100/195]  eta: 0:00:49  lr: 0.001074  loss: 0.3020 (0.3102)  time: 0.5109  data: 0.0003  max mem: 9341
[02:21:44.046853] Epoch: [445]  [120/195]  eta: 0:00:39  lr: 0.001073  loss: 0.2975 (0.3090)  time: 0.5110  data: 0.0003  max mem: 9341
[02:21:54.267067] Epoch: [445]  [140/195]  eta: 0:00:28  lr: 0.001073  loss: 0.3079 (0.3090)  time: 0.5109  data: 0.0003  max mem: 9341
[02:22:04.527562] Epoch: [445]  [160/195]  eta: 0:00:18  lr: 0.001072  loss: 0.3204 (0.3100)  time: 0.5129  data: 0.0002  max mem: 9341
[02:22:14.701738] Epoch: [445]  [180/195]  eta: 0:00:07  lr: 0.001072  loss: 0.3190 (0.3105)  time: 0.5086  data: 0.0002  max mem: 9341
[02:22:21.831432] Epoch: [445]  [194/195]  eta: 0:00:00  lr: 0.001071  loss: 0.3137 (0.3108)  time: 0.5105  data: 0.0001  max mem: 9341
[02:22:22.007803] Epoch: [445] Total time: 0:01:41 (0.5192 s / it)
[02:22:22.024154] Averaged stats: lr: 0.001071  loss: 0.3137 (0.3093)
[02:22:26.849483] {"train_lr": 0.001073943604209761, "train_loss": 0.30932046306821015, "epoch": 445}
[02:22:26.849907] [02:22:26.850010] Training epoch 445 for 0:01:46
[02:22:26.850066] [02:22:26.855233] log_dir: ./exp/debug/cifar100-LT/debug
[02:22:28.796620] Epoch: [446]  [  0/195]  eta: 0:06:18  lr: 0.001071  loss: 0.3116 (0.3116)  time: 1.9402  data: 1.4415  max mem: 9341
[02:22:39.012595] Epoch: [446]  [ 20/195]  eta: 0:01:41  lr: 0.001071  loss: 0.3052 (0.3080)  time: 0.5107  data: 0.0002  max mem: 9341
[02:22:49.230788] Epoch: [446]  [ 40/195]  eta: 0:01:24  lr: 0.001070  loss: 0.2924 (0.3046)  time: 0.5108  data: 0.0002  max mem: 9341
[02:22:59.451429] Epoch: [446]  [ 60/195]  eta: 0:01:12  lr: 0.001070  loss: 0.3049 (0.3075)  time: 0.5110  data: 0.0002  max mem: 9341
[02:23:09.705247] Epoch: [446]  [ 80/195]  eta: 0:01:00  lr: 0.001069  loss: 0.3045 (0.3080)  time: 0.5126  data: 0.0002  max mem: 9341
[02:23:19.916593] Epoch: [446]  [100/195]  eta: 0:00:49  lr: 0.001069  loss: 0.3117 (0.3087)  time: 0.5105  data: 0.0002  max mem: 9341
[02:23:30.129494] Epoch: [446]  [120/195]  eta: 0:00:39  lr: 0.001068  loss: 0.3091 (0.3093)  time: 0.5106  data: 0.0002  max mem: 9341
[02:23:40.341355] Epoch: [446]  [140/195]  eta: 0:00:28  lr: 0.001068  loss: 0.3112 (0.3094)  time: 0.5105  data: 0.0002  max mem: 9341
[02:23:50.602834] Epoch: [446]  [160/195]  eta: 0:00:18  lr: 0.001067  loss: 0.3148 (0.3099)  time: 0.5130  data: 0.0002  max mem: 9341
[02:24:00.768847] Epoch: [446]  [180/195]  eta: 0:00:07  lr: 0.001067  loss: 0.3164 (0.3106)  time: 0.5083  data: 0.0001  max mem: 9341
[02:24:07.895592] Epoch: [446]  [194/195]  eta: 0:00:00  lr: 0.001066  loss: 0.3141 (0.3105)  time: 0.5101  data: 0.0001  max mem: 9341
[02:24:08.066054] Epoch: [446] Total time: 0:01:41 (0.5190 s / it)
[02:24:08.086830] Averaged stats: lr: 0.001066  loss: 0.3141 (0.3094)
[02:24:12.894299] {"train_lr": 0.0010690117339956604, "train_loss": 0.30938619256783756, "epoch": 446}
[02:24:12.894632] [02:24:12.894724] Training epoch 446 for 0:01:46
[02:24:12.894778] [02:24:12.899949] log_dir: ./exp/debug/cifar100-LT/debug
[02:24:14.637807] Epoch: [447]  [  0/195]  eta: 0:05:38  lr: 0.001066  loss: 0.3234 (0.3234)  time: 1.7369  data: 1.2244  max mem: 9341
[02:24:24.869170] Epoch: [447]  [ 20/195]  eta: 0:01:39  lr: 0.001066  loss: 0.3132 (0.3156)  time: 0.5115  data: 0.0002  max mem: 9341
[02:24:35.075545] Epoch: [447]  [ 40/195]  eta: 0:01:23  lr: 0.001066  loss: 0.3063 (0.3100)  time: 0.5102  data: 0.0002  max mem: 9341
[02:24:45.295980] Epoch: [447]  [ 60/195]  eta: 0:01:11  lr: 0.001065  loss: 0.2994 (0.3075)  time: 0.5110  data: 0.0002  max mem: 9341
[02:24:55.556631] Epoch: [447]  [ 80/195]  eta: 0:01:00  lr: 0.001064  loss: 0.3142 (0.3100)  time: 0.5130  data: 0.0002  max mem: 9341
[02:25:05.839494] Epoch: [447]  [100/195]  eta: 0:00:49  lr: 0.001064  loss: 0.3126 (0.3106)  time: 0.5141  data: 0.0002  max mem: 9341
[02:25:16.068782] Epoch: [447]  [120/195]  eta: 0:00:39  lr: 0.001064  loss: 0.2917 (0.3089)  time: 0.5114  data: 0.0002  max mem: 9341
[02:25:26.317038] Epoch: [447]  [140/195]  eta: 0:00:28  lr: 0.001063  loss: 0.3048 (0.3087)  time: 0.5124  data: 0.0002  max mem: 9341
[02:25:36.621224] Epoch: [447]  [160/195]  eta: 0:00:18  lr: 0.001062  loss: 0.3144 (0.3089)  time: 0.5151  data: 0.0002  max mem: 9341
[02:25:46.811829] Epoch: [447]  [180/195]  eta: 0:00:07  lr: 0.001062  loss: 0.3014 (0.3080)  time: 0.5095  data: 0.0001  max mem: 9341
[02:25:53.965795] Epoch: [447]  [194/195]  eta: 0:00:00  lr: 0.001061  loss: 0.3030 (0.3076)  time: 0.5126  data: 0.0001  max mem: 9341
[02:25:54.156477] Epoch: [447] Total time: 0:01:41 (0.5193 s / it)
[02:25:54.160518] Averaged stats: lr: 0.001061  loss: 0.3030 (0.3096)
[02:25:58.924655] {"train_lr": 0.0010640821020096254, "train_loss": 0.3095793988078068, "epoch": 447}
[02:25:58.924965] [02:25:58.925061] Training epoch 447 for 0:01:46
[02:25:58.925116] [02:25:58.929522] log_dir: ./exp/debug/cifar100-LT/debug
[02:26:00.773426] Epoch: [448]  [  0/195]  eta: 0:05:59  lr: 0.001061  loss: 0.3425 (0.3425)  time: 1.8424  data: 1.3391  max mem: 9341
[02:26:10.998009] Epoch: [448]  [ 20/195]  eta: 0:01:40  lr: 0.001061  loss: 0.3012 (0.3061)  time: 0.5111  data: 0.0003  max mem: 9341
[02:26:21.215759] Epoch: [448]  [ 40/195]  eta: 0:01:24  lr: 0.001061  loss: 0.3063 (0.3067)  time: 0.5108  data: 0.0002  max mem: 9341
[02:26:31.429304] Epoch: [448]  [ 60/195]  eta: 0:01:11  lr: 0.001060  loss: 0.2984 (0.3064)  time: 0.5106  data: 0.0002  max mem: 9341
[02:26:41.684082] Epoch: [448]  [ 80/195]  eta: 0:01:00  lr: 0.001059  loss: 0.3056 (0.3068)  time: 0.5127  data: 0.0003  max mem: 9341
[02:26:51.894782] Epoch: [448]  [100/195]  eta: 0:00:49  lr: 0.001059  loss: 0.2934 (0.3049)  time: 0.5105  data: 0.0002  max mem: 9341
[02:27:02.111950] Epoch: [448]  [120/195]  eta: 0:00:39  lr: 0.001059  loss: 0.3086 (0.3051)  time: 0.5108  data: 0.0003  max mem: 9341
[02:27:12.322011] Epoch: [448]  [140/195]  eta: 0:00:28  lr: 0.001058  loss: 0.3118 (0.3055)  time: 0.5104  data: 0.0003  max mem: 9341
[02:27:22.581158] Epoch: [448]  [160/195]  eta: 0:00:18  lr: 0.001057  loss: 0.3105 (0.3054)  time: 0.5129  data: 0.0002  max mem: 9341
[02:27:32.757402] Epoch: [448]  [180/195]  eta: 0:00:07  lr: 0.001057  loss: 0.3032 (0.3057)  time: 0.5088  data: 0.0002  max mem: 9341
[02:27:39.889053] Epoch: [448]  [194/195]  eta: 0:00:00  lr: 0.001057  loss: 0.3097 (0.3063)  time: 0.5106  data: 0.0001  max mem: 9341
[02:27:40.066518] Epoch: [448] Total time: 0:01:41 (0.5187 s / it)
[02:27:40.081217] Averaged stats: lr: 0.001057  loss: 0.3097 (0.3077)
[02:27:44.788758] {"train_lr": 0.0010591547924854467, "train_loss": 0.30769524142528193, "epoch": 448}
[02:27:44.789098] [02:27:44.789202] Training epoch 448 for 0:01:45
[02:27:44.789258] [02:27:44.794111] log_dir: ./exp/debug/cifar100-LT/debug
[02:27:46.541286] Epoch: [449]  [  0/195]  eta: 0:05:40  lr: 0.001056  loss: 0.2774 (0.2774)  time: 1.7457  data: 1.2361  max mem: 9341
[02:27:56.756163] Epoch: [449]  [ 20/195]  eta: 0:01:39  lr: 0.001056  loss: 0.3062 (0.3085)  time: 0.5107  data: 0.0002  max mem: 9341
[02:28:06.970807] Epoch: [449]  [ 40/195]  eta: 0:01:23  lr: 0.001056  loss: 0.2971 (0.3068)  time: 0.5107  data: 0.0003  max mem: 9341
[02:28:17.184175] Epoch: [449]  [ 60/195]  eta: 0:01:11  lr: 0.001055  loss: 0.2940 (0.3056)  time: 0.5106  data: 0.0002  max mem: 9341
[02:28:27.445559] Epoch: [449]  [ 80/195]  eta: 0:01:00  lr: 0.001054  loss: 0.3025 (0.3061)  time: 0.5130  data: 0.0003  max mem: 9341
[02:28:37.658259] Epoch: [449]  [100/195]  eta: 0:00:49  lr: 0.001054  loss: 0.3113 (0.3072)  time: 0.5106  data: 0.0003  max mem: 9341
[02:28:47.867697] Epoch: [449]  [120/195]  eta: 0:00:39  lr: 0.001054  loss: 0.3153 (0.3087)  time: 0.5104  data: 0.0002  max mem: 9341
[02:28:58.087380] Epoch: [449]  [140/195]  eta: 0:00:28  lr: 0.001053  loss: 0.3157 (0.3097)  time: 0.5109  data: 0.0002  max mem: 9341
[02:29:08.346262] Epoch: [449]  [160/195]  eta: 0:00:18  lr: 0.001052  loss: 0.3225 (0.3115)  time: 0.5129  data: 0.0002  max mem: 9341
[02:29:18.519799] Epoch: [449]  [180/195]  eta: 0:00:07  lr: 0.001052  loss: 0.3168 (0.3119)  time: 0.5086  data: 0.0001  max mem: 9341
[02:29:25.652171] Epoch: [449]  [194/195]  eta: 0:00:00  lr: 0.001052  loss: 0.3073 (0.3121)  time: 0.5105  data: 0.0001  max mem: 9341
[02:29:25.829664] Epoch: [449] Total time: 0:01:41 (0.5181 s / it)
[02:29:25.861959] Averaged stats: lr: 0.001052  loss: 0.3073 (0.3114)
[02:29:30.607278] {"train_lr": 0.0010542298896172636, "train_loss": 0.3113802995437231, "epoch": 449}
[02:29:30.607539] [02:29:30.607644] Training epoch 449 for 0:01:45
[02:29:30.607698] [02:29:30.612138] log_dir: ./exp/debug/cifar100-LT/debug
[02:29:32.497541] Epoch: [450]  [  0/195]  eta: 0:06:07  lr: 0.001052  loss: 0.3201 (0.3201)  time: 1.8847  data: 1.3852  max mem: 9341
[02:29:42.712953] Epoch: [450]  [ 20/195]  eta: 0:01:40  lr: 0.001051  loss: 0.3157 (0.3143)  time: 0.5107  data: 0.0002  max mem: 9341
[02:29:52.929827] Epoch: [450]  [ 40/195]  eta: 0:01:24  lr: 0.001051  loss: 0.3202 (0.3146)  time: 0.5108  data: 0.0003  max mem: 9341
[02:30:03.148985] Epoch: [450]  [ 60/195]  eta: 0:01:11  lr: 0.001050  loss: 0.3108 (0.3153)  time: 0.5109  data: 0.0002  max mem: 9341
[02:30:13.411398] Epoch: [450]  [ 80/195]  eta: 0:01:00  lr: 0.001050  loss: 0.3086 (0.3139)  time: 0.5131  data: 0.0003  max mem: 9341
[02:30:23.619363] Epoch: [450]  [100/195]  eta: 0:00:49  lr: 0.001049  loss: 0.3158 (0.3137)  time: 0.5103  data: 0.0003  max mem: 9341
[02:30:33.828952] Epoch: [450]  [120/195]  eta: 0:00:39  lr: 0.001049  loss: 0.3074 (0.3138)  time: 0.5104  data: 0.0002  max mem: 9341
[02:30:44.039049] Epoch: [450]  [140/195]  eta: 0:00:28  lr: 0.001048  loss: 0.3000 (0.3114)  time: 0.5105  data: 0.0003  max mem: 9341
[02:30:54.294313] Epoch: [450]  [160/195]  eta: 0:00:18  lr: 0.001048  loss: 0.3162 (0.3118)  time: 0.5127  data: 0.0003  max mem: 9341
[02:31:04.460135] Epoch: [450]  [180/195]  eta: 0:00:07  lr: 0.001047  loss: 0.3056 (0.3113)  time: 0.5082  data: 0.0002  max mem: 9341
[02:31:11.590491] Epoch: [450]  [194/195]  eta: 0:00:00  lr: 0.001047  loss: 0.2997 (0.3108)  time: 0.5104  data: 0.0001  max mem: 9341
[02:31:11.758884] Epoch: [450] Total time: 0:01:41 (0.5187 s / it)
[02:31:11.774152] Averaged stats: lr: 0.001047  loss: 0.2997 (0.3107)
[02:31:16.376205] {"train_lr": 0.0010493074775580725, "train_loss": 0.31071428975615745, "epoch": 450}
[02:31:16.376470] [02:31:16.376575] Training epoch 450 for 0:01:45
[02:31:16.376629] [02:31:16.381021] log_dir: ./exp/debug/cifar100-LT/debug
[02:31:18.281623] Epoch: [451]  [  0/195]  eta: 0:06:10  lr: 0.001047  loss: 0.2746 (0.2746)  time: 1.8995  data: 1.3988  max mem: 9341
[02:31:28.523992] Epoch: [451]  [ 20/195]  eta: 0:01:41  lr: 0.001046  loss: 0.3148 (0.3160)  time: 0.5120  data: 0.0003  max mem: 9341
[02:31:38.736757] Epoch: [451]  [ 40/195]  eta: 0:01:24  lr: 0.001046  loss: 0.3119 (0.3149)  time: 0.5106  data: 0.0002  max mem: 9341
[02:31:48.942195] Epoch: [451]  [ 60/195]  eta: 0:01:12  lr: 0.001045  loss: 0.3068 (0.3112)  time: 0.5102  data: 0.0002  max mem: 9341
[02:31:59.199127] Epoch: [451]  [ 80/195]  eta: 0:01:00  lr: 0.001045  loss: 0.3156 (0.3115)  time: 0.5128  data: 0.0003  max mem: 9341
[02:32:09.407597] Epoch: [451]  [100/195]  eta: 0:00:49  lr: 0.001044  loss: 0.3145 (0.3128)  time: 0.5104  data: 0.0003  max mem: 9341
[02:32:19.621031] Epoch: [451]  [120/195]  eta: 0:00:39  lr: 0.001044  loss: 0.2991 (0.3112)  time: 0.5106  data: 0.0003  max mem: 9341
[02:32:29.828442] Epoch: [451]  [140/195]  eta: 0:00:28  lr: 0.001043  loss: 0.3048 (0.3105)  time: 0.5103  data: 0.0002  max mem: 9341
[02:32:40.082381] Epoch: [451]  [160/195]  eta: 0:00:18  lr: 0.001043  loss: 0.2973 (0.3092)  time: 0.5126  data: 0.0002  max mem: 9341
[02:32:50.247380] Epoch: [451]  [180/195]  eta: 0:00:07  lr: 0.001042  loss: 0.2989 (0.3088)  time: 0.5082  data: 0.0001  max mem: 9341
[02:32:57.372205] Epoch: [451]  [194/195]  eta: 0:00:00  lr: 0.001042  loss: 0.2971 (0.3082)  time: 0.5102  data: 0.0001  max mem: 9341
[02:32:57.546338] Epoch: [451] Total time: 0:01:41 (0.5188 s / it)
[02:32:57.561360] Averaged stats: lr: 0.001042  loss: 0.2971 (0.3082)
[02:33:02.251974] {"train_lr": 0.001044387640418318, "train_loss": 0.30823441917697586, "epoch": 451}
[02:33:02.252274] [02:33:02.252392] Training epoch 451 for 0:01:45
[02:33:02.252447] [02:33:02.256890] log_dir: ./exp/debug/cifar100-LT/debug
[02:33:03.860616] Epoch: [452]  [  0/195]  eta: 0:05:12  lr: 0.001042  loss: 0.3372 (0.3372)  time: 1.6025  data: 1.1047  max mem: 9341
[02:33:14.069034] Epoch: [452]  [ 20/195]  eta: 0:01:38  lr: 0.001041  loss: 0.2991 (0.3033)  time: 0.5104  data: 0.0002  max mem: 9341
[02:33:24.279492] Epoch: [452]  [ 40/195]  eta: 0:01:23  lr: 0.001041  loss: 0.3067 (0.3055)  time: 0.5105  data: 0.0002  max mem: 9341
[02:33:34.491235] Epoch: [452]  [ 60/195]  eta: 0:01:11  lr: 0.001041  loss: 0.3074 (0.3069)  time: 0.5105  data: 0.0003  max mem: 9341
[02:33:44.769047] Epoch: [452]  [ 80/195]  eta: 0:01:00  lr: 0.001040  loss: 0.3037 (0.3076)  time: 0.5138  data: 0.0003  max mem: 9341
[02:33:54.991853] Epoch: [452]  [100/195]  eta: 0:00:49  lr: 0.001039  loss: 0.3183 (0.3087)  time: 0.5111  data: 0.0002  max mem: 9341
[02:34:05.206115] Epoch: [452]  [120/195]  eta: 0:00:39  lr: 0.001039  loss: 0.2940 (0.3070)  time: 0.5106  data: 0.0003  max mem: 9341
[02:34:15.419011] Epoch: [452]  [140/195]  eta: 0:00:28  lr: 0.001039  loss: 0.3033 (0.3067)  time: 0.5106  data: 0.0002  max mem: 9341
[02:34:25.675361] Epoch: [452]  [160/195]  eta: 0:00:18  lr: 0.001038  loss: 0.2944 (0.3061)  time: 0.5127  data: 0.0002  max mem: 9341
[02:34:35.838011] Epoch: [452]  [180/195]  eta: 0:00:07  lr: 0.001037  loss: 0.2997 (0.3058)  time: 0.5081  data: 0.0002  max mem: 9341
[02:34:42.963112] Epoch: [452]  [194/195]  eta: 0:00:00  lr: 0.001037  loss: 0.2958 (0.3057)  time: 0.5100  data: 0.0001  max mem: 9341
[02:34:43.131005] Epoch: [452] Total time: 0:01:40 (0.5173 s / it)
[02:34:43.148710] Averaged stats: lr: 0.001037  loss: 0.2958 (0.3061)
[02:34:47.873298] {"train_lr": 0.0010394704622644388, "train_loss": 0.30609032137271686, "epoch": 452}
[02:34:47.873569] [02:34:47.873673] Training epoch 452 for 0:01:45
[02:34:47.873728] [02:34:47.878594] log_dir: ./exp/debug/cifar100-LT/debug
[02:34:49.628410] Epoch: [453]  [  0/195]  eta: 0:05:41  lr: 0.001037  loss: 0.3162 (0.3162)  time: 1.7488  data: 1.2351  max mem: 9341
[02:34:59.844226] Epoch: [453]  [ 20/195]  eta: 0:01:39  lr: 0.001036  loss: 0.3001 (0.3015)  time: 0.5107  data: 0.0002  max mem: 9341
[02:35:10.064169] Epoch: [453]  [ 40/195]  eta: 0:01:23  lr: 0.001036  loss: 0.3010 (0.3030)  time: 0.5109  data: 0.0002  max mem: 9341
[02:35:20.276956] Epoch: [453]  [ 60/195]  eta: 0:01:11  lr: 0.001036  loss: 0.3036 (0.3037)  time: 0.5106  data: 0.0002  max mem: 9341
[02:35:30.530206] Epoch: [453]  [ 80/195]  eta: 0:01:00  lr: 0.001035  loss: 0.2987 (0.3033)  time: 0.5126  data: 0.0002  max mem: 9341
[02:35:40.746254] Epoch: [453]  [100/195]  eta: 0:00:49  lr: 0.001034  loss: 0.3117 (0.3046)  time: 0.5107  data: 0.0002  max mem: 9341
[02:35:50.967992] Epoch: [453]  [120/195]  eta: 0:00:39  lr: 0.001034  loss: 0.3045 (0.3043)  time: 0.5110  data: 0.0002  max mem: 9341
[02:36:01.179687] Epoch: [453]  [140/195]  eta: 0:00:28  lr: 0.001034  loss: 0.2935 (0.3034)  time: 0.5105  data: 0.0002  max mem: 9341
[02:36:11.431618] Epoch: [453]  [160/195]  eta: 0:00:18  lr: 0.001033  loss: 0.3128 (0.3047)  time: 0.5125  data: 0.0002  max mem: 9341
[02:36:21.604576] Epoch: [453]  [180/195]  eta: 0:00:07  lr: 0.001032  loss: 0.3079 (0.3053)  time: 0.5086  data: 0.0001  max mem: 9341
[02:36:28.732881] Epoch: [453]  [194/195]  eta: 0:00:00  lr: 0.001032  loss: 0.3024 (0.3051)  time: 0.5104  data: 0.0001  max mem: 9341
[02:36:28.889232] Epoch: [453] Total time: 0:01:41 (0.5180 s / it)
[02:36:28.913248] Averaged stats: lr: 0.001032  loss: 0.3024 (0.3079)
[02:36:33.639643] {"train_lr": 0.0010345560271174383, "train_loss": 0.30788563852890943, "epoch": 453}
[02:36:33.639978] [02:36:33.640064] Training epoch 453 for 0:01:45
[02:36:33.640132] [02:36:33.644602] log_dir: ./exp/debug/cifar100-LT/debug
[02:36:35.585881] Epoch: [454]  [  0/195]  eta: 0:06:18  lr: 0.001032  loss: 0.3188 (0.3188)  time: 1.9399  data: 1.4462  max mem: 9341
[02:36:45.806524] Epoch: [454]  [ 20/195]  eta: 0:01:41  lr: 0.001031  loss: 0.2971 (0.3052)  time: 0.5109  data: 0.0002  max mem: 9341
[02:36:56.016517] Epoch: [454]  [ 40/195]  eta: 0:01:24  lr: 0.001031  loss: 0.3026 (0.3063)  time: 0.5104  data: 0.0002  max mem: 9341
[02:37:06.227947] Epoch: [454]  [ 60/195]  eta: 0:01:12  lr: 0.001031  loss: 0.3081 (0.3070)  time: 0.5105  data: 0.0002  max mem: 9341
[02:37:16.484032] Epoch: [454]  [ 80/195]  eta: 0:01:00  lr: 0.001030  loss: 0.3041 (0.3063)  time: 0.5127  data: 0.0002  max mem: 9341
[02:37:26.692048] Epoch: [454]  [100/195]  eta: 0:00:49  lr: 0.001029  loss: 0.3034 (0.3064)  time: 0.5103  data: 0.0002  max mem: 9341
[02:37:36.904748] Epoch: [454]  [120/195]  eta: 0:00:39  lr: 0.001029  loss: 0.2990 (0.3059)  time: 0.5106  data: 0.0002  max mem: 9341
[02:37:47.113378] Epoch: [454]  [140/195]  eta: 0:00:28  lr: 0.001029  loss: 0.3099 (0.3068)  time: 0.5104  data: 0.0002  max mem: 9341
[02:37:57.369461] Epoch: [454]  [160/195]  eta: 0:00:18  lr: 0.001028  loss: 0.3144 (0.3082)  time: 0.5127  data: 0.0002  max mem: 9341
[02:38:07.534450] Epoch: [454]  [180/195]  eta: 0:00:07  lr: 0.001027  loss: 0.3121 (0.3084)  time: 0.5082  data: 0.0001  max mem: 9341
[02:38:14.656940] Epoch: [454]  [194/195]  eta: 0:00:00  lr: 0.001027  loss: 0.3078 (0.3083)  time: 0.5101  data: 0.0001  max mem: 9341
[02:38:14.833731] Epoch: [454] Total time: 0:01:41 (0.5189 s / it)
[02:38:14.847481] Averaged stats: lr: 0.001027  loss: 0.3078 (0.3088)
[02:38:19.577126] {"train_lr": 0.0010296444189514646, "train_loss": 0.3087868006183551, "epoch": 454}
[02:38:19.577391] [02:38:19.577482] Training epoch 454 for 0:01:45
[02:38:19.577536] [02:38:19.582291] log_dir: ./exp/debug/cifar100-LT/debug
[02:38:21.111748] Epoch: [455]  [  0/195]  eta: 0:04:57  lr: 0.001027  loss: 0.2985 (0.2985)  time: 1.5281  data: 1.0358  max mem: 9341
[02:38:31.327530] Epoch: [455]  [ 20/195]  eta: 0:01:37  lr: 0.001027  loss: 0.3076 (0.3108)  time: 0.5107  data: 0.0002  max mem: 9341
[02:38:41.547419] Epoch: [455]  [ 40/195]  eta: 0:01:23  lr: 0.001026  loss: 0.3063 (0.3083)  time: 0.5109  data: 0.0002  max mem: 9341
[02:38:51.768720] Epoch: [455]  [ 60/195]  eta: 0:01:11  lr: 0.001026  loss: 0.3097 (0.3081)  time: 0.5110  data: 0.0002  max mem: 9341
[02:39:02.031525] Epoch: [455]  [ 80/195]  eta: 0:01:00  lr: 0.001025  loss: 0.3117 (0.3090)  time: 0.5131  data: 0.0002  max mem: 9341
[02:39:12.248496] Epoch: [455]  [100/195]  eta: 0:00:49  lr: 0.001025  loss: 0.3095 (0.3081)  time: 0.5108  data: 0.0002  max mem: 9341
[02:39:22.472458] Epoch: [455]  [120/195]  eta: 0:00:38  lr: 0.001024  loss: 0.2944 (0.3069)  time: 0.5111  data: 0.0002  max mem: 9341
[02:39:32.699205] Epoch: [455]  [140/195]  eta: 0:00:28  lr: 0.001024  loss: 0.2967 (0.3060)  time: 0.5113  data: 0.0002  max mem: 9341
[02:39:42.967708] Epoch: [455]  [160/195]  eta: 0:00:18  lr: 0.001023  loss: 0.3151 (0.3070)  time: 0.5134  data: 0.0002  max mem: 9341
[02:39:53.152296] Epoch: [455]  [180/195]  eta: 0:00:07  lr: 0.001023  loss: 0.3217 (0.3079)  time: 0.5092  data: 0.0001  max mem: 9341
[02:40:00.292289] Epoch: [455]  [194/195]  eta: 0:00:00  lr: 0.001022  loss: 0.3009 (0.3073)  time: 0.5113  data: 0.0001  max mem: 9341
[02:40:00.468286] Epoch: [455] Total time: 0:01:40 (0.5174 s / it)
[02:40:00.480349] Averaged stats: lr: 0.001022  loss: 0.3009 (0.3072)
[02:40:05.147437] {"train_lr": 0.0010247357216923432, "train_loss": 0.30716745880169743, "epoch": 455}
[02:40:05.147790] [02:40:05.147881] Training epoch 455 for 0:01:45
[02:40:05.147935] [02:40:05.153090] log_dir: ./exp/debug/cifar100-LT/debug
[02:40:07.062485] Epoch: [456]  [  0/195]  eta: 0:06:12  lr: 0.001022  loss: 0.2912 (0.2912)  time: 1.9086  data: 1.4085  max mem: 9341
[02:40:17.274772] Epoch: [456]  [ 20/195]  eta: 0:01:40  lr: 0.001022  loss: 0.3038 (0.3052)  time: 0.5106  data: 0.0002  max mem: 9341
[02:40:27.514500] Epoch: [456]  [ 40/195]  eta: 0:01:24  lr: 0.001021  loss: 0.3082 (0.3057)  time: 0.5119  data: 0.0002  max mem: 9341
[02:40:37.734942] Epoch: [456]  [ 60/195]  eta: 0:01:12  lr: 0.001021  loss: 0.3077 (0.3059)  time: 0.5110  data: 0.0002  max mem: 9341
[02:40:47.989645] Epoch: [456]  [ 80/195]  eta: 0:01:00  lr: 0.001020  loss: 0.2980 (0.3059)  time: 0.5127  data: 0.0002  max mem: 9341
[02:40:58.209216] Epoch: [456]  [100/195]  eta: 0:00:49  lr: 0.001020  loss: 0.3131 (0.3084)  time: 0.5109  data: 0.0002  max mem: 9341
[02:41:08.428598] Epoch: [456]  [120/195]  eta: 0:00:39  lr: 0.001019  loss: 0.3093 (0.3081)  time: 0.5109  data: 0.0002  max mem: 9341
[02:41:18.640315] Epoch: [456]  [140/195]  eta: 0:00:28  lr: 0.001019  loss: 0.2970 (0.3084)  time: 0.5105  data: 0.0003  max mem: 9341
[02:41:28.898343] Epoch: [456]  [160/195]  eta: 0:00:18  lr: 0.001018  loss: 0.3061 (0.3085)  time: 0.5128  data: 0.0002  max mem: 9341
[02:41:39.081828] Epoch: [456]  [180/195]  eta: 0:00:07  lr: 0.001018  loss: 0.3005 (0.3078)  time: 0.5091  data: 0.0001  max mem: 9341
[02:41:46.218565] Epoch: [456]  [194/195]  eta: 0:00:00  lr: 0.001017  loss: 0.3017 (0.3077)  time: 0.5112  data: 0.0001  max mem: 9341
[02:41:46.400524] Epoch: [456] Total time: 0:01:41 (0.5192 s / it)
[02:41:46.406334] Averaged stats: lr: 0.001017  loss: 0.3017 (0.3073)
[02:41:51.095814] {"train_lr": 0.001019830019216168, "train_loss": 0.3073182099331648, "epoch": 456}
[02:41:51.096150] [02:41:51.096237] Training epoch 456 for 0:01:45
[02:41:51.096291] [02:41:51.100779] log_dir: ./exp/debug/cifar100-LT/debug
[02:41:52.796415] Epoch: [457]  [  0/195]  eta: 0:05:30  lr: 0.001017  loss: 0.2994 (0.2994)  time: 1.6946  data: 1.1834  max mem: 9341
[02:42:03.036208] Epoch: [457]  [ 20/195]  eta: 0:01:39  lr: 0.001017  loss: 0.2967 (0.2992)  time: 0.5119  data: 0.0004  max mem: 9341
[02:42:13.271855] Epoch: [457]  [ 40/195]  eta: 0:01:23  lr: 0.001016  loss: 0.3067 (0.3030)  time: 0.5117  data: 0.0003  max mem: 9341
[02:42:23.516106] Epoch: [457]  [ 60/195]  eta: 0:01:11  lr: 0.001016  loss: 0.3079 (0.3035)  time: 0.5122  data: 0.0003  max mem: 9341
[02:42:33.820110] Epoch: [457]  [ 80/195]  eta: 0:01:00  lr: 0.001015  loss: 0.3187 (0.3058)  time: 0.5151  data: 0.0003  max mem: 9341
[02:42:44.056193] Epoch: [457]  [100/195]  eta: 0:00:49  lr: 0.001015  loss: 0.2979 (0.3048)  time: 0.5117  data: 0.0003  max mem: 9341
[02:42:54.289051] Epoch: [457]  [120/195]  eta: 0:00:39  lr: 0.001014  loss: 0.2975 (0.3041)  time: 0.5116  data: 0.0003  max mem: 9341
[02:43:04.523010] Epoch: [457]  [140/195]  eta: 0:00:28  lr: 0.001014  loss: 0.3025 (0.3041)  time: 0.5116  data: 0.0003  max mem: 9341
[02:43:14.781166] Epoch: [457]  [160/195]  eta: 0:00:18  lr: 0.001013  loss: 0.3097 (0.3052)  time: 0.5129  data: 0.0003  max mem: 9341
[02:43:24.948215] Epoch: [457]  [180/195]  eta: 0:00:07  lr: 0.001013  loss: 0.3022 (0.3053)  time: 0.5083  data: 0.0002  max mem: 9341
[02:43:32.077500] Epoch: [457]  [194/195]  eta: 0:00:00  lr: 0.001012  loss: 0.3061 (0.3056)  time: 0.5104  data: 0.0001  max mem: 9341
[02:43:32.265675] Epoch: [457] Total time: 0:01:41 (0.5188 s / it)
[02:43:32.273714] Averaged stats: lr: 0.001012  loss: 0.3061 (0.3051)
[02:43:37.119879] {"train_lr": 0.0010149273953478524, "train_loss": 0.305143696279862, "epoch": 457}
[02:43:37.120185] [02:43:37.120292] Training epoch 457 for 0:01:46
[02:43:37.120407] [02:43:37.125288] log_dir: ./exp/debug/cifar100-LT/debug
[02:43:38.822730] Epoch: [458]  [  0/195]  eta: 0:05:30  lr: 0.001012  loss: 0.3169 (0.3169)  time: 1.6961  data: 1.1796  max mem: 9341
[02:43:49.051221] Epoch: [458]  [ 20/195]  eta: 0:01:39  lr: 0.001012  loss: 0.3054 (0.3085)  time: 0.5114  data: 0.0002  max mem: 9341
[02:43:59.271548] Epoch: [458]  [ 40/195]  eta: 0:01:23  lr: 0.001011  loss: 0.3028 (0.3058)  time: 0.5109  data: 0.0003  max mem: 9341
[02:44:09.492607] Epoch: [458]  [ 60/195]  eta: 0:01:11  lr: 0.001011  loss: 0.3055 (0.3060)  time: 0.5110  data: 0.0003  max mem: 9341
[02:44:19.758238] Epoch: [458]  [ 80/195]  eta: 0:01:00  lr: 0.001010  loss: 0.3031 (0.3064)  time: 0.5132  data: 0.0003  max mem: 9341
[02:44:29.995995] Epoch: [458]  [100/195]  eta: 0:00:49  lr: 0.001010  loss: 0.3054 (0.3074)  time: 0.5118  data: 0.0003  max mem: 9341
[02:44:40.234192] Epoch: [458]  [120/195]  eta: 0:00:39  lr: 0.001009  loss: 0.3034 (0.3070)  time: 0.5119  data: 0.0003  max mem: 9341
[02:44:50.471463] Epoch: [458]  [140/195]  eta: 0:00:28  lr: 0.001009  loss: 0.3166 (0.3078)  time: 0.5118  data: 0.0003  max mem: 9341
[02:45:00.766312] Epoch: [458]  [160/195]  eta: 0:00:18  lr: 0.001008  loss: 0.3041 (0.3079)  time: 0.5147  data: 0.0003  max mem: 9341
[02:45:10.959674] Epoch: [458]  [180/195]  eta: 0:00:07  lr: 0.001008  loss: 0.3005 (0.3074)  time: 0.5096  data: 0.0002  max mem: 9341
[02:45:18.114488] Epoch: [458]  [194/195]  eta: 0:00:00  lr: 0.001007  loss: 0.3017 (0.3078)  time: 0.5128  data: 0.0001  max mem: 9341
[02:45:18.303915] Epoch: [458] Total time: 0:01:41 (0.5189 s / it)
[02:45:18.313375] Averaged stats: lr: 0.001007  loss: 0.3017 (0.3070)
[02:45:23.080663] {"train_lr": 0.0010100279338597256, "train_loss": 0.30697742542013146, "epoch": 458}
[02:45:23.080926] [02:45:23.081032] Training epoch 458 for 0:01:45
[02:45:23.081087] [02:45:23.085596] log_dir: ./exp/debug/cifar100-LT/debug
[02:45:24.814287] Epoch: [459]  [  0/195]  eta: 0:05:36  lr: 0.001007  loss: 0.2695 (0.2695)  time: 1.7275  data: 1.2174  max mem: 9341
[02:45:35.037289] Epoch: [459]  [ 20/195]  eta: 0:01:39  lr: 0.001007  loss: 0.3042 (0.3046)  time: 0.5111  data: 0.0002  max mem: 9341
[02:45:45.248009] Epoch: [459]  [ 40/195]  eta: 0:01:23  lr: 0.001007  loss: 0.2987 (0.3045)  time: 0.5105  data: 0.0002  max mem: 9341
[02:45:55.459059] Epoch: [459]  [ 60/195]  eta: 0:01:11  lr: 0.001006  loss: 0.3067 (0.3047)  time: 0.5105  data: 0.0002  max mem: 9341
[02:46:05.715197] Epoch: [459]  [ 80/195]  eta: 0:01:00  lr: 0.001005  loss: 0.3076 (0.3036)  time: 0.5127  data: 0.0002  max mem: 9341
[02:46:15.929135] Epoch: [459]  [100/195]  eta: 0:00:49  lr: 0.001005  loss: 0.3119 (0.3045)  time: 0.5106  data: 0.0002  max mem: 9341
[02:46:26.141767] Epoch: [459]  [120/195]  eta: 0:00:39  lr: 0.001005  loss: 0.2970 (0.3038)  time: 0.5106  data: 0.0002  max mem: 9341
[02:46:36.352832] Epoch: [459]  [140/195]  eta: 0:00:28  lr: 0.001004  loss: 0.3038 (0.3044)  time: 0.5105  data: 0.0002  max mem: 9341
[02:46:46.605063] Epoch: [459]  [160/195]  eta: 0:00:18  lr: 0.001003  loss: 0.2970 (0.3044)  time: 0.5126  data: 0.0002  max mem: 9341
[02:46:56.776340] Epoch: [459]  [180/195]  eta: 0:00:07  lr: 0.001003  loss: 0.2970 (0.3037)  time: 0.5085  data: 0.0001  max mem: 9341
[02:47:03.905184] Epoch: [459]  [194/195]  eta: 0:00:00  lr: 0.001003  loss: 0.3078 (0.3043)  time: 0.5104  data: 0.0001  max mem: 9341
[02:47:04.076842] Epoch: [459] Total time: 0:01:40 (0.5179 s / it)
[02:47:04.084604] Averaged stats: lr: 0.001003  loss: 0.3078 (0.3049)
[02:47:08.837086] {"train_lr": 0.0010051317184700548, "train_loss": 0.30490086970803065, "epoch": 459}
[02:47:08.837348] [02:47:08.837432] Training epoch 459 for 0:01:45
[02:47:08.837485] [02:47:08.841920] log_dir: ./exp/debug/cifar100-LT/debug
[02:47:10.651436] Epoch: [460]  [  0/195]  eta: 0:05:52  lr: 0.001002  loss: 0.2917 (0.2917)  time: 1.8080  data: 1.3158  max mem: 9341
[02:47:20.864766] Epoch: [460]  [ 20/195]  eta: 0:01:40  lr: 0.001002  loss: 0.3048 (0.3030)  time: 0.5106  data: 0.0002  max mem: 9341
[02:47:31.086418] Epoch: [460]  [ 40/195]  eta: 0:01:24  lr: 0.001002  loss: 0.3029 (0.3041)  time: 0.5110  data: 0.0002  max mem: 9341
[02:47:41.303799] Epoch: [460]  [ 60/195]  eta: 0:01:11  lr: 0.001001  loss: 0.3051 (0.3033)  time: 0.5108  data: 0.0002  max mem: 9341
[02:47:51.580610] Epoch: [460]  [ 80/195]  eta: 0:01:00  lr: 0.001000  loss: 0.3044 (0.3050)  time: 0.5138  data: 0.0002  max mem: 9341
[02:48:01.798051] Epoch: [460]  [100/195]  eta: 0:00:49  lr: 0.001000  loss: 0.3001 (0.3044)  time: 0.5108  data: 0.0002  max mem: 9341
[02:48:12.015875] Epoch: [460]  [120/195]  eta: 0:00:39  lr: 0.001000  loss: 0.2990 (0.3045)  time: 0.5108  data: 0.0002  max mem: 9341
[02:48:22.229067] Epoch: [460]  [140/195]  eta: 0:00:28  lr: 0.000999  loss: 0.3060 (0.3058)  time: 0.5106  data: 0.0002  max mem: 9341
[02:48:32.535779] Epoch: [460]  [160/195]  eta: 0:00:18  lr: 0.000998  loss: 0.2975 (0.3050)  time: 0.5153  data: 0.0002  max mem: 9341
[02:48:42.733678] Epoch: [460]  [180/195]  eta: 0:00:07  lr: 0.000998  loss: 0.3043 (0.3054)  time: 0.5098  data: 0.0001  max mem: 9341
[02:48:49.884293] Epoch: [460]  [194/195]  eta: 0:00:00  lr: 0.000998  loss: 0.3035 (0.3053)  time: 0.5128  data: 0.0001  max mem: 9341
[02:48:50.067531] Epoch: [460] Total time: 0:01:41 (0.5191 s / it)
[02:48:50.068386] Averaged stats: lr: 0.000998  loss: 0.3035 (0.3056)
[02:48:54.850519] {"train_lr": 0.001000238832841653, "train_loss": 0.3055524732822027, "epoch": 460}
[02:48:54.850772] [02:48:54.850863] Training epoch 460 for 0:01:46
[02:48:54.850916] [02:48:54.855333] log_dir: ./exp/debug/cifar100-LT/debug
[02:48:56.676184] Epoch: [461]  [  0/195]  eta: 0:05:54  lr: 0.000998  loss: 0.3138 (0.3138)  time: 1.8196  data: 1.3323  max mem: 9341
[02:49:06.911817] Epoch: [461]  [ 20/195]  eta: 0:01:40  lr: 0.000997  loss: 0.2985 (0.3050)  time: 0.5117  data: 0.0003  max mem: 9341
[02:49:17.128313] Epoch: [461]  [ 40/195]  eta: 0:01:24  lr: 0.000997  loss: 0.2977 (0.3018)  time: 0.5107  data: 0.0003  max mem: 9341
[02:49:27.369326] Epoch: [461]  [ 60/195]  eta: 0:01:11  lr: 0.000996  loss: 0.3054 (0.3048)  time: 0.5120  data: 0.0002  max mem: 9341
[02:49:37.680321] Epoch: [461]  [ 80/195]  eta: 0:01:00  lr: 0.000996  loss: 0.3113 (0.3069)  time: 0.5155  data: 0.0002  max mem: 9341
[02:49:47.917823] Epoch: [461]  [100/195]  eta: 0:00:49  lr: 0.000995  loss: 0.3138 (0.3079)  time: 0.5118  data: 0.0002  max mem: 9341
[02:49:58.148787] Epoch: [461]  [120/195]  eta: 0:00:39  lr: 0.000995  loss: 0.3102 (0.3081)  time: 0.5115  data: 0.0003  max mem: 9341
[02:50:08.377011] Epoch: [461]  [140/195]  eta: 0:00:28  lr: 0.000994  loss: 0.3038 (0.3076)  time: 0.5113  data: 0.0003  max mem: 9341
[02:50:18.671359] Epoch: [461]  [160/195]  eta: 0:00:18  lr: 0.000994  loss: 0.2984 (0.3068)  time: 0.5146  data: 0.0003  max mem: 9341
[02:50:28.865753] Epoch: [461]  [180/195]  eta: 0:00:07  lr: 0.000993  loss: 0.3091 (0.3072)  time: 0.5097  data: 0.0002  max mem: 9341
[02:50:36.019264] Epoch: [461]  [194/195]  eta: 0:00:00  lr: 0.000993  loss: 0.2989 (0.3068)  time: 0.5128  data: 0.0001  max mem: 9341
[02:50:36.200675] Epoch: [461] Total time: 0:01:41 (0.5197 s / it)
[02:50:36.212578] Averaged stats: lr: 0.000993  loss: 0.2989 (0.3053)
[02:50:40.960118] {"train_lr": 0.000995349360580442, "train_loss": 0.30526044017229326, "epoch": 461}
[02:50:40.960387] [02:50:40.960495] Training epoch 461 for 0:01:46
[02:50:40.960550] [02:50:40.964967] log_dir: ./exp/debug/cifar100-LT/debug
[02:50:42.809562] Epoch: [462]  [  0/195]  eta: 0:05:59  lr: 0.000993  loss: 0.2873 (0.2873)  time: 1.8427  data: 1.3347  max mem: 9341
[02:50:53.025862] Epoch: [462]  [ 20/195]  eta: 0:01:40  lr: 0.000992  loss: 0.3021 (0.3002)  time: 0.5107  data: 0.0003  max mem: 9341
[02:51:03.241374] Epoch: [462]  [ 40/195]  eta: 0:01:24  lr: 0.000992  loss: 0.2985 (0.3003)  time: 0.5107  data: 0.0002  max mem: 9341
[02:51:13.459478] Epoch: [462]  [ 60/195]  eta: 0:01:11  lr: 0.000992  loss: 0.2974 (0.3002)  time: 0.5108  data: 0.0003  max mem: 9341
[02:51:23.747589] Epoch: [462]  [ 80/195]  eta: 0:01:00  lr: 0.000991  loss: 0.3050 (0.3006)  time: 0.5143  data: 0.0003  max mem: 9341
[02:51:33.986068] Epoch: [462]  [100/195]  eta: 0:00:49  lr: 0.000990  loss: 0.3012 (0.3003)  time: 0.5119  data: 0.0003  max mem: 9341
[02:51:44.225876] Epoch: [462]  [120/195]  eta: 0:00:39  lr: 0.000990  loss: 0.3056 (0.3019)  time: 0.5119  data: 0.0003  max mem: 9341
[02:51:54.462288] Epoch: [462]  [140/195]  eta: 0:00:28  lr: 0.000990  loss: 0.3068 (0.3029)  time: 0.5118  data: 0.0003  max mem: 9341
[02:52:04.768135] Epoch: [462]  [160/195]  eta: 0:00:18  lr: 0.000989  loss: 0.3054 (0.3040)  time: 0.5152  data: 0.0003  max mem: 9341
[02:52:14.968020] Epoch: [462]  [180/195]  eta: 0:00:07  lr: 0.000988  loss: 0.3048 (0.3045)  time: 0.5099  data: 0.0001  max mem: 9341
[02:52:22.123079] Epoch: [462]  [194/195]  eta: 0:00:00  lr: 0.000988  loss: 0.3136 (0.3048)  time: 0.5129  data: 0.0001  max mem: 9341
[02:52:22.299634] Epoch: [462] Total time: 0:01:41 (0.5197 s / it)
[02:52:22.305688] Averaged stats: lr: 0.000988  loss: 0.3136 (0.3043)
[02:52:27.202357] {"train_lr": 0.0009904633852340057, "train_loss": 0.3043246342203556, "epoch": 462}
[02:52:27.202760] [02:52:27.202869] Training epoch 462 for 0:01:46
[02:52:27.202923] [02:52:27.208131] log_dir: ./exp/debug/cifar100-LT/debug
[02:52:28.939696] Epoch: [463]  [  0/195]  eta: 0:05:37  lr: 0.000988  loss: 0.3087 (0.3087)  time: 1.7301  data: 1.2074  max mem: 9341
[02:52:39.149706] Epoch: [463]  [ 20/195]  eta: 0:01:39  lr: 0.000987  loss: 0.3014 (0.3051)  time: 0.5104  data: 0.0002  max mem: 9341
[02:52:49.364469] Epoch: [463]  [ 40/195]  eta: 0:01:23  lr: 0.000987  loss: 0.3063 (0.3074)  time: 0.5107  data: 0.0002  max mem: 9341
[02:52:59.579745] Epoch: [463]  [ 60/195]  eta: 0:01:11  lr: 0.000987  loss: 0.3071 (0.3093)  time: 0.5107  data: 0.0002  max mem: 9341
[02:53:09.861466] Epoch: [463]  [ 80/195]  eta: 0:01:00  lr: 0.000986  loss: 0.2978 (0.3073)  time: 0.5140  data: 0.0002  max mem: 9341
[02:53:20.075565] Epoch: [463]  [100/195]  eta: 0:00:49  lr: 0.000985  loss: 0.3059 (0.3077)  time: 0.5106  data: 0.0002  max mem: 9341
[02:53:30.290955] Epoch: [463]  [120/195]  eta: 0:00:39  lr: 0.000985  loss: 0.3076 (0.3084)  time: 0.5107  data: 0.0002  max mem: 9341
[02:53:40.504172] Epoch: [463]  [140/195]  eta: 0:00:28  lr: 0.000985  loss: 0.3090 (0.3086)  time: 0.5106  data: 0.0002  max mem: 9341
[02:53:50.765531] Epoch: [463]  [160/195]  eta: 0:00:18  lr: 0.000984  loss: 0.3056 (0.3083)  time: 0.5130  data: 0.0002  max mem: 9341
[02:54:00.941445] Epoch: [463]  [180/195]  eta: 0:00:07  lr: 0.000983  loss: 0.3031 (0.3085)  time: 0.5087  data: 0.0002  max mem: 9341
[02:54:08.065353] Epoch: [463]  [194/195]  eta: 0:00:00  lr: 0.000983  loss: 0.2999 (0.3080)  time: 0.5102  data: 0.0001  max mem: 9341
[02:54:08.218667] Epoch: [463] Total time: 0:01:41 (0.5180 s / it)
[02:54:08.251786] Averaged stats: lr: 0.000983  loss: 0.2999 (0.3042)
[02:54:13.007646] {"train_lr": 0.0009855809902901862, "train_loss": 0.30420530980978255, "epoch": 463}
[02:54:13.007904] [02:54:13.007986] Training epoch 463 for 0:01:45
[02:54:13.008040] [02:54:13.012483] log_dir: ./exp/debug/cifar100-LT/debug
[02:54:14.858255] Epoch: [464]  [  0/195]  eta: 0:05:59  lr: 0.000983  loss: 0.2973 (0.2973)  time: 1.8436  data: 1.3495  max mem: 9341
[02:54:25.121187] Epoch: [464]  [ 20/195]  eta: 0:01:40  lr: 0.000983  loss: 0.3106 (0.3081)  time: 0.5131  data: 0.0003  max mem: 9341
[02:54:35.338284] Epoch: [464]  [ 40/195]  eta: 0:01:24  lr: 0.000982  loss: 0.3072 (0.3075)  time: 0.5108  data: 0.0002  max mem: 9341
[02:54:45.548348] Epoch: [464]  [ 60/195]  eta: 0:01:11  lr: 0.000982  loss: 0.2970 (0.3039)  time: 0.5104  data: 0.0003  max mem: 9341
[02:54:55.803236] Epoch: [464]  [ 80/195]  eta: 0:01:00  lr: 0.000981  loss: 0.2999 (0.3046)  time: 0.5127  data: 0.0003  max mem: 9341
[02:55:06.037604] Epoch: [464]  [100/195]  eta: 0:00:49  lr: 0.000981  loss: 0.2993 (0.3044)  time: 0.5116  data: 0.0002  max mem: 9341
[02:55:16.256641] Epoch: [464]  [120/195]  eta: 0:00:39  lr: 0.000980  loss: 0.2979 (0.3027)  time: 0.5109  data: 0.0003  max mem: 9341
[02:55:26.469996] Epoch: [464]  [140/195]  eta: 0:00:28  lr: 0.000980  loss: 0.2951 (0.3026)  time: 0.5106  data: 0.0002  max mem: 9341
[02:55:36.724418] Epoch: [464]  [160/195]  eta: 0:00:18  lr: 0.000979  loss: 0.2985 (0.3027)  time: 0.5126  data: 0.0003  max mem: 9341
[02:55:46.893821] Epoch: [464]  [180/195]  eta: 0:00:07  lr: 0.000979  loss: 0.3012 (0.3033)  time: 0.5084  data: 0.0002  max mem: 9341
[02:55:54.019886] Epoch: [464]  [194/195]  eta: 0:00:00  lr: 0.000978  loss: 0.3033 (0.3035)  time: 0.5102  data: 0.0001  max mem: 9341
[02:55:54.223695] Epoch: [464] Total time: 0:01:41 (0.5190 s / it)
[02:55:54.224720] Averaged stats: lr: 0.000978  loss: 0.3033 (0.3041)
[02:55:58.900636] {"train_lr": 0.0009807022591756395, "train_loss": 0.30405961399277054, "epoch": 464}
[02:55:58.900901] [02:55:58.901009] Training epoch 464 for 0:01:45
[02:55:58.901080] [02:55:58.906182] log_dir: ./exp/debug/cifar100-LT/debug
[02:56:00.678550] Epoch: [465]  [  0/195]  eta: 0:05:45  lr: 0.000978  loss: 0.3684 (0.3684)  time: 1.7708  data: 1.2573  max mem: 9341
[02:56:10.898002] Epoch: [465]  [ 20/195]  eta: 0:01:39  lr: 0.000978  loss: 0.3080 (0.3121)  time: 0.5109  data: 0.0002  max mem: 9341
[02:56:21.115926] Epoch: [465]  [ 40/195]  eta: 0:01:23  lr: 0.000977  loss: 0.3065 (0.3104)  time: 0.5108  data: 0.0002  max mem: 9341
[02:56:31.330922] Epoch: [465]  [ 60/195]  eta: 0:01:11  lr: 0.000977  loss: 0.2993 (0.3069)  time: 0.5107  data: 0.0002  max mem: 9341
[02:56:41.589807] Epoch: [465]  [ 80/195]  eta: 0:01:00  lr: 0.000976  loss: 0.3108 (0.3072)  time: 0.5129  data: 0.0002  max mem: 9341
[02:56:51.805330] Epoch: [465]  [100/195]  eta: 0:00:49  lr: 0.000976  loss: 0.3052 (0.3068)  time: 0.5107  data: 0.0002  max mem: 9341
[02:57:02.027138] Epoch: [465]  [120/195]  eta: 0:00:39  lr: 0.000975  loss: 0.3033 (0.3071)  time: 0.5110  data: 0.0002  max mem: 9341
[02:57:12.267536] Epoch: [465]  [140/195]  eta: 0:00:28  lr: 0.000975  loss: 0.3053 (0.3067)  time: 0.5119  data: 0.0002  max mem: 9341
[02:57:22.542639] Epoch: [465]  [160/195]  eta: 0:00:18  lr: 0.000974  loss: 0.3131 (0.3077)  time: 0.5137  data: 0.0002  max mem: 9341
[02:57:32.722089] Epoch: [465]  [180/195]  eta: 0:00:07  lr: 0.000974  loss: 0.3015 (0.3070)  time: 0.5089  data: 0.0001  max mem: 9341
[02:57:39.865672] Epoch: [465]  [194/195]  eta: 0:00:00  lr: 0.000973  loss: 0.2994 (0.3061)  time: 0.5112  data: 0.0001  max mem: 9341
[02:57:40.049883] Epoch: [465] Total time: 0:01:41 (0.5187 s / it)
[02:57:40.051851] Averaged stats: lr: 0.000973  loss: 0.2994 (0.3051)
[02:57:44.783441] {"train_lr": 0.0009758272752544222, "train_loss": 0.3050693426758815, "epoch": 465}
[02:57:44.783732] [02:57:44.783819] Training epoch 465 for 0:01:45
[02:57:44.783872] [02:57:44.788534] log_dir: ./exp/debug/cifar100-LT/debug
[02:57:46.556949] Epoch: [466]  [  0/195]  eta: 0:05:44  lr: 0.000973  loss: 0.2699 (0.2699)  time: 1.7657  data: 1.2575  max mem: 9341
[02:57:56.784756] Epoch: [466]  [ 20/195]  eta: 0:01:39  lr: 0.000973  loss: 0.2964 (0.3004)  time: 0.5113  data: 0.0002  max mem: 9341
[02:58:07.000663] Epoch: [466]  [ 40/195]  eta: 0:01:23  lr: 0.000972  loss: 0.3046 (0.3024)  time: 0.5107  data: 0.0002  max mem: 9341
[02:58:17.211206] Epoch: [466]  [ 60/195]  eta: 0:01:11  lr: 0.000972  loss: 0.2999 (0.3025)  time: 0.5105  data: 0.0002  max mem: 9341
[02:58:27.516197] Epoch: [466]  [ 80/195]  eta: 0:01:00  lr: 0.000971  loss: 0.3095 (0.3050)  time: 0.5152  data: 0.0002  max mem: 9341
[02:58:37.753563] Epoch: [466]  [100/195]  eta: 0:00:49  lr: 0.000971  loss: 0.3117 (0.3057)  time: 0.5118  data: 0.0002  max mem: 9341
[02:58:47.969656] Epoch: [466]  [120/195]  eta: 0:00:39  lr: 0.000970  loss: 0.2996 (0.3056)  time: 0.5107  data: 0.0002  max mem: 9341
[02:58:58.187770] Epoch: [466]  [140/195]  eta: 0:00:28  lr: 0.000970  loss: 0.3038 (0.3047)  time: 0.5108  data: 0.0002  max mem: 9341
[02:59:08.443274] Epoch: [466]  [160/195]  eta: 0:00:18  lr: 0.000969  loss: 0.3036 (0.3048)  time: 0.5127  data: 0.0002  max mem: 9341
[02:59:18.612873] Epoch: [466]  [180/195]  eta: 0:00:07  lr: 0.000969  loss: 0.3147 (0.3052)  time: 0.5084  data: 0.0001  max mem: 9341
[02:59:25.739229] Epoch: [466]  [194/195]  eta: 0:00:00  lr: 0.000968  loss: 0.3111 (0.3056)  time: 0.5102  data: 0.0001  max mem: 9341
[02:59:25.930541] Epoch: [466] Total time: 0:01:41 (0.5187 s / it)
[02:59:25.936151] Averaged stats: lr: 0.000968  loss: 0.3111 (0.3048)
[02:59:30.698766] {"train_lr": 0.0009709561218265568, "train_loss": 0.3047762689873194, "epoch": 466}
[02:59:30.699026] [02:59:30.699110] Training epoch 466 for 0:01:45
[02:59:30.699163] [02:59:30.703611] log_dir: ./exp/debug/cifar100-LT/debug
[02:59:32.566953] Epoch: [467]  [  0/195]  eta: 0:06:03  lr: 0.000968  loss: 0.3163 (0.3163)  time: 1.8623  data: 1.3509  max mem: 9341
[02:59:42.796331] Epoch: [467]  [ 20/195]  eta: 0:01:40  lr: 0.000968  loss: 0.3044 (0.3087)  time: 0.5114  data: 0.0002  max mem: 9341
[02:59:53.013151] Epoch: [467]  [ 40/195]  eta: 0:01:24  lr: 0.000968  loss: 0.3028 (0.3056)  time: 0.5107  data: 0.0002  max mem: 9341
[03:00:03.224394] Epoch: [467]  [ 60/195]  eta: 0:01:11  lr: 0.000967  loss: 0.3038 (0.3040)  time: 0.5105  data: 0.0003  max mem: 9341
[03:00:13.481648] Epoch: [467]  [ 80/195]  eta: 0:01:00  lr: 0.000966  loss: 0.3074 (0.3037)  time: 0.5128  data: 0.0003  max mem: 9341
[03:00:23.696168] Epoch: [467]  [100/195]  eta: 0:00:49  lr: 0.000966  loss: 0.3097 (0.3054)  time: 0.5106  data: 0.0003  max mem: 9341
[03:00:33.913962] Epoch: [467]  [120/195]  eta: 0:00:39  lr: 0.000966  loss: 0.2975 (0.3047)  time: 0.5108  data: 0.0003  max mem: 9341
[03:00:44.126162] Epoch: [467]  [140/195]  eta: 0:00:28  lr: 0.000965  loss: 0.3076 (0.3054)  time: 0.5105  data: 0.0003  max mem: 9341
[03:00:54.386126] Epoch: [467]  [160/195]  eta: 0:00:18  lr: 0.000964  loss: 0.3147 (0.3058)  time: 0.5129  data: 0.0003  max mem: 9341
[03:01:04.554913] Epoch: [467]  [180/195]  eta: 0:00:07  lr: 0.000964  loss: 0.3155 (0.3069)  time: 0.5084  data: 0.0002  max mem: 9341
[03:01:11.685771] Epoch: [467]  [194/195]  eta: 0:00:00  lr: 0.000964  loss: 0.3140 (0.3068)  time: 0.5105  data: 0.0001  max mem: 9341
[03:01:11.881219] Epoch: [467] Total time: 0:01:41 (0.5189 s / it)
[03:01:11.894030] Averaged stats: lr: 0.000964  loss: 0.3140 (0.3053)
[03:01:16.668219] {"train_lr": 0.0009660888821266142, "train_loss": 0.3053327640470786, "epoch": 467}
[03:01:16.668544] [03:01:16.668649] Training epoch 467 for 0:01:45
[03:01:16.668705] [03:01:16.673795] log_dir: ./exp/debug/cifar100-LT/debug
[03:01:18.350912] Epoch: [468]  [  0/195]  eta: 0:05:26  lr: 0.000963  loss: 0.2880 (0.2880)  time: 1.6757  data: 1.1652  max mem: 9341
[03:01:28.561387] Epoch: [468]  [ 20/195]  eta: 0:01:39  lr: 0.000963  loss: 0.3033 (0.3062)  time: 0.5105  data: 0.0003  max mem: 9341
[03:01:38.777237] Epoch: [468]  [ 40/195]  eta: 0:01:23  lr: 0.000963  loss: 0.3048 (0.3050)  time: 0.5107  data: 0.0003  max mem: 9341
[03:01:49.019458] Epoch: [468]  [ 60/195]  eta: 0:01:11  lr: 0.000962  loss: 0.3049 (0.3051)  time: 0.5120  data: 0.0002  max mem: 9341
[03:01:59.281206] Epoch: [468]  [ 80/195]  eta: 0:01:00  lr: 0.000961  loss: 0.3074 (0.3060)  time: 0.5130  data: 0.0002  max mem: 9341
[03:02:09.494866] Epoch: [468]  [100/195]  eta: 0:00:49  lr: 0.000961  loss: 0.3090 (0.3059)  time: 0.5106  data: 0.0002  max mem: 9341
[03:02:19.705559] Epoch: [468]  [120/195]  eta: 0:00:39  lr: 0.000961  loss: 0.3153 (0.3076)  time: 0.5105  data: 0.0002  max mem: 9341
[03:02:29.921783] Epoch: [468]  [140/195]  eta: 0:00:28  lr: 0.000960  loss: 0.3010 (0.3065)  time: 0.5108  data: 0.0002  max mem: 9341
[03:02:40.179081] Epoch: [468]  [160/195]  eta: 0:00:18  lr: 0.000959  loss: 0.3024 (0.3061)  time: 0.5128  data: 0.0002  max mem: 9341
[03:02:50.346323] Epoch: [468]  [180/195]  eta: 0:00:07  lr: 0.000959  loss: 0.3003 (0.3059)  time: 0.5083  data: 0.0001  max mem: 9341
[03:02:57.474409] Epoch: [468]  [194/195]  eta: 0:00:00  lr: 0.000959  loss: 0.3003 (0.3056)  time: 0.5103  data: 0.0001  max mem: 9341
[03:02:57.648830] Epoch: [468] Total time: 0:01:40 (0.5178 s / it)
[03:02:57.673520] Averaged stats: lr: 0.000959  loss: 0.3003 (0.3060)
[03:03:02.326245] {"train_lr": 0.0009612256393222935, "train_loss": 0.3060121055596914, "epoch": 468}
[03:03:02.326510] [03:03:02.326612] Training epoch 468 for 0:01:45
[03:03:02.326667] [03:03:02.331078] log_dir: ./exp/debug/cifar100-LT/debug
[03:03:04.088237] Epoch: [469]  [  0/195]  eta: 0:05:42  lr: 0.000959  loss: 0.3034 (0.3034)  time: 1.7560  data: 1.2363  max mem: 9341
[03:03:14.303638] Epoch: [469]  [ 20/195]  eta: 0:01:39  lr: 0.000958  loss: 0.3049 (0.3086)  time: 0.5107  data: 0.0002  max mem: 9341
[03:03:24.514177] Epoch: [469]  [ 40/195]  eta: 0:01:23  lr: 0.000958  loss: 0.2989 (0.3039)  time: 0.5105  data: 0.0002  max mem: 9341
[03:03:34.724421] Epoch: [469]  [ 60/195]  eta: 0:01:11  lr: 0.000957  loss: 0.3038 (0.3041)  time: 0.5105  data: 0.0002  max mem: 9341
[03:03:44.978125] Epoch: [469]  [ 80/195]  eta: 0:01:00  lr: 0.000957  loss: 0.3115 (0.3062)  time: 0.5126  data: 0.0002  max mem: 9341
[03:03:55.198105] Epoch: [469]  [100/195]  eta: 0:00:49  lr: 0.000956  loss: 0.3018 (0.3061)  time: 0.5109  data: 0.0002  max mem: 9341
[03:04:05.415311] Epoch: [469]  [120/195]  eta: 0:00:39  lr: 0.000956  loss: 0.3043 (0.3056)  time: 0.5108  data: 0.0002  max mem: 9341
[03:04:15.632190] Epoch: [469]  [140/195]  eta: 0:00:28  lr: 0.000955  loss: 0.2996 (0.3053)  time: 0.5108  data: 0.0002  max mem: 9341
[03:04:25.891757] Epoch: [469]  [160/195]  eta: 0:00:18  lr: 0.000955  loss: 0.3129 (0.3058)  time: 0.5129  data: 0.0002  max mem: 9341
[03:04:36.057099] Epoch: [469]  [180/195]  eta: 0:00:07  lr: 0.000954  loss: 0.3096 (0.3062)  time: 0.5082  data: 0.0001  max mem: 9341
[03:04:43.184595] Epoch: [469]  [194/195]  eta: 0:00:00  lr: 0.000954  loss: 0.3089 (0.3063)  time: 0.5102  data: 0.0001  max mem: 9341
[03:04:43.360772] Epoch: [469] Total time: 0:01:41 (0.5181 s / it)
[03:04:43.370026] Averaged stats: lr: 0.000954  loss: 0.3089 (0.3053)
[03:04:48.086796] {"train_lr": 0.0009563664765129961, "train_loss": 0.30525562566442366, "epoch": 469}
[03:04:48.087061] [03:04:48.087144] Training epoch 469 for 0:01:45
[03:04:48.087197] [03:04:48.091784] log_dir: ./exp/debug/cifar100-LT/debug
[03:04:49.920025] Epoch: [470]  [  0/195]  eta: 0:05:56  lr: 0.000954  loss: 0.2999 (0.2999)  time: 1.8272  data: 1.3324  max mem: 9341
[03:05:00.153763] Epoch: [470]  [ 20/195]  eta: 0:01:40  lr: 0.000953  loss: 0.3047 (0.3078)  time: 0.5116  data: 0.0002  max mem: 9341
[03:05:10.375139] Epoch: [470]  [ 40/195]  eta: 0:01:24  lr: 0.000953  loss: 0.2994 (0.3054)  time: 0.5110  data: 0.0003  max mem: 9341
[03:05:20.596036] Epoch: [470]  [ 60/195]  eta: 0:01:11  lr: 0.000953  loss: 0.3052 (0.3061)  time: 0.5110  data: 0.0002  max mem: 9341
[03:05:30.898224] Epoch: [470]  [ 80/195]  eta: 0:01:00  lr: 0.000952  loss: 0.3015 (0.3057)  time: 0.5151  data: 0.0002  max mem: 9341
[03:05:41.116422] Epoch: [470]  [100/195]  eta: 0:00:49  lr: 0.000951  loss: 0.3067 (0.3058)  time: 0.5108  data: 0.0003  max mem: 9341
[03:05:51.333997] Epoch: [470]  [120/195]  eta: 0:00:39  lr: 0.000951  loss: 0.2985 (0.3048)  time: 0.5108  data: 0.0003  max mem: 9341
[03:06:01.545354] Epoch: [470]  [140/195]  eta: 0:00:28  lr: 0.000951  loss: 0.3037 (0.3041)  time: 0.5105  data: 0.0002  max mem: 9341
[03:06:11.799056] Epoch: [470]  [160/195]  eta: 0:00:18  lr: 0.000950  loss: 0.3081 (0.3041)  time: 0.5126  data: 0.0002  max mem: 9341
[03:06:21.971398] Epoch: [470]  [180/195]  eta: 0:00:07  lr: 0.000949  loss: 0.2909 (0.3032)  time: 0.5086  data: 0.0002  max mem: 9341
[03:06:29.101554] Epoch: [470]  [194/195]  eta: 0:00:00  lr: 0.000949  loss: 0.3032 (0.3033)  time: 0.5105  data: 0.0001  max mem: 9341
[03:06:29.270553] Epoch: [470] Total time: 0:01:41 (0.5189 s / it)
[03:06:29.290610] Averaged stats: lr: 0.000949  loss: 0.3032 (0.3034)
[03:06:34.143621] {"train_lr": 0.0009515114767284091, "train_loss": 0.3033919246724019, "epoch": 470}
[03:06:34.144023] [03:06:34.144151] Training epoch 470 for 0:01:46
[03:06:34.144208] [03:06:34.149339] log_dir: ./exp/debug/cifar100-LT/debug
[03:06:35.776029] Epoch: [471]  [  0/195]  eta: 0:05:16  lr: 0.000949  loss: 0.3010 (0.3010)  time: 1.6253  data: 1.1235  max mem: 9341
[03:06:46.068914] Epoch: [471]  [ 20/195]  eta: 0:01:39  lr: 0.000948  loss: 0.2918 (0.2917)  time: 0.5146  data: 0.0002  max mem: 9341
[03:06:56.281577] Epoch: [471]  [ 40/195]  eta: 0:01:23  lr: 0.000948  loss: 0.3072 (0.2997)  time: 0.5106  data: 0.0002  max mem: 9341
[03:07:06.491725] Epoch: [471]  [ 60/195]  eta: 0:01:11  lr: 0.000948  loss: 0.2991 (0.2991)  time: 0.5105  data: 0.0002  max mem: 9341
[03:07:16.745606] Epoch: [471]  [ 80/195]  eta: 0:01:00  lr: 0.000947  loss: 0.3052 (0.3019)  time: 0.5126  data: 0.0002  max mem: 9341
[03:07:26.959091] Epoch: [471]  [100/195]  eta: 0:00:49  lr: 0.000947  loss: 0.2925 (0.3003)  time: 0.5106  data: 0.0002  max mem: 9341
[03:07:37.174465] Epoch: [471]  [120/195]  eta: 0:00:39  lr: 0.000946  loss: 0.2954 (0.3005)  time: 0.5107  data: 0.0002  max mem: 9341
[03:07:47.385171] Epoch: [471]  [140/195]  eta: 0:00:28  lr: 0.000946  loss: 0.3021 (0.3009)  time: 0.5105  data: 0.0002  max mem: 9341
[03:07:57.641049] Epoch: [471]  [160/195]  eta: 0:00:18  lr: 0.000945  loss: 0.3051 (0.3016)  time: 0.5127  data: 0.0002  max mem: 9341
[03:08:07.814747] Epoch: [471]  [180/195]  eta: 0:00:07  lr: 0.000945  loss: 0.2965 (0.3018)  time: 0.5086  data: 0.0001  max mem: 9341
[03:08:14.948889] Epoch: [471]  [194/195]  eta: 0:00:00  lr: 0.000944  loss: 0.3070 (0.3025)  time: 0.5107  data: 0.0001  max mem: 9341
[03:08:15.112980] Epoch: [471] Total time: 0:01:40 (0.5178 s / it)
[03:08:15.128809] Averaged stats: lr: 0.000944  loss: 0.3070 (0.3032)
[03:08:19.994742] {"train_lr": 0.0009466607229270815, "train_loss": 0.3031875330095108, "epoch": 471}
[03:08:19.995086] [03:08:19.995168] Training epoch 471 for 0:01:45
[03:08:19.995223] [03:08:19.999745] log_dir: ./exp/debug/cifar100-LT/debug
[03:08:21.758963] Epoch: [472]  [  0/195]  eta: 0:05:42  lr: 0.000944  loss: 0.3214 (0.3214)  time: 1.7579  data: 1.2565  max mem: 9341
[03:08:31.998512] Epoch: [472]  [ 20/195]  eta: 0:01:39  lr: 0.000944  loss: 0.2986 (0.3006)  time: 0.5119  data: 0.0003  max mem: 9341
[03:08:42.208849] Epoch: [472]  [ 40/195]  eta: 0:01:23  lr: 0.000943  loss: 0.3032 (0.3034)  time: 0.5105  data: 0.0002  max mem: 9341
[03:08:52.420127] Epoch: [472]  [ 60/195]  eta: 0:01:11  lr: 0.000943  loss: 0.3075 (0.3056)  time: 0.5105  data: 0.0003  max mem: 9341
[03:09:02.703461] Epoch: [472]  [ 80/195]  eta: 0:01:00  lr: 0.000942  loss: 0.2990 (0.3054)  time: 0.5141  data: 0.0002  max mem: 9341
[03:09:12.918220] Epoch: [472]  [100/195]  eta: 0:00:49  lr: 0.000942  loss: 0.3026 (0.3055)  time: 0.5107  data: 0.0002  max mem: 9341
[03:09:23.136455] Epoch: [472]  [120/195]  eta: 0:00:39  lr: 0.000941  loss: 0.3006 (0.3058)  time: 0.5109  data: 0.0002  max mem: 9341
[03:09:33.355024] Epoch: [472]  [140/195]  eta: 0:00:28  lr: 0.000941  loss: 0.2941 (0.3047)  time: 0.5109  data: 0.0002  max mem: 9341
[03:09:43.616975] Epoch: [472]  [160/195]  eta: 0:00:18  lr: 0.000940  loss: 0.3077 (0.3050)  time: 0.5130  data: 0.0003  max mem: 9341
[03:09:53.790783] Epoch: [472]  [180/195]  eta: 0:00:07  lr: 0.000940  loss: 0.3109 (0.3061)  time: 0.5086  data: 0.0002  max mem: 9341
[03:10:00.918664] Epoch: [472]  [194/195]  eta: 0:00:00  lr: 0.000939  loss: 0.3105 (0.3069)  time: 0.5104  data: 0.0001  max mem: 9341
[03:10:01.102495] Epoch: [472] Total time: 0:01:41 (0.5185 s / it)
[03:10:01.111106] Averaged stats: lr: 0.000939  loss: 0.3105 (0.3073)
[03:10:05.836754] {"train_lr": 0.0009418142979950079, "train_loss": 0.3072719740944031, "epoch": 472}
[03:10:05.837085] [03:10:05.837190] Training epoch 472 for 0:01:45
[03:10:05.837245] [03:10:05.841694] log_dir: ./exp/debug/cifar100-LT/debug
[03:10:07.559595] Epoch: [473]  [  0/195]  eta: 0:05:34  lr: 0.000939  loss: 0.3100 (0.3100)  time: 1.7165  data: 1.2061  max mem: 9341
[03:10:17.778591] Epoch: [473]  [ 20/195]  eta: 0:01:39  lr: 0.000939  loss: 0.3000 (0.3027)  time: 0.5109  data: 0.0003  max mem: 9341
[03:10:27.991098] Epoch: [473]  [ 40/195]  eta: 0:01:23  lr: 0.000938  loss: 0.3047 (0.3048)  time: 0.5106  data: 0.0002  max mem: 9341
[03:10:38.203585] Epoch: [473]  [ 60/195]  eta: 0:01:11  lr: 0.000938  loss: 0.3053 (0.3077)  time: 0.5105  data: 0.0002  max mem: 9341
[03:10:48.462130] Epoch: [473]  [ 80/195]  eta: 0:01:00  lr: 0.000937  loss: 0.3058 (0.3086)  time: 0.5126  data: 0.0002  max mem: 9341
[03:10:58.676866] Epoch: [473]  [100/195]  eta: 0:00:49  lr: 0.000937  loss: 0.3051 (0.3079)  time: 0.5107  data: 0.0002  max mem: 9341
[03:11:08.893522] Epoch: [473]  [120/195]  eta: 0:00:39  lr: 0.000936  loss: 0.2987 (0.3072)  time: 0.5108  data: 0.0003  max mem: 9341
[03:11:19.107229] Epoch: [473]  [140/195]  eta: 0:00:28  lr: 0.000936  loss: 0.2985 (0.3065)  time: 0.5106  data: 0.0002  max mem: 9341
[03:11:29.361707] Epoch: [473]  [160/195]  eta: 0:00:18  lr: 0.000935  loss: 0.3049 (0.3065)  time: 0.5127  data: 0.0002  max mem: 9341
[03:11:39.531520] Epoch: [473]  [180/195]  eta: 0:00:07  lr: 0.000935  loss: 0.3048 (0.3062)  time: 0.5084  data: 0.0002  max mem: 9341
[03:11:46.661704] Epoch: [473]  [194/195]  eta: 0:00:00  lr: 0.000934  loss: 0.2981 (0.3052)  time: 0.5104  data: 0.0001  max mem: 9341
[03:11:46.834708] Epoch: [473] Total time: 0:01:40 (0.5179 s / it)
[03:11:46.843402] Averaged stats: lr: 0.000934  loss: 0.2981 (0.3050)
[03:11:51.672796] {"train_lr": 0.0009369722847442275, "train_loss": 0.3050466017272228, "epoch": 473}
[03:11:51.673409] [03:11:51.673523] Training epoch 473 for 0:01:45
[03:11:51.673591] [03:11:51.678062] log_dir: ./exp/debug/cifar100-LT/debug
[03:11:53.388784] Epoch: [474]  [  0/195]  eta: 0:05:33  lr: 0.000934  loss: 0.2820 (0.2820)  time: 1.7093  data: 1.1971  max mem: 9341
[03:12:03.597514] Epoch: [474]  [ 20/195]  eta: 0:01:39  lr: 0.000934  loss: 0.2941 (0.2969)  time: 0.5104  data: 0.0002  max mem: 9341
[03:12:13.836198] Epoch: [474]  [ 40/195]  eta: 0:01:23  lr: 0.000934  loss: 0.3009 (0.3009)  time: 0.5119  data: 0.0002  max mem: 9341
[03:12:24.052315] Epoch: [474]  [ 60/195]  eta: 0:01:11  lr: 0.000933  loss: 0.3025 (0.3010)  time: 0.5107  data: 0.0002  max mem: 9341
[03:12:34.316975] Epoch: [474]  [ 80/195]  eta: 0:01:00  lr: 0.000932  loss: 0.2925 (0.2997)  time: 0.5132  data: 0.0002  max mem: 9341
[03:12:44.583049] Epoch: [474]  [100/195]  eta: 0:00:49  lr: 0.000932  loss: 0.3045 (0.3006)  time: 0.5132  data: 0.0002  max mem: 9341
[03:12:54.800203] Epoch: [474]  [120/195]  eta: 0:00:39  lr: 0.000932  loss: 0.3012 (0.3008)  time: 0.5108  data: 0.0002  max mem: 9341
[03:13:05.024148] Epoch: [474]  [140/195]  eta: 0:00:28  lr: 0.000931  loss: 0.3102 (0.3018)  time: 0.5111  data: 0.0002  max mem: 9341
[03:13:15.279474] Epoch: [474]  [160/195]  eta: 0:00:18  lr: 0.000930  loss: 0.3024 (0.3025)  time: 0.5127  data: 0.0002  max mem: 9341
[03:13:25.458060] Epoch: [474]  [180/195]  eta: 0:00:07  lr: 0.000930  loss: 0.2997 (0.3029)  time: 0.5089  data: 0.0001  max mem: 9341
[03:13:32.593454] Epoch: [474]  [194/195]  eta: 0:00:00  lr: 0.000930  loss: 0.2951 (0.3029)  time: 0.5108  data: 0.0001  max mem: 9341
[03:13:32.771822] Epoch: [474] Total time: 0:01:41 (0.5184 s / it)
[03:13:32.783828] Averaged stats: lr: 0.000930  loss: 0.2951 (0.3032)
[03:13:37.596743] {"train_lr": 0.0009321347659113806, "train_loss": 0.303209183517939, "epoch": 474}
[03:13:37.597003] [03:13:37.597089] Training epoch 474 for 0:01:45
[03:13:37.597142] [03:13:37.601570] log_dir: ./exp/debug/cifar100-LT/debug
[03:13:39.134192] Epoch: [475]  [  0/195]  eta: 0:04:58  lr: 0.000930  loss: 0.3182 (0.3182)  time: 1.5305  data: 1.0397  max mem: 9341
[03:13:49.378282] Epoch: [475]  [ 20/195]  eta: 0:01:38  lr: 0.000929  loss: 0.2950 (0.3054)  time: 0.5121  data: 0.0003  max mem: 9341
[03:13:59.655020] Epoch: [475]  [ 40/195]  eta: 0:01:23  lr: 0.000929  loss: 0.3138 (0.3072)  time: 0.5138  data: 0.0002  max mem: 9341
[03:14:09.891055] Epoch: [475]  [ 60/195]  eta: 0:01:11  lr: 0.000928  loss: 0.3031 (0.3066)  time: 0.5117  data: 0.0002  max mem: 9341
[03:14:20.151574] Epoch: [475]  [ 80/195]  eta: 0:01:00  lr: 0.000928  loss: 0.3092 (0.3066)  time: 0.5130  data: 0.0003  max mem: 9341
[03:14:30.361136] Epoch: [475]  [100/195]  eta: 0:00:49  lr: 0.000927  loss: 0.2972 (0.3064)  time: 0.5104  data: 0.0002  max mem: 9341
[03:14:40.579144] Epoch: [475]  [120/195]  eta: 0:00:39  lr: 0.000927  loss: 0.2899 (0.3042)  time: 0.5108  data: 0.0003  max mem: 9341
[03:14:50.794901] Epoch: [475]  [140/195]  eta: 0:00:28  lr: 0.000926  loss: 0.2961 (0.3029)  time: 0.5107  data: 0.0002  max mem: 9341
[03:15:01.061012] Epoch: [475]  [160/195]  eta: 0:00:18  lr: 0.000926  loss: 0.3034 (0.3031)  time: 0.5132  data: 0.0003  max mem: 9341
[03:15:11.233279] Epoch: [475]  [180/195]  eta: 0:00:07  lr: 0.000925  loss: 0.2983 (0.3029)  time: 0.5086  data: 0.0002  max mem: 9341
[03:15:18.360920] Epoch: [475]  [194/195]  eta: 0:00:00  lr: 0.000925  loss: 0.2985 (0.3028)  time: 0.5103  data: 0.0001  max mem: 9341
[03:15:18.534223] Epoch: [475] Total time: 0:01:40 (0.5176 s / it)
[03:15:18.566482] Averaged stats: lr: 0.000925  loss: 0.2985 (0.3040)
[03:15:23.267090] {"train_lr": 0.0009273018241563207, "train_loss": 0.30397876990147127, "epoch": 475}
[03:15:23.267401] [03:15:23.267504] Training epoch 475 for 0:01:45
[03:15:23.267569] [03:15:23.273821] log_dir: ./exp/debug/cifar100-LT/debug
[03:15:24.944231] Epoch: [476]  [  0/195]  eta: 0:05:25  lr: 0.000925  loss: 0.2823 (0.2823)  time: 1.6695  data: 1.1735  max mem: 9341
[03:15:35.162018] Epoch: [476]  [ 20/195]  eta: 0:01:39  lr: 0.000924  loss: 0.3077 (0.3055)  time: 0.5108  data: 0.0003  max mem: 9341
[03:15:45.379910] Epoch: [476]  [ 40/195]  eta: 0:01:23  lr: 0.000924  loss: 0.2938 (0.3016)  time: 0.5108  data: 0.0003  max mem: 9341
[03:15:55.610254] Epoch: [476]  [ 60/195]  eta: 0:01:11  lr: 0.000924  loss: 0.3013 (0.3026)  time: 0.5115  data: 0.0002  max mem: 9341
[03:16:05.895348] Epoch: [476]  [ 80/195]  eta: 0:01:00  lr: 0.000923  loss: 0.2959 (0.3011)  time: 0.5142  data: 0.0002  max mem: 9341
[03:16:16.113991] Epoch: [476]  [100/195]  eta: 0:00:49  lr: 0.000922  loss: 0.3032 (0.3022)  time: 0.5109  data: 0.0002  max mem: 9341
[03:16:26.327398] Epoch: [476]  [120/195]  eta: 0:00:39  lr: 0.000922  loss: 0.2934 (0.3014)  time: 0.5106  data: 0.0002  max mem: 9341
[03:16:36.550122] Epoch: [476]  [140/195]  eta: 0:00:28  lr: 0.000922  loss: 0.3051 (0.3021)  time: 0.5111  data: 0.0002  max mem: 9341
[03:16:46.806530] Epoch: [476]  [160/195]  eta: 0:00:18  lr: 0.000921  loss: 0.2986 (0.3012)  time: 0.5128  data: 0.0003  max mem: 9341
[03:16:56.986664] Epoch: [476]  [180/195]  eta: 0:00:07  lr: 0.000920  loss: 0.3092 (0.3024)  time: 0.5089  data: 0.0002  max mem: 9341
[03:17:04.122092] Epoch: [476]  [194/195]  eta: 0:00:00  lr: 0.000920  loss: 0.2918 (0.3020)  time: 0.5109  data: 0.0001  max mem: 9341
[03:17:04.305133] Epoch: [476] Total time: 0:01:41 (0.5181 s / it)
[03:17:04.327028] Averaged stats: lr: 0.000920  loss: 0.2918 (0.3023)
[03:17:08.986734] {"train_lr": 0.0009224735420606856, "train_loss": 0.3022823479695198, "epoch": 476}
[03:17:08.987032] [03:17:08.987142] Training epoch 476 for 0:01:45
[03:17:08.987197] [03:17:08.992215] log_dir: ./exp/debug/cifar100-LT/debug
[03:17:10.852268] Epoch: [477]  [  0/195]  eta: 0:06:02  lr: 0.000920  loss: 0.3383 (0.3383)  time: 1.8586  data: 1.3633  max mem: 9341
[03:17:21.062935] Epoch: [477]  [ 20/195]  eta: 0:01:40  lr: 0.000919  loss: 0.2992 (0.3037)  time: 0.5105  data: 0.0002  max mem: 9341
[03:17:31.317560] Epoch: [477]  [ 40/195]  eta: 0:01:24  lr: 0.000919  loss: 0.2968 (0.3023)  time: 0.5127  data: 0.0002  max mem: 9341
[03:17:41.531431] Epoch: [477]  [ 60/195]  eta: 0:01:12  lr: 0.000919  loss: 0.2997 (0.3030)  time: 0.5106  data: 0.0002  max mem: 9341
[03:17:51.783020] Epoch: [477]  [ 80/195]  eta: 0:01:00  lr: 0.000918  loss: 0.2961 (0.3017)  time: 0.5125  data: 0.0002  max mem: 9341
[03:18:01.992156] Epoch: [477]  [100/195]  eta: 0:00:49  lr: 0.000917  loss: 0.3014 (0.3025)  time: 0.5103  data: 0.0002  max mem: 9341
[03:18:12.197600] Epoch: [477]  [120/195]  eta: 0:00:39  lr: 0.000917  loss: 0.3023 (0.3023)  time: 0.5102  data: 0.0003  max mem: 9341
[03:18:22.412433] Epoch: [477]  [140/195]  eta: 0:00:28  lr: 0.000917  loss: 0.3045 (0.3023)  time: 0.5107  data: 0.0003  max mem: 9341
[03:18:32.666297] Epoch: [477]  [160/195]  eta: 0:00:18  lr: 0.000916  loss: 0.3033 (0.3024)  time: 0.5126  data: 0.0003  max mem: 9341
[03:18:42.834922] Epoch: [477]  [180/195]  eta: 0:00:07  lr: 0.000916  loss: 0.2903 (0.3017)  time: 0.5084  data: 0.0002  max mem: 9341
[03:18:49.959798] Epoch: [477]  [194/195]  eta: 0:00:00  lr: 0.000915  loss: 0.3015 (0.3024)  time: 0.5101  data: 0.0001  max mem: 9341
[03:18:50.144392] Epoch: [477] Total time: 0:01:41 (0.5187 s / it)
[03:18:50.153277] Averaged stats: lr: 0.000915  loss: 0.3015 (0.3013)
[03:18:54.960584] {"train_lr": 0.0009176500021264941, "train_loss": 0.3012659139931202, "epoch": 477}
[03:18:54.960890] [03:18:54.960994] Training epoch 477 for 0:01:45
[03:18:54.961049] [03:18:54.965584] log_dir: ./exp/debug/cifar100-LT/debug
[03:18:56.660451] Epoch: [478]  [  0/195]  eta: 0:05:30  lr: 0.000915  loss: 0.3694 (0.3694)  time: 1.6931  data: 1.1919  max mem: 9341
[03:19:06.898820] Epoch: [478]  [ 20/195]  eta: 0:01:39  lr: 0.000915  loss: 0.3071 (0.3080)  time: 0.5119  data: 0.0002  max mem: 9341
[03:19:17.133898] Epoch: [478]  [ 40/195]  eta: 0:01:23  lr: 0.000914  loss: 0.3127 (0.3077)  time: 0.5117  data: 0.0002  max mem: 9341
[03:19:27.371184] Epoch: [478]  [ 60/195]  eta: 0:01:11  lr: 0.000914  loss: 0.3014 (0.3065)  time: 0.5118  data: 0.0002  max mem: 9341
[03:19:37.645827] Epoch: [478]  [ 80/195]  eta: 0:01:00  lr: 0.000913  loss: 0.3079 (0.3062)  time: 0.5137  data: 0.0002  max mem: 9341
[03:19:47.859971] Epoch: [478]  [100/195]  eta: 0:00:49  lr: 0.000913  loss: 0.3044 (0.3058)  time: 0.5107  data: 0.0002  max mem: 9341
[03:19:58.082857] Epoch: [478]  [120/195]  eta: 0:00:39  lr: 0.000912  loss: 0.2984 (0.3053)  time: 0.5111  data: 0.0002  max mem: 9341
[03:20:08.295415] Epoch: [478]  [140/195]  eta: 0:00:28  lr: 0.000912  loss: 0.2980 (0.3050)  time: 0.5106  data: 0.0002  max mem: 9341
[03:20:18.552370] Epoch: [478]  [160/195]  eta: 0:00:18  lr: 0.000911  loss: 0.3116 (0.3054)  time: 0.5128  data: 0.0002  max mem: 9341
[03:20:28.740399] Epoch: [478]  [180/195]  eta: 0:00:07  lr: 0.000911  loss: 0.2966 (0.3047)  time: 0.5094  data: 0.0001  max mem: 9341
[03:20:35.872124] Epoch: [478]  [194/195]  eta: 0:00:00  lr: 0.000910  loss: 0.2962 (0.3041)  time: 0.5114  data: 0.0001  max mem: 9341
[03:20:36.048046] Epoch: [478] Total time: 0:01:41 (0.5184 s / it)
[03:20:36.061557] Averaged stats: lr: 0.000910  loss: 0.2962 (0.3014)
[03:20:40.849267] {"train_lr": 0.0009128312867747365, "train_loss": 0.30139363654530965, "epoch": 478}
[03:20:40.849548] [03:20:40.849647] Training epoch 478 for 0:01:45
[03:20:40.849701] [03:20:40.854653] log_dir: ./exp/debug/cifar100-LT/debug
[03:20:42.521465] Epoch: [479]  [  0/195]  eta: 0:05:24  lr: 0.000910  loss: 0.2916 (0.2916)  time: 1.6647  data: 1.1543  max mem: 9341
[03:20:52.742957] Epoch: [479]  [ 20/195]  eta: 0:01:39  lr: 0.000910  loss: 0.2943 (0.2959)  time: 0.5110  data: 0.0003  max mem: 9341
[03:21:02.965906] Epoch: [479]  [ 40/195]  eta: 0:01:23  lr: 0.000909  loss: 0.3053 (0.3010)  time: 0.5111  data: 0.0003  max mem: 9341
[03:21:13.187097] Epoch: [479]  [ 60/195]  eta: 0:01:11  lr: 0.000909  loss: 0.2981 (0.3025)  time: 0.5110  data: 0.0003  max mem: 9341
[03:21:23.445903] Epoch: [479]  [ 80/195]  eta: 0:01:00  lr: 0.000908  loss: 0.2982 (0.3022)  time: 0.5129  data: 0.0003  max mem: 9341
[03:21:33.658473] Epoch: [479]  [100/195]  eta: 0:00:49  lr: 0.000908  loss: 0.2965 (0.3026)  time: 0.5106  data: 0.0002  max mem: 9341
[03:21:43.868194] Epoch: [479]  [120/195]  eta: 0:00:39  lr: 0.000907  loss: 0.3047 (0.3038)  time: 0.5104  data: 0.0003  max mem: 9341
[03:21:54.078009] Epoch: [479]  [140/195]  eta: 0:00:28  lr: 0.000907  loss: 0.3021 (0.3035)  time: 0.5104  data: 0.0003  max mem: 9341
[03:22:04.336498] Epoch: [479]  [160/195]  eta: 0:00:18  lr: 0.000906  loss: 0.2928 (0.3027)  time: 0.5129  data: 0.0003  max mem: 9341
[03:22:14.510318] Epoch: [479]  [180/195]  eta: 0:00:07  lr: 0.000906  loss: 0.2996 (0.3034)  time: 0.5086  data: 0.0002  max mem: 9341
[03:22:21.639646] Epoch: [479]  [194/195]  eta: 0:00:00  lr: 0.000905  loss: 0.2999 (0.3040)  time: 0.5105  data: 0.0001  max mem: 9341
[03:22:21.814442] Epoch: [479] Total time: 0:01:40 (0.5177 s / it)
[03:22:21.830185] Averaged stats: lr: 0.000905  loss: 0.2999 (0.3012)
[03:22:26.532716] {"train_lr": 0.0009080174783439623, "train_loss": 0.30124009364308457, "epoch": 479}
[03:22:26.532981] [03:22:26.533085] Training epoch 479 for 0:01:45
[03:22:26.533139] [03:22:26.537549] log_dir: ./exp/debug/cifar100-LT/debug
[03:22:28.321766] Epoch: [480]  [  0/195]  eta: 0:05:47  lr: 0.000905  loss: 0.2875 (0.2875)  time: 1.7816  data: 1.2622  max mem: 9341
[03:22:38.550482] Epoch: [480]  [ 20/195]  eta: 0:01:40  lr: 0.000905  loss: 0.3027 (0.2999)  time: 0.5114  data: 0.0003  max mem: 9341
[03:22:48.790270] Epoch: [480]  [ 40/195]  eta: 0:01:24  lr: 0.000905  loss: 0.2954 (0.2982)  time: 0.5119  data: 0.0002  max mem: 9341
[03:22:59.030299] Epoch: [480]  [ 60/195]  eta: 0:01:11  lr: 0.000904  loss: 0.2921 (0.2992)  time: 0.5119  data: 0.0002  max mem: 9341
[03:23:09.333248] Epoch: [480]  [ 80/195]  eta: 0:01:00  lr: 0.000903  loss: 0.3075 (0.3011)  time: 0.5151  data: 0.0002  max mem: 9341
[03:23:19.571431] Epoch: [480]  [100/195]  eta: 0:00:49  lr: 0.000903  loss: 0.2976 (0.2996)  time: 0.5118  data: 0.0002  max mem: 9341
[03:23:29.808596] Epoch: [480]  [120/195]  eta: 0:00:39  lr: 0.000903  loss: 0.2892 (0.2990)  time: 0.5118  data: 0.0003  max mem: 9341
[03:23:40.045073] Epoch: [480]  [140/195]  eta: 0:00:28  lr: 0.000902  loss: 0.3006 (0.2993)  time: 0.5117  data: 0.0002  max mem: 9341
[03:23:50.347199] Epoch: [480]  [160/195]  eta: 0:00:18  lr: 0.000901  loss: 0.2931 (0.2987)  time: 0.5150  data: 0.0002  max mem: 9341
[03:24:00.542108] Epoch: [480]  [180/195]  eta: 0:00:07  lr: 0.000901  loss: 0.2964 (0.2990)  time: 0.5097  data: 0.0002  max mem: 9341
[03:24:07.696918] Epoch: [480]  [194/195]  eta: 0:00:00  lr: 0.000901  loss: 0.2945 (0.2987)  time: 0.5128  data: 0.0001  max mem: 9341
[03:24:07.887745] Epoch: [480] Total time: 0:01:41 (0.5197 s / it)
[03:24:07.896954] Averaged stats: lr: 0.000901  loss: 0.2945 (0.3015)
[03:24:12.503753] {"train_lr": 0.0009032086590888782, "train_loss": 0.301461873566493, "epoch": 480}
[03:24:12.504190] [03:24:12.504305] Training epoch 480 for 0:01:45
[03:24:12.504374] [03:24:12.508733] log_dir: ./exp/debug/cifar100-LT/debug
[03:24:14.511819] Epoch: [481]  [  0/195]  eta: 0:06:30  lr: 0.000901  loss: 0.3145 (0.3145)  time: 2.0011  data: 1.4959  max mem: 9341
[03:24:24.722882] Epoch: [481]  [ 20/195]  eta: 0:01:41  lr: 0.000900  loss: 0.3014 (0.3020)  time: 0.5105  data: 0.0003  max mem: 9341
[03:24:34.936793] Epoch: [481]  [ 40/195]  eta: 0:01:24  lr: 0.000900  loss: 0.3041 (0.3049)  time: 0.5106  data: 0.0002  max mem: 9341
[03:24:45.152576] Epoch: [481]  [ 60/195]  eta: 0:01:12  lr: 0.000899  loss: 0.3014 (0.3044)  time: 0.5107  data: 0.0002  max mem: 9341
[03:24:55.409647] Epoch: [481]  [ 80/195]  eta: 0:01:00  lr: 0.000899  loss: 0.3163 (0.3071)  time: 0.5128  data: 0.0003  max mem: 9341
[03:25:05.615291] Epoch: [481]  [100/195]  eta: 0:00:49  lr: 0.000898  loss: 0.3036 (0.3066)  time: 0.5102  data: 0.0003  max mem: 9341
[03:25:15.826501] Epoch: [481]  [120/195]  eta: 0:00:39  lr: 0.000898  loss: 0.2906 (0.3047)  time: 0.5105  data: 0.0003  max mem: 9341
[03:25:26.032481] Epoch: [481]  [140/195]  eta: 0:00:28  lr: 0.000897  loss: 0.3194 (0.3061)  time: 0.5102  data: 0.0003  max mem: 9341
[03:25:36.292626] Epoch: [481]  [160/195]  eta: 0:00:18  lr: 0.000897  loss: 0.3036 (0.3059)  time: 0.5129  data: 0.0003  max mem: 9341
[03:25:46.460520] Epoch: [481]  [180/195]  eta: 0:00:07  lr: 0.000896  loss: 0.2953 (0.3054)  time: 0.5083  data: 0.0002  max mem: 9341
[03:25:53.589931] Epoch: [481]  [194/195]  eta: 0:00:00  lr: 0.000896  loss: 0.2953 (0.3050)  time: 0.5104  data: 0.0001  max mem: 9341
[03:25:53.761611] Epoch: [481] Total time: 0:01:41 (0.5192 s / it)
[03:25:53.764143] Averaged stats: lr: 0.000896  loss: 0.2953 (0.3052)
[03:25:58.428618] {"train_lr": 0.0008984049111789325, "train_loss": 0.30522789695324043, "epoch": 481}
[03:25:58.428889] [03:25:58.428994] Training epoch 481 for 0:01:45
[03:25:58.429049] [03:25:58.433503] log_dir: ./exp/debug/cifar100-LT/debug
[03:26:00.207541] Epoch: [482]  [  0/195]  eta: 0:05:45  lr: 0.000896  loss: 0.2962 (0.2962)  time: 1.7728  data: 1.2696  max mem: 9341
[03:26:10.414600] Epoch: [482]  [ 20/195]  eta: 0:01:39  lr: 0.000895  loss: 0.3063 (0.3034)  time: 0.5103  data: 0.0002  max mem: 9341
[03:26:20.632531] Epoch: [482]  [ 40/195]  eta: 0:01:23  lr: 0.000895  loss: 0.3141 (0.3055)  time: 0.5108  data: 0.0002  max mem: 9341
[03:26:30.844365] Epoch: [482]  [ 60/195]  eta: 0:01:11  lr: 0.000895  loss: 0.3021 (0.3051)  time: 0.5105  data: 0.0002  max mem: 9341
[03:26:41.100650] Epoch: [482]  [ 80/195]  eta: 0:01:00  lr: 0.000894  loss: 0.2979 (0.3048)  time: 0.5128  data: 0.0002  max mem: 9341
[03:26:51.310791] Epoch: [482]  [100/195]  eta: 0:00:49  lr: 0.000893  loss: 0.3046 (0.3049)  time: 0.5104  data: 0.0002  max mem: 9341
[03:27:01.520557] Epoch: [482]  [120/195]  eta: 0:00:39  lr: 0.000893  loss: 0.3006 (0.3049)  time: 0.5104  data: 0.0002  max mem: 9341
[03:27:11.733380] Epoch: [482]  [140/195]  eta: 0:00:28  lr: 0.000893  loss: 0.2983 (0.3045)  time: 0.5106  data: 0.0002  max mem: 9341
[03:27:21.988061] Epoch: [482]  [160/195]  eta: 0:00:18  lr: 0.000892  loss: 0.2956 (0.3030)  time: 0.5127  data: 0.0002  max mem: 9341
[03:27:32.149112] Epoch: [482]  [180/195]  eta: 0:00:07  lr: 0.000891  loss: 0.3022 (0.3028)  time: 0.5080  data: 0.0002  max mem: 9341
[03:27:39.278339] Epoch: [482]  [194/195]  eta: 0:00:00  lr: 0.000891  loss: 0.2895 (0.3023)  time: 0.5103  data: 0.0001  max mem: 9341
[03:27:39.454995] Epoch: [482] Total time: 0:01:41 (0.5181 s / it)
[03:27:39.463242] Averaged stats: lr: 0.000891  loss: 0.2895 (0.3024)
[03:27:44.110814] {"train_lr": 0.0008936063166969287, "train_loss": 0.3024201926512596, "epoch": 482}
[03:27:44.111128] [03:27:44.111226] Training epoch 482 for 0:01:45
[03:27:44.111334] [03:27:44.115826] log_dir: ./exp/debug/cifar100-LT/debug
[03:27:45.759065] Epoch: [483]  [  0/195]  eta: 0:05:20  lr: 0.000891  loss: 0.3075 (0.3075)  time: 1.6418  data: 1.1324  max mem: 9341
[03:27:55.976265] Epoch: [483]  [ 20/195]  eta: 0:01:38  lr: 0.000891  loss: 0.2979 (0.3020)  time: 0.5108  data: 0.0003  max mem: 9341
[03:28:06.186664] Epoch: [483]  [ 40/195]  eta: 0:01:23  lr: 0.000890  loss: 0.2972 (0.3006)  time: 0.5105  data: 0.0002  max mem: 9341
[03:28:16.404398] Epoch: [483]  [ 60/195]  eta: 0:01:11  lr: 0.000890  loss: 0.2975 (0.2996)  time: 0.5108  data: 0.0002  max mem: 9341
[03:28:26.664376] Epoch: [483]  [ 80/195]  eta: 0:01:00  lr: 0.000889  loss: 0.2917 (0.2981)  time: 0.5129  data: 0.0002  max mem: 9341
[03:28:36.876886] Epoch: [483]  [100/195]  eta: 0:00:49  lr: 0.000889  loss: 0.3017 (0.2988)  time: 0.5106  data: 0.0002  max mem: 9341
[03:28:47.102432] Epoch: [483]  [120/195]  eta: 0:00:39  lr: 0.000888  loss: 0.2936 (0.2990)  time: 0.5112  data: 0.0003  max mem: 9341
[03:28:57.317117] Epoch: [483]  [140/195]  eta: 0:00:28  lr: 0.000888  loss: 0.3041 (0.2995)  time: 0.5107  data: 0.0002  max mem: 9341
[03:29:07.617710] Epoch: [483]  [160/195]  eta: 0:00:18  lr: 0.000887  loss: 0.3114 (0.3005)  time: 0.5150  data: 0.0002  max mem: 9341
[03:29:17.812134] Epoch: [483]  [180/195]  eta: 0:00:07  lr: 0.000887  loss: 0.3007 (0.3010)  time: 0.5097  data: 0.0002  max mem: 9341
[03:29:24.962496] Epoch: [483]  [194/195]  eta: 0:00:00  lr: 0.000886  loss: 0.2963 (0.3008)  time: 0.5126  data: 0.0001  max mem: 9341
[03:29:25.129746] Epoch: [483] Total time: 0:01:41 (0.5180 s / it)
[03:29:25.149975] Averaged stats: lr: 0.000886  loss: 0.2963 (0.3024)
[03:29:29.911136] {"train_lr": 0.0008888129576375995, "train_loss": 0.3023968467918726, "epoch": 483}
[03:29:29.911414] [03:29:29.911518] Training epoch 483 for 0:01:45
[03:29:29.911572] [03:29:29.916553] log_dir: ./exp/debug/cifar100-LT/debug
[03:29:31.737708] Epoch: [484]  [  0/195]  eta: 0:05:54  lr: 0.000886  loss: 0.3084 (0.3084)  time: 1.8200  data: 1.3250  max mem: 9341
[03:29:41.942140] Epoch: [484]  [ 20/195]  eta: 0:01:40  lr: 0.000886  loss: 0.3088 (0.3083)  time: 0.5102  data: 0.0002  max mem: 9341
[03:29:52.151839] Epoch: [484]  [ 40/195]  eta: 0:01:24  lr: 0.000885  loss: 0.3009 (0.3042)  time: 0.5104  data: 0.0002  max mem: 9341
[03:30:02.359038] Epoch: [484]  [ 60/195]  eta: 0:01:11  lr: 0.000885  loss: 0.3010 (0.3046)  time: 0.5103  data: 0.0002  max mem: 9341
[03:30:12.616984] Epoch: [484]  [ 80/195]  eta: 0:01:00  lr: 0.000884  loss: 0.3159 (0.3061)  time: 0.5128  data: 0.0002  max mem: 9341
[03:30:22.829009] Epoch: [484]  [100/195]  eta: 0:00:49  lr: 0.000884  loss: 0.2970 (0.3053)  time: 0.5105  data: 0.0002  max mem: 9341
[03:30:33.041680] Epoch: [484]  [120/195]  eta: 0:00:39  lr: 0.000883  loss: 0.2854 (0.3042)  time: 0.5106  data: 0.0002  max mem: 9341
[03:30:43.251412] Epoch: [484]  [140/195]  eta: 0:00:28  lr: 0.000883  loss: 0.3026 (0.3041)  time: 0.5104  data: 0.0002  max mem: 9341
[03:30:53.508876] Epoch: [484]  [160/195]  eta: 0:00:18  lr: 0.000882  loss: 0.2970 (0.3039)  time: 0.5128  data: 0.0002  max mem: 9341
[03:31:03.677668] Epoch: [484]  [180/195]  eta: 0:00:07  lr: 0.000882  loss: 0.3070 (0.3047)  time: 0.5084  data: 0.0001  max mem: 9341
[03:31:10.804212] Epoch: [484]  [194/195]  eta: 0:00:00  lr: 0.000882  loss: 0.3064 (0.3049)  time: 0.5103  data: 0.0001  max mem: 9341
[03:31:10.965510] Epoch: [484] Total time: 0:01:41 (0.5182 s / it)
[03:31:10.978001] Averaged stats: lr: 0.000882  loss: 0.3064 (0.3026)
[03:31:15.676385] {"train_lr": 0.0008840249159062333, "train_loss": 0.30257085538827455, "epoch": 484}
[03:31:15.676650] [03:31:15.676734] Training epoch 484 for 0:01:45
[03:31:15.676788] [03:31:15.681237] log_dir: ./exp/debug/cifar100-LT/debug
[03:31:17.570677] Epoch: [485]  [  0/195]  eta: 0:06:08  lr: 0.000881  loss: 0.2973 (0.2973)  time: 1.8883  data: 1.3816  max mem: 9341
[03:31:27.798905] Epoch: [485]  [ 20/195]  eta: 0:01:40  lr: 0.000881  loss: 0.3091 (0.3071)  time: 0.5113  data: 0.0003  max mem: 9341
[03:31:38.008344] Epoch: [485]  [ 40/195]  eta: 0:01:24  lr: 0.000881  loss: 0.3141 (0.3089)  time: 0.5104  data: 0.0003  max mem: 9341
[03:31:48.215782] Epoch: [485]  [ 60/195]  eta: 0:01:11  lr: 0.000880  loss: 0.2932 (0.3044)  time: 0.5103  data: 0.0003  max mem: 9341
[03:31:58.471207] Epoch: [485]  [ 80/195]  eta: 0:01:00  lr: 0.000879  loss: 0.2982 (0.3024)  time: 0.5127  data: 0.0003  max mem: 9341
[03:32:08.686050] Epoch: [485]  [100/195]  eta: 0:00:49  lr: 0.000879  loss: 0.2986 (0.3026)  time: 0.5106  data: 0.0003  max mem: 9341
[03:32:18.892323] Epoch: [485]  [120/195]  eta: 0:00:39  lr: 0.000879  loss: 0.2954 (0.3012)  time: 0.5103  data: 0.0003  max mem: 9341
[03:32:29.102149] Epoch: [485]  [140/195]  eta: 0:00:28  lr: 0.000878  loss: 0.3060 (0.3020)  time: 0.5104  data: 0.0002  max mem: 9341
[03:32:39.357498] Epoch: [485]  [160/195]  eta: 0:00:18  lr: 0.000878  loss: 0.2983 (0.3021)  time: 0.5127  data: 0.0003  max mem: 9341
[03:32:49.523685] Epoch: [485]  [180/195]  eta: 0:00:07  lr: 0.000877  loss: 0.3017 (0.3025)  time: 0.5082  data: 0.0002  max mem: 9341
[03:32:56.658221] Epoch: [485]  [194/195]  eta: 0:00:00  lr: 0.000877  loss: 0.3099 (0.3027)  time: 0.5107  data: 0.0001  max mem: 9341
[03:32:56.838299] Epoch: [485] Total time: 0:01:41 (0.5188 s / it)
[03:32:56.848192] Averaged stats: lr: 0.000877  loss: 0.3099 (0.3029)
[03:33:01.572495] {"train_lr": 0.000879242273317246, "train_loss": 0.30285750935092953, "epoch": 485}
[03:33:01.572779] [03:33:01.572884] Training epoch 485 for 0:01:45
[03:33:01.572999] [03:33:01.577447] log_dir: ./exp/debug/cifar100-LT/debug
[03:33:03.168767] Epoch: [486]  [  0/195]  eta: 0:05:09  lr: 0.000877  loss: 0.2842 (0.2842)  time: 1.5896  data: 1.0884  max mem: 9341
[03:33:13.409244] Epoch: [486]  [ 20/195]  eta: 0:01:38  lr: 0.000876  loss: 0.3041 (0.3048)  time: 0.5120  data: 0.0002  max mem: 9341
[03:33:23.620807] Epoch: [486]  [ 40/195]  eta: 0:01:23  lr: 0.000876  loss: 0.2912 (0.3008)  time: 0.5105  data: 0.0002  max mem: 9341
[03:33:33.841191] Epoch: [486]  [ 60/195]  eta: 0:01:11  lr: 0.000875  loss: 0.3059 (0.3022)  time: 0.5109  data: 0.0003  max mem: 9341
[03:33:44.099901] Epoch: [486]  [ 80/195]  eta: 0:01:00  lr: 0.000875  loss: 0.3038 (0.3027)  time: 0.5129  data: 0.0003  max mem: 9341
[03:33:54.317368] Epoch: [486]  [100/195]  eta: 0:00:49  lr: 0.000874  loss: 0.3059 (0.3032)  time: 0.5108  data: 0.0003  max mem: 9341
[03:34:04.537655] Epoch: [486]  [120/195]  eta: 0:00:39  lr: 0.000874  loss: 0.2913 (0.3023)  time: 0.5110  data: 0.0003  max mem: 9341
[03:34:14.752864] Epoch: [486]  [140/195]  eta: 0:00:28  lr: 0.000874  loss: 0.3053 (0.3032)  time: 0.5107  data: 0.0002  max mem: 9341
[03:34:25.008661] Epoch: [486]  [160/195]  eta: 0:00:18  lr: 0.000873  loss: 0.3006 (0.3030)  time: 0.5127  data: 0.0003  max mem: 9341
[03:34:35.171256] Epoch: [486]  [180/195]  eta: 0:00:07  lr: 0.000872  loss: 0.3063 (0.3035)  time: 0.5081  data: 0.0002  max mem: 9341
[03:34:42.301951] Epoch: [486]  [194/195]  eta: 0:00:00  lr: 0.000872  loss: 0.2993 (0.3032)  time: 0.5104  data: 0.0001  max mem: 9341
[03:34:42.492620] Epoch: [486] Total time: 0:01:40 (0.5175 s / it)
[03:34:42.497963] Averaged stats: lr: 0.000872  loss: 0.2993 (0.3010)
[03:34:47.212845] {"train_lr": 0.0008744651115928087, "train_loss": 0.3010436980006022, "epoch": 486}
[03:34:47.213113] [03:34:47.213215] Training epoch 486 for 0:01:45
[03:34:47.213267] [03:34:47.217670] log_dir: ./exp/debug/cifar100-LT/debug
[03:34:49.185644] Epoch: [487]  [  0/195]  eta: 0:06:23  lr: 0.000872  loss: 0.3048 (0.3048)  time: 1.9666  data: 1.4590  max mem: 9341
[03:34:59.425988] Epoch: [487]  [ 20/195]  eta: 0:01:41  lr: 0.000871  loss: 0.2960 (0.3057)  time: 0.5120  data: 0.0002  max mem: 9341
[03:35:09.648426] Epoch: [487]  [ 40/195]  eta: 0:01:24  lr: 0.000871  loss: 0.2915 (0.3017)  time: 0.5111  data: 0.0002  max mem: 9341
[03:35:19.866019] Epoch: [487]  [ 60/195]  eta: 0:01:12  lr: 0.000871  loss: 0.2893 (0.2989)  time: 0.5108  data: 0.0002  max mem: 9341
[03:35:30.136475] Epoch: [487]  [ 80/195]  eta: 0:01:00  lr: 0.000870  loss: 0.2800 (0.2955)  time: 0.5135  data: 0.0002  max mem: 9341
[03:35:40.350303] Epoch: [487]  [100/195]  eta: 0:00:49  lr: 0.000870  loss: 0.2855 (0.2947)  time: 0.5106  data: 0.0002  max mem: 9341
[03:35:50.570510] Epoch: [487]  [120/195]  eta: 0:00:39  lr: 0.000869  loss: 0.3014 (0.2965)  time: 0.5110  data: 0.0002  max mem: 9341
[03:36:00.785046] Epoch: [487]  [140/195]  eta: 0:00:28  lr: 0.000869  loss: 0.3038 (0.2979)  time: 0.5106  data: 0.0002  max mem: 9341
[03:36:11.042706] Epoch: [487]  [160/195]  eta: 0:00:18  lr: 0.000868  loss: 0.2918 (0.2977)  time: 0.5128  data: 0.0002  max mem: 9341
[03:36:21.213733] Epoch: [487]  [180/195]  eta: 0:00:07  lr: 0.000868  loss: 0.2936 (0.2977)  time: 0.5085  data: 0.0001  max mem: 9341
[03:36:28.347628] Epoch: [487]  [194/195]  eta: 0:00:00  lr: 0.000867  loss: 0.2989 (0.2982)  time: 0.5106  data: 0.0001  max mem: 9341
[03:36:28.520153] Epoch: [487] Total time: 0:01:41 (0.5195 s / it)
[03:36:28.538308] Averaged stats: lr: 0.000867  loss: 0.2989 (0.2982)
[03:36:33.287541] {"train_lr": 0.0008696935123614321, "train_loss": 0.2982285241859082, "epoch": 487}
[03:36:33.287810] [03:36:33.287901] Training epoch 487 for 0:01:46
[03:36:33.287955] [03:36:33.292958] log_dir: ./exp/debug/cifar100-LT/debug
[03:36:35.189262] Epoch: [488]  [  0/195]  eta: 0:06:09  lr: 0.000867  loss: 0.3407 (0.3407)  time: 1.8948  data: 1.3966  max mem: 9341
[03:36:45.399930] Epoch: [488]  [ 20/195]  eta: 0:01:40  lr: 0.000867  loss: 0.2913 (0.2984)  time: 0.5105  data: 0.0002  max mem: 9341
[03:36:55.615316] Epoch: [488]  [ 40/195]  eta: 0:01:24  lr: 0.000866  loss: 0.2984 (0.2983)  time: 0.5107  data: 0.0002  max mem: 9341
[03:37:05.831779] Epoch: [488]  [ 60/195]  eta: 0:01:12  lr: 0.000866  loss: 0.3197 (0.3028)  time: 0.5108  data: 0.0002  max mem: 9341
[03:37:16.133585] Epoch: [488]  [ 80/195]  eta: 0:01:00  lr: 0.000865  loss: 0.3000 (0.3028)  time: 0.5150  data: 0.0002  max mem: 9341
[03:37:26.371051] Epoch: [488]  [100/195]  eta: 0:00:49  lr: 0.000865  loss: 0.3007 (0.3025)  time: 0.5118  data: 0.0002  max mem: 9341
[03:37:36.612214] Epoch: [488]  [120/195]  eta: 0:00:39  lr: 0.000864  loss: 0.2964 (0.3017)  time: 0.5120  data: 0.0003  max mem: 9341
[03:37:46.843372] Epoch: [488]  [140/195]  eta: 0:00:28  lr: 0.000864  loss: 0.2967 (0.3009)  time: 0.5115  data: 0.0003  max mem: 9341
[03:37:57.140571] Epoch: [488]  [160/195]  eta: 0:00:18  lr: 0.000863  loss: 0.3033 (0.3016)  time: 0.5148  data: 0.0003  max mem: 9341
[03:38:07.322897] Epoch: [488]  [180/195]  eta: 0:00:07  lr: 0.000863  loss: 0.2988 (0.3011)  time: 0.5091  data: 0.0002  max mem: 9341
[03:38:14.474749] Epoch: [488]  [194/195]  eta: 0:00:00  lr: 0.000862  loss: 0.3003 (0.3013)  time: 0.5124  data: 0.0001  max mem: 9341
[03:38:14.629434] Epoch: [488] Total time: 0:01:41 (0.5197 s / it)
[03:38:14.677457] Averaged stats: lr: 0.000862  loss: 0.3003 (0.3002)
[03:38:19.377548] {"train_lr": 0.0008649275571565867, "train_loss": 0.30023021524151167, "epoch": 488}
[03:38:19.377807] [03:38:19.377909] Training epoch 488 for 0:01:46
[03:38:19.377964] [03:38:19.382312] log_dir: ./exp/debug/cifar100-LT/debug
[03:38:21.300102] Epoch: [489]  [  0/195]  eta: 0:06:13  lr: 0.000862  loss: 0.2826 (0.2826)  time: 1.9169  data: 1.2866  max mem: 9341
[03:38:31.547199] Epoch: [489]  [ 20/195]  eta: 0:01:41  lr: 0.000862  loss: 0.2990 (0.2978)  time: 0.5123  data: 0.0003  max mem: 9341
[03:38:41.768220] Epoch: [489]  [ 40/195]  eta: 0:01:24  lr: 0.000862  loss: 0.3053 (0.3006)  time: 0.5110  data: 0.0002  max mem: 9341
[03:38:51.983537] Epoch: [489]  [ 60/195]  eta: 0:01:12  lr: 0.000861  loss: 0.2967 (0.2999)  time: 0.5107  data: 0.0003  max mem: 9341
[03:39:02.240668] Epoch: [489]  [ 80/195]  eta: 0:01:00  lr: 0.000860  loss: 0.3130 (0.3029)  time: 0.5128  data: 0.0002  max mem: 9341
[03:39:12.454842] Epoch: [489]  [100/195]  eta: 0:00:49  lr: 0.000860  loss: 0.3106 (0.3045)  time: 0.5106  data: 0.0003  max mem: 9341
[03:39:22.664860] Epoch: [489]  [120/195]  eta: 0:00:39  lr: 0.000860  loss: 0.3086 (0.3057)  time: 0.5104  data: 0.0003  max mem: 9341
[03:39:32.873983] Epoch: [489]  [140/195]  eta: 0:00:28  lr: 0.000859  loss: 0.3141 (0.3061)  time: 0.5104  data: 0.0002  max mem: 9341
[03:39:43.123438] Epoch: [489]  [160/195]  eta: 0:00:18  lr: 0.000858  loss: 0.3027 (0.3063)  time: 0.5124  data: 0.0002  max mem: 9341
[03:39:53.283028] Epoch: [489]  [180/195]  eta: 0:00:07  lr: 0.000858  loss: 0.3052 (0.3068)  time: 0.5079  data: 0.0002  max mem: 9341
[03:40:00.410088] Epoch: [489]  [194/195]  eta: 0:00:00  lr: 0.000858  loss: 0.3032 (0.3065)  time: 0.5101  data: 0.0001  max mem: 9341
[03:40:00.586680] Epoch: [489] Total time: 0:01:41 (0.5190 s / it)
[03:40:00.598645] Averaged stats: lr: 0.000858  loss: 0.3032 (0.3062)
[03:40:05.415989] {"train_lr": 0.0008601673274152892, "train_loss": 0.306169909850145, "epoch": 489}
[03:40:05.416429] [03:40:05.416543] Training epoch 489 for 0:01:46
[03:40:05.416599] [03:40:05.421076] log_dir: ./exp/debug/cifar100-LT/debug
[03:40:07.004837] Epoch: [490]  [  0/195]  eta: 0:05:08  lr: 0.000858  loss: 0.3292 (0.3292)  time: 1.5820  data: 1.0666  max mem: 9341
[03:40:17.217739] Epoch: [490]  [ 20/195]  eta: 0:01:38  lr: 0.000857  loss: 0.3043 (0.3049)  time: 0.5106  data: 0.0002  max mem: 9341
[03:40:27.430226] Epoch: [490]  [ 40/195]  eta: 0:01:23  lr: 0.000857  loss: 0.3075 (0.3020)  time: 0.5106  data: 0.0002  max mem: 9341
[03:40:37.645038] Epoch: [490]  [ 60/195]  eta: 0:01:11  lr: 0.000856  loss: 0.2911 (0.2989)  time: 0.5107  data: 0.0002  max mem: 9341
[03:40:47.907916] Epoch: [490]  [ 80/195]  eta: 0:01:00  lr: 0.000856  loss: 0.2987 (0.2994)  time: 0.5131  data: 0.0003  max mem: 9341
[03:40:58.115413] Epoch: [490]  [100/195]  eta: 0:00:49  lr: 0.000855  loss: 0.3019 (0.3001)  time: 0.5103  data: 0.0003  max mem: 9341
[03:41:08.325871] Epoch: [490]  [120/195]  eta: 0:00:38  lr: 0.000855  loss: 0.2945 (0.2992)  time: 0.5105  data: 0.0003  max mem: 9341
[03:41:18.538101] Epoch: [490]  [140/195]  eta: 0:00:28  lr: 0.000854  loss: 0.2996 (0.2989)  time: 0.5106  data: 0.0003  max mem: 9341
[03:41:28.793380] Epoch: [490]  [160/195]  eta: 0:00:18  lr: 0.000854  loss: 0.3029 (0.2995)  time: 0.5127  data: 0.0002  max mem: 9341
[03:41:38.960774] Epoch: [490]  [180/195]  eta: 0:00:07  lr: 0.000853  loss: 0.2970 (0.2991)  time: 0.5083  data: 0.0001  max mem: 9341
[03:41:46.089613] Epoch: [490]  [194/195]  eta: 0:00:00  lr: 0.000853  loss: 0.3004 (0.2991)  time: 0.5104  data: 0.0001  max mem: 9341
[03:41:46.259136] Epoch: [490] Total time: 0:01:40 (0.5171 s / it)
[03:41:46.281748] Averaged stats: lr: 0.000853  loss: 0.3004 (0.3034)
[03:41:50.980433] {"train_lr": 0.0008554129044767368, "train_loss": 0.30344199788493986, "epoch": 490}
[03:41:50.980696] [03:41:50.980801] Training epoch 490 for 0:01:45
[03:41:50.980857] [03:41:50.985242] log_dir: ./exp/debug/cifar100-LT/debug
[03:41:52.596849] Epoch: [491]  [  0/195]  eta: 0:05:13  lr: 0.000853  loss: 0.3196 (0.3196)  time: 1.6102  data: 1.1075  max mem: 9341
[03:42:02.825438] Epoch: [491]  [ 20/195]  eta: 0:01:38  lr: 0.000852  loss: 0.3039 (0.3018)  time: 0.5114  data: 0.0002  max mem: 9341
[03:42:13.046136] Epoch: [491]  [ 40/195]  eta: 0:01:23  lr: 0.000852  loss: 0.2949 (0.3008)  time: 0.5110  data: 0.0002  max mem: 9341
[03:42:23.259872] Epoch: [491]  [ 60/195]  eta: 0:01:11  lr: 0.000852  loss: 0.2936 (0.3000)  time: 0.5106  data: 0.0003  max mem: 9341
[03:42:33.515416] Epoch: [491]  [ 80/195]  eta: 0:01:00  lr: 0.000851  loss: 0.3069 (0.3017)  time: 0.5127  data: 0.0003  max mem: 9341
[03:42:43.727035] Epoch: [491]  [100/195]  eta: 0:00:49  lr: 0.000851  loss: 0.3101 (0.3037)  time: 0.5105  data: 0.0002  max mem: 9341
[03:42:53.943830] Epoch: [491]  [120/195]  eta: 0:00:39  lr: 0.000850  loss: 0.3057 (0.3027)  time: 0.5108  data: 0.0003  max mem: 9341
[03:43:04.185396] Epoch: [491]  [140/195]  eta: 0:00:28  lr: 0.000850  loss: 0.2968 (0.3021)  time: 0.5120  data: 0.0003  max mem: 9341
[03:43:14.484339] Epoch: [491]  [160/195]  eta: 0:00:18  lr: 0.000849  loss: 0.2941 (0.3022)  time: 0.5149  data: 0.0002  max mem: 9341
[03:43:24.676886] Epoch: [491]  [180/195]  eta: 0:00:07  lr: 0.000849  loss: 0.3048 (0.3028)  time: 0.5096  data: 0.0002  max mem: 9341
[03:43:31.827579] Epoch: [491]  [194/195]  eta: 0:00:00  lr: 0.000848  loss: 0.2977 (0.3026)  time: 0.5126  data: 0.0001  max mem: 9341
[03:43:31.999980] Epoch: [491] Total time: 0:01:41 (0.5180 s / it)
[03:43:32.012915] Averaged stats: lr: 0.000848  loss: 0.2977 (0.3025)
[03:43:36.722552] {"train_lr": 0.0008506643695808997, "train_loss": 0.3024736818021689, "epoch": 491}
[03:43:36.722886] [03:43:36.722992] Training epoch 491 for 0:01:45
[03:43:36.723046] [03:43:36.727501] log_dir: ./exp/debug/cifar100-LT/debug
[03:43:38.665775] Epoch: [492]  [  0/195]  eta: 0:06:17  lr: 0.000848  loss: 0.2690 (0.2690)  time: 1.9368  data: 1.4456  max mem: 9341
[03:43:48.880012] Epoch: [492]  [ 20/195]  eta: 0:01:41  lr: 0.000848  loss: 0.2978 (0.2980)  time: 0.5106  data: 0.0003  max mem: 9341
[03:43:59.095666] Epoch: [492]  [ 40/195]  eta: 0:01:24  lr: 0.000847  loss: 0.3041 (0.2996)  time: 0.5107  data: 0.0003  max mem: 9341
[03:44:09.309869] Epoch: [492]  [ 60/195]  eta: 0:01:12  lr: 0.000847  loss: 0.3117 (0.3023)  time: 0.5106  data: 0.0002  max mem: 9341
[03:44:19.591960] Epoch: [492]  [ 80/195]  eta: 0:01:00  lr: 0.000846  loss: 0.3017 (0.3030)  time: 0.5140  data: 0.0003  max mem: 9341
[03:44:29.810046] Epoch: [492]  [100/195]  eta: 0:00:49  lr: 0.000846  loss: 0.3077 (0.3039)  time: 0.5108  data: 0.0002  max mem: 9341
[03:44:40.022695] Epoch: [492]  [120/195]  eta: 0:00:39  lr: 0.000845  loss: 0.2930 (0.3032)  time: 0.5106  data: 0.0003  max mem: 9341
[03:44:50.239968] Epoch: [492]  [140/195]  eta: 0:00:28  lr: 0.000845  loss: 0.3052 (0.3033)  time: 0.5108  data: 0.0002  max mem: 9341
[03:45:00.541721] Epoch: [492]  [160/195]  eta: 0:00:18  lr: 0.000844  loss: 0.2997 (0.3027)  time: 0.5150  data: 0.0003  max mem: 9341
[03:45:10.729605] Epoch: [492]  [180/195]  eta: 0:00:07  lr: 0.000844  loss: 0.2917 (0.3023)  time: 0.5093  data: 0.0002  max mem: 9341
[03:45:17.882744] Epoch: [492]  [194/195]  eta: 0:00:00  lr: 0.000843  loss: 0.3005 (0.3023)  time: 0.5126  data: 0.0001  max mem: 9341
[03:45:18.071605] Epoch: [492] Total time: 0:01:41 (0.5197 s / it)
[03:45:18.073418] Averaged stats: lr: 0.000843  loss: 0.3005 (0.3008)
[03:45:22.849621] {"train_lr": 0.0008459218038671378, "train_loss": 0.30079587852725614, "epoch": 492}
[03:45:22.850011] [03:45:22.850134] Training epoch 492 for 0:01:46
[03:45:22.850189] [03:45:22.854682] log_dir: ./exp/debug/cifar100-LT/debug
[03:45:24.693778] Epoch: [493]  [  0/195]  eta: 0:05:58  lr: 0.000843  loss: 0.3091 (0.3091)  time: 1.8371  data: 1.3354  max mem: 9341
[03:45:34.915690] Epoch: [493]  [ 20/195]  eta: 0:01:40  lr: 0.000843  loss: 0.3059 (0.3058)  time: 0.5110  data: 0.0003  max mem: 9341
[03:45:45.130026] Epoch: [493]  [ 40/195]  eta: 0:01:24  lr: 0.000843  loss: 0.2974 (0.3036)  time: 0.5106  data: 0.0003  max mem: 9341
[03:45:55.363461] Epoch: [493]  [ 60/195]  eta: 0:01:11  lr: 0.000842  loss: 0.2909 (0.3023)  time: 0.5116  data: 0.0002  max mem: 9341
[03:46:05.662899] Epoch: [493]  [ 80/195]  eta: 0:01:00  lr: 0.000841  loss: 0.3027 (0.3014)  time: 0.5149  data: 0.0002  max mem: 9341
[03:46:15.892982] Epoch: [493]  [100/195]  eta: 0:00:49  lr: 0.000841  loss: 0.3018 (0.3023)  time: 0.5114  data: 0.0002  max mem: 9341
[03:46:26.124959] Epoch: [493]  [120/195]  eta: 0:00:39  lr: 0.000841  loss: 0.2913 (0.3015)  time: 0.5115  data: 0.0003  max mem: 9341
[03:46:36.363238] Epoch: [493]  [140/195]  eta: 0:00:28  lr: 0.000840  loss: 0.2937 (0.3008)  time: 0.5119  data: 0.0002  max mem: 9341
[03:46:46.670797] Epoch: [493]  [160/195]  eta: 0:00:18  lr: 0.000839  loss: 0.2937 (0.3001)  time: 0.5153  data: 0.0003  max mem: 9341
[03:46:56.866021] Epoch: [493]  [180/195]  eta: 0:00:07  lr: 0.000839  loss: 0.3022 (0.3006)  time: 0.5097  data: 0.0002  max mem: 9341
[03:47:04.019233] Epoch: [493]  [194/195]  eta: 0:00:00  lr: 0.000839  loss: 0.2956 (0.3003)  time: 0.5128  data: 0.0001  max mem: 9341
[03:47:04.190972] Epoch: [493] Total time: 0:01:41 (0.5197 s / it)
[03:47:04.203042] Averaged stats: lr: 0.000839  loss: 0.2956 (0.3002)
[03:47:08.879797] {"train_lr": 0.0008411852883728076, "train_loss": 0.3001501015936717, "epoch": 493}
[03:47:08.880061] [03:47:08.880191] Training epoch 493 for 0:01:46
[03:47:08.880247] [03:47:08.884695] log_dir: ./exp/debug/cifar100-LT/debug
[03:47:10.660726] Epoch: [494]  [  0/195]  eta: 0:05:45  lr: 0.000839  loss: 0.2857 (0.2857)  time: 1.7743  data: 1.2757  max mem: 9341
[03:47:20.872759] Epoch: [494]  [ 20/195]  eta: 0:01:39  lr: 0.000838  loss: 0.3028 (0.3038)  time: 0.5105  data: 0.0002  max mem: 9341
[03:47:31.084807] Epoch: [494]  [ 40/195]  eta: 0:01:23  lr: 0.000838  loss: 0.3021 (0.3038)  time: 0.5105  data: 0.0002  max mem: 9341
[03:47:41.297332] Epoch: [494]  [ 60/195]  eta: 0:01:11  lr: 0.000837  loss: 0.3052 (0.3033)  time: 0.5105  data: 0.0002  max mem: 9341
[03:47:51.573243] Epoch: [494]  [ 80/195]  eta: 0:01:00  lr: 0.000837  loss: 0.2889 (0.3004)  time: 0.5137  data: 0.0003  max mem: 9341
[03:48:01.783412] Epoch: [494]  [100/195]  eta: 0:00:49  lr: 0.000836  loss: 0.2859 (0.2988)  time: 0.5104  data: 0.0002  max mem: 9341
[03:48:11.993548] Epoch: [494]  [120/195]  eta: 0:00:39  lr: 0.000836  loss: 0.2992 (0.2991)  time: 0.5104  data: 0.0003  max mem: 9341
[03:48:22.200796] Epoch: [494]  [140/195]  eta: 0:00:28  lr: 0.000836  loss: 0.2939 (0.2992)  time: 0.5103  data: 0.0003  max mem: 9341
[03:48:32.457037] Epoch: [494]  [160/195]  eta: 0:00:18  lr: 0.000835  loss: 0.2912 (0.2990)  time: 0.5127  data: 0.0003  max mem: 9341
[03:48:42.624683] Epoch: [494]  [180/195]  eta: 0:00:07  lr: 0.000834  loss: 0.2941 (0.2990)  time: 0.5083  data: 0.0002  max mem: 9341
[03:48:49.753572] Epoch: [494]  [194/195]  eta: 0:00:00  lr: 0.000834  loss: 0.2930 (0.2986)  time: 0.5104  data: 0.0001  max mem: 9341
[03:48:49.927114] Epoch: [494] Total time: 0:01:41 (0.5182 s / it)
[03:48:49.942243] Averaged stats: lr: 0.000834  loss: 0.2930 (0.2995)
[03:48:54.670645] {"train_lr": 0.0008364549040318974, "train_loss": 0.2995268305715842, "epoch": 494}
[03:48:54.670972] [03:48:54.671075] Training epoch 494 for 0:01:45
[03:48:54.671130] [03:48:54.675569] log_dir: ./exp/debug/cifar100-LT/debug
[03:48:56.620136] Epoch: [495]  [  0/195]  eta: 0:06:18  lr: 0.000834  loss: 0.3153 (0.3153)  time: 1.9433  data: 1.4433  max mem: 9341
[03:49:06.835401] Epoch: [495]  [ 20/195]  eta: 0:01:41  lr: 0.000834  loss: 0.2937 (0.2991)  time: 0.5107  data: 0.0002  max mem: 9341
[03:49:17.058186] Epoch: [495]  [ 40/195]  eta: 0:01:24  lr: 0.000833  loss: 0.3067 (0.3004)  time: 0.5111  data: 0.0002  max mem: 9341
[03:49:27.276165] Epoch: [495]  [ 60/195]  eta: 0:01:12  lr: 0.000833  loss: 0.2903 (0.2986)  time: 0.5108  data: 0.0003  max mem: 9341
[03:49:37.538806] Epoch: [495]  [ 80/195]  eta: 0:01:00  lr: 0.000832  loss: 0.3038 (0.3008)  time: 0.5131  data: 0.0003  max mem: 9341
[03:49:47.753815] Epoch: [495]  [100/195]  eta: 0:00:49  lr: 0.000832  loss: 0.2990 (0.3014)  time: 0.5107  data: 0.0003  max mem: 9341
[03:49:57.968190] Epoch: [495]  [120/195]  eta: 0:00:39  lr: 0.000831  loss: 0.3082 (0.3029)  time: 0.5106  data: 0.0002  max mem: 9341
[03:50:08.187729] Epoch: [495]  [140/195]  eta: 0:00:28  lr: 0.000831  loss: 0.2935 (0.3020)  time: 0.5109  data: 0.0002  max mem: 9341
[03:50:18.440473] Epoch: [495]  [160/195]  eta: 0:00:18  lr: 0.000830  loss: 0.2956 (0.3017)  time: 0.5126  data: 0.0002  max mem: 9341
[03:50:28.617640] Epoch: [495]  [180/195]  eta: 0:00:07  lr: 0.000830  loss: 0.3004 (0.3022)  time: 0.5088  data: 0.0002  max mem: 9341
[03:50:35.751205] Epoch: [495]  [194/195]  eta: 0:00:00  lr: 0.000829  loss: 0.3027 (0.3024)  time: 0.5107  data: 0.0001  max mem: 9341
[03:50:35.921016] Epoch: [495] Total time: 0:01:41 (0.5192 s / it)
[03:50:35.940581] Averaged stats: lr: 0.000829  loss: 0.3027 (0.3019)
[03:50:40.606639] {"train_lr": 0.0008317307316736257, "train_loss": 0.3019082546616212, "epoch": 495}
[03:50:40.606965] [03:50:40.607068] Training epoch 495 for 0:01:45
[03:50:40.607124] [03:50:40.611533] log_dir: ./exp/debug/cifar100-LT/debug
[03:50:42.223610] Epoch: [496]  [  0/195]  eta: 0:05:14  lr: 0.000829  loss: 0.3228 (0.3228)  time: 1.6107  data: 1.1208  max mem: 9341
[03:50:52.460438] Epoch: [496]  [ 20/195]  eta: 0:01:38  lr: 0.000829  loss: 0.2999 (0.2996)  time: 0.5118  data: 0.0002  max mem: 9341
[03:51:02.690064] Epoch: [496]  [ 40/195]  eta: 0:01:23  lr: 0.000828  loss: 0.2971 (0.2981)  time: 0.5114  data: 0.0002  max mem: 9341
[03:51:12.928913] Epoch: [496]  [ 60/195]  eta: 0:01:11  lr: 0.000828  loss: 0.2941 (0.2986)  time: 0.5119  data: 0.0002  max mem: 9341
[03:51:23.227598] Epoch: [496]  [ 80/195]  eta: 0:01:00  lr: 0.000827  loss: 0.3044 (0.3012)  time: 0.5149  data: 0.0002  max mem: 9341
[03:51:33.468386] Epoch: [496]  [100/195]  eta: 0:00:49  lr: 0.000827  loss: 0.2876 (0.2996)  time: 0.5120  data: 0.0002  max mem: 9341
[03:51:43.708631] Epoch: [496]  [120/195]  eta: 0:00:39  lr: 0.000826  loss: 0.3005 (0.2996)  time: 0.5119  data: 0.0002  max mem: 9341
[03:51:53.941181] Epoch: [496]  [140/195]  eta: 0:00:28  lr: 0.000826  loss: 0.3021 (0.3001)  time: 0.5116  data: 0.0002  max mem: 9341
[03:52:04.247351] Epoch: [496]  [160/195]  eta: 0:00:18  lr: 0.000825  loss: 0.3049 (0.3004)  time: 0.5152  data: 0.0002  max mem: 9341
[03:52:14.445466] Epoch: [496]  [180/195]  eta: 0:00:07  lr: 0.000825  loss: 0.3035 (0.3005)  time: 0.5098  data: 0.0002  max mem: 9341
[03:52:21.603017] Epoch: [496]  [194/195]  eta: 0:00:00  lr: 0.000825  loss: 0.2880 (0.3000)  time: 0.5132  data: 0.0001  max mem: 9341
[03:52:21.773259] Epoch: [496] Total time: 0:01:41 (0.5188 s / it)
[03:52:21.789783] Averaged stats: lr: 0.000825  loss: 0.2880 (0.2993)
[03:52:26.509743] {"train_lr": 0.0008270128520210589, "train_loss": 0.2993084453046322, "epoch": 496}
[03:52:26.510005] [03:52:26.510088] Training epoch 496 for 0:01:45
[03:52:26.510152] [03:52:26.514778] log_dir: ./exp/debug/cifar100-LT/debug
[03:52:28.267305] Epoch: [497]  [  0/195]  eta: 0:05:41  lr: 0.000824  loss: 0.3130 (0.3130)  time: 1.7513  data: 1.2451  max mem: 9341
[03:52:38.508906] Epoch: [497]  [ 20/195]  eta: 0:01:39  lr: 0.000824  loss: 0.2965 (0.2985)  time: 0.5120  data: 0.0002  max mem: 9341
[03:52:48.734682] Epoch: [497]  [ 40/195]  eta: 0:01:23  lr: 0.000824  loss: 0.3003 (0.3019)  time: 0.5112  data: 0.0003  max mem: 9341
[03:52:58.952553] Epoch: [497]  [ 60/195]  eta: 0:01:11  lr: 0.000823  loss: 0.3024 (0.3046)  time: 0.5108  data: 0.0003  max mem: 9341
[03:53:09.211747] Epoch: [497]  [ 80/195]  eta: 0:01:00  lr: 0.000823  loss: 0.2977 (0.3035)  time: 0.5129  data: 0.0003  max mem: 9341
[03:53:19.432481] Epoch: [497]  [100/195]  eta: 0:00:49  lr: 0.000822  loss: 0.2950 (0.3020)  time: 0.5110  data: 0.0003  max mem: 9341
[03:53:29.646940] Epoch: [497]  [120/195]  eta: 0:00:39  lr: 0.000822  loss: 0.2893 (0.3005)  time: 0.5107  data: 0.0003  max mem: 9341
[03:53:39.868520] Epoch: [497]  [140/195]  eta: 0:00:28  lr: 0.000821  loss: 0.2973 (0.3001)  time: 0.5110  data: 0.0003  max mem: 9341
[03:53:50.127868] Epoch: [497]  [160/195]  eta: 0:00:18  lr: 0.000821  loss: 0.2989 (0.3000)  time: 0.5129  data: 0.0003  max mem: 9341
[03:54:00.298591] Epoch: [497]  [180/195]  eta: 0:00:07  lr: 0.000820  loss: 0.3045 (0.3002)  time: 0.5085  data: 0.0002  max mem: 9341
[03:54:07.429883] Epoch: [497]  [194/195]  eta: 0:00:00  lr: 0.000820  loss: 0.2911 (0.2996)  time: 0.5106  data: 0.0001  max mem: 9341
[03:54:07.635403] Epoch: [497] Total time: 0:01:41 (0.5186 s / it)
[03:54:07.664831] Averaged stats: lr: 0.000820  loss: 0.2911 (0.2991)
[03:54:12.340614] {"train_lr": 0.0008223013456897443, "train_loss": 0.2991367484132449, "epoch": 497}
[03:54:12.340876] [03:54:12.340981] Training epoch 497 for 0:01:45
[03:54:12.341034] [03:54:12.345457] log_dir: ./exp/debug/cifar100-LT/debug
[03:54:14.086895] Epoch: [498]  [  0/195]  eta: 0:05:39  lr: 0.000820  loss: 0.3162 (0.3162)  time: 1.7406  data: 1.2286  max mem: 9341
[03:54:24.348163] Epoch: [498]  [ 20/195]  eta: 0:01:40  lr: 0.000819  loss: 0.2953 (0.2933)  time: 0.5130  data: 0.0002  max mem: 9341
[03:54:34.561766] Epoch: [498]  [ 40/195]  eta: 0:01:23  lr: 0.000819  loss: 0.3046 (0.2962)  time: 0.5106  data: 0.0002  max mem: 9341
[03:54:44.775906] Epoch: [498]  [ 60/195]  eta: 0:01:11  lr: 0.000819  loss: 0.2998 (0.2993)  time: 0.5106  data: 0.0002  max mem: 9341
[03:54:55.033408] Epoch: [498]  [ 80/195]  eta: 0:01:00  lr: 0.000818  loss: 0.2976 (0.2972)  time: 0.5128  data: 0.0002  max mem: 9341
[03:55:05.246002] Epoch: [498]  [100/195]  eta: 0:00:49  lr: 0.000817  loss: 0.2951 (0.2978)  time: 0.5106  data: 0.0002  max mem: 9341
[03:55:15.459281] Epoch: [498]  [120/195]  eta: 0:00:39  lr: 0.000817  loss: 0.2887 (0.2975)  time: 0.5106  data: 0.0002  max mem: 9341
[03:55:25.668488] Epoch: [498]  [140/195]  eta: 0:00:28  lr: 0.000817  loss: 0.3028 (0.2989)  time: 0.5104  data: 0.0002  max mem: 9341
[03:55:35.928668] Epoch: [498]  [160/195]  eta: 0:00:18  lr: 0.000816  loss: 0.3017 (0.2991)  time: 0.5130  data: 0.0002  max mem: 9341
[03:55:46.095032] Epoch: [498]  [180/195]  eta: 0:00:07  lr: 0.000816  loss: 0.2916 (0.2983)  time: 0.5083  data: 0.0001  max mem: 9341
[03:55:53.219700] Epoch: [498]  [194/195]  eta: 0:00:00  lr: 0.000815  loss: 0.2893 (0.2979)  time: 0.5103  data: 0.0001  max mem: 9341
[03:55:53.395610] Epoch: [498] Total time: 0:01:41 (0.5182 s / it)
[03:55:53.406823] Averaged stats: lr: 0.000815  loss: 0.2893 (0.2975)
[03:55:58.238393] {"train_lr": 0.0008175962931863259, "train_loss": 0.29746668799183307, "epoch": 498}
[03:55:58.238754] [03:55:58.238840] Training epoch 498 for 0:01:45
[03:55:58.238893] [03:55:58.243466] log_dir: ./exp/debug/cifar100-LT/debug
[03:56:00.084365] Epoch: [499]  [  0/195]  eta: 0:05:58  lr: 0.000815  loss: 0.3276 (0.3276)  time: 1.8385  data: 1.3366  max mem: 9341
[03:56:10.373450] Epoch: [499]  [ 20/195]  eta: 0:01:41  lr: 0.000815  loss: 0.2961 (0.2988)  time: 0.5144  data: 0.0002  max mem: 9341
[03:56:20.597145] Epoch: [499]  [ 40/195]  eta: 0:01:24  lr: 0.000814  loss: 0.2856 (0.2943)  time: 0.5111  data: 0.0002  max mem: 9341
[03:56:30.816207] Epoch: [499]  [ 60/195]  eta: 0:01:12  lr: 0.000814  loss: 0.2922 (0.2963)  time: 0.5109  data: 0.0002  max mem: 9341
[03:56:41.072849] Epoch: [499]  [ 80/195]  eta: 0:01:00  lr: 0.000813  loss: 0.2922 (0.2956)  time: 0.5128  data: 0.0002  max mem: 9341
[03:56:51.287548] Epoch: [499]  [100/195]  eta: 0:00:49  lr: 0.000813  loss: 0.2962 (0.2957)  time: 0.5107  data: 0.0002  max mem: 9341
[03:57:01.505812] Epoch: [499]  [120/195]  eta: 0:00:39  lr: 0.000812  loss: 0.2983 (0.2965)  time: 0.5108  data: 0.0002  max mem: 9341
[03:57:11.715032] Epoch: [499]  [140/195]  eta: 0:00:28  lr: 0.000812  loss: 0.3008 (0.2974)  time: 0.5104  data: 0.0002  max mem: 9341
[03:57:21.980671] Epoch: [499]  [160/195]  eta: 0:00:18  lr: 0.000811  loss: 0.2976 (0.2981)  time: 0.5132  data: 0.0002  max mem: 9341
[03:57:32.154042] Epoch: [499]  [180/195]  eta: 0:00:07  lr: 0.000811  loss: 0.2966 (0.2983)  time: 0.5086  data: 0.0002  max mem: 9341
[03:57:39.284890] Epoch: [499]  [194/195]  eta: 0:00:00  lr: 0.000810  loss: 0.2971 (0.2984)  time: 0.5107  data: 0.0001  max mem: 9341
[03:57:39.464720] Epoch: [499] Total time: 0:01:41 (0.5191 s / it)
[03:57:39.471916] Averaged stats: lr: 0.000810  loss: 0.2971 (0.2996)
[03:57:44.208292] {"train_lr": 0.0008128977749071673, "train_loss": 0.29961855763044115, "epoch": 499}
[03:57:44.208632] [03:57:44.208719] Training epoch 499 for 0:01:45
[03:57:44.208785] [03:57:44.213212] log_dir: ./exp/debug/cifar100-LT/debug
[03:57:45.929423] Epoch: [500]  [  0/195]  eta: 0:05:34  lr: 0.000810  loss: 0.2787 (0.2787)  time: 1.7148  data: 1.2037  max mem: 9341
[03:57:56.146453] Epoch: [500]  [ 20/195]  eta: 0:01:39  lr: 0.000810  loss: 0.3005 (0.2941)  time: 0.5108  data: 0.0003  max mem: 9341
[03:58:06.355417] Epoch: [500]  [ 40/195]  eta: 0:01:23  lr: 0.000810  loss: 0.2887 (0.2952)  time: 0.5104  data: 0.0002  max mem: 9341
[03:58:16.575939] Epoch: [500]  [ 60/195]  eta: 0:01:11  lr: 0.000809  loss: 0.3048 (0.2999)  time: 0.5110  data: 0.0002  max mem: 9341
[03:58:26.825357] Epoch: [500]  [ 80/195]  eta: 0:01:00  lr: 0.000808  loss: 0.3044 (0.3019)  time: 0.5124  data: 0.0002  max mem: 9341
[03:58:37.031958] Epoch: [500]  [100/195]  eta: 0:00:49  lr: 0.000808  loss: 0.2985 (0.3016)  time: 0.5103  data: 0.0003  max mem: 9341
[03:58:47.246730] Epoch: [500]  [120/195]  eta: 0:00:39  lr: 0.000808  loss: 0.2992 (0.3015)  time: 0.5107  data: 0.0002  max mem: 9341
[03:58:57.461001] Epoch: [500]  [140/195]  eta: 0:00:28  lr: 0.000807  loss: 0.2971 (0.3008)  time: 0.5107  data: 0.0002  max mem: 9341
[03:59:07.720050] Epoch: [500]  [160/195]  eta: 0:00:18  lr: 0.000807  loss: 0.2881 (0.3004)  time: 0.5129  data: 0.0002  max mem: 9341
[03:59:17.888455] Epoch: [500]  [180/195]  eta: 0:00:07  lr: 0.000806  loss: 0.2979 (0.3003)  time: 0.5084  data: 0.0002  max mem: 9341
[03:59:25.017586] Epoch: [500]  [194/195]  eta: 0:00:00  lr: 0.000806  loss: 0.2950 (0.3000)  time: 0.5104  data: 0.0001  max mem: 9341
[03:59:25.192018] Epoch: [500] Total time: 0:01:40 (0.5178 s / it)
[03:59:25.202264] Averaged stats: lr: 0.000806  loss: 0.2950 (0.2997)
[03:59:29.883839] {"train_lr": 0.0008082058711369877, "train_loss": 0.2997480907310278, "epoch": 500}
[03:59:29.884205] [03:59:29.884321] Training epoch 500 for 0:01:45
[03:59:29.884377] [03:59:29.888780] log_dir: ./exp/debug/cifar100-LT/debug
[03:59:31.834324] Epoch: [501]  [  0/195]  eta: 0:06:19  lr: 0.000806  loss: 0.2835 (0.2835)  time: 1.9442  data: 1.4467  max mem: 9341
[03:59:42.047039] Epoch: [501]  [ 20/195]  eta: 0:01:41  lr: 0.000805  loss: 0.2909 (0.2921)  time: 0.5106  data: 0.0002  max mem: 9341
[03:59:52.257322] Epoch: [501]  [ 40/195]  eta: 0:01:24  lr: 0.000805  loss: 0.2938 (0.2958)  time: 0.5105  data: 0.0002  max mem: 9341
[04:00:02.476022] Epoch: [501]  [ 60/195]  eta: 0:01:12  lr: 0.000805  loss: 0.2877 (0.2943)  time: 0.5109  data: 0.0002  max mem: 9341
[04:00:12.783632] Epoch: [501]  [ 80/195]  eta: 0:01:00  lr: 0.000804  loss: 0.2920 (0.2939)  time: 0.5153  data: 0.0002  max mem: 9341
[04:00:23.021631] Epoch: [501]  [100/195]  eta: 0:00:49  lr: 0.000803  loss: 0.2978 (0.2953)  time: 0.5118  data: 0.0003  max mem: 9341
[04:00:33.260903] Epoch: [501]  [120/195]  eta: 0:00:39  lr: 0.000803  loss: 0.2962 (0.2953)  time: 0.5119  data: 0.0002  max mem: 9341
[04:00:43.492960] Epoch: [501]  [140/195]  eta: 0:00:28  lr: 0.000803  loss: 0.2888 (0.2949)  time: 0.5115  data: 0.0002  max mem: 9341
[04:00:53.794266] Epoch: [501]  [160/195]  eta: 0:00:18  lr: 0.000802  loss: 0.2972 (0.2952)  time: 0.5150  data: 0.0002  max mem: 9341
[04:01:03.980636] Epoch: [501]  [180/195]  eta: 0:00:07  lr: 0.000801  loss: 0.2901 (0.2951)  time: 0.5093  data: 0.0002  max mem: 9341
[04:01:11.132018] Epoch: [501]  [194/195]  eta: 0:00:00  lr: 0.000801  loss: 0.2945 (0.2957)  time: 0.5124  data: 0.0001  max mem: 9341
[04:01:11.294481] Epoch: [501] Total time: 0:01:41 (0.5200 s / it)
[04:01:11.327759] Averaged stats: lr: 0.000801  loss: 0.2945 (0.2996)
[04:01:16.256955] {"train_lr": 0.0008035206620474694, "train_loss": 0.29958218110677526, "epoch": 501}
[04:01:16.257242] [04:01:16.257361] Training epoch 501 for 0:01:46
[04:01:16.257415] [04:01:16.262025] log_dir: ./exp/debug/cifar100-LT/debug
[04:01:18.073907] Epoch: [502]  [  0/195]  eta: 0:05:53  lr: 0.000801  loss: 0.3203 (0.3203)  time: 1.8109  data: 1.3101  max mem: 9341
[04:01:28.290550] Epoch: [502]  [ 20/195]  eta: 0:01:40  lr: 0.000801  loss: 0.2999 (0.3000)  time: 0.5108  data: 0.0002  max mem: 9341
[04:01:38.502466] Epoch: [502]  [ 40/195]  eta: 0:01:24  lr: 0.000800  loss: 0.2865 (0.2958)  time: 0.5105  data: 0.0002  max mem: 9341
[04:01:48.714075] Epoch: [502]  [ 60/195]  eta: 0:01:11  lr: 0.000800  loss: 0.3017 (0.2996)  time: 0.5105  data: 0.0002  max mem: 9341
[04:01:58.978242] Epoch: [502]  [ 80/195]  eta: 0:01:00  lr: 0.000799  loss: 0.3085 (0.3016)  time: 0.5131  data: 0.0002  max mem: 9341
[04:02:09.200255] Epoch: [502]  [100/195]  eta: 0:00:49  lr: 0.000799  loss: 0.3061 (0.3022)  time: 0.5110  data: 0.0002  max mem: 9341
[04:02:19.419074] Epoch: [502]  [120/195]  eta: 0:00:39  lr: 0.000798  loss: 0.2891 (0.3010)  time: 0.5109  data: 0.0002  max mem: 9341
[04:02:29.630128] Epoch: [502]  [140/195]  eta: 0:00:28  lr: 0.000798  loss: 0.2929 (0.3006)  time: 0.5105  data: 0.0002  max mem: 9341
[04:02:39.891712] Epoch: [502]  [160/195]  eta: 0:00:18  lr: 0.000797  loss: 0.2978 (0.3000)  time: 0.5130  data: 0.0002  max mem: 9341
[04:02:50.058801] Epoch: [502]  [180/195]  eta: 0:00:07  lr: 0.000797  loss: 0.2952 (0.3003)  time: 0.5083  data: 0.0001  max mem: 9341
[04:02:57.183987] Epoch: [502]  [194/195]  eta: 0:00:00  lr: 0.000796  loss: 0.2838 (0.2998)  time: 0.5101  data: 0.0001  max mem: 9341
[04:02:57.364545] Epoch: [502] Total time: 0:01:41 (0.5185 s / it)
[04:02:57.365327] Averaged stats: lr: 0.000796  loss: 0.2838 (0.3003)
[04:03:02.074673] {"train_lr": 0.0007988422276959143, "train_loss": 0.3002563796746425, "epoch": 502}
[04:03:02.075041] [04:03:02.075130] Training epoch 502 for 0:01:45
[04:03:02.075184] [04:03:02.079814] log_dir: ./exp/debug/cifar100-LT/debug
[04:03:04.035227] Epoch: [503]  [  0/195]  eta: 0:06:21  lr: 0.000796  loss: 0.3060 (0.3060)  time: 1.9542  data: 1.4479  max mem: 9341
[04:03:14.254306] Epoch: [503]  [ 20/195]  eta: 0:01:41  lr: 0.000796  loss: 0.2988 (0.3009)  time: 0.5109  data: 0.0002  max mem: 9341
[04:03:24.462430] Epoch: [503]  [ 40/195]  eta: 0:01:24  lr: 0.000796  loss: 0.2846 (0.2967)  time: 0.5103  data: 0.0002  max mem: 9341
[04:03:34.696873] Epoch: [503]  [ 60/195]  eta: 0:01:12  lr: 0.000795  loss: 0.2904 (0.2964)  time: 0.5117  data: 0.0002  max mem: 9341
[04:03:44.977982] Epoch: [503]  [ 80/195]  eta: 0:01:00  lr: 0.000794  loss: 0.3014 (0.2966)  time: 0.5140  data: 0.0002  max mem: 9341
[04:03:55.193664] Epoch: [503]  [100/195]  eta: 0:00:49  lr: 0.000794  loss: 0.3116 (0.2984)  time: 0.5107  data: 0.0003  max mem: 9341
[04:04:05.428924] Epoch: [503]  [120/195]  eta: 0:00:39  lr: 0.000794  loss: 0.2888 (0.2975)  time: 0.5117  data: 0.0002  max mem: 9341
[04:04:15.660997] Epoch: [503]  [140/195]  eta: 0:00:28  lr: 0.000793  loss: 0.3096 (0.2989)  time: 0.5115  data: 0.0002  max mem: 9341
[04:04:25.964918] Epoch: [503]  [160/195]  eta: 0:00:18  lr: 0.000792  loss: 0.3048 (0.3001)  time: 0.5151  data: 0.0002  max mem: 9341
[04:04:36.154906] Epoch: [503]  [180/195]  eta: 0:00:07  lr: 0.000792  loss: 0.2985 (0.3002)  time: 0.5095  data: 0.0001  max mem: 9341
[04:04:43.307792] Epoch: [503]  [194/195]  eta: 0:00:00  lr: 0.000792  loss: 0.2964 (0.2999)  time: 0.5127  data: 0.0001  max mem: 9341
[04:04:43.472848] Epoch: [503] Total time: 0:01:41 (0.5200 s / it)
[04:04:43.487656] Averaged stats: lr: 0.000792  loss: 0.2964 (0.2987)
[04:04:48.235045] {"train_lr": 0.000794170648023858, "train_loss": 0.2987325410812329, "epoch": 503}
[04:04:48.235308] [04:04:48.235416] Training epoch 503 for 0:01:46
[04:04:48.235470] [04:04:48.239941] log_dir: ./exp/debug/cifar100-LT/debug
[04:04:49.941606] Epoch: [504]  [  0/195]  eta: 0:05:31  lr: 0.000792  loss: 0.3087 (0.3087)  time: 1.7006  data: 1.2008  max mem: 9341
[04:05:00.179758] Epoch: [504]  [ 20/195]  eta: 0:01:39  lr: 0.000791  loss: 0.2898 (0.2968)  time: 0.5118  data: 0.0002  max mem: 9341
[04:05:10.398512] Epoch: [504]  [ 40/195]  eta: 0:01:23  lr: 0.000791  loss: 0.3102 (0.3016)  time: 0.5109  data: 0.0002  max mem: 9341
[04:05:20.613357] Epoch: [504]  [ 60/195]  eta: 0:01:11  lr: 0.000791  loss: 0.3019 (0.3032)  time: 0.5107  data: 0.0002  max mem: 9341
[04:05:30.874923] Epoch: [504]  [ 80/195]  eta: 0:01:00  lr: 0.000790  loss: 0.3071 (0.3023)  time: 0.5130  data: 0.0002  max mem: 9341
[04:05:41.087550] Epoch: [504]  [100/195]  eta: 0:00:49  lr: 0.000789  loss: 0.2977 (0.3010)  time: 0.5106  data: 0.0002  max mem: 9341
[04:05:51.299393] Epoch: [504]  [120/195]  eta: 0:00:39  lr: 0.000789  loss: 0.3031 (0.3021)  time: 0.5105  data: 0.0002  max mem: 9341
[04:06:01.516261] Epoch: [504]  [140/195]  eta: 0:00:28  lr: 0.000789  loss: 0.2868 (0.3013)  time: 0.5108  data: 0.0002  max mem: 9341
[04:06:11.776265] Epoch: [504]  [160/195]  eta: 0:00:18  lr: 0.000788  loss: 0.2946 (0.3010)  time: 0.5129  data: 0.0002  max mem: 9341
[04:06:21.945979] Epoch: [504]  [180/195]  eta: 0:00:07  lr: 0.000787  loss: 0.2985 (0.3012)  time: 0.5084  data: 0.0001  max mem: 9341
[04:06:29.072399] Epoch: [504]  [194/195]  eta: 0:00:00  lr: 0.000787  loss: 0.2944 (0.3012)  time: 0.5103  data: 0.0001  max mem: 9341
[04:06:29.257912] Epoch: [504] Total time: 0:01:41 (0.5180 s / it)
[04:06:29.269654] Averaged stats: lr: 0.000787  loss: 0.2944 (0.2983)
[04:06:34.008147] {"train_lr": 0.0007895060028557039, "train_loss": 0.2982858057778615, "epoch": 504}
[04:06:34.008486] [04:06:34.008575] Training epoch 504 for 0:01:45
[04:06:34.008629] [04:06:34.013070] log_dir: ./exp/debug/cifar100-LT/debug
[04:06:35.759136] Epoch: [505]  [  0/195]  eta: 0:05:40  lr: 0.000787  loss: 0.2573 (0.2573)  time: 1.7444  data: 1.2463  max mem: 9341
[04:06:45.973001] Epoch: [505]  [ 20/195]  eta: 0:01:39  lr: 0.000787  loss: 0.3085 (0.2996)  time: 0.5106  data: 0.0003  max mem: 9341
[04:06:56.194265] Epoch: [505]  [ 40/195]  eta: 0:01:23  lr: 0.000786  loss: 0.3012 (0.3001)  time: 0.5110  data: 0.0003  max mem: 9341
[04:07:06.412652] Epoch: [505]  [ 60/195]  eta: 0:01:11  lr: 0.000786  loss: 0.2877 (0.2979)  time: 0.5109  data: 0.0003  max mem: 9341
[04:07:16.672568] Epoch: [505]  [ 80/195]  eta: 0:01:00  lr: 0.000785  loss: 0.3021 (0.3004)  time: 0.5129  data: 0.0003  max mem: 9341
[04:07:26.890550] Epoch: [505]  [100/195]  eta: 0:00:49  lr: 0.000785  loss: 0.2911 (0.2992)  time: 0.5108  data: 0.0003  max mem: 9341
[04:07:37.109534] Epoch: [505]  [120/195]  eta: 0:00:39  lr: 0.000784  loss: 0.2934 (0.2994)  time: 0.5109  data: 0.0003  max mem: 9341
[04:07:47.329019] Epoch: [505]  [140/195]  eta: 0:00:28  lr: 0.000784  loss: 0.2870 (0.2985)  time: 0.5109  data: 0.0003  max mem: 9341
[04:07:57.601063] Epoch: [505]  [160/195]  eta: 0:00:18  lr: 0.000783  loss: 0.2919 (0.2982)  time: 0.5135  data: 0.0003  max mem: 9341
[04:08:07.775992] Epoch: [505]  [180/195]  eta: 0:00:07  lr: 0.000783  loss: 0.3041 (0.2980)  time: 0.5087  data: 0.0002  max mem: 9341
[04:08:14.902653] Epoch: [505]  [194/195]  eta: 0:00:00  lr: 0.000782  loss: 0.2951 (0.2977)  time: 0.5104  data: 0.0001  max mem: 9341
[04:08:15.084388] Epoch: [505] Total time: 0:01:41 (0.5183 s / it)
[04:08:15.103916] Averaged stats: lr: 0.000782  loss: 0.2951 (0.2974)
[04:08:20.013650] {"train_lr": 0.0007848483718973719, "train_loss": 0.2973837792300261, "epoch": 505}
[04:08:20.013924] [04:08:20.014033] Training epoch 505 for 0:01:46
[04:08:20.014088] [04:08:20.018584] log_dir: ./exp/debug/cifar100-LT/debug
[04:08:21.580170] Epoch: [506]  [  0/195]  eta: 0:05:04  lr: 0.000782  loss: 0.2938 (0.2938)  time: 1.5603  data: 1.0501  max mem: 9341
[04:08:31.818966] Epoch: [506]  [ 20/195]  eta: 0:01:38  lr: 0.000782  loss: 0.2947 (0.2984)  time: 0.5119  data: 0.0002  max mem: 9341
[04:08:42.036051] Epoch: [506]  [ 40/195]  eta: 0:01:23  lr: 0.000782  loss: 0.2994 (0.2987)  time: 0.5108  data: 0.0002  max mem: 9341
[04:08:52.264770] Epoch: [506]  [ 60/195]  eta: 0:01:11  lr: 0.000781  loss: 0.2872 (0.2956)  time: 0.5114  data: 0.0002  max mem: 9341
[04:09:02.515769] Epoch: [506]  [ 80/195]  eta: 0:01:00  lr: 0.000780  loss: 0.2904 (0.2939)  time: 0.5125  data: 0.0002  max mem: 9341
[04:09:12.731847] Epoch: [506]  [100/195]  eta: 0:00:49  lr: 0.000780  loss: 0.2933 (0.2948)  time: 0.5107  data: 0.0003  max mem: 9341
[04:09:22.942735] Epoch: [506]  [120/195]  eta: 0:00:38  lr: 0.000780  loss: 0.2919 (0.2944)  time: 0.5105  data: 0.0003  max mem: 9341
[04:09:33.153086] Epoch: [506]  [140/195]  eta: 0:00:28  lr: 0.000779  loss: 0.2913 (0.2943)  time: 0.5105  data: 0.0002  max mem: 9341
[04:09:43.411422] Epoch: [506]  [160/195]  eta: 0:00:18  lr: 0.000779  loss: 0.2958 (0.2940)  time: 0.5129  data: 0.0003  max mem: 9341
[04:09:53.576120] Epoch: [506]  [180/195]  eta: 0:00:07  lr: 0.000778  loss: 0.3061 (0.2947)  time: 0.5082  data: 0.0001  max mem: 9341
[04:10:00.702622] Epoch: [506]  [194/195]  eta: 0:00:00  lr: 0.000778  loss: 0.3082 (0.2950)  time: 0.5103  data: 0.0001  max mem: 9341
[04:10:00.876409] Epoch: [506] Total time: 0:01:40 (0.5172 s / it)
[04:10:00.891221] Averaged stats: lr: 0.000778  loss: 0.3082 (0.2964)
[04:10:05.563273] {"train_lr": 0.0007801978347349185, "train_loss": 0.2964251912557162, "epoch": 506}
[04:10:05.563712] [04:10:05.563824] Training epoch 506 for 0:01:45
[04:10:05.563880] [04:10:05.569217] log_dir: ./exp/debug/cifar100-LT/debug
[04:10:07.281698] Epoch: [507]  [  0/195]  eta: 0:05:33  lr: 0.000778  loss: 0.3158 (0.3158)  time: 1.7109  data: 1.1925  max mem: 9341
[04:10:17.514085] Epoch: [507]  [ 20/195]  eta: 0:01:39  lr: 0.000777  loss: 0.2984 (0.3009)  time: 0.5116  data: 0.0002  max mem: 9341
[04:10:27.732144] Epoch: [507]  [ 40/195]  eta: 0:01:23  lr: 0.000777  loss: 0.2908 (0.2990)  time: 0.5108  data: 0.0003  max mem: 9341
[04:10:37.946635] Epoch: [507]  [ 60/195]  eta: 0:01:11  lr: 0.000777  loss: 0.2987 (0.2981)  time: 0.5107  data: 0.0002  max mem: 9341
[04:10:48.213156] Epoch: [507]  [ 80/195]  eta: 0:01:00  lr: 0.000776  loss: 0.2921 (0.2964)  time: 0.5133  data: 0.0002  max mem: 9341
[04:10:58.426474] Epoch: [507]  [100/195]  eta: 0:00:49  lr: 0.000775  loss: 0.2934 (0.2957)  time: 0.5106  data: 0.0002  max mem: 9341
[04:11:08.645217] Epoch: [507]  [120/195]  eta: 0:00:39  lr: 0.000775  loss: 0.3011 (0.2966)  time: 0.5109  data: 0.0003  max mem: 9341
[04:11:18.856819] Epoch: [507]  [140/195]  eta: 0:00:28  lr: 0.000775  loss: 0.3051 (0.2970)  time: 0.5105  data: 0.0002  max mem: 9341
[04:11:29.120860] Epoch: [507]  [160/195]  eta: 0:00:18  lr: 0.000774  loss: 0.2881 (0.2959)  time: 0.5131  data: 0.0003  max mem: 9341
[04:11:39.297018] Epoch: [507]  [180/195]  eta: 0:00:07  lr: 0.000773  loss: 0.2988 (0.2964)  time: 0.5088  data: 0.0002  max mem: 9341
[04:11:46.426312] Epoch: [507]  [194/195]  eta: 0:00:00  lr: 0.000773  loss: 0.3018 (0.2973)  time: 0.5106  data: 0.0001  max mem: 9341
[04:11:46.603302] Epoch: [507] Total time: 0:01:41 (0.5181 s / it)
[04:11:46.617837] Averaged stats: lr: 0.000773  loss: 0.3018 (0.2970)
[04:11:51.369675] {"train_lr": 0.000775554470833197, "train_loss": 0.2969545529056818, "epoch": 507}
[04:11:51.370028] [04:11:51.370129] Training epoch 507 for 0:01:45
[04:11:51.370184] [04:11:51.374604] log_dir: ./exp/debug/cifar100-LT/debug
[04:11:53.173205] Epoch: [508]  [  0/195]  eta: 0:05:50  lr: 0.000773  loss: 0.2412 (0.2412)  time: 1.7975  data: 1.2927  max mem: 9341
[04:12:03.390638] Epoch: [508]  [ 20/195]  eta: 0:01:40  lr: 0.000773  loss: 0.2969 (0.2944)  time: 0.5108  data: 0.0002  max mem: 9341
[04:12:13.609343] Epoch: [508]  [ 40/195]  eta: 0:01:24  lr: 0.000772  loss: 0.2975 (0.2968)  time: 0.5109  data: 0.0002  max mem: 9341
[04:12:23.829362] Epoch: [508]  [ 60/195]  eta: 0:01:11  lr: 0.000772  loss: 0.2962 (0.2981)  time: 0.5109  data: 0.0002  max mem: 9341
[04:12:34.096883] Epoch: [508]  [ 80/195]  eta: 0:01:00  lr: 0.000771  loss: 0.3022 (0.3003)  time: 0.5133  data: 0.0002  max mem: 9341
[04:12:44.314584] Epoch: [508]  [100/195]  eta: 0:00:49  lr: 0.000771  loss: 0.2935 (0.2988)  time: 0.5108  data: 0.0002  max mem: 9341
[04:12:54.525849] Epoch: [508]  [120/195]  eta: 0:00:39  lr: 0.000770  loss: 0.3052 (0.2986)  time: 0.5105  data: 0.0002  max mem: 9341
[04:13:04.742290] Epoch: [508]  [140/195]  eta: 0:00:28  lr: 0.000770  loss: 0.3104 (0.3004)  time: 0.5107  data: 0.0002  max mem: 9341
[04:13:15.002720] Epoch: [508]  [160/195]  eta: 0:00:18  lr: 0.000769  loss: 0.2905 (0.3000)  time: 0.5129  data: 0.0002  max mem: 9341
[04:13:25.170357] Epoch: [508]  [180/195]  eta: 0:00:07  lr: 0.000769  loss: 0.2925 (0.2994)  time: 0.5083  data: 0.0002  max mem: 9341
[04:13:32.295788] Epoch: [508]  [194/195]  eta: 0:00:00  lr: 0.000768  loss: 0.3079 (0.2996)  time: 0.5103  data: 0.0001  max mem: 9341
[04:13:32.469741] Epoch: [508] Total time: 0:01:41 (0.5184 s / it)
[04:13:32.478454] Averaged stats: lr: 0.000768  loss: 0.3079 (0.2983)
[04:13:37.201301] {"train_lr": 0.0007709183595344844, "train_loss": 0.2983418820378108, "epoch": 508}
[04:13:37.201689] [04:13:37.201778] Training epoch 508 for 0:01:45
[04:13:37.201832] [04:13:37.206508] log_dir: ./exp/debug/cifar100-LT/debug
[04:13:38.859554] Epoch: [509]  [  0/195]  eta: 0:05:22  lr: 0.000768  loss: 0.2846 (0.2846)  time: 1.6516  data: 1.1478  max mem: 9341
[04:13:49.174871] Epoch: [509]  [ 20/195]  eta: 0:01:39  lr: 0.000768  loss: 0.2813 (0.2826)  time: 0.5157  data: 0.0002  max mem: 9341
[04:13:59.399517] Epoch: [509]  [ 40/195]  eta: 0:01:23  lr: 0.000768  loss: 0.3032 (0.2920)  time: 0.5111  data: 0.0003  max mem: 9341
[04:14:09.611447] Epoch: [509]  [ 60/195]  eta: 0:01:11  lr: 0.000767  loss: 0.2962 (0.2947)  time: 0.5105  data: 0.0003  max mem: 9341
[04:14:19.876013] Epoch: [509]  [ 80/195]  eta: 0:01:00  lr: 0.000767  loss: 0.3025 (0.2970)  time: 0.5132  data: 0.0002  max mem: 9341
[04:14:30.093359] Epoch: [509]  [100/195]  eta: 0:00:49  lr: 0.000766  loss: 0.3034 (0.2975)  time: 0.5108  data: 0.0002  max mem: 9341
[04:14:40.315186] Epoch: [509]  [120/195]  eta: 0:00:39  lr: 0.000766  loss: 0.3040 (0.2994)  time: 0.5110  data: 0.0002  max mem: 9341
[04:14:50.538147] Epoch: [509]  [140/195]  eta: 0:00:28  lr: 0.000765  loss: 0.2916 (0.2989)  time: 0.5111  data: 0.0002  max mem: 9341
[04:15:00.803942] Epoch: [509]  [160/195]  eta: 0:00:18  lr: 0.000765  loss: 0.2988 (0.2986)  time: 0.5132  data: 0.0002  max mem: 9341
[04:15:10.983319] Epoch: [509]  [180/195]  eta: 0:00:07  lr: 0.000764  loss: 0.2909 (0.2976)  time: 0.5089  data: 0.0001  max mem: 9341
[04:15:18.115201] Epoch: [509]  [194/195]  eta: 0:00:00  lr: 0.000764  loss: 0.2996 (0.2981)  time: 0.5106  data: 0.0001  max mem: 9341
[04:15:18.306775] Epoch: [509] Total time: 0:01:41 (0.5185 s / it)
[04:15:18.324337] Averaged stats: lr: 0.000764  loss: 0.2996 (0.2977)
[04:15:23.058201] {"train_lr": 0.0007662895800571323, "train_loss": 0.297722492290613, "epoch": 509}
[04:15:23.058462] [04:15:23.058562] Training epoch 509 for 0:01:45
[04:15:23.058617] [04:15:23.063000] log_dir: ./exp/debug/cifar100-LT/debug
[04:15:24.954662] Epoch: [510]  [  0/195]  eta: 0:06:08  lr: 0.000764  loss: 0.3135 (0.3135)  time: 1.8897  data: 1.3764  max mem: 9341
[04:15:35.189451] Epoch: [510]  [ 20/195]  eta: 0:01:41  lr: 0.000763  loss: 0.3013 (0.2967)  time: 0.5116  data: 0.0002  max mem: 9341
[04:15:45.413038] Epoch: [510]  [ 40/195]  eta: 0:01:24  lr: 0.000763  loss: 0.2846 (0.2933)  time: 0.5111  data: 0.0002  max mem: 9341
[04:15:55.632625] Epoch: [510]  [ 60/195]  eta: 0:01:12  lr: 0.000763  loss: 0.3005 (0.2964)  time: 0.5109  data: 0.0002  max mem: 9341
[04:16:05.894167] Epoch: [510]  [ 80/195]  eta: 0:01:00  lr: 0.000762  loss: 0.2948 (0.2939)  time: 0.5130  data: 0.0002  max mem: 9341
[04:16:16.109526] Epoch: [510]  [100/195]  eta: 0:00:49  lr: 0.000762  loss: 0.2842 (0.2930)  time: 0.5107  data: 0.0002  max mem: 9341
[04:16:26.327987] Epoch: [510]  [120/195]  eta: 0:00:39  lr: 0.000761  loss: 0.2869 (0.2924)  time: 0.5109  data: 0.0002  max mem: 9341
[04:16:36.553001] Epoch: [510]  [140/195]  eta: 0:00:28  lr: 0.000761  loss: 0.2881 (0.2929)  time: 0.5112  data: 0.0002  max mem: 9341
[04:16:46.813394] Epoch: [510]  [160/195]  eta: 0:00:18  lr: 0.000760  loss: 0.2940 (0.2929)  time: 0.5130  data: 0.0002  max mem: 9341
[04:16:56.986515] Epoch: [510]  [180/195]  eta: 0:00:07  lr: 0.000760  loss: 0.3009 (0.2935)  time: 0.5086  data: 0.0001  max mem: 9341
[04:17:04.121977] Epoch: [510]  [194/195]  eta: 0:00:00  lr: 0.000759  loss: 0.2864 (0.2931)  time: 0.5107  data: 0.0001  max mem: 9341
[04:17:04.295602] Epoch: [510] Total time: 0:01:41 (0.5191 s / it)
[04:17:04.313440] Averaged stats: lr: 0.000759  loss: 0.2864 (0.2975)
[04:17:09.023799] {"train_lr": 0.0007616682114942084, "train_loss": 0.2975337635057095, "epoch": 510}
[04:17:09.024162] [04:17:09.024253] Training epoch 510 for 0:01:45
[04:17:09.024308] [04:17:09.029276] log_dir: ./exp/debug/cifar100-LT/debug
[04:17:10.927209] Epoch: [511]  [  0/195]  eta: 0:06:09  lr: 0.000759  loss: 0.2789 (0.2789)  time: 1.8965  data: 1.3804  max mem: 9341
[04:17:21.144105] Epoch: [511]  [ 20/195]  eta: 0:01:40  lr: 0.000759  loss: 0.2975 (0.3019)  time: 0.5108  data: 0.0002  max mem: 9341
[04:17:31.354283] Epoch: [511]  [ 40/195]  eta: 0:01:24  lr: 0.000758  loss: 0.2860 (0.2974)  time: 0.5104  data: 0.0002  max mem: 9341
[04:17:41.574958] Epoch: [511]  [ 60/195]  eta: 0:01:12  lr: 0.000758  loss: 0.2948 (0.2972)  time: 0.5110  data: 0.0002  max mem: 9341
[04:17:51.840949] Epoch: [511]  [ 80/195]  eta: 0:01:00  lr: 0.000757  loss: 0.3161 (0.3005)  time: 0.5132  data: 0.0002  max mem: 9341
[04:18:02.054510] Epoch: [511]  [100/195]  eta: 0:00:49  lr: 0.000757  loss: 0.2973 (0.3002)  time: 0.5106  data: 0.0005  max mem: 9341
[04:18:12.268849] Epoch: [511]  [120/195]  eta: 0:00:39  lr: 0.000757  loss: 0.2953 (0.3000)  time: 0.5107  data: 0.0002  max mem: 9341
[04:18:22.483030] Epoch: [511]  [140/195]  eta: 0:00:28  lr: 0.000756  loss: 0.3015 (0.2999)  time: 0.5107  data: 0.0002  max mem: 9341
[04:18:32.738136] Epoch: [511]  [160/195]  eta: 0:00:18  lr: 0.000755  loss: 0.3057 (0.3007)  time: 0.5127  data: 0.0002  max mem: 9341
[04:18:42.901600] Epoch: [511]  [180/195]  eta: 0:00:07  lr: 0.000755  loss: 0.3030 (0.3012)  time: 0.5081  data: 0.0002  max mem: 9341
[04:18:50.031513] Epoch: [511]  [194/195]  eta: 0:00:00  lr: 0.000755  loss: 0.2924 (0.3003)  time: 0.5103  data: 0.0001  max mem: 9341
[04:18:50.207745] Epoch: [511] Total time: 0:01:41 (0.5189 s / it)
[04:18:50.220184] Averaged stats: lr: 0.000755  loss: 0.2924 (0.2975)
[04:18:55.076918] {"train_lr": 0.00075705433281215, "train_loss": 0.2975137229340199, "epoch": 511}
[04:18:55.077153] [04:18:55.077249] Training epoch 511 for 0:01:46
[04:18:55.077303] [04:18:55.081686] log_dir: ./exp/debug/cifar100-LT/debug
[04:18:56.929791] Epoch: [512]  [  0/195]  eta: 0:06:00  lr: 0.000755  loss: 0.3282 (0.3282)  time: 1.8467  data: 1.3157  max mem: 9341
[04:19:07.146552] Epoch: [512]  [ 20/195]  eta: 0:01:40  lr: 0.000754  loss: 0.2954 (0.2986)  time: 0.5108  data: 0.0003  max mem: 9341
[04:19:17.364916] Epoch: [512]  [ 40/195]  eta: 0:01:24  lr: 0.000754  loss: 0.2994 (0.2986)  time: 0.5109  data: 0.0002  max mem: 9341
[04:19:27.583953] Epoch: [512]  [ 60/195]  eta: 0:01:11  lr: 0.000753  loss: 0.2900 (0.2972)  time: 0.5109  data: 0.0002  max mem: 9341
[04:19:37.844234] Epoch: [512]  [ 80/195]  eta: 0:01:00  lr: 0.000753  loss: 0.2994 (0.2975)  time: 0.5130  data: 0.0003  max mem: 9341
[04:19:48.058141] Epoch: [512]  [100/195]  eta: 0:00:49  lr: 0.000752  loss: 0.2975 (0.2985)  time: 0.5106  data: 0.0002  max mem: 9341
[04:19:58.272588] Epoch: [512]  [120/195]  eta: 0:00:39  lr: 0.000752  loss: 0.2963 (0.2973)  time: 0.5107  data: 0.0002  max mem: 9341
[04:20:08.485715] Epoch: [512]  [140/195]  eta: 0:00:28  lr: 0.000752  loss: 0.2970 (0.2971)  time: 0.5106  data: 0.0002  max mem: 9341
[04:20:18.744331] Epoch: [512]  [160/195]  eta: 0:00:18  lr: 0.000751  loss: 0.2927 (0.2962)  time: 0.5129  data: 0.0002  max mem: 9341
[04:20:28.919570] Epoch: [512]  [180/195]  eta: 0:00:07  lr: 0.000750  loss: 0.2975 (0.2961)  time: 0.5087  data: 0.0002  max mem: 9341
[04:20:36.050048] Epoch: [512]  [194/195]  eta: 0:00:00  lr: 0.000750  loss: 0.2972 (0.2965)  time: 0.5106  data: 0.0001  max mem: 9341
[04:20:36.224193] Epoch: [512] Total time: 0:01:41 (0.5187 s / it)
[04:20:36.236950] Averaged stats: lr: 0.000750  loss: 0.2972 (0.2981)
[04:20:40.951059] {"train_lr": 0.0007524480228494134, "train_loss": 0.2980794072533265, "epoch": 512}
[04:20:40.951392] [04:20:40.951503] Training epoch 512 for 0:01:45
[04:20:40.951559] [04:20:40.955974] log_dir: ./exp/debug/cifar100-LT/debug
[04:20:42.611281] Epoch: [513]  [  0/195]  eta: 0:05:22  lr: 0.000750  loss: 0.3215 (0.3215)  time: 1.6539  data: 1.1380  max mem: 9341
[04:20:52.839838] Epoch: [513]  [ 20/195]  eta: 0:01:39  lr: 0.000750  loss: 0.3006 (0.2991)  time: 0.5114  data: 0.0002  max mem: 9341
[04:21:03.066236] Epoch: [513]  [ 40/195]  eta: 0:01:23  lr: 0.000749  loss: 0.3075 (0.3025)  time: 0.5112  data: 0.0002  max mem: 9341
[04:21:13.275562] Epoch: [513]  [ 60/195]  eta: 0:01:11  lr: 0.000749  loss: 0.2973 (0.3015)  time: 0.5104  data: 0.0002  max mem: 9341
[04:21:23.549532] Epoch: [513]  [ 80/195]  eta: 0:01:00  lr: 0.000748  loss: 0.2936 (0.3004)  time: 0.5136  data: 0.0002  max mem: 9341
[04:21:33.787297] Epoch: [513]  [100/195]  eta: 0:00:49  lr: 0.000748  loss: 0.2999 (0.2999)  time: 0.5118  data: 0.0002  max mem: 9341
[04:21:44.027211] Epoch: [513]  [120/195]  eta: 0:00:39  lr: 0.000747  loss: 0.2957 (0.3004)  time: 0.5119  data: 0.0002  max mem: 9341
[04:21:54.262312] Epoch: [513]  [140/195]  eta: 0:00:28  lr: 0.000747  loss: 0.2946 (0.3004)  time: 0.5117  data: 0.0002  max mem: 9341
[04:22:04.558032] Epoch: [513]  [160/195]  eta: 0:00:18  lr: 0.000746  loss: 0.2856 (0.2994)  time: 0.5147  data: 0.0002  max mem: 9341
[04:22:14.740934] Epoch: [513]  [180/195]  eta: 0:00:07  lr: 0.000746  loss: 0.2968 (0.2991)  time: 0.5091  data: 0.0001  max mem: 9341
[04:22:21.887980] Epoch: [513]  [194/195]  eta: 0:00:00  lr: 0.000745  loss: 0.2964 (0.2987)  time: 0.5121  data: 0.0001  max mem: 9341
[04:22:22.069395] Epoch: [513] Total time: 0:01:41 (0.5185 s / it)
[04:22:22.073104] Averaged stats: lr: 0.000745  loss: 0.2964 (0.2979)
[04:22:26.813771] {"train_lr": 0.000747849360315132, "train_loss": 0.2978740716018738, "epoch": 513}
[04:22:26.814032] [04:22:26.814117] Training epoch 513 for 0:01:45
[04:22:26.814171] [04:22:26.818771] log_dir: ./exp/debug/cifar100-LT/debug
[04:22:28.602328] Epoch: [514]  [  0/195]  eta: 0:05:47  lr: 0.000745  loss: 0.2498 (0.2498)  time: 1.7822  data: 1.2760  max mem: 9341
[04:22:38.811583] Epoch: [514]  [ 20/195]  eta: 0:01:39  lr: 0.000745  loss: 0.3002 (0.2972)  time: 0.5104  data: 0.0003  max mem: 9341
[04:22:49.020850] Epoch: [514]  [ 40/195]  eta: 0:01:23  lr: 0.000745  loss: 0.2996 (0.2991)  time: 0.5104  data: 0.0002  max mem: 9341
[04:22:59.233747] Epoch: [514]  [ 60/195]  eta: 0:01:11  lr: 0.000744  loss: 0.2970 (0.2983)  time: 0.5106  data: 0.0002  max mem: 9341
[04:23:09.490177] Epoch: [514]  [ 80/195]  eta: 0:01:00  lr: 0.000743  loss: 0.2929 (0.2994)  time: 0.5128  data: 0.0003  max mem: 9341
[04:23:19.706072] Epoch: [514]  [100/195]  eta: 0:00:49  lr: 0.000743  loss: 0.2936 (0.2987)  time: 0.5107  data: 0.0002  max mem: 9341
[04:23:29.927863] Epoch: [514]  [120/195]  eta: 0:00:39  lr: 0.000743  loss: 0.2980 (0.2985)  time: 0.5110  data: 0.0002  max mem: 9341
[04:23:40.158958] Epoch: [514]  [140/195]  eta: 0:00:28  lr: 0.000742  loss: 0.2849 (0.2976)  time: 0.5115  data: 0.0002  max mem: 9341
[04:23:50.421833] Epoch: [514]  [160/195]  eta: 0:00:18  lr: 0.000742  loss: 0.2873 (0.2965)  time: 0.5131  data: 0.0002  max mem: 9341
[04:24:00.596278] Epoch: [514]  [180/195]  eta: 0:00:07  lr: 0.000741  loss: 0.2995 (0.2973)  time: 0.5087  data: 0.0001  max mem: 9341
[04:24:07.729669] Epoch: [514]  [194/195]  eta: 0:00:00  lr: 0.000741  loss: 0.2950 (0.2964)  time: 0.5106  data: 0.0001  max mem: 9341
[04:24:07.908771] Epoch: [514] Total time: 0:01:41 (0.5184 s / it)
[04:24:07.918292] Averaged stats: lr: 0.000741  loss: 0.2950 (0.2982)
[04:24:12.631220] {"train_lr": 0.000743258423787755, "train_loss": 0.2981876145570706, "epoch": 514}
[04:24:12.631484] [04:24:12.631591] Training epoch 514 for 0:01:45
[04:24:12.631645] [04:24:12.636095] log_dir: ./exp/debug/cifar100-LT/debug
[04:24:14.431490] Epoch: [515]  [  0/195]  eta: 0:05:49  lr: 0.000741  loss: 0.3051 (0.3051)  time: 1.7935  data: 1.2940  max mem: 9341
[04:24:24.684725] Epoch: [515]  [ 20/195]  eta: 0:01:40  lr: 0.000740  loss: 0.2863 (0.2951)  time: 0.5125  data: 0.0003  max mem: 9341
[04:24:34.907397] Epoch: [515]  [ 40/195]  eta: 0:01:24  lr: 0.000740  loss: 0.2897 (0.2906)  time: 0.5111  data: 0.0002  max mem: 9341
[04:24:45.120583] Epoch: [515]  [ 60/195]  eta: 0:01:11  lr: 0.000740  loss: 0.3065 (0.2960)  time: 0.5106  data: 0.0002  max mem: 9341
[04:24:55.383942] Epoch: [515]  [ 80/195]  eta: 0:01:00  lr: 0.000739  loss: 0.2867 (0.2950)  time: 0.5131  data: 0.0003  max mem: 9341
[04:25:05.596420] Epoch: [515]  [100/195]  eta: 0:00:49  lr: 0.000739  loss: 0.2853 (0.2937)  time: 0.5106  data: 0.0002  max mem: 9341
[04:25:15.808655] Epoch: [515]  [120/195]  eta: 0:00:39  lr: 0.000738  loss: 0.2913 (0.2938)  time: 0.5106  data: 0.0003  max mem: 9341
[04:25:26.026540] Epoch: [515]  [140/195]  eta: 0:00:28  lr: 0.000738  loss: 0.2960 (0.2942)  time: 0.5108  data: 0.0002  max mem: 9341
[04:25:36.286245] Epoch: [515]  [160/195]  eta: 0:00:18  lr: 0.000737  loss: 0.2921 (0.2944)  time: 0.5129  data: 0.0003  max mem: 9341
[04:25:46.456085] Epoch: [515]  [180/195]  eta: 0:00:07  lr: 0.000737  loss: 0.3013 (0.2953)  time: 0.5084  data: 0.0002  max mem: 9341
[04:25:53.583719] Epoch: [515]  [194/195]  eta: 0:00:00  lr: 0.000736  loss: 0.3060 (0.2959)  time: 0.5105  data: 0.0001  max mem: 9341
[04:25:53.767172] Epoch: [515] Total time: 0:01:41 (0.5186 s / it)
[04:25:53.771011] Averaged stats: lr: 0.000736  loss: 0.3060 (0.2973)
[04:25:58.557640] {"train_lr": 0.0007386752917137235, "train_loss": 0.2973004460143737, "epoch": 515}
[04:25:58.557908] [04:25:58.558014] Training epoch 515 for 0:01:45
[04:25:58.558070] [04:25:58.562613] log_dir: ./exp/debug/cifar100-LT/debug
[04:26:00.171564] Epoch: [516]  [  0/195]  eta: 0:05:13  lr: 0.000736  loss: 0.3032 (0.3032)  time: 1.6080  data: 1.1070  max mem: 9341
[04:26:10.385230] Epoch: [516]  [ 20/195]  eta: 0:01:38  lr: 0.000736  loss: 0.2998 (0.2970)  time: 0.5106  data: 0.0002  max mem: 9341
[04:26:20.594518] Epoch: [516]  [ 40/195]  eta: 0:01:23  lr: 0.000735  loss: 0.2909 (0.2966)  time: 0.5104  data: 0.0002  max mem: 9341
[04:26:30.815969] Epoch: [516]  [ 60/195]  eta: 0:01:11  lr: 0.000735  loss: 0.2990 (0.2976)  time: 0.5110  data: 0.0002  max mem: 9341
[04:26:41.077736] Epoch: [516]  [ 80/195]  eta: 0:01:00  lr: 0.000734  loss: 0.3091 (0.3002)  time: 0.5130  data: 0.0002  max mem: 9341
[04:26:51.297709] Epoch: [516]  [100/195]  eta: 0:00:49  lr: 0.000734  loss: 0.2886 (0.2986)  time: 0.5109  data: 0.0002  max mem: 9341
[04:27:01.522563] Epoch: [516]  [120/195]  eta: 0:00:39  lr: 0.000734  loss: 0.2957 (0.2988)  time: 0.5112  data: 0.0002  max mem: 9341
[04:27:11.736404] Epoch: [516]  [140/195]  eta: 0:00:28  lr: 0.000733  loss: 0.2931 (0.2981)  time: 0.5106  data: 0.0002  max mem: 9341
[04:27:21.989973] Epoch: [516]  [160/195]  eta: 0:00:18  lr: 0.000732  loss: 0.2949 (0.2975)  time: 0.5126  data: 0.0002  max mem: 9341
[04:27:32.161120] Epoch: [516]  [180/195]  eta: 0:00:07  lr: 0.000732  loss: 0.2901 (0.2966)  time: 0.5085  data: 0.0001  max mem: 9341
[04:27:39.291814] Epoch: [516]  [194/195]  eta: 0:00:00  lr: 0.000732  loss: 0.2901 (0.2966)  time: 0.5106  data: 0.0001  max mem: 9341
[04:27:39.477181] Epoch: [516] Total time: 0:01:40 (0.5175 s / it)
[04:27:39.482513] Averaged stats: lr: 0.000732  loss: 0.2901 (0.2958)
[04:27:44.196328] {"train_lr": 0.0007341000424061193, "train_loss": 0.29579418909091215, "epoch": 516}
[04:27:44.196620] [04:27:44.196711] Training epoch 516 for 0:01:45
[04:27:44.196779] [04:27:44.201345] log_dir: ./exp/debug/cifar100-LT/debug
[04:27:45.791444] Epoch: [517]  [  0/195]  eta: 0:05:09  lr: 0.000732  loss: 0.2823 (0.2823)  time: 1.5890  data: 1.0866  max mem: 9341
[04:27:56.009130] Epoch: [517]  [ 20/195]  eta: 0:01:38  lr: 0.000731  loss: 0.2818 (0.2833)  time: 0.5108  data: 0.0003  max mem: 9341
[04:28:06.222769] Epoch: [517]  [ 40/195]  eta: 0:01:23  lr: 0.000731  loss: 0.3012 (0.2923)  time: 0.5106  data: 0.0003  max mem: 9341
[04:28:16.442875] Epoch: [517]  [ 60/195]  eta: 0:01:11  lr: 0.000731  loss: 0.2929 (0.2927)  time: 0.5109  data: 0.0003  max mem: 9341
[04:28:26.701098] Epoch: [517]  [ 80/195]  eta: 0:01:00  lr: 0.000730  loss: 0.2987 (0.2954)  time: 0.5129  data: 0.0003  max mem: 9341
[04:28:36.912629] Epoch: [517]  [100/195]  eta: 0:00:49  lr: 0.000729  loss: 0.2889 (0.2945)  time: 0.5105  data: 0.0003  max mem: 9341
[04:28:47.132214] Epoch: [517]  [120/195]  eta: 0:00:39  lr: 0.000729  loss: 0.2897 (0.2949)  time: 0.5109  data: 0.0003  max mem: 9341
[04:28:57.346463] Epoch: [517]  [140/195]  eta: 0:00:28  lr: 0.000729  loss: 0.2899 (0.2948)  time: 0.5107  data: 0.0003  max mem: 9341
[04:29:07.602572] Epoch: [517]  [160/195]  eta: 0:00:18  lr: 0.000728  loss: 0.2950 (0.2950)  time: 0.5127  data: 0.0003  max mem: 9341
[04:29:17.769245] Epoch: [517]  [180/195]  eta: 0:00:07  lr: 0.000728  loss: 0.2903 (0.2949)  time: 0.5083  data: 0.0002  max mem: 9341
[04:29:24.899052] Epoch: [517]  [194/195]  eta: 0:00:00  lr: 0.000727  loss: 0.3019 (0.2956)  time: 0.5104  data: 0.0001  max mem: 9341
[04:29:25.078587] Epoch: [517] Total time: 0:01:40 (0.5173 s / it)
[04:29:25.082344] Averaged stats: lr: 0.000727  loss: 0.3019 (0.2980)
[04:29:29.807428] {"train_lr": 0.0007295327540433285, "train_loss": 0.2980358883929558, "epoch": 517}
[04:29:29.807757] [04:29:29.807859] Training epoch 517 for 0:01:45
[04:29:29.807913] [04:29:29.812381] log_dir: ./exp/debug/cifar100-LT/debug
[04:29:31.587541] Epoch: [518]  [  0/195]  eta: 0:05:45  lr: 0.000727  loss: 0.2621 (0.2621)  time: 1.7730  data: 1.2643  max mem: 9341
[04:29:41.817935] Epoch: [518]  [ 20/195]  eta: 0:01:40  lr: 0.000727  loss: 0.3017 (0.2970)  time: 0.5115  data: 0.0003  max mem: 9341
[04:29:52.039383] Epoch: [518]  [ 40/195]  eta: 0:01:24  lr: 0.000726  loss: 0.2975 (0.2977)  time: 0.5110  data: 0.0002  max mem: 9341
[04:30:02.263015] Epoch: [518]  [ 60/195]  eta: 0:01:11  lr: 0.000726  loss: 0.2984 (0.2982)  time: 0.5111  data: 0.0002  max mem: 9341
[04:30:12.527772] Epoch: [518]  [ 80/195]  eta: 0:01:00  lr: 0.000725  loss: 0.2829 (0.2956)  time: 0.5132  data: 0.0003  max mem: 9341
[04:30:22.750990] Epoch: [518]  [100/195]  eta: 0:00:49  lr: 0.000725  loss: 0.2908 (0.2953)  time: 0.5111  data: 0.0003  max mem: 9341
[04:30:32.968711] Epoch: [518]  [120/195]  eta: 0:00:39  lr: 0.000724  loss: 0.2855 (0.2947)  time: 0.5108  data: 0.0003  max mem: 9341
[04:30:43.181529] Epoch: [518]  [140/195]  eta: 0:00:28  lr: 0.000724  loss: 0.2956 (0.2949)  time: 0.5106  data: 0.0002  max mem: 9341
[04:30:53.450795] Epoch: [518]  [160/195]  eta: 0:00:18  lr: 0.000723  loss: 0.2924 (0.2952)  time: 0.5134  data: 0.0003  max mem: 9341
[04:31:03.630747] Epoch: [518]  [180/195]  eta: 0:00:07  lr: 0.000723  loss: 0.2952 (0.2950)  time: 0.5089  data: 0.0002  max mem: 9341
[04:31:10.763692] Epoch: [518]  [194/195]  eta: 0:00:00  lr: 0.000723  loss: 0.2914 (0.2949)  time: 0.5107  data: 0.0001  max mem: 9341
[04:31:10.959841] Epoch: [518] Total time: 0:01:41 (0.5187 s / it)
[04:31:10.969416] Averaged stats: lr: 0.000723  loss: 0.2914 (0.2961)
[04:31:15.752582] {"train_lr": 0.0007249735046677058, "train_loss": 0.29610546401295906, "epoch": 518}
[04:31:15.752875] [04:31:15.752988] Training epoch 518 for 0:01:45
[04:31:15.753043] [04:31:15.758085] log_dir: ./exp/debug/cifar100-LT/debug
[04:31:17.616332] Epoch: [519]  [  0/195]  eta: 0:06:02  lr: 0.000723  loss: 0.3042 (0.3042)  time: 1.8572  data: 1.3695  max mem: 9341
[04:31:27.841536] Epoch: [519]  [ 20/195]  eta: 0:01:40  lr: 0.000722  loss: 0.2956 (0.3014)  time: 0.5112  data: 0.0002  max mem: 9341
[04:31:38.057173] Epoch: [519]  [ 40/195]  eta: 0:01:24  lr: 0.000722  loss: 0.3095 (0.3040)  time: 0.5107  data: 0.0002  max mem: 9341
[04:31:48.271709] Epoch: [519]  [ 60/195]  eta: 0:01:11  lr: 0.000721  loss: 0.2901 (0.3005)  time: 0.5106  data: 0.0002  max mem: 9341
[04:31:58.532757] Epoch: [519]  [ 80/195]  eta: 0:01:00  lr: 0.000721  loss: 0.2963 (0.2998)  time: 0.5130  data: 0.0002  max mem: 9341
[04:32:08.747327] Epoch: [519]  [100/195]  eta: 0:00:49  lr: 0.000720  loss: 0.3070 (0.3008)  time: 0.5106  data: 0.0002  max mem: 9341
[04:32:18.962332] Epoch: [519]  [120/195]  eta: 0:00:39  lr: 0.000720  loss: 0.2920 (0.2990)  time: 0.5107  data: 0.0003  max mem: 9341
[04:32:29.174799] Epoch: [519]  [140/195]  eta: 0:00:28  lr: 0.000720  loss: 0.2969 (0.2991)  time: 0.5106  data: 0.0003  max mem: 9341
[04:32:39.471151] Epoch: [519]  [160/195]  eta: 0:00:18  lr: 0.000719  loss: 0.2977 (0.2995)  time: 0.5147  data: 0.0002  max mem: 9341
[04:32:49.661393] Epoch: [519]  [180/195]  eta: 0:00:07  lr: 0.000718  loss: 0.2984 (0.2991)  time: 0.5095  data: 0.0002  max mem: 9341
[04:32:56.809781] Epoch: [519]  [194/195]  eta: 0:00:00  lr: 0.000718  loss: 0.2912 (0.2987)  time: 0.5124  data: 0.0001  max mem: 9341
[04:32:57.006367] Epoch: [519] Total time: 0:01:41 (0.5192 s / it)
[04:32:57.010922] Averaged stats: lr: 0.000718  loss: 0.2912 (0.2959)
[04:33:01.743020] {"train_lr": 0.0007204223721842489, "train_loss": 0.29589471488426894, "epoch": 519}
[04:33:01.743350] [04:33:01.743454] Training epoch 519 for 0:01:45
[04:33:01.743507] [04:33:01.747960] log_dir: ./exp/debug/cifar100-LT/debug
[04:33:03.696473] Epoch: [520]  [  0/195]  eta: 0:06:19  lr: 0.000718  loss: 0.3194 (0.3194)  time: 1.9468  data: 1.4390  max mem: 9341
[04:33:13.939467] Epoch: [520]  [ 20/195]  eta: 0:01:41  lr: 0.000718  loss: 0.2900 (0.2961)  time: 0.5121  data: 0.0003  max mem: 9341
[04:33:24.162458] Epoch: [520]  [ 40/195]  eta: 0:01:24  lr: 0.000717  loss: 0.2958 (0.2956)  time: 0.5111  data: 0.0002  max mem: 9341
[04:33:34.384720] Epoch: [520]  [ 60/195]  eta: 0:01:12  lr: 0.000717  loss: 0.2948 (0.2958)  time: 0.5111  data: 0.0002  max mem: 9341
[04:33:44.653404] Epoch: [520]  [ 80/195]  eta: 0:01:00  lr: 0.000716  loss: 0.2886 (0.2946)  time: 0.5134  data: 0.0002  max mem: 9341
[04:33:54.866576] Epoch: [520]  [100/195]  eta: 0:00:49  lr: 0.000716  loss: 0.3013 (0.2952)  time: 0.5106  data: 0.0002  max mem: 9341
[04:34:05.081680] Epoch: [520]  [120/195]  eta: 0:00:39  lr: 0.000715  loss: 0.2954 (0.2959)  time: 0.5107  data: 0.0002  max mem: 9341
[04:34:15.291675] Epoch: [520]  [140/195]  eta: 0:00:28  lr: 0.000715  loss: 0.2997 (0.2963)  time: 0.5104  data: 0.0002  max mem: 9341
[04:34:25.553139] Epoch: [520]  [160/195]  eta: 0:00:18  lr: 0.000714  loss: 0.2943 (0.2961)  time: 0.5130  data: 0.0002  max mem: 9341
[04:34:35.729999] Epoch: [520]  [180/195]  eta: 0:00:07  lr: 0.000714  loss: 0.2997 (0.2965)  time: 0.5088  data: 0.0002  max mem: 9341
[04:34:42.861198] Epoch: [520]  [194/195]  eta: 0:00:00  lr: 0.000713  loss: 0.2997 (0.2964)  time: 0.5106  data: 0.0001  max mem: 9341
[04:34:43.040615] Epoch: [520] Total time: 0:01:41 (0.5194 s / it)
[04:34:43.048442] Averaged stats: lr: 0.000713  loss: 0.2997 (0.2978)
[04:34:47.737375] {"train_lr": 0.0007158794343592488, "train_loss": 0.29782055839896204, "epoch": 520}
[04:34:47.737759] [04:34:47.737856] Training epoch 520 for 0:01:45
[04:34:47.737911] [04:34:47.742319] log_dir: ./exp/debug/cifar100-LT/debug
[04:34:49.636176] Epoch: [521]  [  0/195]  eta: 0:06:09  lr: 0.000713  loss: 0.3126 (0.3126)  time: 1.8927  data: 1.3922  max mem: 9341
[04:34:59.856386] Epoch: [521]  [ 20/195]  eta: 0:01:40  lr: 0.000713  loss: 0.2815 (0.2887)  time: 0.5110  data: 0.0002  max mem: 9341
[04:35:10.070655] Epoch: [521]  [ 40/195]  eta: 0:01:24  lr: 0.000713  loss: 0.2991 (0.2937)  time: 0.5106  data: 0.0003  max mem: 9341
[04:35:20.279391] Epoch: [521]  [ 60/195]  eta: 0:01:11  lr: 0.000712  loss: 0.2886 (0.2934)  time: 0.5104  data: 0.0002  max mem: 9341
[04:35:30.529421] Epoch: [521]  [ 80/195]  eta: 0:01:00  lr: 0.000712  loss: 0.2981 (0.2942)  time: 0.5124  data: 0.0003  max mem: 9341
[04:35:40.745415] Epoch: [521]  [100/195]  eta: 0:00:49  lr: 0.000711  loss: 0.3004 (0.2956)  time: 0.5107  data: 0.0002  max mem: 9341
[04:35:50.962390] Epoch: [521]  [120/195]  eta: 0:00:39  lr: 0.000711  loss: 0.3054 (0.2974)  time: 0.5108  data: 0.0003  max mem: 9341
[04:36:01.171834] Epoch: [521]  [140/195]  eta: 0:00:28  lr: 0.000710  loss: 0.3026 (0.2977)  time: 0.5104  data: 0.0002  max mem: 9341
[04:36:11.435514] Epoch: [521]  [160/195]  eta: 0:00:18  lr: 0.000710  loss: 0.3092 (0.2990)  time: 0.5131  data: 0.0002  max mem: 9341
[04:36:21.619908] Epoch: [521]  [180/195]  eta: 0:00:07  lr: 0.000709  loss: 0.2887 (0.2980)  time: 0.5092  data: 0.0002  max mem: 9341
[04:36:28.765721] Epoch: [521]  [194/195]  eta: 0:00:00  lr: 0.000709  loss: 0.2980 (0.2980)  time: 0.5121  data: 0.0001  max mem: 9341
[04:36:28.941099] Epoch: [521] Total time: 0:01:41 (0.5190 s / it)
[04:36:28.953942] Averaged stats: lr: 0.000709  loss: 0.2980 (0.2978)
[04:36:33.844182] {"train_lr": 0.0007113447688189806, "train_loss": 0.2978001873080547, "epoch": 521}
[04:36:33.844472] [04:36:33.844594] Training epoch 521 for 0:01:46
[04:36:33.844649] [04:36:33.849057] log_dir: ./exp/debug/cifar100-LT/debug
[04:36:35.501129] Epoch: [522]  [  0/195]  eta: 0:05:21  lr: 0.000709  loss: 0.2951 (0.2951)  time: 1.6500  data: 1.1414  max mem: 9341
[04:36:45.751275] Epoch: [522]  [ 20/195]  eta: 0:01:39  lr: 0.000709  loss: 0.2937 (0.2923)  time: 0.5124  data: 0.0003  max mem: 9341
[04:36:55.965841] Epoch: [522]  [ 40/195]  eta: 0:01:23  lr: 0.000708  loss: 0.3006 (0.2963)  time: 0.5107  data: 0.0002  max mem: 9341
[04:37:06.180551] Epoch: [522]  [ 60/195]  eta: 0:01:11  lr: 0.000708  loss: 0.2922 (0.2954)  time: 0.5107  data: 0.0002  max mem: 9341
[04:37:16.439507] Epoch: [522]  [ 80/195]  eta: 0:01:00  lr: 0.000707  loss: 0.2837 (0.2954)  time: 0.5129  data: 0.0002  max mem: 9341
[04:37:26.651101] Epoch: [522]  [100/195]  eta: 0:00:49  lr: 0.000707  loss: 0.2974 (0.2954)  time: 0.5105  data: 0.0003  max mem: 9341
[04:37:36.866960] Epoch: [522]  [120/195]  eta: 0:00:39  lr: 0.000706  loss: 0.2959 (0.2955)  time: 0.5107  data: 0.0003  max mem: 9341
[04:37:47.078233] Epoch: [522]  [140/195]  eta: 0:00:28  lr: 0.000706  loss: 0.2807 (0.2951)  time: 0.5105  data: 0.0003  max mem: 9341
[04:37:57.338425] Epoch: [522]  [160/195]  eta: 0:00:18  lr: 0.000705  loss: 0.2927 (0.2951)  time: 0.5130  data: 0.0003  max mem: 9341
[04:38:07.505307] Epoch: [522]  [180/195]  eta: 0:00:07  lr: 0.000705  loss: 0.2973 (0.2956)  time: 0.5083  data: 0.0002  max mem: 9341
[04:38:14.638662] Epoch: [522]  [194/195]  eta: 0:00:00  lr: 0.000704  loss: 0.2834 (0.2951)  time: 0.5107  data: 0.0001  max mem: 9341
[04:38:14.817074] Epoch: [522] Total time: 0:01:40 (0.5178 s / it)
[04:38:14.828922] Averaged stats: lr: 0.000704  loss: 0.2834 (0.2961)
[04:38:19.526180] {"train_lr": 0.0007068184530483677, "train_loss": 0.29613911293638057, "epoch": 522}
[04:38:19.526571] [04:38:19.526681] Training epoch 522 for 0:01:45
[04:38:19.526737] [04:38:19.531328] log_dir: ./exp/debug/cifar100-LT/debug
[04:38:21.421155] Epoch: [523]  [  0/195]  eta: 0:06:08  lr: 0.000704  loss: 0.2896 (0.2896)  time: 1.8887  data: 1.3835  max mem: 9341
[04:38:31.634334] Epoch: [523]  [ 20/195]  eta: 0:01:40  lr: 0.000704  loss: 0.2977 (0.2953)  time: 0.5106  data: 0.0002  max mem: 9341
[04:38:41.858754] Epoch: [523]  [ 40/195]  eta: 0:01:24  lr: 0.000704  loss: 0.2892 (0.2931)  time: 0.5112  data: 0.0002  max mem: 9341
[04:38:52.074508] Epoch: [523]  [ 60/195]  eta: 0:01:12  lr: 0.000703  loss: 0.2892 (0.2929)  time: 0.5107  data: 0.0002  max mem: 9341
[04:39:02.333225] Epoch: [523]  [ 80/195]  eta: 0:01:00  lr: 0.000703  loss: 0.2881 (0.2918)  time: 0.5129  data: 0.0002  max mem: 9341
[04:39:12.553874] Epoch: [523]  [100/195]  eta: 0:00:49  lr: 0.000702  loss: 0.2924 (0.2927)  time: 0.5110  data: 0.0002  max mem: 9341
[04:39:22.779639] Epoch: [523]  [120/195]  eta: 0:00:39  lr: 0.000702  loss: 0.2853 (0.2919)  time: 0.5112  data: 0.0002  max mem: 9341
[04:39:32.999861] Epoch: [523]  [140/195]  eta: 0:00:28  lr: 0.000701  loss: 0.2984 (0.2931)  time: 0.5110  data: 0.0002  max mem: 9341
[04:39:43.263583] Epoch: [523]  [160/195]  eta: 0:00:18  lr: 0.000701  loss: 0.2890 (0.2928)  time: 0.5131  data: 0.0002  max mem: 9341
[04:39:53.437343] Epoch: [523]  [180/195]  eta: 0:00:07  lr: 0.000700  loss: 0.2903 (0.2927)  time: 0.5086  data: 0.0001  max mem: 9341
[04:40:00.570126] Epoch: [523]  [194/195]  eta: 0:00:00  lr: 0.000700  loss: 0.2990 (0.2935)  time: 0.5106  data: 0.0001  max mem: 9341
[04:40:00.742471] Epoch: [523] Total time: 0:01:41 (0.5190 s / it)
[04:40:00.760968] Averaged stats: lr: 0.000700  loss: 0.2990 (0.2947)
[04:40:05.536460] {"train_lr": 0.000702300564389662, "train_loss": 0.29471714968482654, "epoch": 523}
[04:40:05.536797] [04:40:05.536882] Training epoch 523 for 0:01:46
[04:40:05.536947] [04:40:05.541463] log_dir: ./exp/debug/cifar100-LT/debug
[04:40:07.231063] Epoch: [524]  [  0/195]  eta: 0:05:29  lr: 0.000700  loss: 0.2778 (0.2778)  time: 1.6879  data: 1.1773  max mem: 9341
[04:40:17.453106] Epoch: [524]  [ 20/195]  eta: 0:01:39  lr: 0.000699  loss: 0.2835 (0.2861)  time: 0.5110  data: 0.0003  max mem: 9341
[04:40:27.669536] Epoch: [524]  [ 40/195]  eta: 0:01:23  lr: 0.000699  loss: 0.2873 (0.2882)  time: 0.5108  data: 0.0002  max mem: 9341
[04:40:37.890038] Epoch: [524]  [ 60/195]  eta: 0:01:11  lr: 0.000699  loss: 0.2933 (0.2891)  time: 0.5110  data: 0.0003  max mem: 9341
[04:40:48.165790] Epoch: [524]  [ 80/195]  eta: 0:01:00  lr: 0.000698  loss: 0.2884 (0.2891)  time: 0.5137  data: 0.0003  max mem: 9341
[04:40:58.391644] Epoch: [524]  [100/195]  eta: 0:00:49  lr: 0.000698  loss: 0.2915 (0.2900)  time: 0.5112  data: 0.0003  max mem: 9341
[04:41:08.609529] Epoch: [524]  [120/195]  eta: 0:00:39  lr: 0.000697  loss: 0.2775 (0.2893)  time: 0.5108  data: 0.0003  max mem: 9341
[04:41:18.828723] Epoch: [524]  [140/195]  eta: 0:00:28  lr: 0.000697  loss: 0.2898 (0.2891)  time: 0.5109  data: 0.0002  max mem: 9341
[04:41:29.093330] Epoch: [524]  [160/195]  eta: 0:00:18  lr: 0.000696  loss: 0.2896 (0.2892)  time: 0.5131  data: 0.0002  max mem: 9341
[04:41:39.269453] Epoch: [524]  [180/195]  eta: 0:00:07  lr: 0.000696  loss: 0.2917 (0.2890)  time: 0.5088  data: 0.0002  max mem: 9341
[04:41:46.403193] Epoch: [524]  [194/195]  eta: 0:00:00  lr: 0.000695  loss: 0.2904 (0.2894)  time: 0.5107  data: 0.0001  max mem: 9341
[04:41:46.580111] Epoch: [524] Total time: 0:01:41 (0.5181 s / it)
[04:41:46.589388] Averaged stats: lr: 0.000695  loss: 0.2904 (0.2940)
[04:41:51.367021] {"train_lr": 0.0006977911800411084, "train_loss": 0.2940279513406448, "epoch": 524}
[04:41:51.367343] [04:41:51.367449] Training epoch 524 for 0:01:45
[04:41:51.367504] [04:41:51.371990] log_dir: ./exp/debug/cifar100-LT/debug
[04:41:53.129160] Epoch: [525]  [  0/195]  eta: 0:05:42  lr: 0.000695  loss: 0.3212 (0.3212)  time: 1.7555  data: 1.2537  max mem: 9341
[04:42:03.367827] Epoch: [525]  [ 20/195]  eta: 0:01:39  lr: 0.000695  loss: 0.2992 (0.3002)  time: 0.5119  data: 0.0002  max mem: 9341
[04:42:13.610592] Epoch: [525]  [ 40/195]  eta: 0:01:24  lr: 0.000695  loss: 0.2964 (0.2980)  time: 0.5121  data: 0.0002  max mem: 9341
[04:42:23.817748] Epoch: [525]  [ 60/195]  eta: 0:01:11  lr: 0.000694  loss: 0.2885 (0.2957)  time: 0.5103  data: 0.0002  max mem: 9341
[04:42:34.073548] Epoch: [525]  [ 80/195]  eta: 0:01:00  lr: 0.000694  loss: 0.2966 (0.2959)  time: 0.5127  data: 0.0002  max mem: 9341
[04:42:44.285710] Epoch: [525]  [100/195]  eta: 0:00:49  lr: 0.000693  loss: 0.3004 (0.2964)  time: 0.5106  data: 0.0002  max mem: 9341
[04:42:54.502868] Epoch: [525]  [120/195]  eta: 0:00:39  lr: 0.000693  loss: 0.2939 (0.2964)  time: 0.5108  data: 0.0002  max mem: 9341
[04:43:04.713273] Epoch: [525]  [140/195]  eta: 0:00:28  lr: 0.000692  loss: 0.2935 (0.2965)  time: 0.5105  data: 0.0002  max mem: 9341
[04:43:14.964809] Epoch: [525]  [160/195]  eta: 0:00:18  lr: 0.000692  loss: 0.2896 (0.2960)  time: 0.5125  data: 0.0002  max mem: 9341
[04:43:25.130566] Epoch: [525]  [180/195]  eta: 0:00:07  lr: 0.000691  loss: 0.2868 (0.2958)  time: 0.5082  data: 0.0001  max mem: 9341
[04:43:32.252880] Epoch: [525]  [194/195]  eta: 0:00:00  lr: 0.000691  loss: 0.2876 (0.2956)  time: 0.5100  data: 0.0001  max mem: 9341
[04:43:32.419768] Epoch: [525] Total time: 0:01:41 (0.5182 s / it)
[04:43:32.449296] Averaged stats: lr: 0.000691  loss: 0.2876 (0.2946)
[04:43:37.283420] {"train_lr": 0.0006932903770556488, "train_loss": 0.29459307870039575, "epoch": 525}
[04:43:37.283753] [04:43:37.283858] Training epoch 525 for 0:01:45
[04:43:37.283913] [04:43:37.289138] log_dir: ./exp/debug/cifar100-LT/debug
[04:43:39.028539] Epoch: [526]  [  0/195]  eta: 0:05:38  lr: 0.000691  loss: 0.3369 (0.3369)  time: 1.7381  data: 1.2218  max mem: 9341
[04:43:49.267042] Epoch: [526]  [ 20/195]  eta: 0:01:39  lr: 0.000690  loss: 0.2863 (0.2972)  time: 0.5119  data: 0.0002  max mem: 9341
[04:43:59.474744] Epoch: [526]  [ 40/195]  eta: 0:01:23  lr: 0.000690  loss: 0.2997 (0.2985)  time: 0.5103  data: 0.0002  max mem: 9341
[04:44:09.679406] Epoch: [526]  [ 60/195]  eta: 0:01:11  lr: 0.000690  loss: 0.2882 (0.2956)  time: 0.5102  data: 0.0002  max mem: 9341
[04:44:19.938793] Epoch: [526]  [ 80/195]  eta: 0:01:00  lr: 0.000689  loss: 0.2828 (0.2938)  time: 0.5129  data: 0.0002  max mem: 9341
[04:44:30.163778] Epoch: [526]  [100/195]  eta: 0:00:49  lr: 0.000689  loss: 0.2897 (0.2930)  time: 0.5112  data: 0.0002  max mem: 9341
[04:44:40.379759] Epoch: [526]  [120/195]  eta: 0:00:39  lr: 0.000688  loss: 0.3008 (0.2938)  time: 0.5107  data: 0.0002  max mem: 9341
[04:44:50.597054] Epoch: [526]  [140/195]  eta: 0:00:28  lr: 0.000688  loss: 0.2985 (0.2944)  time: 0.5108  data: 0.0002  max mem: 9341
[04:45:00.861256] Epoch: [526]  [160/195]  eta: 0:00:18  lr: 0.000687  loss: 0.2919 (0.2941)  time: 0.5132  data: 0.0002  max mem: 9341
[04:45:11.032939] Epoch: [526]  [180/195]  eta: 0:00:07  lr: 0.000687  loss: 0.2942 (0.2948)  time: 0.5085  data: 0.0001  max mem: 9341
[04:45:18.163530] Epoch: [526]  [194/195]  eta: 0:00:00  lr: 0.000686  loss: 0.2933 (0.2947)  time: 0.5106  data: 0.0001  max mem: 9341
[04:45:18.321820] Epoch: [526] Total time: 0:01:41 (0.5181 s / it)
[04:45:18.339860] Averaged stats: lr: 0.000686  loss: 0.2933 (0.2947)
[04:45:23.060634] {"train_lr": 0.0006887982323395822, "train_loss": 0.2946518631890798, "epoch": 526}
[04:45:23.060888] [04:45:23.060974] Training epoch 526 for 0:01:45
[04:45:23.061028] [04:45:23.065526] log_dir: ./exp/debug/cifar100-LT/debug
[04:45:24.877788] Epoch: [527]  [  0/195]  eta: 0:05:52  lr: 0.000686  loss: 0.3199 (0.3199)  time: 1.8100  data: 1.3125  max mem: 9341
[04:45:35.093921] Epoch: [527]  [ 20/195]  eta: 0:01:40  lr: 0.000686  loss: 0.2973 (0.3054)  time: 0.5107  data: 0.0003  max mem: 9341
[04:45:45.316390] Epoch: [527]  [ 40/195]  eta: 0:01:24  lr: 0.000686  loss: 0.2985 (0.3010)  time: 0.5111  data: 0.0003  max mem: 9341
[04:45:55.526397] Epoch: [527]  [ 60/195]  eta: 0:01:11  lr: 0.000685  loss: 0.2867 (0.2982)  time: 0.5104  data: 0.0003  max mem: 9341
[04:46:05.788425] Epoch: [527]  [ 80/195]  eta: 0:01:00  lr: 0.000685  loss: 0.2855 (0.2962)  time: 0.5130  data: 0.0003  max mem: 9341
[04:46:16.011142] Epoch: [527]  [100/195]  eta: 0:00:49  lr: 0.000684  loss: 0.2977 (0.2972)  time: 0.5111  data: 0.0003  max mem: 9341
[04:46:26.226176] Epoch: [527]  [120/195]  eta: 0:00:39  lr: 0.000684  loss: 0.2864 (0.2966)  time: 0.5107  data: 0.0003  max mem: 9341
[04:46:36.439258] Epoch: [527]  [140/195]  eta: 0:00:28  lr: 0.000683  loss: 0.2815 (0.2963)  time: 0.5106  data: 0.0002  max mem: 9341
[04:46:46.705140] Epoch: [527]  [160/195]  eta: 0:00:18  lr: 0.000683  loss: 0.3026 (0.2964)  time: 0.5132  data: 0.0003  max mem: 9341
[04:46:56.883100] Epoch: [527]  [180/195]  eta: 0:00:07  lr: 0.000682  loss: 0.2823 (0.2958)  time: 0.5088  data: 0.0002  max mem: 9341
[04:47:04.019858] Epoch: [527]  [194/195]  eta: 0:00:00  lr: 0.000682  loss: 0.3086 (0.2967)  time: 0.5109  data: 0.0001  max mem: 9341
[04:47:04.192476] Epoch: [527] Total time: 0:01:41 (0.5186 s / it)
[04:47:04.236705] Averaged stats: lr: 0.000682  loss: 0.3086 (0.2970)
[04:47:09.037024] {"train_lr": 0.0006843148226512768, "train_loss": 0.29695922147769194, "epoch": 527}
[04:47:09.037271] [04:47:09.037371] Training epoch 527 for 0:01:45
[04:47:09.037426] [04:47:09.041870] log_dir: ./exp/debug/cifar100-LT/debug
[04:47:10.642809] Epoch: [528]  [  0/195]  eta: 0:05:11  lr: 0.000682  loss: 0.2455 (0.2455)  time: 1.5989  data: 1.0977  max mem: 9341
[04:47:20.864622] Epoch: [528]  [ 20/195]  eta: 0:01:38  lr: 0.000682  loss: 0.2991 (0.2966)  time: 0.5110  data: 0.0002  max mem: 9341
[04:47:31.076523] Epoch: [528]  [ 40/195]  eta: 0:01:23  lr: 0.000681  loss: 0.2963 (0.2995)  time: 0.5105  data: 0.0002  max mem: 9341
[04:47:41.285284] Epoch: [528]  [ 60/195]  eta: 0:01:11  lr: 0.000681  loss: 0.2911 (0.2963)  time: 0.5103  data: 0.0002  max mem: 9341
[04:47:51.543545] Epoch: [528]  [ 80/195]  eta: 0:01:00  lr: 0.000680  loss: 0.2977 (0.2967)  time: 0.5129  data: 0.0002  max mem: 9341
[04:48:01.778324] Epoch: [528]  [100/195]  eta: 0:00:49  lr: 0.000680  loss: 0.2925 (0.2960)  time: 0.5117  data: 0.0002  max mem: 9341
[04:48:12.016890] Epoch: [528]  [120/195]  eta: 0:00:39  lr: 0.000679  loss: 0.2943 (0.2960)  time: 0.5119  data: 0.0002  max mem: 9341
[04:48:22.254773] Epoch: [528]  [140/195]  eta: 0:00:28  lr: 0.000679  loss: 0.2817 (0.2950)  time: 0.5118  data: 0.0002  max mem: 9341
[04:48:32.556632] Epoch: [528]  [160/195]  eta: 0:00:18  lr: 0.000678  loss: 0.3030 (0.2957)  time: 0.5150  data: 0.0002  max mem: 9341
[04:48:42.745643] Epoch: [528]  [180/195]  eta: 0:00:07  lr: 0.000678  loss: 0.2974 (0.2958)  time: 0.5094  data: 0.0001  max mem: 9341
[04:48:49.901102] Epoch: [528]  [194/195]  eta: 0:00:00  lr: 0.000677  loss: 0.2975 (0.2962)  time: 0.5126  data: 0.0001  max mem: 9341
[04:48:50.076659] Epoch: [528] Total time: 0:01:41 (0.5181 s / it)
[04:48:50.087211] Averaged stats: lr: 0.000677  loss: 0.2975 (0.2948)
[04:48:54.770935] {"train_lr": 0.0006798402245998249, "train_loss": 0.29481187011951054, "epoch": 528}
[04:48:54.771254] [04:48:54.771339] Training epoch 528 for 0:01:45
[04:48:54.771392] [04:48:54.775813] log_dir: ./exp/debug/cifar100-LT/debug
[04:48:56.444983] Epoch: [529]  [  0/195]  eta: 0:05:25  lr: 0.000677  loss: 0.2997 (0.2997)  time: 1.6682  data: 1.1647  max mem: 9341
[04:49:06.660924] Epoch: [529]  [ 20/195]  eta: 0:01:39  lr: 0.000677  loss: 0.2827 (0.2883)  time: 0.5107  data: 0.0002  max mem: 9341
[04:49:16.883807] Epoch: [529]  [ 40/195]  eta: 0:01:23  lr: 0.000677  loss: 0.3017 (0.2952)  time: 0.5111  data: 0.0002  max mem: 9341
[04:49:27.097553] Epoch: [529]  [ 60/195]  eta: 0:01:11  lr: 0.000676  loss: 0.2974 (0.2951)  time: 0.5106  data: 0.0002  max mem: 9341
[04:49:37.353003] Epoch: [529]  [ 80/195]  eta: 0:01:00  lr: 0.000676  loss: 0.2865 (0.2930)  time: 0.5127  data: 0.0002  max mem: 9341
[04:49:47.572562] Epoch: [529]  [100/195]  eta: 0:00:49  lr: 0.000675  loss: 0.2841 (0.2927)  time: 0.5109  data: 0.0003  max mem: 9341
[04:49:57.789360] Epoch: [529]  [120/195]  eta: 0:00:39  lr: 0.000675  loss: 0.2914 (0.2919)  time: 0.5108  data: 0.0002  max mem: 9341
[04:50:08.002825] Epoch: [529]  [140/195]  eta: 0:00:28  lr: 0.000674  loss: 0.2946 (0.2922)  time: 0.5106  data: 0.0002  max mem: 9341
[04:50:18.263694] Epoch: [529]  [160/195]  eta: 0:00:18  lr: 0.000674  loss: 0.3001 (0.2932)  time: 0.5130  data: 0.0002  max mem: 9341
[04:50:28.442587] Epoch: [529]  [180/195]  eta: 0:00:07  lr: 0.000673  loss: 0.2893 (0.2928)  time: 0.5089  data: 0.0001  max mem: 9341
[04:50:35.577157] Epoch: [529]  [194/195]  eta: 0:00:00  lr: 0.000673  loss: 0.2832 (0.2923)  time: 0.5109  data: 0.0001  max mem: 9341
[04:50:35.746453] Epoch: [529] Total time: 0:01:40 (0.5178 s / it)
[04:50:35.765468] Averaged stats: lr: 0.000673  loss: 0.2832 (0.2945)
[04:50:40.538012] {"train_lr": 0.0006753745146437634, "train_loss": 0.2944892406081542, "epoch": 529}
[04:50:40.538317] [04:50:40.538412] Training epoch 529 for 0:01:45
[04:50:40.538466] [04:50:40.542894] log_dir: ./exp/debug/cifar100-LT/debug
[04:50:42.265499] Epoch: [530]  [  0/195]  eta: 0:05:35  lr: 0.000673  loss: 0.3284 (0.3284)  time: 1.7212  data: 1.2347  max mem: 9341
[04:50:52.502029] Epoch: [530]  [ 20/195]  eta: 0:01:39  lr: 0.000673  loss: 0.2892 (0.2956)  time: 0.5118  data: 0.0002  max mem: 9341
[04:51:02.718017] Epoch: [530]  [ 40/195]  eta: 0:01:23  lr: 0.000672  loss: 0.2954 (0.2976)  time: 0.5107  data: 0.0002  max mem: 9341
[04:51:12.939562] Epoch: [530]  [ 60/195]  eta: 0:01:11  lr: 0.000672  loss: 0.2925 (0.2959)  time: 0.5110  data: 0.0002  max mem: 9341
[04:51:23.206689] Epoch: [530]  [ 80/195]  eta: 0:01:00  lr: 0.000671  loss: 0.3079 (0.2981)  time: 0.5132  data: 0.0003  max mem: 9341
[04:51:33.420524] Epoch: [530]  [100/195]  eta: 0:00:49  lr: 0.000671  loss: 0.2920 (0.2978)  time: 0.5106  data: 0.0003  max mem: 9341
[04:51:43.631415] Epoch: [530]  [120/195]  eta: 0:00:39  lr: 0.000670  loss: 0.3025 (0.2989)  time: 0.5105  data: 0.0003  max mem: 9341
[04:51:53.847625] Epoch: [530]  [140/195]  eta: 0:00:28  lr: 0.000670  loss: 0.2919 (0.2985)  time: 0.5108  data: 0.0003  max mem: 9341
[04:52:04.154229] Epoch: [530]  [160/195]  eta: 0:00:18  lr: 0.000669  loss: 0.2916 (0.2984)  time: 0.5153  data: 0.0003  max mem: 9341
[04:52:14.350994] Epoch: [530]  [180/195]  eta: 0:00:07  lr: 0.000669  loss: 0.2893 (0.2975)  time: 0.5098  data: 0.0002  max mem: 9341
[04:52:21.499810] Epoch: [530]  [194/195]  eta: 0:00:00  lr: 0.000669  loss: 0.2877 (0.2971)  time: 0.5127  data: 0.0001  max mem: 9341
[04:52:21.685439] Epoch: [530] Total time: 0:01:41 (0.5187 s / it)
[04:52:21.698999] Averaged stats: lr: 0.000669  loss: 0.2877 (0.2955)
[04:52:26.458080] {"train_lr": 0.0006709177690897564, "train_loss": 0.29552428057560554, "epoch": 530}
[04:52:26.458532] [04:52:26.458658] Training epoch 530 for 0:01:45
[04:52:26.458716] [04:52:26.463116] log_dir: ./exp/debug/cifar100-LT/debug
[04:52:28.059431] Epoch: [531]  [  0/195]  eta: 0:05:10  lr: 0.000669  loss: 0.2912 (0.2912)  time: 1.5948  data: 1.0935  max mem: 9341
[04:52:38.272472] Epoch: [531]  [ 20/195]  eta: 0:01:38  lr: 0.000668  loss: 0.2968 (0.2975)  time: 0.5106  data: 0.0002  max mem: 9341
[04:52:48.498315] Epoch: [531]  [ 40/195]  eta: 0:01:23  lr: 0.000668  loss: 0.2965 (0.2929)  time: 0.5112  data: 0.0002  max mem: 9341
[04:52:58.718954] Epoch: [531]  [ 60/195]  eta: 0:01:11  lr: 0.000667  loss: 0.2909 (0.2939)  time: 0.5110  data: 0.0002  max mem: 9341
[04:53:08.976844] Epoch: [531]  [ 80/195]  eta: 0:01:00  lr: 0.000667  loss: 0.2951 (0.2952)  time: 0.5128  data: 0.0002  max mem: 9341
[04:53:19.193590] Epoch: [531]  [100/195]  eta: 0:00:49  lr: 0.000666  loss: 0.2915 (0.2948)  time: 0.5108  data: 0.0002  max mem: 9341
[04:53:29.406454] Epoch: [531]  [120/195]  eta: 0:00:39  lr: 0.000666  loss: 0.2926 (0.2940)  time: 0.5106  data: 0.0002  max mem: 9341
[04:53:39.622513] Epoch: [531]  [140/195]  eta: 0:00:28  lr: 0.000666  loss: 0.2953 (0.2946)  time: 0.5107  data: 0.0002  max mem: 9341
[04:53:49.929596] Epoch: [531]  [160/195]  eta: 0:00:18  lr: 0.000665  loss: 0.2972 (0.2951)  time: 0.5153  data: 0.0002  max mem: 9341
[04:54:00.109104] Epoch: [531]  [180/195]  eta: 0:00:07  lr: 0.000665  loss: 0.2882 (0.2953)  time: 0.5089  data: 0.0001  max mem: 9341
[04:54:07.241814] Epoch: [531]  [194/195]  eta: 0:00:00  lr: 0.000664  loss: 0.2984 (0.2959)  time: 0.5108  data: 0.0001  max mem: 9341
[04:54:07.434416] Epoch: [531] Total time: 0:01:40 (0.5178 s / it)
[04:54:07.443546] Averaged stats: lr: 0.000664  loss: 0.2984 (0.2954)
[04:54:12.228639] {"train_lr": 0.0006664700640912905, "train_loss": 0.2954163831969102, "epoch": 531}
[04:54:12.228913] [04:54:12.228997] Training epoch 531 for 0:01:45
[04:54:12.229051] [04:54:12.233496] log_dir: ./exp/debug/cifar100-LT/debug
[04:54:13.854642] Epoch: [532]  [  0/195]  eta: 0:05:15  lr: 0.000664  loss: 0.2799 (0.2799)  time: 1.6201  data: 1.1097  max mem: 9341
[04:54:24.076700] Epoch: [532]  [ 20/195]  eta: 0:01:38  lr: 0.000664  loss: 0.3072 (0.2996)  time: 0.5110  data: 0.0002  max mem: 9341
[04:54:34.302743] Epoch: [532]  [ 40/195]  eta: 0:01:23  lr: 0.000663  loss: 0.2871 (0.2948)  time: 0.5112  data: 0.0002  max mem: 9341
[04:54:44.522188] Epoch: [532]  [ 60/195]  eta: 0:01:11  lr: 0.000663  loss: 0.2803 (0.2930)  time: 0.5109  data: 0.0003  max mem: 9341
[04:54:54.782654] Epoch: [532]  [ 80/195]  eta: 0:01:00  lr: 0.000662  loss: 0.2979 (0.2944)  time: 0.5130  data: 0.0003  max mem: 9341
[04:55:05.009273] Epoch: [532]  [100/195]  eta: 0:00:49  lr: 0.000662  loss: 0.2985 (0.2954)  time: 0.5113  data: 0.0003  max mem: 9341
[04:55:15.220212] Epoch: [532]  [120/195]  eta: 0:00:39  lr: 0.000662  loss: 0.2986 (0.2960)  time: 0.5105  data: 0.0003  max mem: 9341
[04:55:25.454756] Epoch: [532]  [140/195]  eta: 0:00:28  lr: 0.000661  loss: 0.2859 (0.2966)  time: 0.5117  data: 0.0003  max mem: 9341
[04:55:35.749254] Epoch: [532]  [160/195]  eta: 0:00:18  lr: 0.000660  loss: 0.2923 (0.2959)  time: 0.5147  data: 0.0003  max mem: 9341
[04:55:45.940415] Epoch: [532]  [180/195]  eta: 0:00:07  lr: 0.000660  loss: 0.2919 (0.2954)  time: 0.5095  data: 0.0002  max mem: 9341
[04:55:53.093057] Epoch: [532]  [194/195]  eta: 0:00:00  lr: 0.000660  loss: 0.3011 (0.2964)  time: 0.5128  data: 0.0001  max mem: 9341
[04:55:53.284979] Epoch: [532] Total time: 0:01:41 (0.5182 s / it)
[04:55:53.297349] Averaged stats: lr: 0.000660  loss: 0.3011 (0.2958)
[04:55:58.021404] {"train_lr": 0.0006620314756473669, "train_loss": 0.2957508835272911, "epoch": 532}
[04:55:58.021795] [04:55:58.021899] Training epoch 532 for 0:01:45
[04:55:58.021954] [04:55:58.026549] log_dir: ./exp/debug/cifar100-LT/debug
[04:55:59.911748] Epoch: [533]  [  0/195]  eta: 0:06:07  lr: 0.000660  loss: 0.2603 (0.2603)  time: 1.8837  data: 1.3829  max mem: 9341
[04:56:10.149570] Epoch: [533]  [ 20/195]  eta: 0:01:40  lr: 0.000659  loss: 0.2860 (0.2881)  time: 0.5118  data: 0.0003  max mem: 9341
[04:56:20.388423] Epoch: [533]  [ 40/195]  eta: 0:01:24  lr: 0.000659  loss: 0.2999 (0.2946)  time: 0.5119  data: 0.0002  max mem: 9341
[04:56:30.593349] Epoch: [533]  [ 60/195]  eta: 0:01:12  lr: 0.000659  loss: 0.3097 (0.2982)  time: 0.5102  data: 0.0003  max mem: 9341
[04:56:40.869800] Epoch: [533]  [ 80/195]  eta: 0:01:00  lr: 0.000658  loss: 0.3049 (0.2999)  time: 0.5138  data: 0.0003  max mem: 9341
[04:56:51.108446] Epoch: [533]  [100/195]  eta: 0:00:49  lr: 0.000657  loss: 0.2880 (0.2980)  time: 0.5119  data: 0.0003  max mem: 9341
[04:57:01.347910] Epoch: [533]  [120/195]  eta: 0:00:39  lr: 0.000657  loss: 0.3028 (0.2990)  time: 0.5119  data: 0.0003  max mem: 9341
[04:57:11.586176] Epoch: [533]  [140/195]  eta: 0:00:28  lr: 0.000657  loss: 0.2993 (0.2987)  time: 0.5119  data: 0.0003  max mem: 9341
[04:57:21.884898] Epoch: [533]  [160/195]  eta: 0:00:18  lr: 0.000656  loss: 0.2869 (0.2976)  time: 0.5149  data: 0.0002  max mem: 9341
[04:57:32.084712] Epoch: [533]  [180/195]  eta: 0:00:07  lr: 0.000656  loss: 0.2998 (0.2976)  time: 0.5099  data: 0.0002  max mem: 9341
[04:57:39.239491] Epoch: [533]  [194/195]  eta: 0:00:00  lr: 0.000655  loss: 0.2789 (0.2966)  time: 0.5128  data: 0.0001  max mem: 9341
[04:57:39.428466] Epoch: [533] Total time: 0:01:41 (0.5200 s / it)
[04:57:39.436541] Averaged stats: lr: 0.000655  loss: 0.2789 (0.2976)
[04:57:44.164310] {"train_lr": 0.0006576020796012222, "train_loss": 0.29764099228076446, "epoch": 533}
[04:57:44.164573] [04:57:44.164678] Training epoch 533 for 0:01:46
[04:57:44.164732] [04:57:44.169178] log_dir: ./exp/debug/cifar100-LT/debug
[04:57:45.990654] Epoch: [534]  [  0/195]  eta: 0:05:55  lr: 0.000655  loss: 0.2879 (0.2879)  time: 1.8206  data: 1.3117  max mem: 9341
[04:57:56.217146] Epoch: [534]  [ 20/195]  eta: 0:01:40  lr: 0.000655  loss: 0.2904 (0.2881)  time: 0.5112  data: 0.0003  max mem: 9341
[04:58:06.438946] Epoch: [534]  [ 40/195]  eta: 0:01:24  lr: 0.000654  loss: 0.2936 (0.2935)  time: 0.5110  data: 0.0003  max mem: 9341
[04:58:16.651311] Epoch: [534]  [ 60/195]  eta: 0:01:11  lr: 0.000654  loss: 0.2998 (0.2951)  time: 0.5106  data: 0.0002  max mem: 9341
[04:58:26.912741] Epoch: [534]  [ 80/195]  eta: 0:01:00  lr: 0.000653  loss: 0.2952 (0.2962)  time: 0.5130  data: 0.0002  max mem: 9341
[04:58:37.126083] Epoch: [534]  [100/195]  eta: 0:00:49  lr: 0.000653  loss: 0.2925 (0.2961)  time: 0.5106  data: 0.0002  max mem: 9341
[04:58:47.333894] Epoch: [534]  [120/195]  eta: 0:00:39  lr: 0.000653  loss: 0.3004 (0.2965)  time: 0.5103  data: 0.0003  max mem: 9341
[04:58:57.544920] Epoch: [534]  [140/195]  eta: 0:00:28  lr: 0.000652  loss: 0.3006 (0.2975)  time: 0.5105  data: 0.0003  max mem: 9341
[04:59:07.826438] Epoch: [534]  [160/195]  eta: 0:00:18  lr: 0.000652  loss: 0.2946 (0.2971)  time: 0.5140  data: 0.0003  max mem: 9341
[04:59:18.018698] Epoch: [534]  [180/195]  eta: 0:00:07  lr: 0.000651  loss: 0.2950 (0.2968)  time: 0.5096  data: 0.0002  max mem: 9341
[04:59:25.167857] Epoch: [534]  [194/195]  eta: 0:00:00  lr: 0.000651  loss: 0.3010 (0.2974)  time: 0.5124  data: 0.0001  max mem: 9341
[04:59:25.341935] Epoch: [534] Total time: 0:01:41 (0.5188 s / it)
[04:59:25.358728] Averaged stats: lr: 0.000651  loss: 0.3010 (0.2959)
[04:59:30.099712] {"train_lr": 0.0006531819516390083, "train_loss": 0.2959328509867191, "epoch": 534}
[04:59:30.099981] [04:59:30.100116] Training epoch 534 for 0:01:45
[04:59:30.100172] [04:59:30.104669] log_dir: ./exp/debug/cifar100-LT/debug
[04:59:31.762586] Epoch: [535]  [  0/195]  eta: 0:05:23  lr: 0.000651  loss: 0.3075 (0.3075)  time: 1.6565  data: 1.1520  max mem: 9341
[04:59:41.973055] Epoch: [535]  [ 20/195]  eta: 0:01:38  lr: 0.000650  loss: 0.2911 (0.2968)  time: 0.5105  data: 0.0002  max mem: 9341
[04:59:52.178912] Epoch: [535]  [ 40/195]  eta: 0:01:23  lr: 0.000650  loss: 0.2887 (0.2933)  time: 0.5102  data: 0.0002  max mem: 9341
[05:00:02.397554] Epoch: [535]  [ 60/195]  eta: 0:01:11  lr: 0.000650  loss: 0.2982 (0.2948)  time: 0.5109  data: 0.0002  max mem: 9341
[05:00:12.654571] Epoch: [535]  [ 80/195]  eta: 0:01:00  lr: 0.000649  loss: 0.2874 (0.2938)  time: 0.5128  data: 0.0002  max mem: 9341
[05:00:22.869731] Epoch: [535]  [100/195]  eta: 0:00:49  lr: 0.000649  loss: 0.2896 (0.2932)  time: 0.5107  data: 0.0002  max mem: 9341
[05:00:33.079934] Epoch: [535]  [120/195]  eta: 0:00:39  lr: 0.000648  loss: 0.3015 (0.2942)  time: 0.5105  data: 0.0002  max mem: 9341
[05:00:43.289090] Epoch: [535]  [140/195]  eta: 0:00:28  lr: 0.000648  loss: 0.2838 (0.2932)  time: 0.5104  data: 0.0002  max mem: 9341
[05:00:53.550442] Epoch: [535]  [160/195]  eta: 0:00:18  lr: 0.000647  loss: 0.2884 (0.2931)  time: 0.5130  data: 0.0002  max mem: 9341
[05:01:03.722844] Epoch: [535]  [180/195]  eta: 0:00:07  lr: 0.000647  loss: 0.2917 (0.2934)  time: 0.5086  data: 0.0001  max mem: 9341
[05:01:10.849329] Epoch: [535]  [194/195]  eta: 0:00:00  lr: 0.000646  loss: 0.2827 (0.2932)  time: 0.5103  data: 0.0001  max mem: 9341
[05:01:11.037828] Epoch: [535] Total time: 0:01:40 (0.5176 s / it)
[05:01:11.045783] Averaged stats: lr: 0.000646  loss: 0.2827 (0.2942)
[05:01:15.759399] {"train_lr": 0.0006487711672885182, "train_loss": 0.29417550183641605, "epoch": 535}
[05:01:15.759658] [05:01:15.759745] Training epoch 535 for 0:01:45
[05:01:15.759799] [05:01:15.764236] log_dir: ./exp/debug/cifar100-LT/debug
[05:01:17.607972] Epoch: [536]  [  0/195]  eta: 0:05:59  lr: 0.000646  loss: 0.3307 (0.3307)  time: 1.8426  data: 1.3426  max mem: 9341
[05:01:27.861266] Epoch: [536]  [ 20/195]  eta: 0:01:40  lr: 0.000646  loss: 0.3008 (0.2963)  time: 0.5126  data: 0.0002  max mem: 9341
[05:01:38.088448] Epoch: [536]  [ 40/195]  eta: 0:01:24  lr: 0.000646  loss: 0.2957 (0.2950)  time: 0.5113  data: 0.0002  max mem: 9341
[05:01:48.312958] Epoch: [536]  [ 60/195]  eta: 0:01:12  lr: 0.000645  loss: 0.2981 (0.2956)  time: 0.5111  data: 0.0003  max mem: 9341
[05:01:58.582476] Epoch: [536]  [ 80/195]  eta: 0:01:00  lr: 0.000645  loss: 0.2821 (0.2924)  time: 0.5134  data: 0.0002  max mem: 9341
[05:02:08.794256] Epoch: [536]  [100/195]  eta: 0:00:49  lr: 0.000644  loss: 0.2921 (0.2927)  time: 0.5105  data: 0.0003  max mem: 9341
[05:02:19.013650] Epoch: [536]  [120/195]  eta: 0:00:39  lr: 0.000644  loss: 0.2887 (0.2916)  time: 0.5109  data: 0.0002  max mem: 9341
[05:02:29.238927] Epoch: [536]  [140/195]  eta: 0:00:28  lr: 0.000644  loss: 0.2850 (0.2914)  time: 0.5112  data: 0.0002  max mem: 9341
[05:02:39.506106] Epoch: [536]  [160/195]  eta: 0:00:18  lr: 0.000643  loss: 0.2930 (0.2915)  time: 0.5133  data: 0.0002  max mem: 9341
[05:02:49.691957] Epoch: [536]  [180/195]  eta: 0:00:07  lr: 0.000642  loss: 0.2911 (0.2914)  time: 0.5092  data: 0.0001  max mem: 9341
[05:02:56.835760] Epoch: [536]  [194/195]  eta: 0:00:00  lr: 0.000642  loss: 0.2911 (0.2918)  time: 0.5116  data: 0.0001  max mem: 9341
[05:02:57.009688] Epoch: [536] Total time: 0:01:41 (0.5192 s / it)
[05:02:57.029384] Averaged stats: lr: 0.000642  loss: 0.2911 (0.2935)
[05:03:01.867219] {"train_lr": 0.0006443698019178886, "train_loss": 0.2934642957953306, "epoch": 536}
[05:03:01.867480] [05:03:01.867587] Training epoch 536 for 0:01:46
[05:03:01.867641] [05:03:01.872090] log_dir: ./exp/debug/cifar100-LT/debug
[05:03:03.753583] Epoch: [537]  [  0/195]  eta: 0:06:06  lr: 0.000642  loss: 0.2988 (0.2988)  time: 1.8805  data: 1.3897  max mem: 9341
[05:03:14.028100] Epoch: [537]  [ 20/195]  eta: 0:01:41  lr: 0.000642  loss: 0.2839 (0.2922)  time: 0.5137  data: 0.0002  max mem: 9341
[05:03:24.248132] Epoch: [537]  [ 40/195]  eta: 0:01:24  lr: 0.000641  loss: 0.2882 (0.2912)  time: 0.5109  data: 0.0002  max mem: 9341
[05:03:34.466487] Epoch: [537]  [ 60/195]  eta: 0:01:12  lr: 0.000641  loss: 0.2950 (0.2928)  time: 0.5109  data: 0.0003  max mem: 9341
[05:03:44.723809] Epoch: [537]  [ 80/195]  eta: 0:01:00  lr: 0.000640  loss: 0.2874 (0.2926)  time: 0.5128  data: 0.0002  max mem: 9341
[05:03:54.937803] Epoch: [537]  [100/195]  eta: 0:00:49  lr: 0.000640  loss: 0.2951 (0.2929)  time: 0.5106  data: 0.0002  max mem: 9341
[05:04:05.149737] Epoch: [537]  [120/195]  eta: 0:00:39  lr: 0.000639  loss: 0.3007 (0.2940)  time: 0.5105  data: 0.0002  max mem: 9341
[05:04:15.364434] Epoch: [537]  [140/195]  eta: 0:00:28  lr: 0.000639  loss: 0.2963 (0.2939)  time: 0.5107  data: 0.0002  max mem: 9341
[05:04:25.619429] Epoch: [537]  [160/195]  eta: 0:00:18  lr: 0.000638  loss: 0.2945 (0.2936)  time: 0.5127  data: 0.0002  max mem: 9341
[05:04:35.790635] Epoch: [537]  [180/195]  eta: 0:00:07  lr: 0.000638  loss: 0.2946 (0.2939)  time: 0.5085  data: 0.0001  max mem: 9341
[05:04:42.913358] Epoch: [537]  [194/195]  eta: 0:00:00  lr: 0.000638  loss: 0.3059 (0.2940)  time: 0.5102  data: 0.0001  max mem: 9341
[05:04:43.091178] Epoch: [537] Total time: 0:01:41 (0.5191 s / it)
[05:04:43.100559] Averaged stats: lr: 0.000638  loss: 0.3059 (0.2948)
[05:04:47.810371] {"train_lr": 0.0006399779307343104, "train_loss": 0.2948435126015773, "epoch": 537}
[05:04:47.810624] [05:04:47.810742] Training epoch 537 for 0:01:45
[05:04:47.810798] [05:04:47.815229] log_dir: ./exp/debug/cifar100-LT/debug
[05:04:49.527896] Epoch: [538]  [  0/195]  eta: 0:05:33  lr: 0.000638  loss: 0.3263 (0.3263)  time: 1.7110  data: 1.1996  max mem: 9341
[05:04:59.748390] Epoch: [538]  [ 20/195]  eta: 0:01:39  lr: 0.000637  loss: 0.2988 (0.3033)  time: 0.5110  data: 0.0003  max mem: 9341
[05:05:09.960662] Epoch: [538]  [ 40/195]  eta: 0:01:23  lr: 0.000637  loss: 0.2929 (0.2983)  time: 0.5106  data: 0.0002  max mem: 9341
[05:05:20.181432] Epoch: [538]  [ 60/195]  eta: 0:01:11  lr: 0.000637  loss: 0.2915 (0.2963)  time: 0.5110  data: 0.0002  max mem: 9341
[05:05:30.438114] Epoch: [538]  [ 80/195]  eta: 0:01:00  lr: 0.000636  loss: 0.2868 (0.2958)  time: 0.5128  data: 0.0003  max mem: 9341
[05:05:40.651681] Epoch: [538]  [100/195]  eta: 0:00:49  lr: 0.000635  loss: 0.2972 (0.2957)  time: 0.5106  data: 0.0004  max mem: 9341
[05:05:50.863526] Epoch: [538]  [120/195]  eta: 0:00:39  lr: 0.000635  loss: 0.3003 (0.2967)  time: 0.5105  data: 0.0003  max mem: 9341
[05:06:01.084327] Epoch: [538]  [140/195]  eta: 0:00:28  lr: 0.000635  loss: 0.2861 (0.2962)  time: 0.5110  data: 0.0002  max mem: 9341
[05:06:11.345938] Epoch: [538]  [160/195]  eta: 0:00:18  lr: 0.000634  loss: 0.2964 (0.2964)  time: 0.5130  data: 0.0002  max mem: 9341
[05:06:21.514119] Epoch: [538]  [180/195]  eta: 0:00:07  lr: 0.000634  loss: 0.2897 (0.2955)  time: 0.5084  data: 0.0002  max mem: 9341
[05:06:28.642426] Epoch: [538]  [194/195]  eta: 0:00:00  lr: 0.000633  loss: 0.2958 (0.2954)  time: 0.5103  data: 0.0001  max mem: 9341
[05:06:28.812139] Epoch: [538] Total time: 0:01:40 (0.5179 s / it)
[05:06:28.836466] Averaged stats: lr: 0.000633  loss: 0.2958 (0.2948)
[05:06:33.557890] {"train_lr": 0.0006355956287827364, "train_loss": 0.2948337263594835, "epoch": 538}
[05:06:33.558229] [05:06:33.558336] Training epoch 538 for 0:01:45
[05:06:33.558391] [05:06:33.562824] log_dir: ./exp/debug/cifar100-LT/debug
[05:06:35.292028] Epoch: [539]  [  0/195]  eta: 0:05:36  lr: 0.000633  loss: 0.2756 (0.2756)  time: 1.7280  data: 1.2205  max mem: 9341
[05:06:45.529013] Epoch: [539]  [ 20/195]  eta: 0:01:39  lr: 0.000633  loss: 0.3045 (0.3038)  time: 0.5118  data: 0.0002  max mem: 9341
[05:06:55.737917] Epoch: [539]  [ 40/195]  eta: 0:01:23  lr: 0.000633  loss: 0.2960 (0.3023)  time: 0.5104  data: 0.0002  max mem: 9341
[05:07:05.953624] Epoch: [539]  [ 60/195]  eta: 0:01:11  lr: 0.000632  loss: 0.2839 (0.2968)  time: 0.5107  data: 0.0002  max mem: 9341
[05:07:16.231559] Epoch: [539]  [ 80/195]  eta: 0:01:00  lr: 0.000631  loss: 0.2926 (0.2968)  time: 0.5138  data: 0.0003  max mem: 9341
[05:07:26.464892] Epoch: [539]  [100/195]  eta: 0:00:49  lr: 0.000631  loss: 0.2789 (0.2946)  time: 0.5116  data: 0.0003  max mem: 9341
[05:07:36.702569] Epoch: [539]  [120/195]  eta: 0:00:39  lr: 0.000631  loss: 0.2922 (0.2945)  time: 0.5118  data: 0.0003  max mem: 9341
[05:07:46.930814] Epoch: [539]  [140/195]  eta: 0:00:28  lr: 0.000630  loss: 0.2916 (0.2941)  time: 0.5114  data: 0.0003  max mem: 9341
[05:07:57.230861] Epoch: [539]  [160/195]  eta: 0:00:18  lr: 0.000630  loss: 0.2941 (0.2940)  time: 0.5149  data: 0.0003  max mem: 9341
[05:08:07.417925] Epoch: [539]  [180/195]  eta: 0:00:07  lr: 0.000629  loss: 0.3015 (0.2947)  time: 0.5093  data: 0.0002  max mem: 9341
[05:08:14.566015] Epoch: [539]  [194/195]  eta: 0:00:00  lr: 0.000629  loss: 0.2971 (0.2945)  time: 0.5123  data: 0.0001  max mem: 9341
[05:08:14.741151] Epoch: [539] Total time: 0:01:41 (0.5189 s / it)
[05:08:14.752275] Averaged stats: lr: 0.000629  loss: 0.2971 (0.2939)
[05:08:19.511520] {"train_lr": 0.0006312229709446252, "train_loss": 0.29394969414824096, "epoch": 539}
[05:08:19.511877] [05:08:19.511978] Training epoch 539 for 0:01:45
[05:08:19.512033] [05:08:19.516471] log_dir: ./exp/debug/cifar100-LT/debug
[05:08:21.465072] Epoch: [540]  [  0/195]  eta: 0:06:19  lr: 0.000629  loss: 0.2976 (0.2976)  time: 1.9469  data: 1.4482  max mem: 9341
[05:08:31.695497] Epoch: [540]  [ 20/195]  eta: 0:01:41  lr: 0.000629  loss: 0.2868 (0.2917)  time: 0.5115  data: 0.0002  max mem: 9341
[05:08:41.924483] Epoch: [540]  [ 40/195]  eta: 0:01:24  lr: 0.000628  loss: 0.2811 (0.2886)  time: 0.5114  data: 0.0003  max mem: 9341
[05:08:52.163632] Epoch: [540]  [ 60/195]  eta: 0:01:12  lr: 0.000628  loss: 0.2894 (0.2925)  time: 0.5119  data: 0.0003  max mem: 9341
[05:09:02.424930] Epoch: [540]  [ 80/195]  eta: 0:01:00  lr: 0.000627  loss: 0.3036 (0.2941)  time: 0.5130  data: 0.0003  max mem: 9341
[05:09:12.642468] Epoch: [540]  [100/195]  eta: 0:00:49  lr: 0.000627  loss: 0.2909 (0.2938)  time: 0.5108  data: 0.0002  max mem: 9341
[05:09:22.856766] Epoch: [540]  [120/195]  eta: 0:00:39  lr: 0.000626  loss: 0.2961 (0.2945)  time: 0.5107  data: 0.0003  max mem: 9341
[05:09:33.066658] Epoch: [540]  [140/195]  eta: 0:00:28  lr: 0.000626  loss: 0.2748 (0.2930)  time: 0.5104  data: 0.0003  max mem: 9341
[05:09:43.323684] Epoch: [540]  [160/195]  eta: 0:00:18  lr: 0.000625  loss: 0.2896 (0.2931)  time: 0.5128  data: 0.0002  max mem: 9341
[05:09:53.497821] Epoch: [540]  [180/195]  eta: 0:00:07  lr: 0.000625  loss: 0.2911 (0.2932)  time: 0.5087  data: 0.0002  max mem: 9341
[05:10:00.632163] Epoch: [540]  [194/195]  eta: 0:00:00  lr: 0.000625  loss: 0.3013 (0.2939)  time: 0.5108  data: 0.0001  max mem: 9341
[05:10:00.799669] Epoch: [540] Total time: 0:01:41 (0.5194 s / it)
[05:10:00.825015] Averaged stats: lr: 0.000625  loss: 0.3013 (0.2930)
[05:10:05.565139] {"train_lr": 0.0006268600319366329, "train_loss": 0.29299288052014816, "epoch": 540}
[05:10:05.565409] [05:10:05.565515] Training epoch 540 for 0:01:46
[05:10:05.565580] [05:10:05.570045] log_dir: ./exp/debug/cifar100-LT/debug
[05:10:07.115633] Epoch: [541]  [  0/195]  eta: 0:05:01  lr: 0.000625  loss: 0.2908 (0.2908)  time: 1.5446  data: 1.0443  max mem: 9341
[05:10:17.353660] Epoch: [541]  [ 20/195]  eta: 0:01:38  lr: 0.000624  loss: 0.2916 (0.2939)  time: 0.5118  data: 0.0003  max mem: 9341
[05:10:27.562486] Epoch: [541]  [ 40/195]  eta: 0:01:23  lr: 0.000624  loss: 0.2902 (0.2937)  time: 0.5104  data: 0.0003  max mem: 9341
[05:10:37.783891] Epoch: [541]  [ 60/195]  eta: 0:01:11  lr: 0.000623  loss: 0.2913 (0.2929)  time: 0.5110  data: 0.0003  max mem: 9341
[05:10:48.052118] Epoch: [541]  [ 80/195]  eta: 0:01:00  lr: 0.000623  loss: 0.2941 (0.2935)  time: 0.5134  data: 0.0003  max mem: 9341
[05:10:58.268697] Epoch: [541]  [100/195]  eta: 0:00:49  lr: 0.000622  loss: 0.2923 (0.2940)  time: 0.5108  data: 0.0002  max mem: 9341
[05:11:08.482592] Epoch: [541]  [120/195]  eta: 0:00:38  lr: 0.000622  loss: 0.2957 (0.2950)  time: 0.5106  data: 0.0003  max mem: 9341
[05:11:18.696054] Epoch: [541]  [140/195]  eta: 0:00:28  lr: 0.000622  loss: 0.2895 (0.2946)  time: 0.5106  data: 0.0002  max mem: 9341
[05:11:28.949385] Epoch: [541]  [160/195]  eta: 0:00:18  lr: 0.000621  loss: 0.2921 (0.2943)  time: 0.5126  data: 0.0002  max mem: 9341
[05:11:39.120210] Epoch: [541]  [180/195]  eta: 0:00:07  lr: 0.000621  loss: 0.2991 (0.2944)  time: 0.5085  data: 0.0002  max mem: 9341
[05:11:46.252989] Epoch: [541]  [194/195]  eta: 0:00:00  lr: 0.000620  loss: 0.3021 (0.2951)  time: 0.5105  data: 0.0001  max mem: 9341
[05:11:46.441161] Epoch: [541] Total time: 0:01:40 (0.5173 s / it)
[05:11:46.449480] Averaged stats: lr: 0.000620  loss: 0.3021 (0.2933)
[05:11:51.085259] {"train_lr": 0.0006225068863093447, "train_loss": 0.2933488002763345, "epoch": 541}
[05:11:51.085512] [05:11:51.085616] Training epoch 541 for 0:01:45
[05:11:51.085671] [05:11:51.090132] log_dir: ./exp/debug/cifar100-LT/debug
[05:11:53.077135] Epoch: [542]  [  0/195]  eta: 0:06:27  lr: 0.000620  loss: 0.3281 (0.3281)  time: 1.9857  data: 1.4839  max mem: 9341
[05:12:03.280124] Epoch: [542]  [ 20/195]  eta: 0:01:41  lr: 0.000620  loss: 0.2912 (0.2912)  time: 0.5101  data: 0.0003  max mem: 9341
[05:12:13.486512] Epoch: [542]  [ 40/195]  eta: 0:01:24  lr: 0.000619  loss: 0.2909 (0.2940)  time: 0.5103  data: 0.0003  max mem: 9341
[05:12:23.702458] Epoch: [542]  [ 60/195]  eta: 0:01:12  lr: 0.000619  loss: 0.2898 (0.2930)  time: 0.5107  data: 0.0003  max mem: 9341
[05:12:33.957965] Epoch: [542]  [ 80/195]  eta: 0:01:00  lr: 0.000618  loss: 0.2850 (0.2918)  time: 0.5127  data: 0.0003  max mem: 9341
[05:12:44.171866] Epoch: [542]  [100/195]  eta: 0:00:49  lr: 0.000618  loss: 0.2902 (0.2916)  time: 0.5106  data: 0.0003  max mem: 9341
[05:12:54.385024] Epoch: [542]  [120/195]  eta: 0:00:39  lr: 0.000618  loss: 0.2967 (0.2927)  time: 0.5106  data: 0.0003  max mem: 9341
[05:13:04.594988] Epoch: [542]  [140/195]  eta: 0:00:28  lr: 0.000617  loss: 0.2813 (0.2915)  time: 0.5104  data: 0.0003  max mem: 9341
[05:13:14.853202] Epoch: [542]  [160/195]  eta: 0:00:18  lr: 0.000617  loss: 0.2830 (0.2911)  time: 0.5129  data: 0.0003  max mem: 9341
[05:13:25.032664] Epoch: [542]  [180/195]  eta: 0:00:07  lr: 0.000616  loss: 0.2919 (0.2913)  time: 0.5089  data: 0.0002  max mem: 9341
[05:13:32.170562] Epoch: [542]  [194/195]  eta: 0:00:00  lr: 0.000616  loss: 0.2974 (0.2918)  time: 0.5112  data: 0.0001  max mem: 9341
[05:13:32.366016] Epoch: [542] Total time: 0:01:41 (0.5194 s / it)
[05:13:32.374836] Averaged stats: lr: 0.000616  loss: 0.2974 (0.2930)
[05:13:37.002934] {"train_lr": 0.0006181636084460159, "train_loss": 0.29303097455547406, "epoch": 542}
[05:13:37.003177] [05:13:37.003291] Training epoch 542 for 0:01:45
[05:13:37.003360] [05:13:37.007833] log_dir: ./exp/debug/cifar100-LT/debug
[05:13:38.651875] Epoch: [543]  [  0/195]  eta: 0:05:20  lr: 0.000616  loss: 0.3075 (0.3075)  time: 1.6421  data: 1.1224  max mem: 9341
[05:13:48.871934] Epoch: [543]  [ 20/195]  eta: 0:01:38  lr: 0.000615  loss: 0.2813 (0.2895)  time: 0.5109  data: 0.0002  max mem: 9341
[05:13:59.093015] Epoch: [543]  [ 40/195]  eta: 0:01:23  lr: 0.000615  loss: 0.2922 (0.2913)  time: 0.5110  data: 0.0003  max mem: 9341
[05:14:09.307957] Epoch: [543]  [ 60/195]  eta: 0:01:11  lr: 0.000615  loss: 0.2790 (0.2880)  time: 0.5107  data: 0.0003  max mem: 9341
[05:14:19.565280] Epoch: [543]  [ 80/195]  eta: 0:01:00  lr: 0.000614  loss: 0.2970 (0.2902)  time: 0.5128  data: 0.0002  max mem: 9341
[05:14:29.784533] Epoch: [543]  [100/195]  eta: 0:00:49  lr: 0.000614  loss: 0.2814 (0.2890)  time: 0.5109  data: 0.0002  max mem: 9341
[05:14:40.009919] Epoch: [543]  [120/195]  eta: 0:00:39  lr: 0.000613  loss: 0.2940 (0.2900)  time: 0.5112  data: 0.0002  max mem: 9341
[05:14:50.229966] Epoch: [543]  [140/195]  eta: 0:00:28  lr: 0.000613  loss: 0.2832 (0.2898)  time: 0.5109  data: 0.0003  max mem: 9341
[05:15:00.493027] Epoch: [543]  [160/195]  eta: 0:00:18  lr: 0.000612  loss: 0.2928 (0.2901)  time: 0.5131  data: 0.0003  max mem: 9341
[05:15:10.662351] Epoch: [543]  [180/195]  eta: 0:00:07  lr: 0.000612  loss: 0.2901 (0.2900)  time: 0.5084  data: 0.0002  max mem: 9341
[05:15:17.793130] Epoch: [543]  [194/195]  eta: 0:00:00  lr: 0.000612  loss: 0.2926 (0.2903)  time: 0.5105  data: 0.0001  max mem: 9341
[05:15:17.954047] Epoch: [543] Total time: 0:01:40 (0.5177 s / it)
[05:15:17.968975] Averaged stats: lr: 0.000612  loss: 0.2926 (0.2914)
[05:15:22.724287] {"train_lr": 0.0006138302725612738, "train_loss": 0.2914247549306124, "epoch": 543}
[05:15:22.724529] [05:15:22.724634] Training epoch 543 for 0:01:45
[05:15:22.724705] [05:15:22.729113] log_dir: ./exp/debug/cifar100-LT/debug
[05:15:24.630586] Epoch: [544]  [  0/195]  eta: 0:06:10  lr: 0.000611  loss: 0.2843 (0.2843)  time: 1.9008  data: 1.3065  max mem: 9341
[05:15:34.848255] Epoch: [544]  [ 20/195]  eta: 0:01:40  lr: 0.000611  loss: 0.2805 (0.2812)  time: 0.5108  data: 0.0002  max mem: 9341
[05:15:45.089633] Epoch: [544]  [ 40/195]  eta: 0:01:24  lr: 0.000611  loss: 0.2931 (0.2861)  time: 0.5120  data: 0.0002  max mem: 9341
[05:15:55.321688] Epoch: [544]  [ 60/195]  eta: 0:01:12  lr: 0.000610  loss: 0.2933 (0.2876)  time: 0.5115  data: 0.0002  max mem: 9341
[05:16:05.624427] Epoch: [544]  [ 80/195]  eta: 0:01:00  lr: 0.000610  loss: 0.2909 (0.2876)  time: 0.5151  data: 0.0003  max mem: 9341
[05:16:15.859263] Epoch: [544]  [100/195]  eta: 0:00:49  lr: 0.000609  loss: 0.2860 (0.2886)  time: 0.5117  data: 0.0002  max mem: 9341
[05:16:26.070731] Epoch: [544]  [120/195]  eta: 0:00:39  lr: 0.000609  loss: 0.2862 (0.2890)  time: 0.5105  data: 0.0002  max mem: 9341
[05:16:36.280746] Epoch: [544]  [140/195]  eta: 0:00:28  lr: 0.000609  loss: 0.3008 (0.2906)  time: 0.5104  data: 0.0002  max mem: 9341
[05:16:46.539340] Epoch: [544]  [160/195]  eta: 0:00:18  lr: 0.000608  loss: 0.2971 (0.2905)  time: 0.5129  data: 0.0002  max mem: 9341
[05:16:56.710541] Epoch: [544]  [180/195]  eta: 0:00:07  lr: 0.000608  loss: 0.2946 (0.2907)  time: 0.5085  data: 0.0002  max mem: 9341
[05:17:03.840976] Epoch: [544]  [194/195]  eta: 0:00:00  lr: 0.000607  loss: 0.2895 (0.2907)  time: 0.5105  data: 0.0001  max mem: 9341
[05:17:04.026531] Epoch: [544] Total time: 0:01:41 (0.5195 s / it)
[05:17:04.034128] Averaged stats: lr: 0.000607  loss: 0.2895 (0.2918)
[05:17:08.716499] {"train_lr": 0.000609506952699878, "train_loss": 0.29184682917518495, "epoch": 544}
[05:17:08.716784] [05:17:08.716883] Training epoch 544 for 0:01:45
[05:17:08.716938] [05:17:08.721359] log_dir: ./exp/debug/cifar100-LT/debug
[05:17:10.556503] Epoch: [545]  [  0/195]  eta: 0:05:57  lr: 0.000607  loss: 0.3044 (0.3044)  time: 1.8341  data: 1.3335  max mem: 9341
[05:17:20.794907] Epoch: [545]  [ 20/195]  eta: 0:01:40  lr: 0.000607  loss: 0.2825 (0.2931)  time: 0.5118  data: 0.0002  max mem: 9341
[05:17:31.061496] Epoch: [545]  [ 40/195]  eta: 0:01:24  lr: 0.000606  loss: 0.2865 (0.2932)  time: 0.5133  data: 0.0002  max mem: 9341
[05:17:41.265405] Epoch: [545]  [ 60/195]  eta: 0:01:12  lr: 0.000606  loss: 0.2893 (0.2922)  time: 0.5101  data: 0.0002  max mem: 9341
[05:17:51.525961] Epoch: [545]  [ 80/195]  eta: 0:01:00  lr: 0.000605  loss: 0.2902 (0.2927)  time: 0.5130  data: 0.0002  max mem: 9341
[05:18:01.746636] Epoch: [545]  [100/195]  eta: 0:00:49  lr: 0.000605  loss: 0.2997 (0.2944)  time: 0.5110  data: 0.0002  max mem: 9341
[05:18:11.962522] Epoch: [545]  [120/195]  eta: 0:00:39  lr: 0.000605  loss: 0.2908 (0.2939)  time: 0.5107  data: 0.0002  max mem: 9341
[05:18:22.180421] Epoch: [545]  [140/195]  eta: 0:00:28  lr: 0.000604  loss: 0.2903 (0.2941)  time: 0.5108  data: 0.0002  max mem: 9341
[05:18:32.447448] Epoch: [545]  [160/195]  eta: 0:00:18  lr: 0.000604  loss: 0.2858 (0.2936)  time: 0.5133  data: 0.0002  max mem: 9341
[05:18:42.619023] Epoch: [545]  [180/195]  eta: 0:00:07  lr: 0.000603  loss: 0.2815 (0.2928)  time: 0.5085  data: 0.0001  max mem: 9341
[05:18:49.751755] Epoch: [545]  [194/195]  eta: 0:00:00  lr: 0.000603  loss: 0.2918 (0.2926)  time: 0.5107  data: 0.0001  max mem: 9341
[05:18:49.925986] Epoch: [545] Total time: 0:01:41 (0.5190 s / it)
[05:18:49.942426] Averaged stats: lr: 0.000603  loss: 0.2918 (0.2935)
[05:18:54.553059] {"train_lr": 0.0006051937227354345, "train_loss": 0.2935212726203295, "epoch": 545}
[05:18:54.553320] [05:18:54.553406] Training epoch 545 for 0:01:45
[05:18:54.553459] [05:18:54.557904] log_dir: ./exp/debug/cifar100-LT/debug
[05:18:56.412087] Epoch: [546]  [  0/195]  eta: 0:06:01  lr: 0.000603  loss: 0.2911 (0.2911)  time: 1.8533  data: 1.3552  max mem: 9341
[05:19:06.624952] Epoch: [546]  [ 20/195]  eta: 0:01:40  lr: 0.000603  loss: 0.2912 (0.2971)  time: 0.5106  data: 0.0002  max mem: 9341
[05:19:16.843248] Epoch: [546]  [ 40/195]  eta: 0:01:24  lr: 0.000602  loss: 0.2997 (0.2964)  time: 0.5109  data: 0.0002  max mem: 9341
[05:19:27.060295] Epoch: [546]  [ 60/195]  eta: 0:01:11  lr: 0.000602  loss: 0.3014 (0.2987)  time: 0.5108  data: 0.0002  max mem: 9341
[05:19:37.328056] Epoch: [546]  [ 80/195]  eta: 0:01:00  lr: 0.000601  loss: 0.2938 (0.2978)  time: 0.5133  data: 0.0002  max mem: 9341
[05:19:47.546851] Epoch: [546]  [100/195]  eta: 0:00:49  lr: 0.000601  loss: 0.2869 (0.2958)  time: 0.5109  data: 0.0002  max mem: 9341
[05:19:57.767776] Epoch: [546]  [120/195]  eta: 0:00:39  lr: 0.000600  loss: 0.2942 (0.2955)  time: 0.5110  data: 0.0002  max mem: 9341
[05:20:07.975430] Epoch: [546]  [140/195]  eta: 0:00:28  lr: 0.000600  loss: 0.2896 (0.2952)  time: 0.5103  data: 0.0002  max mem: 9341
[05:20:18.239490] Epoch: [546]  [160/195]  eta: 0:00:18  lr: 0.000599  loss: 0.2915 (0.2948)  time: 0.5131  data: 0.0002  max mem: 9341
[05:20:28.405469] Epoch: [546]  [180/195]  eta: 0:00:07  lr: 0.000599  loss: 0.2832 (0.2940)  time: 0.5082  data: 0.0001  max mem: 9341
[05:20:35.533051] Epoch: [546]  [194/195]  eta: 0:00:00  lr: 0.000599  loss: 0.3024 (0.2949)  time: 0.5103  data: 0.0001  max mem: 9341
[05:20:35.716982] Epoch: [546] Total time: 0:01:41 (0.5188 s / it)
[05:20:35.726059] Averaged stats: lr: 0.000599  loss: 0.3024 (0.2951)
[05:20:40.463975] {"train_lr": 0.0006008906563691421, "train_loss": 0.29506064382119057, "epoch": 546}
[05:20:40.464268] [05:20:40.464356] Training epoch 546 for 0:01:45
[05:20:40.464411] [05:20:40.468860] log_dir: ./exp/debug/cifar100-LT/debug
[05:20:42.329621] Epoch: [547]  [  0/195]  eta: 0:06:02  lr: 0.000599  loss: 0.2865 (0.2865)  time: 1.8595  data: 1.3341  max mem: 9341
[05:20:52.589593] Epoch: [547]  [ 20/195]  eta: 0:01:40  lr: 0.000598  loss: 0.2860 (0.2940)  time: 0.5129  data: 0.0002  max mem: 9341
[05:21:02.815673] Epoch: [547]  [ 40/195]  eta: 0:01:24  lr: 0.000598  loss: 0.2829 (0.2932)  time: 0.5112  data: 0.0003  max mem: 9341
[05:21:13.024280] Epoch: [547]  [ 60/195]  eta: 0:01:12  lr: 0.000598  loss: 0.2960 (0.2943)  time: 0.5104  data: 0.0003  max mem: 9341
[05:21:23.288404] Epoch: [547]  [ 80/195]  eta: 0:01:00  lr: 0.000597  loss: 0.2898 (0.2934)  time: 0.5131  data: 0.0003  max mem: 9341
[05:21:33.506928] Epoch: [547]  [100/195]  eta: 0:00:49  lr: 0.000596  loss: 0.2842 (0.2919)  time: 0.5109  data: 0.0003  max mem: 9341
[05:21:43.722134] Epoch: [547]  [120/195]  eta: 0:00:39  lr: 0.000596  loss: 0.2946 (0.2924)  time: 0.5107  data: 0.0003  max mem: 9341
[05:21:53.946129] Epoch: [547]  [140/195]  eta: 0:00:28  lr: 0.000596  loss: 0.2873 (0.2922)  time: 0.5111  data: 0.0003  max mem: 9341
[05:22:04.203510] Epoch: [547]  [160/195]  eta: 0:00:18  lr: 0.000595  loss: 0.2790 (0.2910)  time: 0.5128  data: 0.0002  max mem: 9341
[05:22:14.378023] Epoch: [547]  [180/195]  eta: 0:00:07  lr: 0.000595  loss: 0.2886 (0.2911)  time: 0.5087  data: 0.0002  max mem: 9341
[05:22:21.512044] Epoch: [547]  [194/195]  eta: 0:00:00  lr: 0.000594  loss: 0.2879 (0.2907)  time: 0.5105  data: 0.0001  max mem: 9341
[05:22:21.687634] Epoch: [547] Total time: 0:01:41 (0.5191 s / it)
[05:22:21.692363] Averaged stats: lr: 0.000594  loss: 0.2879 (0.2926)
[05:22:26.369618] {"train_lr": 0.0005965978271285327, "train_loss": 0.29262058643194344, "epoch": 547}
[05:22:26.369914] [05:22:26.370011] Training epoch 547 for 0:01:45
[05:22:26.370134] [05:22:26.374600] log_dir: ./exp/debug/cifar100-LT/debug
[05:22:28.177385] Epoch: [548]  [  0/195]  eta: 0:05:51  lr: 0.000594  loss: 0.3133 (0.3133)  time: 1.8012  data: 1.3083  max mem: 9341
[05:22:38.387182] Epoch: [548]  [ 20/195]  eta: 0:01:40  lr: 0.000594  loss: 0.2879 (0.2956)  time: 0.5104  data: 0.0002  max mem: 9341
[05:22:48.593020] Epoch: [548]  [ 40/195]  eta: 0:01:23  lr: 0.000594  loss: 0.3006 (0.2971)  time: 0.5102  data: 0.0002  max mem: 9341
[05:22:58.805672] Epoch: [548]  [ 60/195]  eta: 0:01:11  lr: 0.000593  loss: 0.2915 (0.2943)  time: 0.5106  data: 0.0002  max mem: 9341
[05:23:09.063052] Epoch: [548]  [ 80/195]  eta: 0:01:00  lr: 0.000593  loss: 0.2913 (0.2952)  time: 0.5128  data: 0.0002  max mem: 9341
[05:23:19.278470] Epoch: [548]  [100/195]  eta: 0:00:49  lr: 0.000592  loss: 0.2942 (0.2944)  time: 0.5107  data: 0.0002  max mem: 9341
[05:23:29.494877] Epoch: [548]  [120/195]  eta: 0:00:39  lr: 0.000592  loss: 0.2980 (0.2950)  time: 0.5108  data: 0.0002  max mem: 9341
[05:23:39.736249] Epoch: [548]  [140/195]  eta: 0:00:28  lr: 0.000591  loss: 0.2866 (0.2940)  time: 0.5120  data: 0.0002  max mem: 9341
[05:23:49.995920] Epoch: [548]  [160/195]  eta: 0:00:18  lr: 0.000591  loss: 0.3032 (0.2946)  time: 0.5129  data: 0.0002  max mem: 9341
[05:24:00.169777] Epoch: [548]  [180/195]  eta: 0:00:07  lr: 0.000590  loss: 0.2941 (0.2943)  time: 0.5086  data: 0.0001  max mem: 9341
[05:24:07.298987] Epoch: [548]  [194/195]  eta: 0:00:00  lr: 0.000590  loss: 0.3009 (0.2945)  time: 0.5106  data: 0.0001  max mem: 9341
[05:24:07.467313] Epoch: [548] Total time: 0:01:41 (0.5184 s / it)
[05:24:07.488522] Averaged stats: lr: 0.000590  loss: 0.3009 (0.2924)
[05:24:12.245289] {"train_lr": 0.0005923153083662172, "train_loss": 0.29243003149063157, "epoch": 548}
[05:24:12.245647] [05:24:12.245733] Training epoch 548 for 0:01:45
[05:24:12.245786] [05:24:12.250226] log_dir: ./exp/debug/cifar100-LT/debug
[05:24:13.854845] Epoch: [549]  [  0/195]  eta: 0:05:12  lr: 0.000590  loss: 0.2693 (0.2693)  time: 1.6031  data: 1.1142  max mem: 9341
[05:24:24.065901] Epoch: [549]  [ 20/195]  eta: 0:01:38  lr: 0.000590  loss: 0.2964 (0.2938)  time: 0.5105  data: 0.0002  max mem: 9341
[05:24:34.286833] Epoch: [549]  [ 40/195]  eta: 0:01:23  lr: 0.000589  loss: 0.3001 (0.2976)  time: 0.5110  data: 0.0002  max mem: 9341
[05:24:44.505312] Epoch: [549]  [ 60/195]  eta: 0:01:11  lr: 0.000589  loss: 0.2957 (0.2968)  time: 0.5108  data: 0.0002  max mem: 9341
[05:24:54.763837] Epoch: [549]  [ 80/195]  eta: 0:01:00  lr: 0.000588  loss: 0.2907 (0.2956)  time: 0.5129  data: 0.0003  max mem: 9341
[05:25:04.970946] Epoch: [549]  [100/195]  eta: 0:00:49  lr: 0.000588  loss: 0.3071 (0.2970)  time: 0.5103  data: 0.0002  max mem: 9341
[05:25:15.180488] Epoch: [549]  [120/195]  eta: 0:00:39  lr: 0.000588  loss: 0.2919 (0.2975)  time: 0.5104  data: 0.0003  max mem: 9341
[05:25:25.397062] Epoch: [549]  [140/195]  eta: 0:00:28  lr: 0.000587  loss: 0.2844 (0.2966)  time: 0.5108  data: 0.0003  max mem: 9341
[05:25:35.658807] Epoch: [549]  [160/195]  eta: 0:00:18  lr: 0.000587  loss: 0.2877 (0.2960)  time: 0.5130  data: 0.0003  max mem: 9341
[05:25:45.826026] Epoch: [549]  [180/195]  eta: 0:00:07  lr: 0.000586  loss: 0.2858 (0.2951)  time: 0.5083  data: 0.0002  max mem: 9341
[05:25:52.955378] Epoch: [549]  [194/195]  eta: 0:00:00  lr: 0.000586  loss: 0.2863 (0.2948)  time: 0.5105  data: 0.0001  max mem: 9341
[05:25:53.137751] Epoch: [549] Total time: 0:01:40 (0.5174 s / it)
[05:25:53.153128] Averaged stats: lr: 0.000586  loss: 0.2863 (0.2919)
[05:25:57.827335] {"train_lr": 0.0005880431732586213, "train_loss": 0.2919135433550064, "epoch": 549}
[05:25:57.827597] [05:25:57.827703] Training epoch 549 for 0:01:45
[05:25:57.827757] [05:25:57.832197] log_dir: ./exp/debug/cifar100-LT/debug
[05:25:59.660882] Epoch: [550]  [  0/195]  eta: 0:05:56  lr: 0.000586  loss: 0.3056 (0.3056)  time: 1.8273  data: 1.3218  max mem: 9341
[05:26:09.876166] Epoch: [550]  [ 20/195]  eta: 0:01:40  lr: 0.000585  loss: 0.2734 (0.2859)  time: 0.5107  data: 0.0002  max mem: 9341
[05:26:20.094877] Epoch: [550]  [ 40/195]  eta: 0:01:24  lr: 0.000585  loss: 0.2878 (0.2887)  time: 0.5109  data: 0.0002  max mem: 9341
[05:26:30.312170] Epoch: [550]  [ 60/195]  eta: 0:01:11  lr: 0.000585  loss: 0.2904 (0.2890)  time: 0.5108  data: 0.0002  max mem: 9341
[05:26:40.570665] Epoch: [550]  [ 80/195]  eta: 0:01:00  lr: 0.000584  loss: 0.2853 (0.2885)  time: 0.5129  data: 0.0002  max mem: 9341
[05:26:50.778756] Epoch: [550]  [100/195]  eta: 0:00:49  lr: 0.000584  loss: 0.2894 (0.2891)  time: 0.5104  data: 0.0002  max mem: 9341
[05:27:00.998089] Epoch: [550]  [120/195]  eta: 0:00:39  lr: 0.000583  loss: 0.2858 (0.2884)  time: 0.5109  data: 0.0002  max mem: 9341
[05:27:11.212571] Epoch: [550]  [140/195]  eta: 0:00:28  lr: 0.000583  loss: 0.2805 (0.2885)  time: 0.5107  data: 0.0002  max mem: 9341
[05:27:21.468224] Epoch: [550]  [160/195]  eta: 0:00:18  lr: 0.000582  loss: 0.2815 (0.2886)  time: 0.5127  data: 0.0002  max mem: 9341
[05:27:31.635179] Epoch: [550]  [180/195]  eta: 0:00:07  lr: 0.000582  loss: 0.2849 (0.2886)  time: 0.5083  data: 0.0001  max mem: 9341
[05:27:38.771176] Epoch: [550]  [194/195]  eta: 0:00:00  lr: 0.000582  loss: 0.2843 (0.2886)  time: 0.5106  data: 0.0001  max mem: 9341
[05:27:38.940562] Epoch: [550] Total time: 0:01:41 (0.5185 s / it)
[05:27:38.952053] Averaged stats: lr: 0.000582  loss: 0.2843 (0.2899)
[05:27:43.659499] {"train_lr": 0.0005837814948047479, "train_loss": 0.2899454932373304, "epoch": 550}
[05:27:43.659762] [05:27:43.659848] Training epoch 550 for 0:01:45
[05:27:43.659900] [05:27:43.664328] log_dir: ./exp/debug/cifar100-LT/debug
[05:27:45.512334] Epoch: [551]  [  0/195]  eta: 0:06:00  lr: 0.000581  loss: 0.2768 (0.2768)  time: 1.8465  data: 1.3490  max mem: 9341
[05:27:55.728384] Epoch: [551]  [ 20/195]  eta: 0:01:40  lr: 0.000581  loss: 0.2932 (0.2937)  time: 0.5107  data: 0.0002  max mem: 9341
[05:28:05.973814] Epoch: [551]  [ 40/195]  eta: 0:01:24  lr: 0.000581  loss: 0.2907 (0.2933)  time: 0.5122  data: 0.0003  max mem: 9341
[05:28:16.184350] Epoch: [551]  [ 60/195]  eta: 0:01:11  lr: 0.000580  loss: 0.2766 (0.2901)  time: 0.5105  data: 0.0003  max mem: 9341
[05:28:26.449849] Epoch: [551]  [ 80/195]  eta: 0:01:00  lr: 0.000580  loss: 0.2919 (0.2900)  time: 0.5132  data: 0.0003  max mem: 9341
[05:28:36.667154] Epoch: [551]  [100/195]  eta: 0:00:49  lr: 0.000579  loss: 0.2911 (0.2913)  time: 0.5108  data: 0.0003  max mem: 9341
[05:28:46.885252] Epoch: [551]  [120/195]  eta: 0:00:39  lr: 0.000579  loss: 0.2951 (0.2917)  time: 0.5108  data: 0.0002  max mem: 9341
[05:28:57.103769] Epoch: [551]  [140/195]  eta: 0:00:28  lr: 0.000579  loss: 0.2776 (0.2910)  time: 0.5109  data: 0.0003  max mem: 9341
[05:29:07.358102] Epoch: [551]  [160/195]  eta: 0:00:18  lr: 0.000578  loss: 0.2853 (0.2906)  time: 0.5127  data: 0.0003  max mem: 9341
[05:29:17.524061] Epoch: [551]  [180/195]  eta: 0:00:07  lr: 0.000578  loss: 0.2931 (0.2912)  time: 0.5082  data: 0.0002  max mem: 9341
[05:29:24.659805] Epoch: [551]  [194/195]  eta: 0:00:00  lr: 0.000577  loss: 0.2846 (0.2911)  time: 0.5107  data: 0.0001  max mem: 9341
[05:29:24.843811] Epoch: [551] Total time: 0:01:41 (0.5189 s / it)
[05:29:24.864013] Averaged stats: lr: 0.000577  loss: 0.2846 (0.2913)
[05:29:29.590398] {"train_lr": 0.000579530345824926, "train_loss": 0.29125326617788044, "epoch": 551}
[05:29:29.590733] [05:29:29.590841] Training epoch 551 for 0:01:45
[05:29:29.590895] [05:29:29.595352] log_dir: ./exp/debug/cifar100-LT/debug
[05:29:31.456693] Epoch: [552]  [  0/195]  eta: 0:06:02  lr: 0.000577  loss: 0.2963 (0.2963)  time: 1.8600  data: 1.3494  max mem: 9341
[05:29:41.690573] Epoch: [552]  [ 20/195]  eta: 0:01:40  lr: 0.000577  loss: 0.2949 (0.2926)  time: 0.5116  data: 0.0002  max mem: 9341
[05:29:51.908582] Epoch: [552]  [ 40/195]  eta: 0:01:24  lr: 0.000577  loss: 0.2884 (0.2921)  time: 0.5108  data: 0.0002  max mem: 9341
[05:30:02.129615] Epoch: [552]  [ 60/195]  eta: 0:01:11  lr: 0.000576  loss: 0.2936 (0.2943)  time: 0.5110  data: 0.0003  max mem: 9341
[05:30:12.398508] Epoch: [552]  [ 80/195]  eta: 0:01:00  lr: 0.000575  loss: 0.2941 (0.2939)  time: 0.5134  data: 0.0003  max mem: 9341
[05:30:22.618456] Epoch: [552]  [100/195]  eta: 0:00:49  lr: 0.000575  loss: 0.2865 (0.2928)  time: 0.5109  data: 0.0003  max mem: 9341
[05:30:32.828374] Epoch: [552]  [120/195]  eta: 0:00:39  lr: 0.000575  loss: 0.2981 (0.2938)  time: 0.5104  data: 0.0003  max mem: 9341
[05:30:43.044010] Epoch: [552]  [140/195]  eta: 0:00:28  lr: 0.000574  loss: 0.2894 (0.2934)  time: 0.5107  data: 0.0003  max mem: 9341
[05:30:53.302850] Epoch: [552]  [160/195]  eta: 0:00:18  lr: 0.000574  loss: 0.2902 (0.2924)  time: 0.5129  data: 0.0002  max mem: 9341
[05:31:03.474737] Epoch: [552]  [180/195]  eta: 0:00:07  lr: 0.000573  loss: 0.2916 (0.2925)  time: 0.5085  data: 0.0002  max mem: 9341
[05:31:10.610574] Epoch: [552]  [194/195]  eta: 0:00:00  lr: 0.000573  loss: 0.2983 (0.2929)  time: 0.5108  data: 0.0001  max mem: 9341
[05:31:10.796449] Epoch: [552] Total time: 0:01:41 (0.5190 s / it)
[05:31:10.801821] Averaged stats: lr: 0.000573  loss: 0.2983 (0.2936)
[05:31:15.565230] {"train_lr": 0.0005752897989595565, "train_loss": 0.2936000310839751, "epoch": 552}
[05:31:15.565502] [05:31:15.565607] Training epoch 552 for 0:01:45
[05:31:15.565662] [05:31:15.570103] log_dir: ./exp/debug/cifar100-LT/debug
[05:31:17.334499] Epoch: [553]  [  0/195]  eta: 0:05:43  lr: 0.000573  loss: 0.2582 (0.2582)  time: 1.7630  data: 1.2555  max mem: 9341
[05:31:27.550852] Epoch: [553]  [ 20/195]  eta: 0:01:39  lr: 0.000573  loss: 0.2699 (0.2802)  time: 0.5108  data: 0.0002  max mem: 9341
[05:31:37.763978] Epoch: [553]  [ 40/195]  eta: 0:01:23  lr: 0.000572  loss: 0.2924 (0.2855)  time: 0.5106  data: 0.0002  max mem: 9341
[05:31:47.984355] Epoch: [553]  [ 60/195]  eta: 0:01:11  lr: 0.000572  loss: 0.2880 (0.2870)  time: 0.5109  data: 0.0003  max mem: 9341
[05:31:58.243704] Epoch: [553]  [ 80/195]  eta: 0:01:00  lr: 0.000571  loss: 0.2926 (0.2895)  time: 0.5129  data: 0.0003  max mem: 9341
[05:32:08.462960] Epoch: [553]  [100/195]  eta: 0:00:49  lr: 0.000571  loss: 0.2894 (0.2903)  time: 0.5109  data: 0.0002  max mem: 9341
[05:32:18.678191] Epoch: [553]  [120/195]  eta: 0:00:39  lr: 0.000571  loss: 0.2850 (0.2895)  time: 0.5107  data: 0.0003  max mem: 9341
[05:32:28.888830] Epoch: [553]  [140/195]  eta: 0:00:28  lr: 0.000570  loss: 0.2848 (0.2893)  time: 0.5105  data: 0.0003  max mem: 9341
[05:32:39.144935] Epoch: [553]  [160/195]  eta: 0:00:18  lr: 0.000570  loss: 0.2887 (0.2896)  time: 0.5127  data: 0.0002  max mem: 9341
[05:32:49.310864] Epoch: [553]  [180/195]  eta: 0:00:07  lr: 0.000569  loss: 0.2787 (0.2890)  time: 0.5082  data: 0.0002  max mem: 9341
[05:32:56.438734] Epoch: [553]  [194/195]  eta: 0:00:00  lr: 0.000569  loss: 0.2894 (0.2893)  time: 0.5103  data: 0.0001  max mem: 9341
[05:32:56.617827] Epoch: [553] Total time: 0:01:41 (0.5182 s / it)
[05:32:56.629992] Averaged stats: lr: 0.000569  loss: 0.2894 (0.2918)
[05:33:01.343706] {"train_lr": 0.0005710599266678905, "train_loss": 0.2917676607767741, "epoch": 553}
[05:33:01.344027] [05:33:01.344152] Training epoch 553 for 0:01:45
[05:33:01.344208] [05:33:01.348647] log_dir: ./exp/debug/cifar100-LT/debug
[05:33:03.094712] Epoch: [554]  [  0/195]  eta: 0:05:40  lr: 0.000569  loss: 0.2795 (0.2795)  time: 1.7446  data: 1.2369  max mem: 9341
[05:33:13.322988] Epoch: [554]  [ 20/195]  eta: 0:01:39  lr: 0.000568  loss: 0.2883 (0.2855)  time: 0.5114  data: 0.0002  max mem: 9341
[05:33:23.544843] Epoch: [554]  [ 40/195]  eta: 0:01:23  lr: 0.000568  loss: 0.2923 (0.2888)  time: 0.5110  data: 0.0002  max mem: 9341
[05:33:33.764263] Epoch: [554]  [ 60/195]  eta: 0:01:11  lr: 0.000568  loss: 0.2937 (0.2900)  time: 0.5109  data: 0.0002  max mem: 9341
[05:33:44.030584] Epoch: [554]  [ 80/195]  eta: 0:01:00  lr: 0.000567  loss: 0.2915 (0.2893)  time: 0.5132  data: 0.0003  max mem: 9341
[05:33:54.245146] Epoch: [554]  [100/195]  eta: 0:00:49  lr: 0.000567  loss: 0.2881 (0.2907)  time: 0.5107  data: 0.0003  max mem: 9341
[05:34:04.456134] Epoch: [554]  [120/195]  eta: 0:00:39  lr: 0.000566  loss: 0.2931 (0.2908)  time: 0.5105  data: 0.0003  max mem: 9341
[05:34:14.676301] Epoch: [554]  [140/195]  eta: 0:00:28  lr: 0.000566  loss: 0.2843 (0.2906)  time: 0.5109  data: 0.0002  max mem: 9341
[05:34:24.938839] Epoch: [554]  [160/195]  eta: 0:00:18  lr: 0.000565  loss: 0.2872 (0.2909)  time: 0.5131  data: 0.0002  max mem: 9341
[05:34:35.119755] Epoch: [554]  [180/195]  eta: 0:00:07  lr: 0.000565  loss: 0.2854 (0.2913)  time: 0.5090  data: 0.0002  max mem: 9341
[05:34:42.251076] Epoch: [554]  [194/195]  eta: 0:00:00  lr: 0.000565  loss: 0.2854 (0.2913)  time: 0.5107  data: 0.0001  max mem: 9341
[05:34:42.434308] Epoch: [554] Total time: 0:01:41 (0.5184 s / it)
[05:34:42.435025] Averaged stats: lr: 0.000565  loss: 0.2854 (0.2919)
[05:34:47.201963] {"train_lr": 0.0005668408012267724, "train_loss": 0.29188466717799505, "epoch": 554}
[05:34:47.202304] [05:34:47.202411] Training epoch 554 for 0:01:45
[05:34:47.202466] [05:34:47.206871] log_dir: ./exp/debug/cifar100-LT/debug
[05:34:48.812419] Epoch: [555]  [  0/195]  eta: 0:05:12  lr: 0.000565  loss: 0.3068 (0.3068)  time: 1.6041  data: 1.1045  max mem: 9341
[05:34:59.033533] Epoch: [555]  [ 20/195]  eta: 0:01:38  lr: 0.000564  loss: 0.2931 (0.2975)  time: 0.5110  data: 0.0002  max mem: 9341
[05:35:09.249606] Epoch: [555]  [ 40/195]  eta: 0:01:23  lr: 0.000564  loss: 0.2865 (0.2942)  time: 0.5107  data: 0.0002  max mem: 9341
[05:35:19.496563] Epoch: [555]  [ 60/195]  eta: 0:01:11  lr: 0.000564  loss: 0.2888 (0.2943)  time: 0.5122  data: 0.0003  max mem: 9341
[05:35:29.748549] Epoch: [555]  [ 80/195]  eta: 0:01:00  lr: 0.000563  loss: 0.2894 (0.2936)  time: 0.5125  data: 0.0003  max mem: 9341
[05:35:39.954839] Epoch: [555]  [100/195]  eta: 0:00:49  lr: 0.000562  loss: 0.2868 (0.2911)  time: 0.5102  data: 0.0003  max mem: 9341
[05:35:50.166492] Epoch: [555]  [120/195]  eta: 0:00:39  lr: 0.000562  loss: 0.2840 (0.2908)  time: 0.5105  data: 0.0003  max mem: 9341
[05:36:00.379636] Epoch: [555]  [140/195]  eta: 0:00:28  lr: 0.000562  loss: 0.2987 (0.2910)  time: 0.5106  data: 0.0003  max mem: 9341
[05:36:10.640558] Epoch: [555]  [160/195]  eta: 0:00:18  lr: 0.000561  loss: 0.2934 (0.2908)  time: 0.5130  data: 0.0002  max mem: 9341
[05:36:20.807514] Epoch: [555]  [180/195]  eta: 0:00:07  lr: 0.000561  loss: 0.2983 (0.2918)  time: 0.5083  data: 0.0002  max mem: 9341
[05:36:27.934596] Epoch: [555]  [194/195]  eta: 0:00:00  lr: 0.000560  loss: 0.2822 (0.2910)  time: 0.5102  data: 0.0001  max mem: 9341
[05:36:28.112769] Epoch: [555] Total time: 0:01:40 (0.5175 s / it)
[05:36:28.133921] Averaged stats: lr: 0.000560  loss: 0.2822 (0.2914)
[05:36:32.850835] {"train_lr": 0.0005626324947294155, "train_loss": 0.29139437719415395, "epoch": 555}
[05:36:32.851109] [05:36:32.851214] Training epoch 555 for 0:01:45
[05:36:32.851268] [05:36:32.855722] log_dir: ./exp/debug/cifar100-LT/debug
[05:36:34.679683] Epoch: [556]  [  0/195]  eta: 0:05:55  lr: 0.000560  loss: 0.3023 (0.3023)  time: 1.8229  data: 1.3140  max mem: 9341
[05:36:44.897575] Epoch: [556]  [ 20/195]  eta: 0:01:40  lr: 0.000560  loss: 0.2939 (0.3018)  time: 0.5108  data: 0.0002  max mem: 9341
[05:36:55.114859] Epoch: [556]  [ 40/195]  eta: 0:01:24  lr: 0.000560  loss: 0.2822 (0.2925)  time: 0.5108  data: 0.0002  max mem: 9341
[05:37:05.331250] Epoch: [556]  [ 60/195]  eta: 0:01:11  lr: 0.000559  loss: 0.2812 (0.2904)  time: 0.5107  data: 0.0004  max mem: 9341
[05:37:15.587513] Epoch: [556]  [ 80/195]  eta: 0:01:00  lr: 0.000559  loss: 0.2950 (0.2912)  time: 0.5128  data: 0.0003  max mem: 9341
[05:37:25.820051] Epoch: [556]  [100/195]  eta: 0:00:49  lr: 0.000558  loss: 0.2893 (0.2909)  time: 0.5116  data: 0.0002  max mem: 9341
[05:37:36.036439] Epoch: [556]  [120/195]  eta: 0:00:39  lr: 0.000558  loss: 0.2908 (0.2913)  time: 0.5108  data: 0.0002  max mem: 9341
[05:37:46.254297] Epoch: [556]  [140/195]  eta: 0:00:28  lr: 0.000558  loss: 0.2923 (0.2921)  time: 0.5108  data: 0.0002  max mem: 9341
[05:37:56.514181] Epoch: [556]  [160/195]  eta: 0:00:18  lr: 0.000557  loss: 0.2870 (0.2927)  time: 0.5129  data: 0.0002  max mem: 9341
[05:38:06.683957] Epoch: [556]  [180/195]  eta: 0:00:07  lr: 0.000557  loss: 0.2760 (0.2913)  time: 0.5084  data: 0.0001  max mem: 9341
[05:38:13.813613] Epoch: [556]  [194/195]  eta: 0:00:00  lr: 0.000556  loss: 0.2943 (0.2919)  time: 0.5104  data: 0.0001  max mem: 9341
[05:38:13.983970] Epoch: [556] Total time: 0:01:41 (0.5186 s / it)
[05:38:14.001430] Averaged stats: lr: 0.000556  loss: 0.2943 (0.2920)
[05:38:18.829849] {"train_lr": 0.0005584350790841671, "train_loss": 0.2919710750381152, "epoch": 556}
[05:38:18.830141] [05:38:18.830239] Training epoch 556 for 0:01:45
[05:38:18.830293] [05:38:18.834785] log_dir: ./exp/debug/cifar100-LT/debug
[05:38:20.362365] Epoch: [557]  [  0/195]  eta: 0:04:57  lr: 0.000556  loss: 0.2954 (0.2954)  time: 1.5260  data: 1.0239  max mem: 9341
[05:38:30.581029] Epoch: [557]  [ 20/195]  eta: 0:01:37  lr: 0.000556  loss: 0.2829 (0.2884)  time: 0.5109  data: 0.0002  max mem: 9341
[05:38:40.801493] Epoch: [557]  [ 40/195]  eta: 0:01:23  lr: 0.000555  loss: 0.2987 (0.2921)  time: 0.5109  data: 0.0003  max mem: 9341
[05:38:51.032038] Epoch: [557]  [ 60/195]  eta: 0:01:11  lr: 0.000555  loss: 0.2913 (0.2926)  time: 0.5115  data: 0.0002  max mem: 9341
[05:39:01.291845] Epoch: [557]  [ 80/195]  eta: 0:01:00  lr: 0.000554  loss: 0.2857 (0.2930)  time: 0.5129  data: 0.0002  max mem: 9341
[05:39:11.504134] Epoch: [557]  [100/195]  eta: 0:00:49  lr: 0.000554  loss: 0.2918 (0.2928)  time: 0.5106  data: 0.0002  max mem: 9341
[05:39:21.716488] Epoch: [557]  [120/195]  eta: 0:00:38  lr: 0.000554  loss: 0.2766 (0.2912)  time: 0.5106  data: 0.0002  max mem: 9341
[05:39:31.938437] Epoch: [557]  [140/195]  eta: 0:00:28  lr: 0.000553  loss: 0.2951 (0.2919)  time: 0.5110  data: 0.0002  max mem: 9341
[05:39:42.189868] Epoch: [557]  [160/195]  eta: 0:00:18  lr: 0.000553  loss: 0.2906 (0.2923)  time: 0.5125  data: 0.0002  max mem: 9341
[05:39:52.357482] Epoch: [557]  [180/195]  eta: 0:00:07  lr: 0.000552  loss: 0.2936 (0.2924)  time: 0.5083  data: 0.0001  max mem: 9341
[05:39:59.486331] Epoch: [557]  [194/195]  eta: 0:00:00  lr: 0.000552  loss: 0.2911 (0.2920)  time: 0.5103  data: 0.0001  max mem: 9341
[05:39:59.664725] Epoch: [557] Total time: 0:01:40 (0.5171 s / it)
[05:39:59.680829] Averaged stats: lr: 0.000552  loss: 0.2911 (0.2916)
[05:40:04.444186] {"train_lr": 0.0005542486260132814, "train_loss": 0.2915638759159125, "epoch": 557}
[05:40:04.444449] [05:40:04.444557] Training epoch 557 for 0:01:45
[05:40:04.444612] [05:40:04.449057] log_dir: ./exp/debug/cifar100-LT/debug
[05:40:06.205679] Epoch: [558]  [  0/195]  eta: 0:05:42  lr: 0.000552  loss: 0.2835 (0.2835)  time: 1.7556  data: 1.2459  max mem: 9341
[05:40:16.428976] Epoch: [558]  [ 20/195]  eta: 0:01:39  lr: 0.000552  loss: 0.2828 (0.2892)  time: 0.5111  data: 0.0003  max mem: 9341
[05:40:26.639724] Epoch: [558]  [ 40/195]  eta: 0:01:23  lr: 0.000551  loss: 0.2788 (0.2859)  time: 0.5105  data: 0.0002  max mem: 9341
[05:40:36.852989] Epoch: [558]  [ 60/195]  eta: 0:01:11  lr: 0.000551  loss: 0.2962 (0.2879)  time: 0.5106  data: 0.0002  max mem: 9341
[05:40:47.111047] Epoch: [558]  [ 80/195]  eta: 0:01:00  lr: 0.000550  loss: 0.2946 (0.2890)  time: 0.5128  data: 0.0003  max mem: 9341
[05:40:57.320053] Epoch: [558]  [100/195]  eta: 0:00:49  lr: 0.000550  loss: 0.2824 (0.2879)  time: 0.5104  data: 0.0002  max mem: 9341
[05:41:07.538459] Epoch: [558]  [120/195]  eta: 0:00:39  lr: 0.000550  loss: 0.2831 (0.2870)  time: 0.5109  data: 0.0002  max mem: 9341
[05:41:17.754144] Epoch: [558]  [140/195]  eta: 0:00:28  lr: 0.000549  loss: 0.2853 (0.2869)  time: 0.5107  data: 0.0002  max mem: 9341
[05:41:28.015261] Epoch: [558]  [160/195]  eta: 0:00:18  lr: 0.000549  loss: 0.2744 (0.2854)  time: 0.5130  data: 0.0002  max mem: 9341
[05:41:38.194279] Epoch: [558]  [180/195]  eta: 0:00:07  lr: 0.000548  loss: 0.2922 (0.2865)  time: 0.5089  data: 0.0001  max mem: 9341
[05:41:45.321812] Epoch: [558]  [194/195]  eta: 0:00:00  lr: 0.000548  loss: 0.2893 (0.2868)  time: 0.5104  data: 0.0001  max mem: 9341
[05:41:45.492835] Epoch: [558] Total time: 0:01:41 (0.5182 s / it)
[05:41:45.517088] Averaged stats: lr: 0.000548  loss: 0.2893 (0.2906)
[05:41:50.350465] {"train_lr": 0.0005500732070516864, "train_loss": 0.29058275234240755, "epoch": 558}
[05:41:50.350742] [05:41:50.350852] Training epoch 558 for 0:01:45
[05:41:50.350906] [05:41:50.355433] log_dir: ./exp/debug/cifar100-LT/debug
[05:41:52.087328] Epoch: [559]  [  0/195]  eta: 0:05:37  lr: 0.000548  loss: 0.2742 (0.2742)  time: 1.7309  data: 1.2298  max mem: 9341
[05:42:02.303581] Epoch: [559]  [ 20/195]  eta: 0:01:39  lr: 0.000547  loss: 0.3006 (0.3025)  time: 0.5107  data: 0.0003  max mem: 9341
[05:42:12.525287] Epoch: [559]  [ 40/195]  eta: 0:01:23  lr: 0.000547  loss: 0.2852 (0.2966)  time: 0.5110  data: 0.0002  max mem: 9341
[05:42:22.736987] Epoch: [559]  [ 60/195]  eta: 0:01:11  lr: 0.000547  loss: 0.2810 (0.2930)  time: 0.5105  data: 0.0002  max mem: 9341
[05:42:32.995596] Epoch: [559]  [ 80/195]  eta: 0:01:00  lr: 0.000546  loss: 0.2864 (0.2933)  time: 0.5129  data: 0.0003  max mem: 9341
[05:42:43.206373] Epoch: [559]  [100/195]  eta: 0:00:49  lr: 0.000546  loss: 0.2836 (0.2922)  time: 0.5105  data: 0.0002  max mem: 9341
[05:42:53.420663] Epoch: [559]  [120/195]  eta: 0:00:39  lr: 0.000545  loss: 0.2928 (0.2925)  time: 0.5107  data: 0.0003  max mem: 9341
[05:43:03.630041] Epoch: [559]  [140/195]  eta: 0:00:28  lr: 0.000545  loss: 0.2882 (0.2917)  time: 0.5104  data: 0.0002  max mem: 9341
[05:43:13.888166] Epoch: [559]  [160/195]  eta: 0:00:18  lr: 0.000544  loss: 0.2773 (0.2906)  time: 0.5129  data: 0.0002  max mem: 9341
[05:43:24.056878] Epoch: [559]  [180/195]  eta: 0:00:07  lr: 0.000544  loss: 0.3005 (0.2913)  time: 0.5084  data: 0.0002  max mem: 9341
[05:43:31.200151] Epoch: [559]  [194/195]  eta: 0:00:00  lr: 0.000544  loss: 0.2863 (0.2910)  time: 0.5112  data: 0.0001  max mem: 9341
[05:43:31.384193] Epoch: [559] Total time: 0:01:41 (0.5181 s / it)
[05:43:31.395462] Averaged stats: lr: 0.000544  loss: 0.2863 (0.2904)
[05:43:36.113712] {"train_lr": 0.000545908893545771, "train_loss": 0.29043395255620663, "epoch": 559}
[05:43:36.113984] [05:43:36.114093] Training epoch 559 for 0:01:45
[05:43:36.114149] [05:43:36.118635] log_dir: ./exp/debug/cifar100-LT/debug
[05:43:37.811609] Epoch: [560]  [  0/195]  eta: 0:05:29  lr: 0.000544  loss: 0.2688 (0.2688)  time: 1.6919  data: 1.1748  max mem: 9341
[05:43:48.036581] Epoch: [560]  [ 20/195]  eta: 0:01:39  lr: 0.000543  loss: 0.2873 (0.2867)  time: 0.5112  data: 0.0003  max mem: 9341
[05:43:58.256393] Epoch: [560]  [ 40/195]  eta: 0:01:23  lr: 0.000543  loss: 0.2964 (0.2919)  time: 0.5109  data: 0.0002  max mem: 9341
[05:44:08.467571] Epoch: [560]  [ 60/195]  eta: 0:01:11  lr: 0.000543  loss: 0.2966 (0.2928)  time: 0.5105  data: 0.0003  max mem: 9341
[05:44:18.721676] Epoch: [560]  [ 80/195]  eta: 0:01:00  lr: 0.000542  loss: 0.2948 (0.2934)  time: 0.5126  data: 0.0003  max mem: 9341
[05:44:28.936955] Epoch: [560]  [100/195]  eta: 0:00:49  lr: 0.000542  loss: 0.2849 (0.2934)  time: 0.5107  data: 0.0003  max mem: 9341
[05:44:39.151182] Epoch: [560]  [120/195]  eta: 0:00:39  lr: 0.000541  loss: 0.2910 (0.2931)  time: 0.5107  data: 0.0003  max mem: 9341
[05:44:49.368608] Epoch: [560]  [140/195]  eta: 0:00:28  lr: 0.000541  loss: 0.3006 (0.2936)  time: 0.5108  data: 0.0003  max mem: 9341
[05:44:59.627452] Epoch: [560]  [160/195]  eta: 0:00:18  lr: 0.000540  loss: 0.2841 (0.2926)  time: 0.5129  data: 0.0003  max mem: 9341
[05:45:09.796163] Epoch: [560]  [180/195]  eta: 0:00:07  lr: 0.000540  loss: 0.2875 (0.2920)  time: 0.5084  data: 0.0002  max mem: 9341
[05:45:16.930433] Epoch: [560]  [194/195]  eta: 0:00:00  lr: 0.000540  loss: 0.2899 (0.2922)  time: 0.5105  data: 0.0001  max mem: 9341
[05:45:17.121586] Epoch: [560] Total time: 0:01:41 (0.5180 s / it)
[05:45:17.122329] Averaged stats: lr: 0.000540  loss: 0.2899 (0.2905)
[05:45:21.853695] {"train_lr": 0.0005417557566521729, "train_loss": 0.29046142538770653, "epoch": 560}
[05:45:21.853957] [05:45:21.854060] Training epoch 560 for 0:01:45
[05:45:21.854115] [05:45:21.858579] log_dir: ./exp/debug/cifar100-LT/debug
[05:45:23.499902] Epoch: [561]  [  0/195]  eta: 0:05:19  lr: 0.000540  loss: 0.3146 (0.3146)  time: 1.6399  data: 1.1487  max mem: 9341
[05:45:33.735349] Epoch: [561]  [ 20/195]  eta: 0:01:38  lr: 0.000539  loss: 0.2925 (0.2957)  time: 0.5117  data: 0.0002  max mem: 9341
[05:45:43.952220] Epoch: [561]  [ 40/195]  eta: 0:01:23  lr: 0.000539  loss: 0.2897 (0.2930)  time: 0.5108  data: 0.0002  max mem: 9341
[05:45:54.167785] Epoch: [561]  [ 60/195]  eta: 0:01:11  lr: 0.000538  loss: 0.2824 (0.2918)  time: 0.5107  data: 0.0003  max mem: 9341
[05:46:04.446956] Epoch: [561]  [ 80/195]  eta: 0:01:00  lr: 0.000538  loss: 0.2811 (0.2903)  time: 0.5139  data: 0.0003  max mem: 9341
[05:46:14.661177] Epoch: [561]  [100/195]  eta: 0:00:49  lr: 0.000537  loss: 0.2840 (0.2896)  time: 0.5107  data: 0.0003  max mem: 9341
[05:46:24.898809] Epoch: [561]  [120/195]  eta: 0:00:39  lr: 0.000537  loss: 0.2873 (0.2903)  time: 0.5118  data: 0.0003  max mem: 9341
[05:46:35.134130] Epoch: [561]  [140/195]  eta: 0:00:28  lr: 0.000537  loss: 0.2963 (0.2911)  time: 0.5117  data: 0.0002  max mem: 9341
[05:46:45.440715] Epoch: [561]  [160/195]  eta: 0:00:18  lr: 0.000536  loss: 0.2885 (0.2905)  time: 0.5153  data: 0.0003  max mem: 9341
[05:46:55.622360] Epoch: [561]  [180/195]  eta: 0:00:07  lr: 0.000536  loss: 0.2802 (0.2897)  time: 0.5090  data: 0.0002  max mem: 9341
[05:47:02.773147] Epoch: [561]  [194/195]  eta: 0:00:00  lr: 0.000535  loss: 0.3079 (0.2911)  time: 0.5123  data: 0.0001  max mem: 9341
[05:47:02.946670] Epoch: [561] Total time: 0:01:41 (0.5184 s / it)
[05:47:02.963315] Averaged stats: lr: 0.000535  loss: 0.3079 (0.2896)
[05:47:07.717894] {"train_lr": 0.0005376138673365302, "train_loss": 0.2896068728505037, "epoch": 561}
[05:47:07.718144] [05:47:07.718244] Training epoch 561 for 0:01:45
[05:47:07.718299] [05:47:07.722753] log_dir: ./exp/debug/cifar100-LT/debug
[05:47:09.447540] Epoch: [562]  [  0/195]  eta: 0:05:36  lr: 0.000535  loss: 0.2814 (0.2814)  time: 1.7237  data: 1.2173  max mem: 9341
[05:47:19.659460] Epoch: [562]  [ 20/195]  eta: 0:01:39  lr: 0.000535  loss: 0.2765 (0.2839)  time: 0.5105  data: 0.0002  max mem: 9341
[05:47:29.876940] Epoch: [562]  [ 40/195]  eta: 0:01:23  lr: 0.000535  loss: 0.2857 (0.2856)  time: 0.5108  data: 0.0002  max mem: 9341
[05:47:40.093496] Epoch: [562]  [ 60/195]  eta: 0:01:11  lr: 0.000534  loss: 0.2944 (0.2892)  time: 0.5108  data: 0.0002  max mem: 9341
[05:47:50.351107] Epoch: [562]  [ 80/195]  eta: 0:01:00  lr: 0.000534  loss: 0.2722 (0.2865)  time: 0.5128  data: 0.0002  max mem: 9341
[05:48:00.564646] Epoch: [562]  [100/195]  eta: 0:00:49  lr: 0.000533  loss: 0.2841 (0.2863)  time: 0.5106  data: 0.0002  max mem: 9341
[05:48:10.792595] Epoch: [562]  [120/195]  eta: 0:00:39  lr: 0.000533  loss: 0.2791 (0.2863)  time: 0.5113  data: 0.0002  max mem: 9341
[05:48:21.024873] Epoch: [562]  [140/195]  eta: 0:00:28  lr: 0.000533  loss: 0.2950 (0.2872)  time: 0.5116  data: 0.0002  max mem: 9341
[05:48:31.285601] Epoch: [562]  [160/195]  eta: 0:00:18  lr: 0.000532  loss: 0.2916 (0.2875)  time: 0.5130  data: 0.0002  max mem: 9341
[05:48:41.456972] Epoch: [562]  [180/195]  eta: 0:00:07  lr: 0.000532  loss: 0.2800 (0.2872)  time: 0.5085  data: 0.0001  max mem: 9341
[05:48:48.585106] Epoch: [562]  [194/195]  eta: 0:00:00  lr: 0.000531  loss: 0.2953 (0.2884)  time: 0.5104  data: 0.0001  max mem: 9341
[05:48:48.757828] Epoch: [562] Total time: 0:01:41 (0.5181 s / it)
[05:48:48.786191] Averaged stats: lr: 0.000531  loss: 0.2953 (0.2906)
[05:48:53.448549] {"train_lr": 0.0005334832963723124, "train_loss": 0.2906184734251255, "epoch": 562}
[05:48:53.448813] [05:48:53.448900] Training epoch 562 for 0:01:45
[05:48:53.448954] [05:48:53.453533] log_dir: ./exp/debug/cifar100-LT/debug
[05:48:55.242960] Epoch: [563]  [  0/195]  eta: 0:05:48  lr: 0.000531  loss: 0.3267 (0.3267)  time: 1.7883  data: 1.2879  max mem: 9341
[05:49:05.453400] Epoch: [563]  [ 20/195]  eta: 0:01:39  lr: 0.000531  loss: 0.2941 (0.2931)  time: 0.5105  data: 0.0002  max mem: 9341
[05:49:15.669599] Epoch: [563]  [ 40/195]  eta: 0:01:23  lr: 0.000531  loss: 0.2927 (0.2938)  time: 0.5107  data: 0.0003  max mem: 9341
[05:49:25.885244] Epoch: [563]  [ 60/195]  eta: 0:01:11  lr: 0.000530  loss: 0.3030 (0.2963)  time: 0.5107  data: 0.0002  max mem: 9341
[05:49:36.145796] Epoch: [563]  [ 80/195]  eta: 0:01:00  lr: 0.000530  loss: 0.2873 (0.2949)  time: 0.5130  data: 0.0003  max mem: 9341
[05:49:46.364571] Epoch: [563]  [100/195]  eta: 0:00:49  lr: 0.000529  loss: 0.2953 (0.2948)  time: 0.5109  data: 0.0003  max mem: 9341
[05:49:56.586425] Epoch: [563]  [120/195]  eta: 0:00:39  lr: 0.000529  loss: 0.2846 (0.2934)  time: 0.5110  data: 0.0003  max mem: 9341
[05:50:06.803449] Epoch: [563]  [140/195]  eta: 0:00:28  lr: 0.000529  loss: 0.2862 (0.2922)  time: 0.5108  data: 0.0002  max mem: 9341
[05:50:17.069621] Epoch: [563]  [160/195]  eta: 0:00:18  lr: 0.000528  loss: 0.2891 (0.2917)  time: 0.5133  data: 0.0003  max mem: 9341
[05:50:27.245623] Epoch: [563]  [180/195]  eta: 0:00:07  lr: 0.000528  loss: 0.2947 (0.2924)  time: 0.5087  data: 0.0002  max mem: 9341
[05:50:34.376412] Epoch: [563]  [194/195]  eta: 0:00:00  lr: 0.000527  loss: 0.2869 (0.2918)  time: 0.5105  data: 0.0001  max mem: 9341
[05:50:34.550657] Epoch: [563] Total time: 0:01:41 (0.5184 s / it)
[05:50:34.560477] Averaged stats: lr: 0.000527  loss: 0.2869 (0.2924)
[05:50:39.352914] {"train_lr": 0.0005293641143395757, "train_loss": 0.29235672505620197, "epoch": 563}
[05:50:39.353183] [05:50:39.353289] Training epoch 563 for 0:01:45
[05:50:39.353344] [05:50:39.357773] log_dir: ./exp/debug/cifar100-LT/debug
[05:50:41.387026] Epoch: [564]  [  0/195]  eta: 0:06:35  lr: 0.000527  loss: 0.3207 (0.3207)  time: 2.0285  data: 1.5308  max mem: 9341
[05:50:51.599969] Epoch: [564]  [ 20/195]  eta: 0:01:42  lr: 0.000527  loss: 0.2759 (0.2818)  time: 0.5106  data: 0.0002  max mem: 9341
[05:51:01.815460] Epoch: [564]  [ 40/195]  eta: 0:01:24  lr: 0.000526  loss: 0.2838 (0.2845)  time: 0.5107  data: 0.0002  max mem: 9341
[05:51:12.027518] Epoch: [564]  [ 60/195]  eta: 0:01:12  lr: 0.000526  loss: 0.2826 (0.2859)  time: 0.5105  data: 0.0002  max mem: 9341
[05:51:22.307640] Epoch: [564]  [ 80/195]  eta: 0:01:00  lr: 0.000525  loss: 0.2909 (0.2866)  time: 0.5139  data: 0.0003  max mem: 9341
[05:51:32.542445] Epoch: [564]  [100/195]  eta: 0:00:50  lr: 0.000525  loss: 0.2903 (0.2879)  time: 0.5117  data: 0.0003  max mem: 9341
[05:51:42.781426] Epoch: [564]  [120/195]  eta: 0:00:39  lr: 0.000525  loss: 0.2854 (0.2879)  time: 0.5119  data: 0.0002  max mem: 9341
[05:51:53.014703] Epoch: [564]  [140/195]  eta: 0:00:28  lr: 0.000524  loss: 0.2909 (0.2887)  time: 0.5116  data: 0.0003  max mem: 9341
[05:52:03.311919] Epoch: [564]  [160/195]  eta: 0:00:18  lr: 0.000524  loss: 0.2934 (0.2890)  time: 0.5148  data: 0.0003  max mem: 9341
[05:52:13.497917] Epoch: [564]  [180/195]  eta: 0:00:07  lr: 0.000523  loss: 0.2906 (0.2894)  time: 0.5092  data: 0.0002  max mem: 9341
[05:52:20.647141] Epoch: [564]  [194/195]  eta: 0:00:00  lr: 0.000523  loss: 0.2879 (0.2898)  time: 0.5123  data: 0.0001  max mem: 9341
[05:52:20.821699] Epoch: [564] Total time: 0:01:41 (0.5203 s / it)
[05:52:20.826450] Averaged stats: lr: 0.000523  loss: 0.2879 (0.2897)
[05:52:25.622053] {"train_lr": 0.0005252563916237783, "train_loss": 0.2897272449273329, "epoch": 564}
[05:52:25.622316] [05:52:25.622424] Training epoch 564 for 0:01:46
[05:52:25.622479] [05:52:25.626902] log_dir: ./exp/debug/cifar100-LT/debug
[05:52:27.425823] Epoch: [565]  [  0/195]  eta: 0:05:50  lr: 0.000523  loss: 0.3440 (0.3440)  time: 1.7975  data: 1.2926  max mem: 9341
[05:52:37.658859] Epoch: [565]  [ 20/195]  eta: 0:01:40  lr: 0.000523  loss: 0.2916 (0.2926)  time: 0.5116  data: 0.0002  max mem: 9341
[05:52:47.896943] Epoch: [565]  [ 40/195]  eta: 0:01:24  lr: 0.000522  loss: 0.2863 (0.2895)  time: 0.5118  data: 0.0003  max mem: 9341
[05:52:58.135358] Epoch: [565]  [ 60/195]  eta: 0:01:11  lr: 0.000522  loss: 0.2902 (0.2893)  time: 0.5119  data: 0.0003  max mem: 9341
[05:53:08.436809] Epoch: [565]  [ 80/195]  eta: 0:01:00  lr: 0.000521  loss: 0.2846 (0.2889)  time: 0.5150  data: 0.0003  max mem: 9341
[05:53:18.668516] Epoch: [565]  [100/195]  eta: 0:00:49  lr: 0.000521  loss: 0.2904 (0.2888)  time: 0.5115  data: 0.0002  max mem: 9341
[05:53:28.900581] Epoch: [565]  [120/195]  eta: 0:00:39  lr: 0.000521  loss: 0.2863 (0.2879)  time: 0.5115  data: 0.0003  max mem: 9341
[05:53:39.133835] Epoch: [565]  [140/195]  eta: 0:00:28  lr: 0.000520  loss: 0.2920 (0.2894)  time: 0.5116  data: 0.0002  max mem: 9341
[05:53:49.433339] Epoch: [565]  [160/195]  eta: 0:00:18  lr: 0.000520  loss: 0.2835 (0.2890)  time: 0.5149  data: 0.0002  max mem: 9341
[05:53:59.623221] Epoch: [565]  [180/195]  eta: 0:00:07  lr: 0.000519  loss: 0.3005 (0.2904)  time: 0.5094  data: 0.0002  max mem: 9341
[05:54:06.773872] Epoch: [565]  [194/195]  eta: 0:00:00  lr: 0.000519  loss: 0.2924 (0.2905)  time: 0.5124  data: 0.0001  max mem: 9341
[05:54:06.938373] Epoch: [565] Total time: 0:01:41 (0.5195 s / it)
[05:54:06.964274] Averaged stats: lr: 0.000519  loss: 0.2924 (0.2898)
[05:54:11.683483] {"train_lr": 0.0005211601984145679, "train_loss": 0.28983843427820083, "epoch": 565}
[05:54:11.683905] [05:54:11.684006] Training epoch 565 for 0:01:46
[05:54:11.684061] [05:54:11.689159] log_dir: ./exp/debug/cifar100-LT/debug
[05:54:13.463825] Epoch: [566]  [  0/195]  eta: 0:05:45  lr: 0.000519  loss: 0.2853 (0.2853)  time: 1.7721  data: 1.2608  max mem: 9341
[05:54:23.702567] Epoch: [566]  [ 20/195]  eta: 0:01:40  lr: 0.000519  loss: 0.2847 (0.2873)  time: 0.5119  data: 0.0002  max mem: 9341
[05:54:33.912343] Epoch: [566]  [ 40/195]  eta: 0:01:23  lr: 0.000518  loss: 0.2814 (0.2900)  time: 0.5104  data: 0.0003  max mem: 9341
[05:54:44.131951] Epoch: [566]  [ 60/195]  eta: 0:01:11  lr: 0.000518  loss: 0.2745 (0.2881)  time: 0.5109  data: 0.0002  max mem: 9341
[05:54:54.385328] Epoch: [566]  [ 80/195]  eta: 0:01:00  lr: 0.000517  loss: 0.2932 (0.2892)  time: 0.5126  data: 0.0003  max mem: 9341
[05:55:04.596152] Epoch: [566]  [100/195]  eta: 0:00:49  lr: 0.000517  loss: 0.2900 (0.2898)  time: 0.5105  data: 0.0002  max mem: 9341
[05:55:14.813299] Epoch: [566]  [120/195]  eta: 0:00:39  lr: 0.000517  loss: 0.2748 (0.2884)  time: 0.5108  data: 0.0003  max mem: 9341
[05:55:25.029295] Epoch: [566]  [140/195]  eta: 0:00:28  lr: 0.000516  loss: 0.2798 (0.2876)  time: 0.5107  data: 0.0003  max mem: 9341
[05:55:35.290160] Epoch: [566]  [160/195]  eta: 0:00:18  lr: 0.000516  loss: 0.2937 (0.2883)  time: 0.5130  data: 0.0003  max mem: 9341
[05:55:45.459859] Epoch: [566]  [180/195]  eta: 0:00:07  lr: 0.000515  loss: 0.2820 (0.2882)  time: 0.5084  data: 0.0002  max mem: 9341
[05:55:52.586548] Epoch: [566]  [194/195]  eta: 0:00:00  lr: 0.000515  loss: 0.2890 (0.2886)  time: 0.5103  data: 0.0001  max mem: 9341
[05:55:52.761604] Epoch: [566] Total time: 0:01:41 (0.5183 s / it)
[05:55:52.768806] Averaged stats: lr: 0.000515  loss: 0.2890 (0.2882)
[05:55:57.486601] {"train_lr": 0.0005170756047045823, "train_loss": 0.2882092497669734, "epoch": 566}
[05:55:57.486902] [05:55:57.487010] Training epoch 566 for 0:01:45
[05:55:57.487066] [05:55:57.492109] log_dir: ./exp/debug/cifar100-LT/debug
[05:55:59.203838] Epoch: [567]  [  0/195]  eta: 0:05:33  lr: 0.000515  loss: 0.2695 (0.2695)  time: 1.7106  data: 1.2099  max mem: 9341
[05:56:09.414973] Epoch: [567]  [ 20/195]  eta: 0:01:39  lr: 0.000515  loss: 0.2877 (0.2898)  time: 0.5105  data: 0.0002  max mem: 9341
[05:56:19.628767] Epoch: [567]  [ 40/195]  eta: 0:01:23  lr: 0.000514  loss: 0.2881 (0.2916)  time: 0.5106  data: 0.0002  max mem: 9341
[05:56:29.842358] Epoch: [567]  [ 60/195]  eta: 0:01:11  lr: 0.000514  loss: 0.2920 (0.2923)  time: 0.5106  data: 0.0002  max mem: 9341
[05:56:40.107834] Epoch: [567]  [ 80/195]  eta: 0:01:00  lr: 0.000513  loss: 0.2945 (0.2921)  time: 0.5132  data: 0.0002  max mem: 9341
[05:56:50.318962] Epoch: [567]  [100/195]  eta: 0:00:49  lr: 0.000513  loss: 0.2896 (0.2926)  time: 0.5105  data: 0.0002  max mem: 9341
[05:57:00.530681] Epoch: [567]  [120/195]  eta: 0:00:39  lr: 0.000513  loss: 0.2829 (0.2913)  time: 0.5105  data: 0.0002  max mem: 9341
[05:57:10.749027] Epoch: [567]  [140/195]  eta: 0:00:28  lr: 0.000512  loss: 0.2886 (0.2910)  time: 0.5109  data: 0.0002  max mem: 9341
[05:57:21.012548] Epoch: [567]  [160/195]  eta: 0:00:18  lr: 0.000512  loss: 0.2908 (0.2914)  time: 0.5131  data: 0.0002  max mem: 9341
[05:57:31.179865] Epoch: [567]  [180/195]  eta: 0:00:07  lr: 0.000511  loss: 0.2884 (0.2912)  time: 0.5083  data: 0.0001  max mem: 9341
[05:57:38.308359] Epoch: [567]  [194/195]  eta: 0:00:00  lr: 0.000511  loss: 0.2884 (0.2912)  time: 0.5103  data: 0.0001  max mem: 9341
[05:57:38.468403] Epoch: [567] Total time: 0:01:40 (0.5178 s / it)
[05:57:38.502875] Averaged stats: lr: 0.000511  loss: 0.2884 (0.2899)
[05:57:43.213320] {"train_lr": 0.0005130026802882615, "train_loss": 0.2899373160531888, "epoch": 567}
[05:57:43.213583] [05:57:43.213669] Training epoch 567 for 0:01:45
[05:57:43.213722] [05:57:43.218166] log_dir: ./exp/debug/cifar100-LT/debug
[05:57:45.033369] Epoch: [568]  [  0/195]  eta: 0:05:53  lr: 0.000511  loss: 0.3101 (0.3101)  time: 1.8142  data: 1.3021  max mem: 9341
[05:57:55.257616] Epoch: [568]  [ 20/195]  eta: 0:01:40  lr: 0.000510  loss: 0.2891 (0.2916)  time: 0.5112  data: 0.0002  max mem: 9341
[05:58:05.465792] Epoch: [568]  [ 40/195]  eta: 0:01:24  lr: 0.000510  loss: 0.2763 (0.2871)  time: 0.5103  data: 0.0002  max mem: 9341
[05:58:15.680794] Epoch: [568]  [ 60/195]  eta: 0:01:11  lr: 0.000510  loss: 0.2840 (0.2875)  time: 0.5107  data: 0.0002  max mem: 9341
[05:58:25.941703] Epoch: [568]  [ 80/195]  eta: 0:01:00  lr: 0.000509  loss: 0.2929 (0.2876)  time: 0.5130  data: 0.0002  max mem: 9341
[05:58:36.159056] Epoch: [568]  [100/195]  eta: 0:00:49  lr: 0.000509  loss: 0.2905 (0.2890)  time: 0.5108  data: 0.0002  max mem: 9341
[05:58:46.376692] Epoch: [568]  [120/195]  eta: 0:00:39  lr: 0.000508  loss: 0.2912 (0.2893)  time: 0.5108  data: 0.0003  max mem: 9341
[05:58:56.585776] Epoch: [568]  [140/195]  eta: 0:00:28  lr: 0.000508  loss: 0.2951 (0.2899)  time: 0.5104  data: 0.0003  max mem: 9341
[05:59:06.843778] Epoch: [568]  [160/195]  eta: 0:00:18  lr: 0.000507  loss: 0.2973 (0.2905)  time: 0.5128  data: 0.0003  max mem: 9341
[05:59:17.015869] Epoch: [568]  [180/195]  eta: 0:00:07  lr: 0.000507  loss: 0.2868 (0.2908)  time: 0.5085  data: 0.0002  max mem: 9341
[05:59:24.141149] Epoch: [568]  [194/195]  eta: 0:00:00  lr: 0.000507  loss: 0.2980 (0.2910)  time: 0.5104  data: 0.0001  max mem: 9341
[05:59:24.338814] Epoch: [568] Total time: 0:01:41 (0.5186 s / it)
[05:59:24.339760] Averaged stats: lr: 0.000507  loss: 0.2980 (0.2903)
[05:59:29.039247] {"train_lr": 0.0005089414947606426, "train_loss": 0.29031871782663543, "epoch": 568}
[05:59:29.039518] [05:59:29.039625] Training epoch 568 for 0:01:45
[05:59:29.039679] [05:59:29.044277] log_dir: ./exp/debug/cifar100-LT/debug
[05:59:30.895984] Epoch: [569]  [  0/195]  eta: 0:06:00  lr: 0.000507  loss: 0.3050 (0.3050)  time: 1.8509  data: 1.3486  max mem: 9341
[05:59:41.111815] Epoch: [569]  [ 20/195]  eta: 0:01:40  lr: 0.000506  loss: 0.2859 (0.2883)  time: 0.5107  data: 0.0002  max mem: 9341
[05:59:51.321420] Epoch: [569]  [ 40/195]  eta: 0:01:24  lr: 0.000506  loss: 0.2925 (0.2897)  time: 0.5104  data: 0.0002  max mem: 9341
[06:00:01.543997] Epoch: [569]  [ 60/195]  eta: 0:01:11  lr: 0.000506  loss: 0.2962 (0.2901)  time: 0.5111  data: 0.0002  max mem: 9341
[06:00:11.802990] Epoch: [569]  [ 80/195]  eta: 0:01:00  lr: 0.000505  loss: 0.2968 (0.2910)  time: 0.5129  data: 0.0002  max mem: 9341
[06:00:22.024771] Epoch: [569]  [100/195]  eta: 0:00:49  lr: 0.000505  loss: 0.2876 (0.2914)  time: 0.5110  data: 0.0003  max mem: 9341
[06:00:32.261331] Epoch: [569]  [120/195]  eta: 0:00:39  lr: 0.000504  loss: 0.2845 (0.2905)  time: 0.5118  data: 0.0003  max mem: 9341
[06:00:42.503853] Epoch: [569]  [140/195]  eta: 0:00:28  lr: 0.000504  loss: 0.2840 (0.2907)  time: 0.5121  data: 0.0002  max mem: 9341
[06:00:52.806108] Epoch: [569]  [160/195]  eta: 0:00:18  lr: 0.000503  loss: 0.2840 (0.2901)  time: 0.5151  data: 0.0002  max mem: 9341
[06:01:02.999941] Epoch: [569]  [180/195]  eta: 0:00:07  lr: 0.000503  loss: 0.2951 (0.2908)  time: 0.5096  data: 0.0002  max mem: 9341
[06:01:10.151781] Epoch: [569]  [194/195]  eta: 0:00:00  lr: 0.000503  loss: 0.2889 (0.2905)  time: 0.5126  data: 0.0001  max mem: 9341
[06:01:10.318362] Epoch: [569] Total time: 0:01:41 (0.5194 s / it)
[06:01:10.334744] Averaged stats: lr: 0.000503  loss: 0.2889 (0.2901)
[06:01:15.035121] {"train_lr": 0.0005048921175161842, "train_loss": 0.290100895995513, "epoch": 569}
[06:01:15.035403] [06:01:15.035500] Training epoch 569 for 0:01:45
[06:01:15.035553] [06:01:15.040042] log_dir: ./exp/debug/cifar100-LT/debug
[06:01:16.796659] Epoch: [570]  [  0/195]  eta: 0:05:42  lr: 0.000503  loss: 0.2628 (0.2628)  time: 1.7555  data: 1.2677  max mem: 9341
[06:01:27.146502] Epoch: [570]  [ 20/195]  eta: 0:01:40  lr: 0.000502  loss: 0.2878 (0.2869)  time: 0.5174  data: 0.0002  max mem: 9341
[06:01:37.388003] Epoch: [570]  [ 40/195]  eta: 0:01:24  lr: 0.000502  loss: 0.3010 (0.2939)  time: 0.5120  data: 0.0003  max mem: 9341
[06:01:47.625361] Epoch: [570]  [ 60/195]  eta: 0:01:12  lr: 0.000502  loss: 0.2904 (0.2934)  time: 0.5118  data: 0.0002  max mem: 9341
[06:01:57.929793] Epoch: [570]  [ 80/195]  eta: 0:01:00  lr: 0.000501  loss: 0.2924 (0.2944)  time: 0.5152  data: 0.0002  max mem: 9341
[06:02:08.257748] Epoch: [570]  [100/195]  eta: 0:00:50  lr: 0.000501  loss: 0.2832 (0.2926)  time: 0.5163  data: 0.0002  max mem: 9341
[06:02:18.493291] Epoch: [570]  [120/195]  eta: 0:00:39  lr: 0.000500  loss: 0.2934 (0.2920)  time: 0.5117  data: 0.0002  max mem: 9341
[06:02:28.736663] Epoch: [570]  [140/195]  eta: 0:00:28  lr: 0.000500  loss: 0.2874 (0.2918)  time: 0.5121  data: 0.0002  max mem: 9341
[06:02:39.045726] Epoch: [570]  [160/195]  eta: 0:00:18  lr: 0.000499  loss: 0.2865 (0.2914)  time: 0.5154  data: 0.0002  max mem: 9341
[06:02:49.242561] Epoch: [570]  [180/195]  eta: 0:00:07  lr: 0.000499  loss: 0.2910 (0.2913)  time: 0.5098  data: 0.0001  max mem: 9341
[06:02:56.397547] Epoch: [570]  [194/195]  eta: 0:00:00  lr: 0.000499  loss: 0.2835 (0.2910)  time: 0.5127  data: 0.0001  max mem: 9341
[06:02:56.597649] Epoch: [570] Total time: 0:01:41 (0.5208 s / it)
[06:02:56.606942] Averaged stats: lr: 0.000499  loss: 0.2835 (0.2898)
[06:03:01.328910] {"train_lr": 0.0005008546177475694, "train_loss": 0.2897897115502602, "epoch": 570}
[06:03:01.329198] [06:03:01.329310] Training epoch 570 for 0:01:46
[06:03:01.329366] [06:03:01.334038] log_dir: ./exp/debug/cifar100-LT/debug
[06:03:02.985623] Epoch: [571]  [  0/195]  eta: 0:05:21  lr: 0.000499  loss: 0.2901 (0.2901)  time: 1.6498  data: 1.1520  max mem: 9341
[06:03:13.235842] Epoch: [571]  [ 20/195]  eta: 0:01:39  lr: 0.000498  loss: 0.2832 (0.2853)  time: 0.5124  data: 0.0003  max mem: 9341
[06:03:23.456772] Epoch: [571]  [ 40/195]  eta: 0:01:23  lr: 0.000498  loss: 0.2781 (0.2843)  time: 0.5110  data: 0.0002  max mem: 9341
[06:03:33.675418] Epoch: [571]  [ 60/195]  eta: 0:01:11  lr: 0.000498  loss: 0.2864 (0.2871)  time: 0.5109  data: 0.0002  max mem: 9341
[06:03:43.930847] Epoch: [571]  [ 80/195]  eta: 0:01:00  lr: 0.000497  loss: 0.2852 (0.2874)  time: 0.5127  data: 0.0002  max mem: 9341
[06:03:54.147432] Epoch: [571]  [100/195]  eta: 0:00:49  lr: 0.000497  loss: 0.2900 (0.2885)  time: 0.5108  data: 0.0002  max mem: 9341
[06:04:04.355070] Epoch: [571]  [120/195]  eta: 0:00:39  lr: 0.000496  loss: 0.2891 (0.2881)  time: 0.5103  data: 0.0002  max mem: 9341
[06:04:14.564814] Epoch: [571]  [140/195]  eta: 0:00:28  lr: 0.000496  loss: 0.2753 (0.2871)  time: 0.5104  data: 0.0002  max mem: 9341
[06:04:24.825584] Epoch: [571]  [160/195]  eta: 0:00:18  lr: 0.000495  loss: 0.2814 (0.2870)  time: 0.5130  data: 0.0002  max mem: 9341
[06:04:35.004230] Epoch: [571]  [180/195]  eta: 0:00:07  lr: 0.000495  loss: 0.2782 (0.2867)  time: 0.5089  data: 0.0001  max mem: 9341
[06:04:42.138416] Epoch: [571]  [194/195]  eta: 0:00:00  lr: 0.000495  loss: 0.2998 (0.2875)  time: 0.5107  data: 0.0001  max mem: 9341
[06:04:42.321281] Epoch: [571] Total time: 0:01:40 (0.5179 s / it)
[06:04:42.321985] Averaged stats: lr: 0.000495  loss: 0.2998 (0.2881)
[06:04:47.050814] {"train_lr": 0.0004968290644445281, "train_loss": 0.2880966932918781, "epoch": 571}
[06:04:47.051086] [06:04:47.051194] Training epoch 571 for 0:01:45
[06:04:47.051259] [06:04:47.055736] log_dir: ./exp/debug/cifar100-LT/debug
[06:04:48.734664] Epoch: [572]  [  0/195]  eta: 0:05:27  lr: 0.000495  loss: 0.3139 (0.3139)  time: 1.6780  data: 1.1847  max mem: 9341
[06:04:58.954072] Epoch: [572]  [ 20/195]  eta: 0:01:39  lr: 0.000494  loss: 0.2941 (0.2969)  time: 0.5109  data: 0.0002  max mem: 9341
[06:05:09.173601] Epoch: [572]  [ 40/195]  eta: 0:01:23  lr: 0.000494  loss: 0.2915 (0.2949)  time: 0.5109  data: 0.0002  max mem: 9341
[06:05:19.391359] Epoch: [572]  [ 60/195]  eta: 0:01:11  lr: 0.000494  loss: 0.2856 (0.2926)  time: 0.5108  data: 0.0002  max mem: 9341
[06:05:29.646910] Epoch: [572]  [ 80/195]  eta: 0:01:00  lr: 0.000493  loss: 0.2875 (0.2927)  time: 0.5127  data: 0.0002  max mem: 9341
[06:05:39.859938] Epoch: [572]  [100/195]  eta: 0:00:49  lr: 0.000493  loss: 0.2881 (0.2923)  time: 0.5106  data: 0.0002  max mem: 9341
[06:05:50.094920] Epoch: [572]  [120/195]  eta: 0:00:39  lr: 0.000492  loss: 0.2889 (0.2923)  time: 0.5117  data: 0.0002  max mem: 9341
[06:06:00.323004] Epoch: [572]  [140/195]  eta: 0:00:28  lr: 0.000492  loss: 0.2822 (0.2914)  time: 0.5113  data: 0.0002  max mem: 9341
[06:06:10.619232] Epoch: [572]  [160/195]  eta: 0:00:18  lr: 0.000491  loss: 0.2924 (0.2916)  time: 0.5148  data: 0.0002  max mem: 9341
[06:06:20.806366] Epoch: [572]  [180/195]  eta: 0:00:07  lr: 0.000491  loss: 0.2902 (0.2923)  time: 0.5093  data: 0.0001  max mem: 9341
[06:06:27.953533] Epoch: [572]  [194/195]  eta: 0:00:00  lr: 0.000491  loss: 0.2852 (0.2915)  time: 0.5124  data: 0.0001  max mem: 9341
[06:06:28.134795] Epoch: [572] Total time: 0:01:41 (0.5184 s / it)
[06:06:28.141514] Averaged stats: lr: 0.000491  loss: 0.2852 (0.2890)
[06:06:32.830845] {"train_lr": 0.0004928155263926627, "train_loss": 0.2890108570456505, "epoch": 572}
[06:06:32.831131] [06:06:32.831221] Training epoch 572 for 0:01:45
[06:06:32.831275] [06:06:32.835889] log_dir: ./exp/debug/cifar100-LT/debug
[06:06:34.629154] Epoch: [573]  [  0/195]  eta: 0:05:49  lr: 0.000491  loss: 0.3081 (0.3081)  time: 1.7925  data: 1.2867  max mem: 9341
[06:06:44.865063] Epoch: [573]  [ 20/195]  eta: 0:01:40  lr: 0.000490  loss: 0.2864 (0.2878)  time: 0.5117  data: 0.0002  max mem: 9341
[06:06:55.072090] Epoch: [573]  [ 40/195]  eta: 0:01:24  lr: 0.000490  loss: 0.2876 (0.2910)  time: 0.5103  data: 0.0002  max mem: 9341
[06:07:05.279989] Epoch: [573]  [ 60/195]  eta: 0:01:11  lr: 0.000490  loss: 0.2818 (0.2896)  time: 0.5103  data: 0.0002  max mem: 9341
[06:07:15.532639] Epoch: [573]  [ 80/195]  eta: 0:01:00  lr: 0.000489  loss: 0.2833 (0.2888)  time: 0.5126  data: 0.0002  max mem: 9341
[06:07:25.744842] Epoch: [573]  [100/195]  eta: 0:00:49  lr: 0.000489  loss: 0.2903 (0.2890)  time: 0.5106  data: 0.0002  max mem: 9341
[06:07:35.953313] Epoch: [573]  [120/195]  eta: 0:00:39  lr: 0.000488  loss: 0.2873 (0.2889)  time: 0.5104  data: 0.0002  max mem: 9341
[06:07:46.162025] Epoch: [573]  [140/195]  eta: 0:00:28  lr: 0.000488  loss: 0.2833 (0.2886)  time: 0.5104  data: 0.0002  max mem: 9341
[06:07:56.415039] Epoch: [573]  [160/195]  eta: 0:00:18  lr: 0.000487  loss: 0.2939 (0.2892)  time: 0.5126  data: 0.0002  max mem: 9341
[06:08:06.583509] Epoch: [573]  [180/195]  eta: 0:00:07  lr: 0.000487  loss: 0.2925 (0.2893)  time: 0.5084  data: 0.0001  max mem: 9341
[06:08:13.710552] Epoch: [573]  [194/195]  eta: 0:00:00  lr: 0.000487  loss: 0.2928 (0.2893)  time: 0.5103  data: 0.0001  max mem: 9341
[06:08:13.877766] Epoch: [573] Total time: 0:01:41 (0.5182 s / it)
[06:08:13.890690] Averaged stats: lr: 0.000487  loss: 0.2928 (0.2878)
[06:08:18.538403] {"train_lr": 0.000488814072172261, "train_loss": 0.2878356200953325, "epoch": 573}
[06:08:18.538670] [06:08:18.538754] Training epoch 573 for 0:01:45
[06:08:18.538807] [06:08:18.543327] log_dir: ./exp/debug/cifar100-LT/debug
[06:08:20.236778] Epoch: [574]  [  0/195]  eta: 0:05:29  lr: 0.000487  loss: 0.3088 (0.3088)  time: 1.6921  data: 1.1829  max mem: 9341
[06:08:30.495320] Epoch: [574]  [ 20/195]  eta: 0:01:39  lr: 0.000486  loss: 0.2879 (0.2976)  time: 0.5129  data: 0.0002  max mem: 9341
[06:08:40.712311] Epoch: [574]  [ 40/195]  eta: 0:01:23  lr: 0.000486  loss: 0.2916 (0.2954)  time: 0.5108  data: 0.0002  max mem: 9341
[06:08:50.930926] Epoch: [574]  [ 60/195]  eta: 0:01:11  lr: 0.000486  loss: 0.2894 (0.2926)  time: 0.5109  data: 0.0002  max mem: 9341
[06:09:01.231477] Epoch: [574]  [ 80/195]  eta: 0:01:00  lr: 0.000485  loss: 0.2876 (0.2927)  time: 0.5150  data: 0.0002  max mem: 9341
[06:09:11.460201] Epoch: [574]  [100/195]  eta: 0:00:49  lr: 0.000485  loss: 0.2847 (0.2903)  time: 0.5114  data: 0.0002  max mem: 9341
[06:09:21.682935] Epoch: [574]  [120/195]  eta: 0:00:39  lr: 0.000484  loss: 0.2963 (0.2910)  time: 0.5111  data: 0.0002  max mem: 9341
[06:09:31.898682] Epoch: [574]  [140/195]  eta: 0:00:28  lr: 0.000484  loss: 0.2911 (0.2906)  time: 0.5107  data: 0.0002  max mem: 9341
[06:09:42.160019] Epoch: [574]  [160/195]  eta: 0:00:18  lr: 0.000483  loss: 0.2838 (0.2896)  time: 0.5130  data: 0.0002  max mem: 9341
[06:09:52.337534] Epoch: [574]  [180/195]  eta: 0:00:07  lr: 0.000483  loss: 0.2888 (0.2897)  time: 0.5088  data: 0.0001  max mem: 9341
[06:09:59.477175] Epoch: [574]  [194/195]  eta: 0:00:00  lr: 0.000483  loss: 0.2858 (0.2894)  time: 0.5111  data: 0.0001  max mem: 9341
[06:09:59.646320] Epoch: [574] Total time: 0:01:41 (0.5185 s / it)
[06:09:59.662109] Averaged stats: lr: 0.000483  loss: 0.2858 (0.2891)
[06:10:04.286428] {"train_lr": 0.00048482477015714, "train_loss": 0.2891045629596099, "epoch": 574}
[06:10:04.286699] [06:10:04.286804] Training epoch 574 for 0:01:45
[06:10:04.286859] [06:10:04.291353] log_dir: ./exp/debug/cifar100-LT/debug
[06:10:05.826163] Epoch: [575]  [  0/195]  eta: 0:04:59  lr: 0.000483  loss: 0.3050 (0.3050)  time: 1.5338  data: 1.0169  max mem: 9341
[06:10:16.062451] Epoch: [575]  [ 20/195]  eta: 0:01:38  lr: 0.000482  loss: 0.2882 (0.2854)  time: 0.5118  data: 0.0002  max mem: 9341
[06:10:26.298375] Epoch: [575]  [ 40/195]  eta: 0:01:23  lr: 0.000482  loss: 0.2790 (0.2810)  time: 0.5117  data: 0.0002  max mem: 9341
[06:10:36.534326] Epoch: [575]  [ 60/195]  eta: 0:01:11  lr: 0.000482  loss: 0.2859 (0.2821)  time: 0.5117  data: 0.0002  max mem: 9341
[06:10:46.819845] Epoch: [575]  [ 80/195]  eta: 0:01:00  lr: 0.000481  loss: 0.2882 (0.2847)  time: 0.5142  data: 0.0002  max mem: 9341
[06:10:57.042269] Epoch: [575]  [100/195]  eta: 0:00:49  lr: 0.000481  loss: 0.2825 (0.2842)  time: 0.5111  data: 0.0002  max mem: 9341
[06:11:07.257239] Epoch: [575]  [120/195]  eta: 0:00:39  lr: 0.000480  loss: 0.2863 (0.2849)  time: 0.5107  data: 0.0002  max mem: 9341
[06:11:17.466836] Epoch: [575]  [140/195]  eta: 0:00:28  lr: 0.000480  loss: 0.2816 (0.2859)  time: 0.5104  data: 0.0002  max mem: 9341
[06:11:27.725330] Epoch: [575]  [160/195]  eta: 0:00:18  lr: 0.000479  loss: 0.2818 (0.2854)  time: 0.5129  data: 0.0002  max mem: 9341
[06:11:37.898306] Epoch: [575]  [180/195]  eta: 0:00:07  lr: 0.000479  loss: 0.2911 (0.2863)  time: 0.5086  data: 0.0001  max mem: 9341
[06:11:45.034390] Epoch: [575]  [194/195]  eta: 0:00:00  lr: 0.000479  loss: 0.2863 (0.2870)  time: 0.5109  data: 0.0001  max mem: 9341
[06:11:45.205681] Epoch: [575] Total time: 0:01:40 (0.5175 s / it)
[06:11:45.221376] Averaged stats: lr: 0.000479  loss: 0.2863 (0.2878)
[06:11:49.962392] {"train_lr": 0.00048084768851345904, "train_loss": 0.28775959748488206, "epoch": 575}
[06:11:49.962753] [06:11:49.962840] Training epoch 575 for 0:01:45
[06:11:49.962894] [06:11:49.967344] log_dir: ./exp/debug/cifar100-LT/debug
[06:11:51.884235] Epoch: [576]  [  0/195]  eta: 0:06:13  lr: 0.000479  loss: 0.2760 (0.2760)  time: 1.9156  data: 1.4229  max mem: 9341
[06:12:02.122689] Epoch: [576]  [ 20/195]  eta: 0:01:41  lr: 0.000478  loss: 0.2869 (0.2918)  time: 0.5119  data: 0.0002  max mem: 9341
[06:12:12.367081] Epoch: [576]  [ 40/195]  eta: 0:01:24  lr: 0.000478  loss: 0.2899 (0.2917)  time: 0.5122  data: 0.0002  max mem: 9341
[06:12:22.613568] Epoch: [576]  [ 60/195]  eta: 0:01:12  lr: 0.000478  loss: 0.2822 (0.2916)  time: 0.5123  data: 0.0002  max mem: 9341
[06:12:32.923938] Epoch: [576]  [ 80/195]  eta: 0:01:00  lr: 0.000477  loss: 0.2837 (0.2919)  time: 0.5155  data: 0.0002  max mem: 9341
[06:12:43.160550] Epoch: [576]  [100/195]  eta: 0:00:50  lr: 0.000477  loss: 0.2908 (0.2916)  time: 0.5118  data: 0.0002  max mem: 9341
[06:12:53.398313] Epoch: [576]  [120/195]  eta: 0:00:39  lr: 0.000476  loss: 0.2908 (0.2916)  time: 0.5118  data: 0.0002  max mem: 9341
[06:13:03.643202] Epoch: [576]  [140/195]  eta: 0:00:28  lr: 0.000476  loss: 0.3054 (0.2936)  time: 0.5122  data: 0.0002  max mem: 9341
[06:13:13.947791] Epoch: [576]  [160/195]  eta: 0:00:18  lr: 0.000475  loss: 0.2719 (0.2915)  time: 0.5152  data: 0.0002  max mem: 9341
[06:13:24.146857] Epoch: [576]  [180/195]  eta: 0:00:07  lr: 0.000475  loss: 0.2857 (0.2908)  time: 0.5099  data: 0.0001  max mem: 9341
[06:13:31.315527] Epoch: [576]  [194/195]  eta: 0:00:00  lr: 0.000475  loss: 0.2885 (0.2904)  time: 0.5137  data: 0.0001  max mem: 9341
[06:13:31.486970] Epoch: [576] Total time: 0:01:41 (0.5206 s / it)
[06:13:31.491036] Averaged stats: lr: 0.000475  loss: 0.2885 (0.2888)
[06:13:36.225022] {"train_lr": 0.0004768828951985739, "train_loss": 0.2888268555586155, "epoch": 576}
[06:13:36.225287] [06:13:36.225372] Training epoch 576 for 0:01:46
[06:13:36.225425] [06:13:36.229836] log_dir: ./exp/debug/cifar100-LT/debug
[06:13:37.971269] Epoch: [577]  [  0/195]  eta: 0:05:39  lr: 0.000475  loss: 0.2729 (0.2729)  time: 1.7405  data: 1.2237  max mem: 9341
[06:13:48.191199] Epoch: [577]  [ 20/195]  eta: 0:01:39  lr: 0.000474  loss: 0.2878 (0.2933)  time: 0.5109  data: 0.0002  max mem: 9341
[06:13:58.405804] Epoch: [577]  [ 40/195]  eta: 0:01:23  lr: 0.000474  loss: 0.2888 (0.2916)  time: 0.5107  data: 0.0002  max mem: 9341
[06:14:08.625378] Epoch: [577]  [ 60/195]  eta: 0:01:11  lr: 0.000474  loss: 0.2827 (0.2893)  time: 0.5109  data: 0.0002  max mem: 9341
[06:14:18.884505] Epoch: [577]  [ 80/195]  eta: 0:01:00  lr: 0.000473  loss: 0.2868 (0.2902)  time: 0.5129  data: 0.0002  max mem: 9341
[06:14:29.093946] Epoch: [577]  [100/195]  eta: 0:00:49  lr: 0.000473  loss: 0.2718 (0.2877)  time: 0.5104  data: 0.0002  max mem: 9341
[06:14:39.308154] Epoch: [577]  [120/195]  eta: 0:00:39  lr: 0.000472  loss: 0.2830 (0.2872)  time: 0.5106  data: 0.0002  max mem: 9341
[06:14:49.517621] Epoch: [577]  [140/195]  eta: 0:00:28  lr: 0.000472  loss: 0.2788 (0.2863)  time: 0.5104  data: 0.0002  max mem: 9341
[06:14:59.771286] Epoch: [577]  [160/195]  eta: 0:00:18  lr: 0.000472  loss: 0.2735 (0.2853)  time: 0.5126  data: 0.0002  max mem: 9341
[06:15:09.950200] Epoch: [577]  [180/195]  eta: 0:00:07  lr: 0.000471  loss: 0.2844 (0.2848)  time: 0.5089  data: 0.0001  max mem: 9341
[06:15:17.084516] Epoch: [577]  [194/195]  eta: 0:00:00  lr: 0.000471  loss: 0.2785 (0.2851)  time: 0.5107  data: 0.0001  max mem: 9341
[06:15:17.263288] Epoch: [577] Total time: 0:01:41 (0.5181 s / it)
[06:15:17.276183] Averaged stats: lr: 0.000471  loss: 0.2785 (0.2860)
[06:15:21.908282] {"train_lr": 0.00047293045795986126, "train_loss": 0.286034459907275, "epoch": 577}
[06:15:21.908662] [06:15:21.908751] Training epoch 577 for 0:01:45
[06:15:21.908804] [06:15:21.913366] log_dir: ./exp/debug/cifar100-LT/debug
[06:15:23.682176] Epoch: [578]  [  0/195]  eta: 0:05:44  lr: 0.000471  loss: 0.3143 (0.3143)  time: 1.7674  data: 1.2817  max mem: 9341
[06:15:33.889278] Epoch: [578]  [ 20/195]  eta: 0:01:39  lr: 0.000470  loss: 0.2750 (0.2823)  time: 0.5103  data: 0.0002  max mem: 9341
[06:15:44.099867] Epoch: [578]  [ 40/195]  eta: 0:01:23  lr: 0.000470  loss: 0.2839 (0.2834)  time: 0.5105  data: 0.0002  max mem: 9341
[06:15:54.313854] Epoch: [578]  [ 60/195]  eta: 0:01:11  lr: 0.000470  loss: 0.2803 (0.2845)  time: 0.5106  data: 0.0002  max mem: 9341
[06:16:04.572675] Epoch: [578]  [ 80/195]  eta: 0:01:00  lr: 0.000469  loss: 0.2843 (0.2850)  time: 0.5129  data: 0.0002  max mem: 9341
[06:16:14.785705] Epoch: [578]  [100/195]  eta: 0:00:49  lr: 0.000469  loss: 0.2722 (0.2832)  time: 0.5106  data: 0.0002  max mem: 9341
[06:16:24.993468] Epoch: [578]  [120/195]  eta: 0:00:39  lr: 0.000469  loss: 0.2768 (0.2833)  time: 0.5103  data: 0.0002  max mem: 9341
[06:16:35.208218] Epoch: [578]  [140/195]  eta: 0:00:28  lr: 0.000468  loss: 0.2950 (0.2858)  time: 0.5107  data: 0.0002  max mem: 9341
[06:16:45.460431] Epoch: [578]  [160/195]  eta: 0:00:18  lr: 0.000468  loss: 0.2834 (0.2861)  time: 0.5126  data: 0.0002  max mem: 9341
[06:16:55.630558] Epoch: [578]  [180/195]  eta: 0:00:07  lr: 0.000467  loss: 0.2885 (0.2866)  time: 0.5085  data: 0.0001  max mem: 9341
[06:17:02.757841] Epoch: [578]  [194/195]  eta: 0:00:00  lr: 0.000467  loss: 0.2802 (0.2864)  time: 0.5103  data: 0.0001  max mem: 9341
[06:17:02.925852] Epoch: [578] Total time: 0:01:41 (0.5180 s / it)
[06:17:02.930625] Averaged stats: lr: 0.000467  loss: 0.2802 (0.2883)
[06:17:07.608405] {"train_lr": 0.00046899044433356893, "train_loss": 0.2882926490635444, "epoch": 578}
[06:17:07.608769] [06:17:07.608859] Training epoch 578 for 0:01:45
[06:17:07.608913] [06:17:07.613335] log_dir: ./exp/debug/cifar100-LT/debug
[06:17:09.428314] Epoch: [579]  [  0/195]  eta: 0:05:53  lr: 0.000467  loss: 0.2846 (0.2846)  time: 1.8135  data: 1.3023  max mem: 9341
[06:17:19.641037] Epoch: [579]  [ 20/195]  eta: 0:01:40  lr: 0.000467  loss: 0.2944 (0.2944)  time: 0.5106  data: 0.0002  max mem: 9341
[06:17:29.847512] Epoch: [579]  [ 40/195]  eta: 0:01:24  lr: 0.000466  loss: 0.2884 (0.2944)  time: 0.5102  data: 0.0002  max mem: 9341
[06:17:40.072955] Epoch: [579]  [ 60/195]  eta: 0:01:11  lr: 0.000466  loss: 0.2848 (0.2929)  time: 0.5112  data: 0.0002  max mem: 9341
[06:17:50.369714] Epoch: [579]  [ 80/195]  eta: 0:01:00  lr: 0.000465  loss: 0.2823 (0.2907)  time: 0.5148  data: 0.0002  max mem: 9341
[06:18:00.600477] Epoch: [579]  [100/195]  eta: 0:00:49  lr: 0.000465  loss: 0.2872 (0.2909)  time: 0.5115  data: 0.0002  max mem: 9341
[06:18:10.832968] Epoch: [579]  [120/195]  eta: 0:00:39  lr: 0.000465  loss: 0.2914 (0.2911)  time: 0.5116  data: 0.0002  max mem: 9341
[06:18:21.068888] Epoch: [579]  [140/195]  eta: 0:00:28  lr: 0.000464  loss: 0.2862 (0.2907)  time: 0.5117  data: 0.0002  max mem: 9341
[06:18:31.363523] Epoch: [579]  [160/195]  eta: 0:00:18  lr: 0.000464  loss: 0.2960 (0.2908)  time: 0.5147  data: 0.0002  max mem: 9341
[06:18:41.554310] Epoch: [579]  [180/195]  eta: 0:00:07  lr: 0.000463  loss: 0.2848 (0.2904)  time: 0.5095  data: 0.0001  max mem: 9341
[06:18:48.698765] Epoch: [579]  [194/195]  eta: 0:00:00  lr: 0.000463  loss: 0.2944 (0.2903)  time: 0.5122  data: 0.0001  max mem: 9341
[06:18:48.847616] Epoch: [579] Total time: 0:01:41 (0.5191 s / it)
[06:18:48.881159] Averaged stats: lr: 0.000463  loss: 0.2944 (0.2892)
[06:18:53.595999] {"train_lr": 0.0004650629216436595, "train_loss": 0.28920875917642547, "epoch": 579}
[06:18:53.596303] [06:18:53.596397] Training epoch 579 for 0:01:45
[06:18:53.596452] [06:18:53.600909] log_dir: ./exp/debug/cifar100-LT/debug
[06:18:55.475053] Epoch: [580]  [  0/195]  eta: 0:06:05  lr: 0.000463  loss: 0.3187 (0.3187)  time: 1.8729  data: 1.3794  max mem: 9341
[06:19:05.687619] Epoch: [580]  [ 20/195]  eta: 0:01:40  lr: 0.000463  loss: 0.2755 (0.2798)  time: 0.5106  data: 0.0002  max mem: 9341
[06:19:15.907015] Epoch: [580]  [ 40/195]  eta: 0:01:24  lr: 0.000462  loss: 0.2826 (0.2819)  time: 0.5109  data: 0.0002  max mem: 9341
[06:19:26.128976] Epoch: [580]  [ 60/195]  eta: 0:01:11  lr: 0.000462  loss: 0.2909 (0.2835)  time: 0.5110  data: 0.0003  max mem: 9341
[06:19:36.388437] Epoch: [580]  [ 80/195]  eta: 0:01:00  lr: 0.000461  loss: 0.2836 (0.2849)  time: 0.5129  data: 0.0003  max mem: 9341
[06:19:46.605111] Epoch: [580]  [100/195]  eta: 0:00:49  lr: 0.000461  loss: 0.2839 (0.2849)  time: 0.5108  data: 0.0003  max mem: 9341
[06:19:56.826566] Epoch: [580]  [120/195]  eta: 0:00:39  lr: 0.000461  loss: 0.2853 (0.2866)  time: 0.5110  data: 0.0003  max mem: 9341
[06:20:07.038741] Epoch: [580]  [140/195]  eta: 0:00:28  lr: 0.000460  loss: 0.2916 (0.2871)  time: 0.5105  data: 0.0003  max mem: 9341
[06:20:17.298784] Epoch: [580]  [160/195]  eta: 0:00:18  lr: 0.000460  loss: 0.2742 (0.2865)  time: 0.5129  data: 0.0003  max mem: 9341
[06:20:27.482760] Epoch: [580]  [180/195]  eta: 0:00:07  lr: 0.000459  loss: 0.2756 (0.2856)  time: 0.5092  data: 0.0002  max mem: 9341
[06:20:34.622842] Epoch: [580]  [194/195]  eta: 0:00:00  lr: 0.000459  loss: 0.2785 (0.2849)  time: 0.5112  data: 0.0001  max mem: 9341
[06:20:34.801101] Epoch: [580] Total time: 0:01:41 (0.5190 s / it)
[06:20:34.810723] Averaged stats: lr: 0.000459  loss: 0.2785 (0.2863)
[06:20:39.481316] {"train_lr": 0.00046114795700065805, "train_loss": 0.2863404453182832, "epoch": 580}
[06:20:39.481666] [06:20:39.481773] Training epoch 580 for 0:01:45
[06:20:39.481842] [06:20:39.486381] log_dir: ./exp/debug/cifar100-LT/debug
[06:20:41.269243] Epoch: [581]  [  0/195]  eta: 0:05:47  lr: 0.000459  loss: 0.2916 (0.2916)  time: 1.7815  data: 1.2716  max mem: 9341
[06:20:51.492295] Epoch: [581]  [ 20/195]  eta: 0:01:40  lr: 0.000459  loss: 0.2863 (0.2920)  time: 0.5111  data: 0.0002  max mem: 9341
[06:21:01.708805] Epoch: [581]  [ 40/195]  eta: 0:01:23  lr: 0.000458  loss: 0.2960 (0.2944)  time: 0.5108  data: 0.0002  max mem: 9341
[06:21:11.922684] Epoch: [581]  [ 60/195]  eta: 0:01:11  lr: 0.000458  loss: 0.2851 (0.2919)  time: 0.5106  data: 0.0002  max mem: 9341
[06:21:22.186179] Epoch: [581]  [ 80/195]  eta: 0:01:00  lr: 0.000457  loss: 0.2780 (0.2905)  time: 0.5131  data: 0.0002  max mem: 9341
[06:21:32.410739] Epoch: [581]  [100/195]  eta: 0:00:49  lr: 0.000457  loss: 0.2805 (0.2892)  time: 0.5112  data: 0.0002  max mem: 9341
[06:21:42.644749] Epoch: [581]  [120/195]  eta: 0:00:39  lr: 0.000457  loss: 0.2777 (0.2866)  time: 0.5116  data: 0.0002  max mem: 9341
[06:21:52.880124] Epoch: [581]  [140/195]  eta: 0:00:28  lr: 0.000456  loss: 0.2794 (0.2863)  time: 0.5117  data: 0.0002  max mem: 9341
[06:22:03.158806] Epoch: [581]  [160/195]  eta: 0:00:18  lr: 0.000456  loss: 0.2894 (0.2864)  time: 0.5139  data: 0.0002  max mem: 9341
[06:22:13.356996] Epoch: [581]  [180/195]  eta: 0:00:07  lr: 0.000456  loss: 0.2914 (0.2870)  time: 0.5099  data: 0.0001  max mem: 9341
[06:22:20.513567] Epoch: [581]  [194/195]  eta: 0:00:00  lr: 0.000455  loss: 0.2821 (0.2869)  time: 0.5122  data: 0.0001  max mem: 9341
[06:22:20.675448] Epoch: [581] Total time: 0:01:41 (0.5189 s / it)
[06:22:20.697070] Averaged stats: lr: 0.000455  loss: 0.2821 (0.2872)
[06:22:25.485488] {"train_lr": 0.0004572456173005104, "train_loss": 0.2871862977170027, "epoch": 581}
[06:22:25.485781] [06:22:25.485873] Training epoch 581 for 0:01:46
[06:22:25.485932] [06:22:25.490448] log_dir: ./exp/debug/cifar100-LT/debug
[06:22:27.115080] Epoch: [582]  [  0/195]  eta: 0:05:16  lr: 0.000455  loss: 0.3349 (0.3349)  time: 1.6238  data: 1.1079  max mem: 9341
[06:22:37.341744] Epoch: [582]  [ 20/195]  eta: 0:01:38  lr: 0.000455  loss: 0.2821 (0.2854)  time: 0.5113  data: 0.0002  max mem: 9341
[06:22:47.565438] Epoch: [582]  [ 40/195]  eta: 0:01:23  lr: 0.000455  loss: 0.2923 (0.2864)  time: 0.5111  data: 0.0002  max mem: 9341
[06:22:57.787348] Epoch: [582]  [ 60/195]  eta: 0:01:11  lr: 0.000454  loss: 0.2801 (0.2849)  time: 0.5110  data: 0.0002  max mem: 9341
[06:23:08.050935] Epoch: [582]  [ 80/195]  eta: 0:01:00  lr: 0.000454  loss: 0.2911 (0.2863)  time: 0.5131  data: 0.0002  max mem: 9341
[06:23:18.271926] Epoch: [582]  [100/195]  eta: 0:00:49  lr: 0.000453  loss: 0.2906 (0.2872)  time: 0.5110  data: 0.0002  max mem: 9341
[06:23:28.502257] Epoch: [582]  [120/195]  eta: 0:00:39  lr: 0.000453  loss: 0.2938 (0.2881)  time: 0.5115  data: 0.0002  max mem: 9341
[06:23:38.743688] Epoch: [582]  [140/195]  eta: 0:00:28  lr: 0.000453  loss: 0.2845 (0.2878)  time: 0.5120  data: 0.0002  max mem: 9341
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 25543 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 25545 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 25546 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 1 (pid: 25544) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
./main_pretrain.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_06:23:40
  host      : user
  rank      : 1 (local_rank: 1)
  exitcode  : -9 (pid: 25544)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 25544
======================================================
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 1): env://, gpu 1
[06:25:22.575723] job dir: /home/vision/wonjun/LiVT-main
[06:25:22.575831] [06:25:22.575985] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=8,
adamW2=0.999,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt/debug/cifar100-LT/vit_base_patch16/debug',
clip_grad=None,
color_jitter=None,
cutmix=0.0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=100,
eval=False,
finetune='./ckpt/debug/cifar100-LT/debug/checkpoint.pth',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.65,
local_rank=0,
log_dir='./exp/debug/cifar100-LT/vit_base_patch16/debug',
loss='CE',
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=1,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=4)
[06:25:22.576048] [06:25:22.900478] Files already downloaded and verified
[06:25:23.648483] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[06:25:23.988988] Files already downloaded and verified
[06:25:24.354130] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[06:25:24.354794] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f8d0e6bded0>
[06:25:24.354914] [06:25:24.359107] Train on 10847 Image w.r.t. 100 classes
[06:25:24.359183] [06:25:27.057200] Load pre-trained checkpoint from: ./ckpt/debug/cifar100-LT/debug/checkpoint.pth
[06:25:27.057704] [06:25:27.131501] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.qkv.scaling_factor', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.attn.proj.scaling_factor', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc1.scaling_factor', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.0.mlp.fc2.scaling_factor', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.qkv.scaling_factor', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.attn.proj.scaling_factor', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc1.scaling_factor', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.1.mlp.fc2.scaling_factor', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.qkv.scaling_factor', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.attn.proj.scaling_factor', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc1.scaling_factor', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.2.mlp.fc2.scaling_factor', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.qkv.scaling_factor', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.attn.proj.scaling_factor', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc1.scaling_factor', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.3.mlp.fc2.scaling_factor', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.qkv.scaling_factor', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.attn.proj.scaling_factor', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc1.scaling_factor', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.4.mlp.fc2.scaling_factor', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.qkv.scaling_factor', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.attn.proj.scaling_factor', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc1.scaling_factor', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.5.mlp.fc2.scaling_factor', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.qkv.scaling_factor', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.attn.proj.scaling_factor', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc1.scaling_factor', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.6.mlp.fc2.scaling_factor', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.qkv.scaling_factor', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.attn.proj.scaling_factor', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc1.scaling_factor', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_blocks.7.mlp.fc2.scaling_factor', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[06:25:27.131862] [06:25:27.190266] Model = vit_base_patch16
[06:25:27.190450] [06:25:27.190531] number of params (M): 85.96
[06:25:27.190583] [06:25:27.190628] base lr: 1.00e-03
[06:25:27.190691] [06:25:27.190728] actual lr: 8.00e-03
[06:25:27.190772] [06:25:27.190822] accumulate grad iterations: 8
[06:25:27.190865] [06:25:27.190898] effective batch size: 2048
[06:25:27.190942] [06:25:27.560805] criterion = CrossEntropyLoss()
[06:25:27.560924] [06:25:27.561280] Save config to: ./exp/debug/cifar100-LT/vit_base_patch16/debug/args.txt
[06:25:27.561324] Start training for 100 epochs
[06:25:27.561377] [06:25:27.562918] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:25:32.424189] Epoch: [0]  [ 0/42]  eta: 0:03:24  lr: 0.000000  loss: 4.6055 (4.6055)  time: 4.8602  data: 2.5248  max mem: 9052
[06:25:52.297489] Epoch: [0]  [41/42]  eta: 0:00:00  lr: 0.001524  loss: 4.4510 (4.5333)  time: 0.4852  data: 0.0001  max mem: 10039
[06:25:52.497820] Epoch: [0] Total time: 0:00:24 (0.5937 s / it)
[06:25:52.498824] Averaged stats: lr: 0.001524  loss: 4.4510 (4.5367)
[06:25:54.427443] Test:  [ 0/40]  eta: 0:01:17  loss: 4.6289 (4.6289)  acc1: 6.2500 (6.2500)  acc5: 12.5000 (12.5000)  time: 1.9254  data: 1.7557  max mem: 10039
[06:26:00.392386] Test:  [39/40]  eta: 0:00:00  loss: 4.6011 (4.6249)  acc1: 4.6875 (4.9200)  acc5: 14.0625 (12.7600)  time: 0.1516  data: 0.0001  max mem: 10039
[06:26:00.520783] Test: Total time: 0:00:08 (0.2005 s / it)
[06:26:00.696004] * Acc@1 4.670 Acc@5 12.470 loss 4.618
[06:26:00.696227] Accuracy of the network on the 10000 test images: 4.7%
[06:26:00.696429] [06:26:04.090019] Max accuracy: 4.67%
[06:26:04.090325] [06:26:04.091496] {"train_lr": 0.0006530612244897961, "train_loss": 4.536651792980376, "test_loss": 4.617624664306641, "test_acc1": 4.67, "test_acc5": 12.47, "epoch": 0, "n_parameters": 85958500}
[06:26:04.091572] [06:26:04.091635] Training epoch 0 for 0:00:36
[06:26:04.091688] [06:26:04.094808] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:26:06.129253] Epoch: [1]  [ 0/42]  eta: 0:01:25  lr: 0.001600  loss: 4.2594 (4.2594)  time: 2.0335  data: 1.5573  max mem: 10040
[06:26:26.080752] Epoch: [1]  [41/42]  eta: 0:00:00  lr: 0.003124  loss: 3.9710 (4.0383)  time: 0.4877  data: 0.0001  max mem: 10046
[06:26:26.338651] Epoch: [1] Total time: 0:00:22 (0.5296 s / it)
[06:26:26.353142] Averaged stats: lr: 0.003124  loss: 3.9710 (4.0493)
[06:26:28.301651] Test:  [ 0/40]  eta: 0:01:17  loss: 5.7382 (5.7382)  acc1: 3.1250 (3.1250)  acc5: 10.9375 (10.9375)  time: 1.9448  data: 1.7662  max mem: 10046
[06:26:34.232591] Test:  [39/40]  eta: 0:00:00  loss: 5.4823 (5.5751)  acc1: 1.5625 (2.4400)  acc5: 9.3750 (9.6000)  time: 0.1483  data: 0.0001  max mem: 10046
[06:26:34.380425] Test: Total time: 0:00:08 (0.2006 s / it)
[06:26:34.476135] * Acc@1 2.330 Acc@5 9.710 loss 5.548
[06:26:34.476320] Accuracy of the network on the 10000 test images: 2.3%
[06:26:34.476535] [06:26:34.476612] Max accuracy: 4.67%
[06:26:34.476672] [06:26:34.477490] {"train_lr": 0.0022530612244897968, "train_loss": 4.04926145644415, "test_loss": 5.547707843780517, "test_acc1": 2.33, "test_acc5": 9.71, "epoch": 1, "n_parameters": 85958500}
[06:26:34.477561] [06:26:34.477622] Training epoch 1 for 0:00:30
[06:26:34.477674] [06:26:34.480569] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:26:37.123287] Epoch: [2]  [ 0/42]  eta: 0:01:50  lr: 0.003200  loss: 4.0012 (4.0012)  time: 2.6417  data: 2.0856  max mem: 10046
[06:26:57.109345] Epoch: [2]  [41/42]  eta: 0:00:00  lr: 0.004724  loss: 3.7863 (3.8679)  time: 0.4883  data: 0.0001  max mem: 10046
[06:26:57.390481] Epoch: [2] Total time: 0:00:22 (0.5455 s / it)
[06:26:57.391348] Averaged stats: lr: 0.004724  loss: 3.7863 (3.8916)
[06:26:59.136877] Test:  [ 0/40]  eta: 0:01:09  loss: 4.9482 (4.9482)  acc1: 4.6875 (4.6875)  acc5: 15.6250 (15.6250)  time: 1.7416  data: 1.5721  max mem: 10046
[06:27:05.079802] Test:  [39/40]  eta: 0:00:00  loss: 4.6866 (4.7659)  acc1: 6.2500 (5.7200)  acc5: 17.1875 (17.4800)  time: 0.1489  data: 0.0001  max mem: 10046
[06:27:05.204684] Test: Total time: 0:00:07 (0.1953 s / it)
[06:27:05.492732] * Acc@1 5.800 Acc@5 17.620 loss 4.767
[06:27:05.492953] Accuracy of the network on the 10000 test images: 5.8%
[06:27:05.493139] [06:27:08.901213] Max accuracy: 5.80%
[06:27:08.901512] [06:27:08.902373] {"train_lr": 0.003853061224489796, "train_loss": 3.8915624845595587, "test_loss": 4.766565418243408, "test_acc1": 5.8, "test_acc5": 17.62, "epoch": 2, "n_parameters": 85958500}
[06:27:08.902447] [06:27:08.902509] Training epoch 2 for 0:00:34
[06:27:08.902577] [06:27:08.905508] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:27:11.021552] Epoch: [3]  [ 0/42]  eta: 0:01:28  lr: 0.004800  loss: 3.8519 (3.8519)  time: 2.1148  data: 1.6408  max mem: 10046
[06:27:31.035893] Epoch: [3]  [41/42]  eta: 0:00:00  lr: 0.006324  loss: 3.7009 (3.8001)  time: 0.4889  data: 0.0001  max mem: 10046
[06:27:31.306103] Epoch: [3] Total time: 0:00:22 (0.5333 s / it)
[06:27:31.307010] Averaged stats: lr: 0.006324  loss: 3.7009 (3.7614)
[06:27:33.358098] Test:  [ 0/40]  eta: 0:01:21  loss: 5.3577 (5.3577)  acc1: 10.9375 (10.9375)  acc5: 23.4375 (23.4375)  time: 2.0475  data: 1.8701  max mem: 10046
[06:27:39.301055] Test:  [39/40]  eta: 0:00:00  loss: 5.0828 (5.1807)  acc1: 6.2500 (7.4000)  acc5: 23.4375 (21.9200)  time: 0.1490  data: 0.0001  max mem: 10046
[06:27:39.447832] Test: Total time: 0:00:08 (0.2035 s / it)
[06:27:39.449512] * Acc@1 7.920 Acc@5 21.570 loss 5.163
[06:27:39.449714] Accuracy of the network on the 10000 test images: 7.9%
[06:27:39.449972] [06:27:42.931542] Max accuracy: 7.92%
[06:27:42.931801] [06:27:42.932678] {"train_lr": 0.005453061224489799, "train_loss": 3.761402550197783, "test_loss": 5.16278977394104, "test_acc1": 7.92, "test_acc5": 21.57, "epoch": 3, "n_parameters": 85958500}
[06:27:42.932802] [06:27:42.932871] Training epoch 3 for 0:00:34
[06:27:42.932924] [06:27:42.935771] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:27:45.022356] Epoch: [4]  [ 0/42]  eta: 0:01:27  lr: 0.006400  loss: 3.4822 (3.4822)  time: 2.0855  data: 1.6055  max mem: 10046
[06:28:05.049041] Epoch: [4]  [41/42]  eta: 0:00:00  lr: 0.007924  loss: 3.5970 (3.6094)  time: 0.4893  data: 0.0001  max mem: 10046
[06:28:05.292087] Epoch: [4] Total time: 0:00:22 (0.5323 s / it)
[06:28:05.293046] Averaged stats: lr: 0.007924  loss: 3.5970 (3.5953)
[06:28:07.294212] Test:  [ 0/40]  eta: 0:01:19  loss: 4.8840 (4.8840)  acc1: 10.9375 (10.9375)  acc5: 25.0000 (25.0000)  time: 1.9978  data: 1.8202  max mem: 10046
[06:28:13.230563] Test:  [39/40]  eta: 0:00:00  loss: 4.6141 (4.7448)  acc1: 7.8125 (8.1600)  acc5: 23.4375 (23.4800)  time: 0.1488  data: 0.0001  max mem: 10046
[06:28:13.400234] Test: Total time: 0:00:08 (0.2026 s / it)
[06:28:13.402998] * Acc@1 8.220 Acc@5 23.300 loss 4.716
[06:28:13.403132] Accuracy of the network on the 10000 test images: 8.2%
[06:28:13.403329] [06:28:16.431117] Max accuracy: 8.22%
[06:28:16.431451] [06:28:16.432489] {"train_lr": 0.007053061224489795, "train_loss": 3.595255874452137, "test_loss": 4.716222751140594, "test_acc1": 8.22, "test_acc5": 23.3, "epoch": 4, "n_parameters": 85958500}
[06:28:16.432632] [06:28:16.432717] Training epoch 4 for 0:00:33
[06:28:16.432792] [06:28:16.438256] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:28:18.459209] Epoch: [5]  [ 0/42]  eta: 0:01:24  lr: 0.008000  loss: 3.7700 (3.7700)  time: 2.0196  data: 1.5336  max mem: 10046
[06:28:38.500002] Epoch: [5]  [41/42]  eta: 0:00:00  lr: 0.007998  loss: 3.4266 (3.4920)  time: 0.4902  data: 0.0001  max mem: 10046
[06:28:38.738573] Epoch: [5] Total time: 0:00:22 (0.5310 s / it)
[06:28:38.762855] Averaged stats: lr: 0.007998  loss: 3.4266 (3.5078)
[06:28:40.877933] Test:  [ 0/40]  eta: 0:01:24  loss: 4.7711 (4.7711)  acc1: 14.0625 (14.0625)  acc5: 34.3750 (34.3750)  time: 2.1115  data: 1.9341  max mem: 10046
[06:28:46.826158] Test:  [39/40]  eta: 0:00:00  loss: 4.4876 (4.6541)  acc1: 10.9375 (11.8800)  acc5: 28.1250 (28.5200)  time: 0.1492  data: 0.0001  max mem: 10046
[06:28:46.972222] Test: Total time: 0:00:08 (0.2052 s / it)
[06:28:46.973685] * Acc@1 11.640 Acc@5 28.410 loss 4.615
[06:28:46.973826] Accuracy of the network on the 10000 test images: 11.6%
[06:28:46.974012] [06:28:50.392471] Max accuracy: 11.64%
[06:28:50.392748] [06:28:50.393606] {"train_lr": 0.007999452179690645, "train_loss": 3.5077752811568126, "test_loss": 4.614793860912323, "test_acc1": 11.64, "test_acc5": 28.41, "epoch": 5, "n_parameters": 85958500}
[06:28:50.393690] [06:28:50.393753] Training epoch 5 for 0:00:33
[06:28:50.393807] [06:28:50.396843] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:28:52.446288] Epoch: [6]  [ 0/42]  eta: 0:01:26  lr: 0.007998  loss: 3.2735 (3.2735)  time: 2.0484  data: 1.5614  max mem: 10046
[06:29:12.490884] Epoch: [6]  [41/42]  eta: 0:00:00  lr: 0.007992  loss: 3.3031 (3.3824)  time: 0.4903  data: 0.0001  max mem: 10046
[06:29:12.735592] Epoch: [6] Total time: 0:00:22 (0.5319 s / it)
[06:29:12.736487] Averaged stats: lr: 0.007992  loss: 3.3031 (3.3964)
[06:29:14.575442] Test:  [ 0/40]  eta: 0:01:13  loss: 4.3652 (4.3652)  acc1: 12.5000 (12.5000)  acc5: 31.2500 (31.2500)  time: 1.8348  data: 1.6562  max mem: 10046
[06:29:20.506218] Test:  [39/40]  eta: 0:00:00  loss: 4.0701 (4.1919)  acc1: 14.0625 (14.1600)  acc5: 34.3750 (32.5600)  time: 0.1488  data: 0.0001  max mem: 10046
[06:29:20.637223] Test: Total time: 0:00:07 (0.1975 s / it)
[06:29:20.938847] * Acc@1 13.520 Acc@5 32.310 loss 4.184
[06:29:20.939082] Accuracy of the network on the 10000 test images: 13.5%
[06:29:20.939287] [06:29:24.325858] Max accuracy: 13.52%
[06:29:24.326133] [06:29:24.327019] {"train_lr": 0.007995481023328083, "train_loss": 3.3963508776256015, "test_loss": 4.1840237230062485, "test_acc1": 13.52, "test_acc5": 32.31, "epoch": 6, "n_parameters": 85958500}
[06:29:24.327146] [06:29:24.327223] Training epoch 6 for 0:00:33
[06:29:24.327280] [06:29:24.330334] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:29:26.284220] Epoch: [7]  [ 0/42]  eta: 0:01:22  lr: 0.007991  loss: 3.1277 (3.1277)  time: 1.9529  data: 1.4656  max mem: 10046
[06:29:46.442252] Epoch: [7]  [41/42]  eta: 0:00:00  lr: 0.007981  loss: 3.2332 (3.2405)  time: 0.4919  data: 0.0001  max mem: 10046
[06:29:46.678264] Epoch: [7] Total time: 0:00:22 (0.5321 s / it)
[06:29:46.709706] Averaged stats: lr: 0.007981  loss: 3.2332 (3.2550)
[06:29:48.457870] Test:  [ 0/40]  eta: 0:01:09  loss: 4.1924 (4.1924)  acc1: 15.6250 (15.6250)  acc5: 32.8125 (32.8125)  time: 1.7396  data: 1.5573  max mem: 10046
[06:29:54.396903] Test:  [39/40]  eta: 0:00:00  loss: 3.9974 (4.1005)  acc1: 15.6250 (15.6000)  acc5: 35.9375 (35.2000)  time: 0.1491  data: 0.0001  max mem: 10046
[06:29:54.523508] Test: Total time: 0:00:07 (0.1953 s / it)
[06:29:55.011156] * Acc@1 15.300 Acc@5 35.960 loss 4.097
[06:29:55.011364] Accuracy of the network on the 10000 test images: 15.3%
[06:29:55.011555] [06:29:58.293385] Max accuracy: 15.30%
[06:29:58.293744] [06:29:58.294855] {"train_lr": 0.007987141413870226, "train_loss": 3.255026624316261, "test_loss": 4.097350814938546, "test_acc1": 15.3, "test_acc5": 35.96, "epoch": 7, "n_parameters": 85958500}
[06:29:58.294952] [06:29:58.295041] Training epoch 7 for 0:00:33
[06:29:58.295113] [06:29:58.299516] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:30:00.512001] Epoch: [8]  [ 0/42]  eta: 0:01:32  lr: 0.007980  loss: 3.1020 (3.1020)  time: 2.2108  data: 1.7212  max mem: 10046
[06:30:20.573936] Epoch: [8]  [41/42]  eta: 0:00:00  lr: 0.007966  loss: 3.1302 (3.1681)  time: 0.4900  data: 0.0001  max mem: 10046
[06:30:20.841006] Epoch: [8] Total time: 0:00:22 (0.5367 s / it)
[06:30:20.842566] Averaged stats: lr: 0.007966  loss: 3.1302 (3.2030)
[06:30:22.675213] Test:  [ 0/40]  eta: 0:01:13  loss: 4.4063 (4.4063)  acc1: 15.6250 (15.6250)  acc5: 37.5000 (37.5000)  time: 1.8283  data: 1.6485  max mem: 10046
[06:30:28.618976] Test:  [39/40]  eta: 0:00:00  loss: 4.1006 (4.2624)  acc1: 15.6250 (15.1200)  acc5: 35.9375 (35.6400)  time: 0.1492  data: 0.0001  max mem: 10046
[06:30:28.760700] Test: Total time: 0:00:07 (0.1979 s / it)
[06:30:29.040056] * Acc@1 14.940 Acc@5 34.960 loss 4.272
[06:30:29.040296] Accuracy of the network on the 10000 test images: 14.9%
[06:30:29.040535] [06:30:29.040612] Max accuracy: 15.30%
[06:30:29.040670] [06:30:29.041537] {"train_lr": 0.007974442470557574, "train_loss": 3.2029795731816972, "test_loss": 4.272046121954918, "test_acc1": 14.94, "test_acc5": 34.96, "epoch": 8, "n_parameters": 85958500}
[06:30:29.041607] [06:30:29.041671] Training epoch 8 for 0:00:30
[06:30:29.041724] [06:30:29.044592] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:30:31.587355] Epoch: [9]  [ 0/42]  eta: 0:01:46  lr: 0.007965  loss: 3.0015 (3.0015)  time: 2.5417  data: 1.8729  max mem: 10046
[06:30:51.692227] Epoch: [9]  [41/42]  eta: 0:00:00  lr: 0.007946  loss: 3.0893 (3.1289)  time: 0.4912  data: 0.0001  max mem: 10046
[06:30:51.933053] Epoch: [9] Total time: 0:00:22 (0.5450 s / it)
[06:30:51.935867] Averaged stats: lr: 0.007946  loss: 3.0893 (3.1101)
[06:30:53.512719] Test:  [ 0/40]  eta: 0:01:02  loss: 4.1382 (4.1382)  acc1: 17.1875 (17.1875)  acc5: 35.9375 (35.9375)  time: 1.5730  data: 1.4172  max mem: 10046
[06:30:59.472948] Test:  [39/40]  eta: 0:00:00  loss: 3.8645 (3.9625)  acc1: 15.6250 (16.7600)  acc5: 42.1875 (38.6000)  time: 0.1495  data: 0.0001  max mem: 10046
[06:30:59.596641] Test: Total time: 0:00:07 (0.1915 s / it)
[06:31:00.131754] * Acc@1 16.400 Acc@5 39.060 loss 3.974
[06:31:00.131961] Accuracy of the network on the 10000 test images: 16.4%
[06:31:00.132197] [06:31:03.547896] Max accuracy: 16.40%
[06:31:03.548210] [06:31:03.549106] {"train_lr": 0.007957398079498206, "train_loss": 3.110135875997089, "test_loss": 3.973594145476818, "test_acc1": 16.4, "test_acc5": 39.06, "epoch": 9, "n_parameters": 85958500}
[06:31:03.549202] [06:31:03.549300] Training epoch 9 for 0:00:34
[06:31:03.549372] [06:31:03.552295] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:31:05.582093] Epoch: [10]  [ 0/42]  eta: 0:01:25  lr: 0.007945  loss: 2.9043 (2.9043)  time: 2.0290  data: 1.5423  max mem: 10046
[06:31:25.684142] Epoch: [10]  [41/42]  eta: 0:00:00  lr: 0.007923  loss: 2.9856 (3.0668)  time: 0.4912  data: 0.0001  max mem: 10046
[06:31:25.957779] Epoch: [10] Total time: 0:00:22 (0.5335 s / it)
[06:31:25.963290] Averaged stats: lr: 0.007923  loss: 2.9856 (3.0584)
[06:31:27.797127] Test:  [ 0/40]  eta: 0:01:13  loss: 3.9694 (3.9694)  acc1: 18.7500 (18.7500)  acc5: 43.7500 (43.7500)  time: 1.8304  data: 1.6589  max mem: 10046
[06:31:33.757340] Test:  [39/40]  eta: 0:00:00  loss: 3.8488 (3.9936)  acc1: 17.1875 (17.5200)  acc5: 42.1875 (40.0800)  time: 0.1489  data: 0.0001  max mem: 10046
[06:31:33.879568] Test: Total time: 0:00:07 (0.1978 s / it)
[06:31:34.216722] * Acc@1 18.110 Acc@5 41.020 loss 3.984
[06:31:34.216940] Accuracy of the network on the 10000 test images: 18.1%
[06:31:34.217148] [06:31:37.567335] Max accuracy: 18.11%
[06:31:37.567608] [06:31:37.568476] {"train_lr": 0.007936026878483507, "train_loss": 3.0583953899996623, "test_loss": 3.983704550564289, "test_acc1": 18.11, "test_acc5": 41.02, "epoch": 10, "n_parameters": 85958500}
[06:31:37.568560] [06:31:37.568627] Training epoch 10 for 0:00:34
[06:31:37.568680] [06:31:37.571526] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:31:39.671162] Epoch: [11]  [ 0/42]  eta: 0:01:28  lr: 0.007922  loss: 3.2175 (3.2175)  time: 2.0987  data: 1.6171  max mem: 10046
[06:31:59.744687] Epoch: [11]  [41/42]  eta: 0:00:00  lr: 0.007895  loss: 3.0585 (2.9975)  time: 0.4905  data: 0.0001  max mem: 10046
[06:31:59.982928] Epoch: [11] Total time: 0:00:22 (0.5336 s / it)
[06:31:59.998759] Averaged stats: lr: 0.007895  loss: 3.0585 (3.0189)
[06:32:01.840648] Test:  [ 0/40]  eta: 0:01:13  loss: 3.9485 (3.9485)  acc1: 17.1875 (17.1875)  acc5: 42.1875 (42.1875)  time: 1.8378  data: 1.6664  max mem: 10046
[06:32:07.778550] Test:  [39/40]  eta: 0:00:00  loss: 3.5884 (3.7849)  acc1: 20.3125 (19.7600)  acc5: 43.7500 (43.3600)  time: 0.1488  data: 0.0001  max mem: 10046
[06:32:07.930217] Test: Total time: 0:00:07 (0.1982 s / it)
[06:32:08.075889] * Acc@1 19.160 Acc@5 43.150 loss 3.793
[06:32:08.076117] Accuracy of the network on the 10000 test images: 19.2%
[06:32:08.076323] [06:32:11.359430] Max accuracy: 19.16%
[06:32:11.359694] [06:32:11.360592] {"train_lr": 0.007910352236608017, "train_loss": 3.0189164706638882, "test_loss": 3.79256289601326, "test_acc1": 19.16, "test_acc5": 43.15, "epoch": 11, "n_parameters": 85958500}
[06:32:11.360682] [06:32:11.360745] Training epoch 11 for 0:00:33
[06:32:11.360798] [06:32:11.363661] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:32:13.315256] Epoch: [12]  [ 0/42]  eta: 0:01:21  lr: 0.007893  loss: 3.0063 (3.0063)  time: 1.9506  data: 1.4684  max mem: 10046
[06:32:33.460780] Epoch: [12]  [41/42]  eta: 0:00:00  lr: 0.007862  loss: 2.9781 (3.0183)  time: 0.4926  data: 0.0001  max mem: 10046
[06:32:33.723534] Epoch: [12] Total time: 0:00:22 (0.5324 s / it)
[06:32:33.741561] Averaged stats: lr: 0.007862  loss: 2.9781 (2.9609)
[06:32:35.823211] Test:  [ 0/40]  eta: 0:01:23  loss: 3.8216 (3.8216)  acc1: 18.7500 (18.7500)  acc5: 39.0625 (39.0625)  time: 2.0784  data: 1.9034  max mem: 10046
[06:32:41.758709] Test:  [39/40]  eta: 0:00:00  loss: 3.6269 (3.7856)  acc1: 21.8750 (20.2000)  acc5: 46.8750 (44.5600)  time: 0.1492  data: 0.0001  max mem: 10046
[06:32:41.883744] Test: Total time: 0:00:08 (0.2035 s / it)
[06:32:42.094471] * Acc@1 20.090 Acc@5 44.040 loss 3.782
[06:32:42.094693] Accuracy of the network on the 10000 test images: 20.1%
[06:32:42.094906] [06:32:44.707974] Max accuracy: 20.09%
[06:32:44.708329] [06:32:44.709374] {"train_lr": 0.007880402228715652, "train_loss": 2.9608817540463948, "test_loss": 3.7816490441560746, "test_acc1": 20.09, "test_acc5": 44.04, "epoch": 12, "n_parameters": 85958500}
[06:32:44.709528] [06:32:44.709614] Training epoch 12 for 0:00:33
[06:32:44.709697] [06:32:44.714263] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:32:46.892178] Epoch: [13]  [ 0/42]  eta: 0:01:31  lr: 0.007861  loss: 2.4897 (2.4897)  time: 2.1767  data: 1.6913  max mem: 10046
[06:33:07.030148] Epoch: [13]  [41/42]  eta: 0:00:00  lr: 0.007826  loss: 2.8115 (2.9089)  time: 0.4928  data: 0.0001  max mem: 10046
[06:33:07.275325] Epoch: [13] Total time: 0:00:22 (0.5372 s / it)
[06:33:07.283517] Averaged stats: lr: 0.007826  loss: 2.8115 (2.9067)
[06:33:09.372226] Test:  [ 0/40]  eta: 0:01:23  loss: 4.0816 (4.0816)  acc1: 20.3125 (20.3125)  acc5: 39.0625 (39.0625)  time: 2.0852  data: 1.9061  max mem: 10046
[06:33:15.317402] Test:  [39/40]  eta: 0:00:00  loss: 3.7705 (3.9073)  acc1: 20.3125 (20.3200)  acc5: 45.3125 (44.8000)  time: 0.1492  data: 0.0001  max mem: 10046
[06:33:15.447246] Test: Total time: 0:00:08 (0.2040 s / it)
[06:33:15.448533] * Acc@1 20.130 Acc@5 45.320 loss 3.876
[06:33:15.448714] Accuracy of the network on the 10000 test images: 20.1%
[06:33:15.448920] [06:33:18.735930] Max accuracy: 20.13%
[06:33:18.736290] [06:33:18.737375] {"train_lr": 0.00784620960470034, "train_loss": 2.9067251086235046, "test_loss": 3.875660076737404, "test_acc1": 20.13, "test_acc5": 45.32, "epoch": 13, "n_parameters": 85958500}
[06:33:18.737474] [06:33:18.737558] Training epoch 13 for 0:00:34
[06:33:18.737634] [06:33:18.742230] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:33:20.791546] Epoch: [14]  [ 0/42]  eta: 0:01:26  lr: 0.007824  loss: 2.7174 (2.7174)  time: 2.0484  data: 1.5669  max mem: 10046
[06:33:40.868921] Epoch: [14]  [41/42]  eta: 0:00:00  lr: 0.007785  loss: 2.8813 (2.8179)  time: 0.4910  data: 0.0001  max mem: 10046
[06:33:41.119477] Epoch: [14] Total time: 0:00:22 (0.5328 s / it)
[06:33:41.122409] Averaged stats: lr: 0.007785  loss: 2.8813 (2.8632)
[06:33:43.152865] Test:  [ 0/40]  eta: 0:01:21  loss: 3.9402 (3.9402)  acc1: 20.3125 (20.3125)  acc5: 45.3125 (45.3125)  time: 2.0269  data: 1.8566  max mem: 10046
[06:33:49.094125] Test:  [39/40]  eta: 0:00:00  loss: 3.6238 (3.7400)  acc1: 23.4375 (22.0000)  acc5: 48.4375 (48.3200)  time: 0.1493  data: 0.0001  max mem: 10046
[06:33:49.231267] Test: Total time: 0:00:08 (0.2027 s / it)
[06:33:49.289253] * Acc@1 21.670 Acc@5 48.000 loss 3.715
[06:33:49.289470] Accuracy of the network on the 10000 test images: 21.7%
[06:33:49.289678] [06:33:52.585523] Max accuracy: 21.67%
[06:33:52.585784] [06:33:52.586642] {"train_lr": 0.007807811753694423, "train_loss": 2.8631550002665747, "test_loss": 3.715454435348511, "test_acc1": 21.67, "test_acc5": 48.0, "epoch": 14, "n_parameters": 85958500}
[06:33:52.586714] [06:33:52.586776] Training epoch 14 for 0:00:33
[06:33:52.586829] [06:33:52.589800] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:33:54.725807] Epoch: [15]  [ 0/42]  eta: 0:01:29  lr: 0.007783  loss: 3.1308 (3.1308)  time: 2.1351  data: 1.6662  max mem: 10046
[06:34:14.820917] Epoch: [15]  [41/42]  eta: 0:00:00  lr: 0.007741  loss: 2.8213 (2.8573)  time: 0.4909  data: 0.0001  max mem: 10046
[06:34:15.062815] Epoch: [15] Total time: 0:00:22 (0.5351 s / it)
[06:34:15.080846] Averaged stats: lr: 0.007741  loss: 2.8213 (2.8375)
[06:34:16.889967] Test:  [ 0/40]  eta: 0:01:12  loss: 3.9329 (3.9329)  acc1: 20.3125 (20.3125)  acc5: 43.7500 (43.7500)  time: 1.8057  data: 1.6275  max mem: 10046
[06:34:22.832573] Test:  [39/40]  eta: 0:00:00  loss: 3.5554 (3.6988)  acc1: 21.8750 (21.8000)  acc5: 50.0000 (49.3600)  time: 0.1493  data: 0.0001  max mem: 10046
[06:34:22.962214] Test: Total time: 0:00:07 (0.1970 s / it)
[06:34:23.313593] * Acc@1 22.430 Acc@5 49.160 loss 3.655
[06:34:23.313805] Accuracy of the network on the 10000 test images: 22.4%
[06:34:23.314013] [06:34:26.570779] Max accuracy: 22.43%
[06:34:26.571072] [06:34:26.571915] {"train_lr": 0.007765250663184244, "train_loss": 2.837503704286757, "test_loss": 3.655086313188076, "test_acc1": 22.43, "test_acc5": 49.16, "epoch": 15, "n_parameters": 85958500}
[06:34:26.571986] [06:34:26.572049] Training epoch 15 for 0:00:33
[06:34:26.572122] [06:34:26.575081] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:34:28.810009] Epoch: [16]  [ 0/42]  eta: 0:01:33  lr: 0.007738  loss: 2.7859 (2.7859)  time: 2.2340  data: 1.7461  max mem: 10046
[06:34:48.880788] Epoch: [16]  [41/42]  eta: 0:00:00  lr: 0.007692  loss: 2.7388 (2.7965)  time: 0.4906  data: 0.0001  max mem: 10046
[06:34:49.124693] Epoch: [16] Total time: 0:00:22 (0.5369 s / it)
[06:34:49.134734] Averaged stats: lr: 0.007692  loss: 2.7388 (2.8102)
[06:34:50.991415] Test:  [ 0/40]  eta: 0:01:14  loss: 3.7112 (3.7112)  acc1: 20.3125 (20.3125)  acc5: 46.8750 (46.8750)  time: 1.8530  data: 1.6777  max mem: 10046
[06:34:56.923453] Test:  [39/40]  eta: 0:00:00  loss: 3.3477 (3.5087)  acc1: 25.0000 (24.3600)  acc5: 50.0000 (49.5600)  time: 0.1488  data: 0.0001  max mem: 10046
[06:34:57.046126] Test: Total time: 0:00:07 (0.1977 s / it)
[06:34:57.157368] * Acc@1 23.590 Acc@5 49.190 loss 3.500
[06:34:57.157587] Accuracy of the network on the 10000 test images: 23.6%
[06:34:57.157832] [06:34:59.891234] Max accuracy: 23.59%
[06:34:59.891575] [06:34:59.892921] {"train_lr": 0.0077185728730973764, "train_loss": 2.810242078133992, "test_loss": 3.4997791826725004, "test_acc1": 23.59, "test_acc5": 49.19, "epoch": 16, "n_parameters": 85958500}
[06:34:59.893073] [06:34:59.893163] Training epoch 16 for 0:00:33
[06:34:59.893240] [06:34:59.897600] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:35:02.060124] Epoch: [17]  [ 0/42]  eta: 0:01:30  lr: 0.007689  loss: 2.6732 (2.6732)  time: 2.1617  data: 1.6747  max mem: 10046
[06:35:22.138042] Epoch: [17]  [41/42]  eta: 0:00:00  lr: 0.007639  loss: 2.7265 (2.7343)  time: 0.4910  data: 0.0001  max mem: 10046
[06:35:22.379577] Epoch: [17] Total time: 0:00:22 (0.5353 s / it)
[06:35:22.383193] Averaged stats: lr: 0.007639  loss: 2.7265 (2.7331)
[06:35:24.425062] Test:  [ 0/40]  eta: 0:01:21  loss: 3.7120 (3.7120)  acc1: 21.8750 (21.8750)  acc5: 46.8750 (46.8750)  time: 2.0381  data: 1.8630  max mem: 10046
[06:35:30.427609] Test:  [39/40]  eta: 0:00:00  loss: 3.4135 (3.5845)  acc1: 25.0000 (24.0000)  acc5: 51.5625 (50.5200)  time: 0.1504  data: 0.0001  max mem: 10046
[06:35:30.601051] Test: Total time: 0:00:08 (0.2054 s / it)
[06:35:30.602321] * Acc@1 23.110 Acc@5 50.280 loss 3.610
[06:35:30.602491] Accuracy of the network on the 10000 test images: 23.1%
[06:35:30.602708] [06:35:30.602776] Max accuracy: 23.59%
[06:35:30.602831] [06:35:30.603904] {"train_lr": 0.007667829424911969, "train_loss": 2.733059536843073, "test_loss": 3.610186244547367, "test_acc1": 23.11, "test_acc5": 50.28, "epoch": 17, "n_parameters": 85958500}
[06:35:30.603975] [06:35:30.604032] Training epoch 17 for 0:00:30
[06:35:30.604093] [06:35:30.606968] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:35:32.918497] Epoch: [18]  [ 0/42]  eta: 0:01:37  lr: 0.007636  loss: 3.0862 (3.0862)  time: 2.3101  data: 1.8210  max mem: 10046
[06:35:53.024513] Epoch: [18]  [41/42]  eta: 0:00:00  lr: 0.007582  loss: 2.6690 (2.7280)  time: 0.4909  data: 0.0001  max mem: 10046
[06:35:53.242649] Epoch: [18] Total time: 0:00:22 (0.5389 s / it)
[06:35:53.261956] Averaged stats: lr: 0.007582  loss: 2.6690 (2.7326)
[06:35:55.415318] Test:  [ 0/40]  eta: 0:01:25  loss: 3.8301 (3.8301)  acc1: 20.3125 (20.3125)  acc5: 50.0000 (50.0000)  time: 2.1500  data: 1.9727  max mem: 10046
[06:36:01.355642] Test:  [39/40]  eta: 0:00:00  loss: 3.4299 (3.6058)  acc1: 26.5625 (24.0400)  acc5: 50.0000 (51.8400)  time: 0.1491  data: 0.0001  max mem: 10046
[06:36:01.487360] Test: Total time: 0:00:08 (0.2056 s / it)
[06:36:01.488740] * Acc@1 23.600 Acc@5 51.350 loss 3.593
[06:36:01.488915] Accuracy of the network on the 10000 test images: 23.6%
[06:36:01.489091] [06:36:04.823168] Max accuracy: 23.60%
[06:36:04.823441] [06:36:04.824343] {"train_lr": 0.007613075805843609, "train_loss": 2.732576307796297, "test_loss": 3.5934064015746117, "test_acc1": 23.6, "test_acc5": 51.35, "epoch": 18, "n_parameters": 85958500}
[06:36:04.824415] [06:36:04.824475] Training epoch 18 for 0:00:34
[06:36:04.824527] [06:36:04.827359] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:36:06.915308] Epoch: [19]  [ 0/42]  eta: 0:01:27  lr: 0.007579  loss: 2.5973 (2.5973)  time: 2.0871  data: 1.5988  max mem: 10046
[06:36:27.008012] Epoch: [19]  [41/42]  eta: 0:00:00  lr: 0.007521  loss: 2.7193 (2.6998)  time: 0.4909  data: 0.0001  max mem: 10046
[06:36:27.255103] Epoch: [19] Total time: 0:00:22 (0.5340 s / it)
[06:36:27.263648] Averaged stats: lr: 0.007521  loss: 2.7193 (2.7061)
[06:36:29.271639] Test:  [ 0/40]  eta: 0:01:20  loss: 3.7093 (3.7093)  acc1: 23.4375 (23.4375)  acc5: 45.3125 (45.3125)  time: 2.0042  data: 1.8265  max mem: 10046
[06:36:35.212753] Test:  [39/40]  eta: 0:00:00  loss: 3.2598 (3.4466)  acc1: 26.5625 (25.7600)  acc5: 53.1250 (51.7600)  time: 0.1490  data: 0.0001  max mem: 10046
[06:36:35.365439] Test: Total time: 0:00:08 (0.2025 s / it)
[06:36:35.366738] * Acc@1 25.460 Acc@5 51.760 loss 3.435
[06:36:35.366887] Accuracy of the network on the 10000 test images: 25.5%
[06:36:35.367088] [06:36:38.650954] Max accuracy: 25.46%
[06:36:38.651293] [06:36:38.652266] {"train_lr": 0.007554371888170873, "train_loss": 2.7061204385189783, "test_loss": 3.435309612751007, "test_acc1": 25.46, "test_acc5": 51.76, "epoch": 19, "n_parameters": 85958500}
[06:36:38.652366] [06:36:38.652499] Training epoch 19 for 0:00:33
[06:36:38.652575] [06:36:38.655473] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:36:40.837569] Epoch: [20]  [ 0/42]  eta: 0:01:31  lr: 0.007518  loss: 2.5578 (2.5578)  time: 2.1813  data: 1.7054  max mem: 10046
[06:37:00.909338] Epoch: [20]  [41/42]  eta: 0:00:00  lr: 0.007456  loss: 2.6114 (2.6664)  time: 0.4909  data: 0.0001  max mem: 10046
[06:37:01.178537] Epoch: [20] Total time: 0:00:22 (0.5363 s / it)
[06:37:01.181376] Averaged stats: lr: 0.007456  loss: 2.6114 (2.6755)
[06:37:02.798701] Test:  [ 0/40]  eta: 0:01:04  loss: 3.7364 (3.7364)  acc1: 23.4375 (23.4375)  acc5: 48.4375 (48.4375)  time: 1.6133  data: 1.4460  max mem: 10046
[06:37:08.971884] Test:  [39/40]  eta: 0:00:00  loss: 3.4014 (3.5521)  acc1: 26.5625 (26.4400)  acc5: 53.1250 (52.4400)  time: 0.1489  data: 0.0001  max mem: 10046
[06:37:09.089486] Test: Total time: 0:00:07 (0.1976 s / it)
[06:37:09.413257] * Acc@1 25.820 Acc@5 52.970 loss 3.542
[06:37:09.413446] Accuracy of the network on the 10000 test images: 25.8%
[06:37:09.413656] [06:37:12.860578] Max accuracy: 25.82%
[06:37:12.860867] [06:37:12.861855] {"train_lr": 0.007491781863765832, "train_loss": 2.6755126317342124, "test_loss": 3.542027023434639, "test_acc1": 25.82, "test_acc5": 52.97, "epoch": 20, "n_parameters": 85958500}
[06:37:12.861950] [06:37:12.862027] Training epoch 20 for 0:00:34
[06:37:12.862083] [06:37:12.865047] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:37:15.010804] Epoch: [21]  [ 0/42]  eta: 0:01:30  lr: 0.007453  loss: 2.5268 (2.5268)  time: 2.1445  data: 1.6608  max mem: 10046
[06:37:35.087144] Epoch: [21]  [41/42]  eta: 0:00:00  lr: 0.007388  loss: 2.6920 (2.6655)  time: 0.4906  data: 0.0001  max mem: 10046
[06:37:35.329030] Epoch: [21] Total time: 0:00:22 (0.5349 s / it)
[06:37:35.330559] Averaged stats: lr: 0.007388  loss: 2.6920 (2.6460)
[06:37:37.519598] Test:  [ 0/40]  eta: 0:01:27  loss: 3.5795 (3.5795)  acc1: 25.0000 (25.0000)  acc5: 50.0000 (50.0000)  time: 2.1857  data: 2.0065  max mem: 10046
[06:37:43.458527] Test:  [39/40]  eta: 0:00:00  loss: 3.4097 (3.5220)  acc1: 25.0000 (25.2000)  acc5: 51.5625 (52.2000)  time: 0.1489  data: 0.0001  max mem: 10046
[06:37:43.581504] Test: Total time: 0:00:08 (0.2062 s / it)
[06:37:43.582837] * Acc@1 24.510 Acc@5 52.030 loss 3.522
[06:37:43.582988] Accuracy of the network on the 10000 test images: 24.5%
[06:37:43.583194] [06:37:43.583260] Max accuracy: 25.82%
[06:37:43.583319] [06:37:43.584147] {"train_lr": 0.007425374173901249, "train_loss": 2.646018530641283, "test_loss": 3.522021920979023, "test_acc1": 24.51, "test_acc5": 52.03, "epoch": 21, "n_parameters": 85958500}
[06:37:43.584235] [06:37:43.584293] Training epoch 21 for 0:00:30
[06:37:43.584345] [06:37:43.587158] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:37:45.966935] Epoch: [22]  [ 0/42]  eta: 0:01:39  lr: 0.007384  loss: 2.4331 (2.4331)  time: 2.3788  data: 1.8778  max mem: 10046
[06:38:06.144988] Epoch: [22]  [41/42]  eta: 0:00:00  lr: 0.007316  loss: 2.6226 (2.6414)  time: 0.4919  data: 0.0001  max mem: 10046
[06:38:06.398583] Epoch: [22] Total time: 0:00:22 (0.5431 s / it)
[06:38:06.402372] Averaged stats: lr: 0.007316  loss: 2.6226 (2.6269)
[06:38:08.480242] Test:  [ 0/40]  eta: 0:01:22  loss: 3.6868 (3.6868)  acc1: 25.0000 (25.0000)  acc5: 48.4375 (48.4375)  time: 2.0741  data: 1.8926  max mem: 10046
[06:38:14.429660] Test:  [39/40]  eta: 0:00:00  loss: 3.4662 (3.5246)  acc1: 25.0000 (25.3600)  acc5: 54.6875 (52.9200)  time: 0.1492  data: 0.0001  max mem: 10046
[06:38:14.579431] Test: Total time: 0:00:08 (0.2044 s / it)
[06:38:14.614913] * Acc@1 25.310 Acc@5 52.820 loss 3.513
[06:38:14.615108] Accuracy of the network on the 10000 test images: 25.3%
[06:38:14.615363] [06:38:14.615438] Max accuracy: 25.82%
[06:38:14.615496] [06:38:14.616356] {"train_lr": 0.007355221434411005, "train_loss": 2.6268657857463475, "test_loss": 3.513132619857788, "test_acc1": 25.31, "test_acc5": 52.82, "epoch": 22, "n_parameters": 85958500}
[06:38:14.616431] [06:38:14.616493] Training epoch 22 for 0:00:31
[06:38:14.616547] [06:38:14.619428] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:38:17.102294] Epoch: [23]  [ 0/42]  eta: 0:01:44  lr: 0.007312  loss: 2.9699 (2.9699)  time: 2.4817  data: 1.7769  max mem: 10046
[06:38:37.211371] Epoch: [23]  [41/42]  eta: 0:00:00  lr: 0.007240  loss: 2.4871 (2.6074)  time: 0.4914  data: 0.0001  max mem: 10046
[06:38:37.423228] Epoch: [23] Total time: 0:00:22 (0.5429 s / it)
[06:38:37.442770] Averaged stats: lr: 0.007240  loss: 2.4871 (2.5858)
[06:38:39.407533] Test:  [ 0/40]  eta: 0:01:18  loss: 3.6236 (3.6236)  acc1: 29.6875 (29.6875)  acc5: 46.8750 (46.8750)  time: 1.9611  data: 1.7884  max mem: 10046
[06:38:45.355942] Test:  [39/40]  eta: 0:00:00  loss: 3.3352 (3.4120)  acc1: 26.5625 (26.7600)  acc5: 54.6875 (53.4800)  time: 0.1492  data: 0.0001  max mem: 10046
[06:38:45.500832] Test: Total time: 0:00:08 (0.2014 s / it)
[06:38:45.502189] * Acc@1 26.940 Acc@5 54.100 loss 3.411
[06:38:45.502330] Accuracy of the network on the 10000 test images: 26.9%
[06:38:45.502529] [06:38:48.884883] Max accuracy: 26.94%
[06:38:48.885173] [06:38:48.886108] {"train_lr": 0.007281400356285755, "train_loss": 2.5857684995446886, "test_loss": 3.41075841486454, "test_acc1": 26.94, "test_acc5": 54.1, "epoch": 23, "n_parameters": 85958500}
[06:38:48.886183] [06:38:48.886247] Training epoch 23 for 0:00:34
[06:38:48.886301] [06:38:48.889215] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:38:51.043704] Epoch: [24]  [ 0/42]  eta: 0:01:30  lr: 0.007236  loss: 2.3234 (2.3234)  time: 2.1536  data: 1.6693  max mem: 10046
[06:39:11.150933] Epoch: [24]  [41/42]  eta: 0:00:00  lr: 0.007161  loss: 2.6093 (2.5515)  time: 0.4910  data: 0.0001  max mem: 10046
[06:39:11.357300] Epoch: [24] Total time: 0:00:22 (0.5350 s / it)
[06:39:11.383011] Averaged stats: lr: 0.007161  loss: 2.6093 (2.5670)
[06:39:13.229791] Test:  [ 0/40]  eta: 0:01:13  loss: 3.6675 (3.6675)  acc1: 26.5625 (26.5625)  acc5: 56.2500 (56.2500)  time: 1.8433  data: 1.6683  max mem: 10046
[06:39:19.173534] Test:  [39/40]  eta: 0:00:00  loss: 3.3927 (3.4926)  acc1: 28.1250 (26.8400)  acc5: 56.2500 (54.0000)  time: 0.1495  data: 0.0001  max mem: 10046
[06:39:19.309244] Test: Total time: 0:00:07 (0.1981 s / it)
[06:39:19.507040] * Acc@1 26.930 Acc@5 54.630 loss 3.463
[06:39:19.507252] Accuracy of the network on the 10000 test images: 26.9%
[06:39:19.507449] [06:39:19.507524] Max accuracy: 26.94%
[06:39:19.507581] [06:39:19.508428] {"train_lr": 0.0072039916617906, "train_loss": 2.566971290679205, "test_loss": 3.463333159685135, "test_acc1": 26.93, "test_acc5": 54.63, "epoch": 24, "n_parameters": 85958500}
[06:39:19.508513] [06:39:19.508573] Training epoch 24 for 0:00:30
[06:39:19.508624] [06:39:19.511444] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:39:22.228166] Epoch: [25]  [ 0/42]  eta: 0:01:54  lr: 0.007157  loss: 2.5607 (2.5607)  time: 2.7157  data: 2.0171  max mem: 10046
[06:39:42.376161] Epoch: [25]  [41/42]  eta: 0:00:00  lr: 0.007078  loss: 2.5598 (2.5554)  time: 0.4933  data: 0.0001  max mem: 10046
[06:39:42.609503] Epoch: [25] Total time: 0:00:23 (0.5500 s / it)
[06:39:42.615557] Averaged stats: lr: 0.007078  loss: 2.5598 (2.5473)
[06:39:44.443886] Test:  [ 0/40]  eta: 0:01:12  loss: 3.6678 (3.6678)  acc1: 26.5625 (26.5625)  acc5: 57.8125 (57.8125)  time: 1.8248  data: 1.6481  max mem: 10046
[06:39:50.388315] Test:  [39/40]  eta: 0:00:00  loss: 3.3141 (3.4621)  acc1: 28.1250 (28.5200)  acc5: 56.2500 (55.5600)  time: 0.1492  data: 0.0001  max mem: 10046
[06:39:50.522578] Test: Total time: 0:00:07 (0.1976 s / it)
[06:39:50.767513] * Acc@1 27.840 Acc@5 56.020 loss 3.446
[06:39:50.767735] Accuracy of the network on the 10000 test images: 27.8%
[06:39:50.767967] [06:39:54.182532] Max accuracy: 27.84%
[06:39:54.182821] [06:39:54.183748] {"train_lr": 0.007123079996196465, "train_loss": 2.547263657762891, "test_loss": 3.4455387383699416, "test_acc1": 27.84, "test_acc5": 56.02, "epoch": 25, "n_parameters": 85958500}
[06:39:54.183819] [06:39:54.183882] Training epoch 25 for 0:00:34
[06:39:54.183948] [06:39:54.186865] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:39:56.175808] Epoch: [26]  [ 0/42]  eta: 0:01:23  lr: 0.007074  loss: 2.3567 (2.3567)  time: 1.9878  data: 1.5094  max mem: 10046
[06:40:16.286660] Epoch: [26]  [41/42]  eta: 0:00:00  lr: 0.006992  loss: 2.4756 (2.5189)  time: 0.4918  data: 0.0001  max mem: 10046
[06:40:16.540542] Epoch: [26] Total time: 0:00:22 (0.5322 s / it)
[06:40:16.544898] Averaged stats: lr: 0.006992  loss: 2.4756 (2.5007)
[06:40:18.273562] Test:  [ 0/40]  eta: 0:01:08  loss: 3.5610 (3.5610)  acc1: 31.2500 (31.2500)  acc5: 53.1250 (53.1250)  time: 1.7239  data: 1.5460  max mem: 10046
[06:40:24.302100] Test:  [39/40]  eta: 0:00:00  loss: 3.2708 (3.3775)  acc1: 28.1250 (29.0800)  acc5: 59.3750 (56.2000)  time: 0.1500  data: 0.0001  max mem: 10046
[06:40:24.426673] Test: Total time: 0:00:07 (0.1970 s / it)
[06:40:24.753033] * Acc@1 28.140 Acc@5 56.190 loss 3.368
[06:40:24.753208] Accuracy of the network on the 10000 test images: 28.1%
[06:40:24.753407] [06:40:28.083941] Max accuracy: 28.14%
[06:40:28.084262] [06:40:28.085146] {"train_lr": 0.0070387538352217486, "train_loss": 2.5007466915107908, "test_loss": 3.367839257419109, "test_acc1": 28.14, "test_acc5": 56.19, "epoch": 26, "n_parameters": 85958500}
[06:40:28.085227] [06:40:28.085293] Training epoch 26 for 0:00:33
[06:40:28.085346] [06:40:28.088329] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[06:40:30.132933] Epoch: [27]  [ 0/42]  eta: 0:01:25  lr: 0.006987  loss: 2.6923 (2.6923)  time: 2.0436  data: 1.5609  max mem: 10046
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 41040 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 41042 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 41043 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -9) local_rank: 1 (pid: 41041) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
======================================================
./main_finetune.py FAILED
------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_06:40:39
  host      : user
  rank      : 1 (local_rank: 1)
  exitcode  : -9 (pid: 41041)
  error_file: <N/A>
  traceback : Signal 9 (SIGKILL) received by PID 41041
======================================================
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
[06:40:57.771549] job dir: /home/vision/wonjun/LiVT-main
[06:40:57.771672] [06:40:57.771837] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=16,
batch_size=64,
blr=0.00015,
ckpt_dir='./ckpt/debug/cifar100-LT/debug',
color_jitter=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=800,
gpu=0,
imbf=100,
input_size=224,
local_rank=0,
log_dir='./exp/debug/cifar100-LT/debug',
lr=None,
mask_ratio=0.75,
min_lr=0.0,
model='mae_vit_base_patch16',
norm_pix_loss=True,
num_workers=16,
pin_mem=True,
prit=200,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=40,
weight_decay=0.05,
world_size=4)
[06:40:57.771904] [06:40:58.103669] Files already downloaded and verified
[06:40:58.850164] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[06:40:58.875919] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[06:40:58.876747] [06:40:58.877500] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fc900156050>
[06:40:58.877569] [06:40:58.879365] log_writer is <torch.utils.tensorboard.writer.SummaryWriter object at 0x7fc900156110>
[06:40:58.879440] [06:41:01.201073] Model = mae_vit_base_patch16
[06:41:01.201290] [06:41:01.201364] base lr: 1.50e-04
[06:41:01.201418] [06:41:01.201461] actual lr: 2.40e-03
[06:41:01.201509] [06:41:01.201548] accumulate grad iterations: 16
[06:41:01.201593] [06:41:01.201630] effective batch size: 4096
[06:41:01.201675] [06:41:01.219735] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.0024
    maximize: False
    weight_decay: 0.01
)
[06:41:01.219894] [06:41:01.220187] Save config to: ./exp/debug/cifar100-LT/debug/args.txt
[06:41:01.220226] Start training for 800 epochs
[06:41:01.220278] [06:41:01.221985] log_dir: ./exp/debug/cifar100-LT/debug
[06:41:04.820805] attn initing scaling_factor...
[06:41:04.830931] attn initing scaling_factor...
[06:41:04.831754] mlp initing scaling_factor...
[06:41:04.833495] mlp initing scaling_factor...
[06:41:04.835096] attn initing scaling_factor...
[06:41:04.837613] attn initing scaling_factor...
[06:41:04.838241] mlp initing scaling_factor...
[06:41:04.840012] mlp initing scaling_factor...
[06:41:04.841532] attn initing scaling_factor...
[06:41:04.843915] attn initing scaling_factor...
[06:41:04.844594] mlp initing scaling_factor...
[06:41:04.846360] mlp initing scaling_factor...
[06:41:04.847812] attn initing scaling_factor...
[06:41:04.850661] attn initing scaling_factor...
[06:41:04.851284] mlp initing scaling_factor...
[06:41:04.853143] mlp initing scaling_factor...
[06:41:04.854763] attn initing scaling_factor...
[06:41:04.857505] attn initing scaling_factor...
[06:41:04.858151] mlp initing scaling_factor...
[06:41:04.859823] mlp initing scaling_factor...
[06:41:04.861477] attn initing scaling_factor...
[06:41:04.864053] attn initing scaling_factor...
[06:41:04.864696] mlp initing scaling_factor...
[06:41:04.866576] mlp initing scaling_factor...
[06:41:04.868085] attn initing scaling_factor...
[06:41:04.870625] attn initing scaling_factor...
[06:41:04.871246] mlp initing scaling_factor...
[06:41:04.873175] mlp initing scaling_factor...
[06:41:04.874622] attn initing scaling_factor...
[06:41:04.877204] attn initing scaling_factor...
[06:41:04.877846] mlp initing scaling_factor...
[06:41:04.879771] mlp initing scaling_factor...
[06:41:04.881272] attn initing scaling_factor...
[06:41:04.883885] attn initing scaling_factor...
[06:41:04.884532] mlp initing scaling_factor...
[06:41:04.886482] mlp initing scaling_factor...
[06:41:04.888082] attn initing scaling_factor...
[06:41:04.890592] attn initing scaling_factor...
[06:41:04.891223] mlp initing scaling_factor...
[06:41:04.893055] mlp initing scaling_factor...
[06:41:04.894574] attn initing scaling_factor...
[06:41:04.897242] attn initing scaling_factor...
[06:41:04.897860] mlp initing scaling_factor...
[06:41:04.899791] mlp initing scaling_factor...
[06:41:04.901234] attn initing scaling_factor...
[06:41:04.903805] attn initing scaling_factor...
[06:41:04.904437] mlp initing scaling_factor...
[06:41:04.906331] mlp initing scaling_factor...
[06:41:04.910913] attn initing scaling_factor...
[06:41:04.921435] attn initing scaling_factor...
[06:41:04.922601] mlp initing scaling_factor...
[06:41:04.924262] mlp initing scaling_factor...
[06:41:04.925523] attn initing scaling_factor...
[06:41:04.934707] attn initing scaling_factor...
[06:41:04.935881] mlp initing scaling_factor...
[06:41:04.937558] mlp initing scaling_factor...
[06:41:04.938826] attn initing scaling_factor...
[06:41:04.948018] attn initing scaling_factor...
[06:41:04.949410] mlp initing scaling_factor...
[06:41:04.951073] mlp initing scaling_factor...
[06:41:04.952354] attn initing scaling_factor...
[06:41:04.961511] attn initing scaling_factor...
[06:41:04.962685] mlp initing scaling_factor...
[06:41:04.964340] mlp initing scaling_factor...
[06:41:04.965593] attn initing scaling_factor...
[06:41:04.974780] attn initing scaling_factor...
[06:41:04.976209] mlp initing scaling_factor...
[06:41:04.977871] mlp initing scaling_factor...
[06:41:04.979142] attn initing scaling_factor...
[06:41:04.988338] attn initing scaling_factor...
[06:41:04.989517] mlp initing scaling_factor...
[06:41:04.991163] mlp initing scaling_factor...
[06:41:04.992431] attn initing scaling_factor...
[06:41:05.001592] attn initing scaling_factor...
[06:41:05.002753] mlp initing scaling_factor...
[06:41:05.004405] mlp initing scaling_factor...
[06:41:05.005693] attn initing scaling_factor...
[06:41:05.014903] attn initing scaling_factor...
[06:41:05.016466] mlp initing scaling_factor...
[06:41:05.018111] mlp initing scaling_factor...
[06:41:05.840118] Epoch: [0]  [ 0/42]  eta: 0:03:13  lr: 0.000000  loss: 1.7968 (1.7968)  time: 4.6171  data: 2.0894  max mem: 8037
[06:41:16.030398] Epoch: [0]  [20/42]  eta: 0:00:15  lr: 0.000023  loss: 1.7973 (1.7974)  time: 0.5095  data: 0.0002  max mem: 9341
[06:41:26.177676] Epoch: [0]  [40/42]  eta: 0:00:01  lr: 0.000046  loss: 1.7901 (1.7618)  time: 0.5073  data: 0.0001  max mem: 9341
[06:41:26.685057] Epoch: [0]  [41/42]  eta: 0:00:00  lr: 0.000046  loss: 1.6415 (1.7588)  time: 0.5073  data: 0.0001  max mem: 9341
[06:41:26.831171] Epoch: [0] Total time: 0:00:25 (0.6097 s / it)
[06:41:26.854392] Averaged stats: lr: 0.000046  loss: 1.6415 (1.7594)
[06:41:31.482770] {"train_lr": 1.9591836734693884e-05, "train_loss": 1.7593972484270732, "epoch": 0}
[06:41:31.483215] [06:41:31.483343] Training epoch 0 for 0:00:30
[06:41:31.483399] [06:41:31.487899] log_dir: ./exp/debug/cifar100-LT/debug
[06:41:33.066661] Epoch: [1]  [ 0/42]  eta: 0:01:06  lr: 0.000060  loss: 1.6386 (1.6386)  time: 1.5778  data: 1.0609  max mem: 9341
[06:41:43.226926] Epoch: [1]  [20/42]  eta: 0:00:12  lr: 0.000083  loss: 1.6309 (1.6112)  time: 0.5080  data: 0.0001  max mem: 9341
[06:41:53.378873] Epoch: [1]  [40/42]  eta: 0:00:01  lr: 0.000106  loss: 1.5274 (1.5559)  time: 0.5076  data: 0.0001  max mem: 9341
[06:41:53.886198] Epoch: [1]  [41/42]  eta: 0:00:00  lr: 0.000106  loss: 1.4561 (1.5535)  time: 0.5076  data: 0.0001  max mem: 9341
[06:41:54.053244] Epoch: [1] Total time: 0:00:22 (0.5373 s / it)
[06:41:54.068684] Averaged stats: lr: 0.000106  loss: 1.4561 (1.5542)
[06:41:58.676060] {"train_lr": 7.959183673469385e-05, "train_loss": 1.554183882616815, "epoch": 1}
[06:41:58.676457] [06:41:58.676545] Training epoch 1 for 0:00:27
[06:41:58.676602] [06:41:58.680973] log_dir: ./exp/debug/cifar100-LT/debug
[06:42:00.132896] Epoch: [2]  [ 0/42]  eta: 0:01:00  lr: 0.000120  loss: 1.4424 (1.4424)  time: 1.4507  data: 0.9318  max mem: 9341
[06:42:10.303083] Epoch: [2]  [20/42]  eta: 0:00:12  lr: 0.000143  loss: 1.4434 (1.4232)  time: 0.5085  data: 0.0001  max mem: 9341
[06:42:20.460484] Epoch: [2]  [40/42]  eta: 0:00:01  lr: 0.000166  loss: 1.3237 (1.3631)  time: 0.5078  data: 0.0001  max mem: 9341
[06:42:20.967533] Epoch: [2]  [41/42]  eta: 0:00:00  lr: 0.000166  loss: 1.2738 (1.3605)  time: 0.5078  data: 0.0001  max mem: 9341
[06:42:21.135650] Epoch: [2] Total time: 0:00:22 (0.5346 s / it)
[06:42:21.136785] Averaged stats: lr: 0.000166  loss: 1.2738 (1.3610)
[06:42:25.747127] {"train_lr": 0.00013959183673469382, "train_loss": 1.3610453896579289, "epoch": 2}
[06:42:25.747476] [06:42:25.747564] Training epoch 2 for 0:00:27
[06:42:25.747621] [06:42:25.752017] log_dir: ./exp/debug/cifar100-LT/debug
[06:42:27.342087] Epoch: [3]  [ 0/42]  eta: 0:01:06  lr: 0.000180  loss: 1.2524 (1.2524)  time: 1.5892  data: 1.0700  max mem: 9341
[06:42:37.531438] Epoch: [3]  [20/42]  eta: 0:00:12  lr: 0.000203  loss: 1.2532 (1.2404)  time: 0.5094  data: 0.0002  max mem: 9341
[06:42:47.706528] Epoch: [3]  [40/42]  eta: 0:00:01  lr: 0.000226  loss: 1.1589 (1.1999)  time: 0.5087  data: 0.0001  max mem: 9341
[06:42:48.213526] Epoch: [3]  [41/42]  eta: 0:00:00  lr: 0.000226  loss: 1.1464 (1.1981)  time: 0.5087  data: 0.0001  max mem: 9341
[06:42:48.382374] Epoch: [3] Total time: 0:00:22 (0.5388 s / it)
[06:42:48.392377] Averaged stats: lr: 0.000226  loss: 1.1464 (1.1978)
[06:42:53.124636] {"train_lr": 0.00019959183673469381, "train_loss": 1.1977867441517966, "epoch": 3}
[06:42:53.125035] [06:42:53.125125] Training epoch 3 for 0:00:27
[06:42:53.125185] [06:42:53.129611] log_dir: ./exp/debug/cifar100-LT/debug
[06:42:54.567546] Epoch: [4]  [ 0/42]  eta: 0:01:00  lr: 0.000240  loss: 1.1329 (1.1329)  time: 1.4366  data: 0.9369  max mem: 9341
[06:43:04.743152] Epoch: [4]  [20/42]  eta: 0:00:12  lr: 0.000263  loss: 1.1158 (1.1096)  time: 0.5087  data: 0.0001  max mem: 9341
[06:43:14.909415] Epoch: [4]  [40/42]  eta: 0:00:01  lr: 0.000286  loss: 1.0510 (1.0831)  time: 0.5083  data: 0.0001  max mem: 9341
[06:43:15.414785] Epoch: [4]  [41/42]  eta: 0:00:00  lr: 0.000286  loss: 1.0485 (1.0818)  time: 0.5081  data: 0.0001  max mem: 9341
[06:43:15.580032] Epoch: [4] Total time: 0:00:22 (0.5345 s / it)
[06:43:15.597839] Averaged stats: lr: 0.000286  loss: 1.0485 (1.0877)
[06:43:20.123710] {"train_lr": 0.0002595918367346936, "train_loss": 1.0877172436032976, "epoch": 4}
[06:43:20.124115] [06:43:20.124207] Training epoch 4 for 0:00:26
[06:43:20.124266] [06:43:20.128709] log_dir: ./exp/debug/cifar100-LT/debug
[06:43:21.722009] Epoch: [5]  [ 0/42]  eta: 0:01:06  lr: 0.000300  loss: 1.0569 (1.0569)  time: 1.5920  data: 1.0970  max mem: 9341
[06:43:31.912803] Epoch: [5]  [20/42]  eta: 0:00:12  lr: 0.000323  loss: 1.0307 (1.0351)  time: 0.5095  data: 0.0001  max mem: 9341
[06:43:42.090913] Epoch: [5]  [40/42]  eta: 0:00:01  lr: 0.000346  loss: 1.0219 (1.0290)  time: 0.5089  data: 0.0001  max mem: 9341
[06:43:42.598573] Epoch: [5]  [41/42]  eta: 0:00:00  lr: 0.000346  loss: 1.0195 (1.0284)  time: 0.5089  data: 0.0001  max mem: 9341
[06:43:42.760854] Epoch: [5] Total time: 0:00:22 (0.5389 s / it)
[06:43:42.769064] Averaged stats: lr: 0.000346  loss: 1.0195 (1.0304)
[06:43:47.458730] {"train_lr": 0.00031959183673469383, "train_loss": 1.0304478264990307, "epoch": 5}
[06:43:47.459089] [06:43:47.459178] Training epoch 5 for 0:00:27
[06:43:47.459238] [06:43:47.463610] log_dir: ./exp/debug/cifar100-LT/debug
[06:43:48.973390] Epoch: [6]  [ 0/42]  eta: 0:01:03  lr: 0.000360  loss: 1.0144 (1.0144)  time: 1.5085  data: 0.9948  max mem: 9341
[06:43:59.135423] Epoch: [6]  [20/42]  eta: 0:00:12  lr: 0.000383  loss: 1.0035 (1.0089)  time: 0.5080  data: 0.0001  max mem: 9341
[06:44:09.282648] Epoch: [6]  [40/42]  eta: 0:00:01  lr: 0.000406  loss: 0.9986 (1.0050)  time: 0.5073  data: 0.0001  max mem: 9341
[06:44:09.790007] Epoch: [6]  [41/42]  eta: 0:00:00  lr: 0.000406  loss: 0.9946 (1.0041)  time: 0.5073  data: 0.0001  max mem: 9341
[06:44:09.953702] Epoch: [6] Total time: 0:00:22 (0.5355 s / it)
[06:44:09.954508] Averaged stats: lr: 0.000406  loss: 0.9946 (1.0083)
[06:44:14.494135] {"train_lr": 0.0003795918367346939, "train_loss": 1.0083239727786608, "epoch": 6}
[06:44:14.494549] [06:44:14.494644] Training epoch 6 for 0:00:27
[06:44:14.494700] [06:44:14.498991] log_dir: ./exp/debug/cifar100-LT/debug
[06:44:16.030304] Epoch: [7]  [ 0/42]  eta: 0:01:04  lr: 0.000420  loss: 1.0159 (1.0159)  time: 1.5300  data: 1.0122  max mem: 9341
[06:44:26.198715] Epoch: [7]  [20/42]  eta: 0:00:12  lr: 0.000443  loss: 1.0061 (1.0052)  time: 0.5084  data: 0.0001  max mem: 9341
[06:44:36.349419] Epoch: [7]  [40/42]  eta: 0:00:01  lr: 0.000466  loss: 1.0174 (1.0105)  time: 0.5075  data: 0.0001  max mem: 9341
[06:44:36.856956] Epoch: [7]  [41/42]  eta: 0:00:00  lr: 0.000466  loss: 1.0174 (1.0103)  time: 0.5075  data: 0.0001  max mem: 9341
[06:44:37.026631] Epoch: [7] Total time: 0:00:22 (0.5364 s / it)
[06:44:37.030506] Averaged stats: lr: 0.000466  loss: 1.0174 (1.0071)
[06:44:41.577460] {"train_lr": 0.0004395918367346936, "train_loss": 1.0071011288535028, "epoch": 7}
[06:44:41.577819] [06:44:41.577907] Training epoch 7 for 0:00:27
[06:44:41.577966] [06:44:41.582322] log_dir: ./exp/debug/cifar100-LT/debug
[06:44:43.026191] Epoch: [8]  [ 0/42]  eta: 0:01:00  lr: 0.000480  loss: 1.0245 (1.0245)  time: 1.4419  data: 0.9223  max mem: 9341
[06:44:53.202173] Epoch: [8]  [20/42]  eta: 0:00:12  lr: 0.000503  loss: 1.0092 (1.0112)  time: 0.5087  data: 0.0001  max mem: 9341
[06:45:03.365159] Epoch: [8]  [40/42]  eta: 0:00:01  lr: 0.000526  loss: 1.0002 (1.0068)  time: 0.5081  data: 0.0001  max mem: 9341
[06:45:03.870783] Epoch: [8]  [41/42]  eta: 0:00:00  lr: 0.000526  loss: 1.0024 (1.0067)  time: 0.5080  data: 0.0001  max mem: 9341
[06:45:04.043061] Epoch: [8] Total time: 0:00:22 (0.5348 s / it)
[06:45:04.053187] Averaged stats: lr: 0.000526  loss: 1.0024 (1.0044)
[06:45:08.670986] {"train_lr": 0.000499591836734694, "train_loss": 1.0044042326155163, "epoch": 8}
[06:45:08.671381] [06:45:08.671472] Training epoch 8 for 0:00:27
[06:45:08.671529] [06:45:08.675969] log_dir: ./exp/debug/cifar100-LT/debug
[06:45:10.224813] Epoch: [9]  [ 0/42]  eta: 0:01:05  lr: 0.000540  loss: 0.9814 (0.9814)  time: 1.5476  data: 1.0398  max mem: 9341
[06:45:20.388005] Epoch: [9]  [20/42]  eta: 0:00:12  lr: 0.000563  loss: 0.9892 (0.9901)  time: 0.5081  data: 0.0001  max mem: 9341
[06:45:30.544522] Epoch: [9]  [40/42]  eta: 0:00:01  lr: 0.000586  loss: 0.9833 (0.9864)  time: 0.5078  data: 0.0001  max mem: 9341
[06:45:31.051010] Epoch: [9]  [41/42]  eta: 0:00:00  lr: 0.000586  loss: 0.9813 (0.9859)  time: 0.5078  data: 0.0001  max mem: 9341
[06:45:31.220144] Epoch: [9] Total time: 0:00:22 (0.5368 s / it)
[06:45:31.237023] Averaged stats: lr: 0.000586  loss: 0.9813 (0.9864)
[06:45:35.847817] {"train_lr": 0.000559591836734694, "train_loss": 0.9863531376634326, "epoch": 9}
[06:45:35.848219] [06:45:35.848313] Training epoch 9 for 0:00:27
[06:45:35.848370] [06:45:35.852784] log_dir: ./exp/debug/cifar100-LT/debug
[06:45:37.357435] Epoch: [10]  [ 0/42]  eta: 0:01:03  lr: 0.000600  loss: 0.9762 (0.9762)  time: 1.5033  data: 1.0030  max mem: 9341
[06:45:47.525972] Epoch: [10]  [20/42]  eta: 0:00:12  lr: 0.000623  loss: 0.9660 (0.9671)  time: 0.5084  data: 0.0001  max mem: 9341
[06:45:57.678883] Epoch: [10]  [40/42]  eta: 0:00:01  lr: 0.000646  loss: 0.9476 (0.9576)  time: 0.5076  data: 0.0001  max mem: 9341
[06:45:58.186964] Epoch: [10]  [41/42]  eta: 0:00:00  lr: 0.000646  loss: 0.9513 (0.9575)  time: 0.5076  data: 0.0001  max mem: 9341
[06:45:58.350701] Epoch: [10] Total time: 0:00:22 (0.5357 s / it)
[06:45:58.363925] Averaged stats: lr: 0.000646  loss: 0.9513 (0.9554)
[06:46:03.012433] {"train_lr": 0.0006195918367346934, "train_loss": 0.955430999753021, "epoch": 10}
[06:46:03.012839] [06:46:03.012931] Training epoch 10 for 0:00:27
[06:46:03.012987] [06:46:03.017421] log_dir: ./exp/debug/cifar100-LT/debug
[06:46:04.617396] Epoch: [11]  [ 0/42]  eta: 0:01:07  lr: 0.000660  loss: 0.9245 (0.9245)  time: 1.5985  data: 1.0873  max mem: 9341
[06:46:14.785913] Epoch: [11]  [20/42]  eta: 0:00:12  lr: 0.000683  loss: 0.9416 (0.9349)  time: 0.5084  data: 0.0002  max mem: 9341
[06:46:24.940351] Epoch: [11]  [40/42]  eta: 0:00:01  lr: 0.000706  loss: 0.9288 (0.9305)  time: 0.5077  data: 0.0001  max mem: 9341
[06:46:25.444796] Epoch: [11]  [41/42]  eta: 0:00:00  lr: 0.000706  loss: 0.9193 (0.9298)  time: 0.5074  data: 0.0001  max mem: 9341
[06:46:25.608024] Epoch: [11] Total time: 0:00:22 (0.5379 s / it)
[06:46:25.616732] Averaged stats: lr: 0.000706  loss: 0.9193 (0.9271)
[06:46:30.305933] {"train_lr": 0.0006795918367346934, "train_loss": 0.9270867354103497, "epoch": 11}
[06:46:30.306324] [06:46:30.306406] Training epoch 11 for 0:00:27
[06:46:30.306457] [06:46:30.310929] log_dir: ./exp/debug/cifar100-LT/debug
[06:46:32.046828] Epoch: [12]  [ 0/42]  eta: 0:01:12  lr: 0.000720  loss: 0.8762 (0.8762)  time: 1.7349  data: 1.2394  max mem: 9341
[06:46:42.208602] Epoch: [12]  [20/42]  eta: 0:00:12  lr: 0.000743  loss: 0.9056 (0.9074)  time: 0.5080  data: 0.0001  max mem: 9341
[06:46:52.354837] Epoch: [12]  [40/42]  eta: 0:00:01  lr: 0.000766  loss: 0.8836 (0.8991)  time: 0.5073  data: 0.0001  max mem: 9341
[06:46:52.861044] Epoch: [12]  [41/42]  eta: 0:00:00  lr: 0.000766  loss: 0.8836 (0.8994)  time: 0.5073  data: 0.0001  max mem: 9341
[06:46:53.027969] Epoch: [12] Total time: 0:00:22 (0.5409 s / it)
[06:46:53.040505] Averaged stats: lr: 0.000766  loss: 0.8836 (0.9008)
[06:46:57.609675] {"train_lr": 0.0007395918367346938, "train_loss": 0.9008380515234811, "epoch": 12}
[06:46:57.610002] [06:46:57.610085] Training epoch 12 for 0:00:27
[06:46:57.610136] [06:46:57.614474] log_dir: ./exp/debug/cifar100-LT/debug
[06:46:59.055960] Epoch: [13]  [ 0/42]  eta: 0:01:00  lr: 0.000780  loss: 0.8792 (0.8792)  time: 1.4405  data: 0.9211  max mem: 9341
[06:47:09.259835] Epoch: [13]  [20/42]  eta: 0:00:12  lr: 0.000803  loss: 0.8841 (0.8908)  time: 0.5101  data: 0.0001  max mem: 9341
[06:47:19.408761] Epoch: [13]  [40/42]  eta: 0:00:01  lr: 0.000826  loss: 0.8761 (0.8860)  time: 0.5074  data: 0.0001  max mem: 9341
[06:47:19.915011] Epoch: [13]  [41/42]  eta: 0:00:00  lr: 0.000826  loss: 0.8761 (0.8856)  time: 0.5074  data: 0.0001  max mem: 9341
[06:47:20.074790] Epoch: [13] Total time: 0:00:22 (0.5348 s / it)
[06:47:20.082579] Averaged stats: lr: 0.000826  loss: 0.8761 (0.8861)
[06:47:24.752532] {"train_lr": 0.0007995918367346942, "train_loss": 0.8861464737426668, "epoch": 13}
[06:47:24.752875] [06:47:24.752959] Training epoch 13 for 0:00:27
[06:47:24.753026] [06:47:24.757337] log_dir: ./exp/debug/cifar100-LT/debug
[06:47:26.242319] Epoch: [14]  [ 0/42]  eta: 0:01:02  lr: 0.000840  loss: 0.8590 (0.8590)  time: 1.4837  data: 0.9813  max mem: 9341
[06:47:36.413269] Epoch: [14]  [20/42]  eta: 0:00:12  lr: 0.000863  loss: 0.8775 (0.8752)  time: 0.5085  data: 0.0001  max mem: 9341
[06:47:46.567893] Epoch: [14]  [40/42]  eta: 0:00:01  lr: 0.000886  loss: 0.8581 (0.8696)  time: 0.5077  data: 0.0001  max mem: 9341
[06:47:47.073821] Epoch: [14]  [41/42]  eta: 0:00:00  lr: 0.000886  loss: 0.8581 (0.8688)  time: 0.5076  data: 0.0001  max mem: 9341
[06:47:47.231165] Epoch: [14] Total time: 0:00:22 (0.5351 s / it)
[06:47:47.238688] Averaged stats: lr: 0.000886  loss: 0.8581 (0.8695)
[06:47:51.867288] {"train_lr": 0.0008595918367346946, "train_loss": 0.8694543093442917, "epoch": 14}
[06:47:51.867666] [06:47:51.867747] Training epoch 14 for 0:00:27
[06:47:51.867798] [06:47:51.872315] log_dir: ./exp/debug/cifar100-LT/debug
[06:47:53.396409] Epoch: [15]  [ 0/42]  eta: 0:01:03  lr: 0.000900  loss: 0.8899 (0.8899)  time: 1.5228  data: 1.0055  max mem: 9341
[06:48:03.565180] Epoch: [15]  [20/42]  eta: 0:00:12  lr: 0.000923  loss: 0.8556 (0.8570)  time: 0.5084  data: 0.0002  max mem: 9341
[06:48:13.722789] Epoch: [15]  [40/42]  eta: 0:00:01  lr: 0.000946  loss: 0.8395 (0.8473)  time: 0.5077  data: 0.0001  max mem: 9341
[06:48:14.226309] Epoch: [15]  [41/42]  eta: 0:00:00  lr: 0.000946  loss: 0.8333 (0.8465)  time: 0.5076  data: 0.0001  max mem: 9341
[06:48:14.380537] Epoch: [15] Total time: 0:00:22 (0.5359 s / it)
[06:48:14.387487] Averaged stats: lr: 0.000946  loss: 0.8333 (0.8428)
[06:48:18.934940] {"train_lr": 0.0009195918367346935, "train_loss": 0.8428160328240621, "epoch": 15}
[06:48:18.935310] [06:48:18.935414] Training epoch 15 for 0:00:27
[06:48:18.935465] [06:48:18.939873] log_dir: ./exp/debug/cifar100-LT/debug
[06:48:20.437046] Epoch: [16]  [ 0/42]  eta: 0:01:02  lr: 0.000960  loss: 0.8269 (0.8269)  time: 1.4956  data: 0.9797  max mem: 9341
[06:48:30.596701] Epoch: [16]  [20/42]  eta: 0:00:12  lr: 0.000983  loss: 0.8203 (0.8194)  time: 0.5079  data: 0.0001  max mem: 9341
[06:48:40.746347] Epoch: [16]  [40/42]  eta: 0:00:01  lr: 0.001006  loss: 0.8239 (0.8215)  time: 0.5074  data: 0.0001  max mem: 9341
[06:48:41.252134] Epoch: [16]  [41/42]  eta: 0:00:00  lr: 0.001006  loss: 0.8224 (0.8215)  time: 0.5074  data: 0.0001  max mem: 9341
[06:48:41.420040] Epoch: [16] Total time: 0:00:22 (0.5352 s / it)
[06:48:41.421744] Averaged stats: lr: 0.001006  loss: 0.8224 (0.8241)
[06:48:45.959975] {"train_lr": 0.0009795918367346932, "train_loss": 0.8240737350923675, "epoch": 16}
[06:48:45.960344] [06:48:45.960447] Training epoch 16 for 0:00:27
[06:48:45.960498] [06:48:45.964835] log_dir: ./exp/debug/cifar100-LT/debug
[06:48:47.679678] Epoch: [17]  [ 0/42]  eta: 0:01:11  lr: 0.001020  loss: 0.8408 (0.8408)  time: 1.7141  data: 1.2040  max mem: 9341
[06:48:57.841580] Epoch: [17]  [20/42]  eta: 0:00:12  lr: 0.001043  loss: 0.8499 (0.8546)  time: 0.5080  data: 0.0002  max mem: 9341
[06:49:07.998831] Epoch: [17]  [40/42]  eta: 0:00:01  lr: 0.001066  loss: 0.9354 (0.8926)  time: 0.5078  data: 0.0001  max mem: 9341
[06:49:08.501999] Epoch: [17]  [41/42]  eta: 0:00:00  lr: 0.001066  loss: 0.9354 (0.8958)  time: 0.5076  data: 0.0001  max mem: 9341
[06:49:08.670414] Epoch: [17] Total time: 0:00:22 (0.5406 s / it)
[06:49:08.678571] Averaged stats: lr: 0.001066  loss: 0.9354 (0.8963)
[06:49:13.257995] {"train_lr": 0.0010395918367346946, "train_loss": 0.8962594256514594, "epoch": 17}
[06:49:13.258330] [06:49:13.258420] Training epoch 17 for 0:00:27
[06:49:13.258474] [06:49:13.263053] log_dir: ./exp/debug/cifar100-LT/debug
[06:49:14.748411] Epoch: [18]  [ 0/42]  eta: 0:01:02  lr: 0.001080  loss: 0.9634 (0.9634)  time: 1.4841  data: 0.9517  max mem: 9341
[06:49:24.916085] Epoch: [18]  [20/42]  eta: 0:00:12  lr: 0.001103  loss: 0.9684 (0.9705)  time: 0.5083  data: 0.0002  max mem: 9341
[06:49:35.069695] Epoch: [18]  [40/42]  eta: 0:00:01  lr: 0.001126  loss: 0.8915 (0.9305)  time: 0.5076  data: 0.0001  max mem: 9341
[06:49:35.576184] Epoch: [18]  [41/42]  eta: 0:00:00  lr: 0.001126  loss: 0.9116 (0.9308)  time: 0.5077  data: 0.0001  max mem: 9341
[06:49:35.741388] Epoch: [18] Total time: 0:00:22 (0.5352 s / it)
[06:49:35.749865] Averaged stats: lr: 0.001126  loss: 0.9116 (0.9302)
[06:49:40.438977] {"train_lr": 0.0010995918367346933, "train_loss": 0.9302064779968489, "epoch": 18}
[06:49:40.439320] [06:49:40.439422] Training epoch 18 for 0:00:27
[06:49:40.439472] [06:49:40.443777] log_dir: ./exp/debug/cifar100-LT/debug
[06:49:41.878934] Epoch: [19]  [ 0/42]  eta: 0:01:00  lr: 0.001140  loss: 0.9219 (0.9219)  time: 1.4340  data: 0.9054  max mem: 9341
[06:49:52.045239] Epoch: [19]  [20/42]  eta: 0:00:12  lr: 0.001163  loss: 0.9206 (0.9116)  time: 0.5083  data: 0.0001  max mem: 9341
[06:50:02.187754] Epoch: [19]  [40/42]  eta: 0:00:01  lr: 0.001186  loss: 0.8614 (0.8891)  time: 0.5071  data: 0.0001  max mem: 9341
[06:50:02.693659] Epoch: [19]  [41/42]  eta: 0:00:00  lr: 0.001186  loss: 0.8571 (0.8863)  time: 0.5071  data: 0.0001  max mem: 9341
[06:50:02.849946] Epoch: [19] Total time: 0:00:22 (0.5335 s / it)
[06:50:02.854820] Averaged stats: lr: 0.001186  loss: 0.8571 (0.8870)
[06:50:07.425549] {"train_lr": 0.0011595918367346934, "train_loss": 0.8870089036368188, "epoch": 19}
[06:50:07.425813] [06:50:07.425894] Training epoch 19 for 0:00:26
[06:50:07.425943] [06:50:07.430251] log_dir: ./exp/debug/cifar100-LT/debug
[06:50:08.952992] Epoch: [20]  [ 0/42]  eta: 0:01:03  lr: 0.001200  loss: 0.8817 (0.8817)  time: 1.5219  data: 1.0204  max mem: 9341
[06:50:19.109133] Epoch: [20]  [20/42]  eta: 0:00:12  lr: 0.001223  loss: 0.8257 (0.8335)  time: 0.5077  data: 0.0001  max mem: 9341
[06:50:29.239846] Epoch: [20]  [40/42]  eta: 0:00:01  lr: 0.001246  loss: 0.8420 (0.8426)  time: 0.5065  data: 0.0001  max mem: 9341
[06:50:29.746255] Epoch: [20]  [41/42]  eta: 0:00:00  lr: 0.001246  loss: 0.8411 (0.8423)  time: 0.5066  data: 0.0001  max mem: 9341
[06:50:29.910193] Epoch: [20] Total time: 0:00:22 (0.5352 s / it)
[06:50:29.912040] Averaged stats: lr: 0.001246  loss: 0.8411 (0.8403)
[06:50:34.489948] {"train_lr": 0.0012195918367346945, "train_loss": 0.8403168388065838, "epoch": 20}
[06:50:34.490442] [06:50:34.490539] Training epoch 20 for 0:00:27
[06:50:34.490593] [06:50:34.495912] log_dir: ./exp/debug/cifar100-LT/debug
[06:50:36.043072] Epoch: [21]  [ 0/42]  eta: 0:01:04  lr: 0.001260  loss: 0.8784 (0.8784)  time: 1.5463  data: 1.0331  max mem: 9341
[06:50:46.206217] Epoch: [21]  [20/42]  eta: 0:00:12  lr: 0.001283  loss: 0.8555 (0.8561)  time: 0.5081  data: 0.0002  max mem: 9341
[06:50:56.359146] Epoch: [21]  [40/42]  eta: 0:00:01  lr: 0.001306  loss: 0.8361 (0.8503)  time: 0.5076  data: 0.0001  max mem: 9341
[06:50:56.865820] Epoch: [21]  [41/42]  eta: 0:00:00  lr: 0.001306  loss: 0.8361 (0.8481)  time: 0.5076  data: 0.0001  max mem: 9341
[06:50:57.034319] Epoch: [21] Total time: 0:00:22 (0.5366 s / it)
[06:50:57.037328] Averaged stats: lr: 0.001306  loss: 0.8361 (0.8342)
[06:51:01.713535] {"train_lr": 0.001279591836734693, "train_loss": 0.8342281853159269, "epoch": 21}
[06:51:01.713783] [06:51:01.713861] Training epoch 21 for 0:00:27
[06:51:01.713911] [06:51:01.718246] log_dir: ./exp/debug/cifar100-LT/debug
[06:51:03.352525] Epoch: [22]  [ 0/42]  eta: 0:01:08  lr: 0.001320  loss: 0.8158 (0.8158)  time: 1.6333  data: 1.1244  max mem: 9341
[06:51:13.535804] Epoch: [22]  [20/42]  eta: 0:00:12  lr: 0.001343  loss: 0.8255 (0.8201)  time: 0.5091  data: 0.0001  max mem: 9341
[06:51:23.706525] Epoch: [22]  [40/42]  eta: 0:00:01  lr: 0.001366  loss: 0.8109 (0.8193)  time: 0.5085  data: 0.0001  max mem: 9341
[06:51:24.212030] Epoch: [22]  [41/42]  eta: 0:00:00  lr: 0.001366  loss: 0.8109 (0.8188)  time: 0.5085  data: 0.0001  max mem: 9341
[06:51:24.384956] Epoch: [22] Total time: 0:00:22 (0.5397 s / it)
[06:51:24.392679] Averaged stats: lr: 0.001366  loss: 0.8109 (0.8222)
[06:51:28.952291] {"train_lr": 0.0013395918367346935, "train_loss": 0.8221685276145027, "epoch": 22}
[06:51:28.952640] [06:51:28.952722] Training epoch 22 for 0:00:27
[06:51:28.952832] [06:51:28.957132] log_dir: ./exp/debug/cifar100-LT/debug
[06:51:30.519823] Epoch: [23]  [ 0/42]  eta: 0:01:05  lr: 0.001380  loss: 0.8149 (0.8149)  time: 1.5615  data: 1.0641  max mem: 9341
[06:51:40.682135] Epoch: [23]  [20/42]  eta: 0:00:12  lr: 0.001403  loss: 0.8252 (0.8250)  time: 0.5081  data: 0.0001  max mem: 9341
[06:51:50.829076] Epoch: [23]  [40/42]  eta: 0:00:01  lr: 0.001426  loss: 0.7984 (0.8182)  time: 0.5073  data: 0.0001  max mem: 9341
[06:51:51.334589] Epoch: [23]  [41/42]  eta: 0:00:00  lr: 0.001426  loss: 0.8001 (0.8180)  time: 0.5073  data: 0.0001  max mem: 9341
[06:51:51.506148] Epoch: [23] Total time: 0:00:22 (0.5369 s / it)
[06:51:51.507030] Averaged stats: lr: 0.001426  loss: 0.8001 (0.8168)
[06:51:56.020918] {"train_lr": 0.001399591836734694, "train_loss": 0.8167553426963943, "epoch": 23}
[06:51:56.021272] [06:51:56.021354] Training epoch 23 for 0:00:27
[06:51:56.021407] [06:51:56.025657] log_dir: ./exp/debug/cifar100-LT/debug
[06:51:57.642839] Epoch: [24]  [ 0/42]  eta: 0:01:07  lr: 0.001440  loss: 0.7897 (0.7897)  time: 1.6164  data: 1.1086  max mem: 9341
[06:52:07.800367] Epoch: [24]  [20/42]  eta: 0:00:12  lr: 0.001463  loss: 0.8123 (0.8108)  time: 0.5078  data: 0.0001  max mem: 9341
[06:52:17.940219] Epoch: [24]  [40/42]  eta: 0:00:01  lr: 0.001486  loss: 0.8054 (0.8098)  time: 0.5069  data: 0.0001  max mem: 9341
[06:52:18.446060] Epoch: [24]  [41/42]  eta: 0:00:00  lr: 0.001486  loss: 0.8034 (0.8085)  time: 0.5070  data: 0.0001  max mem: 9341
[06:52:18.605395] Epoch: [24] Total time: 0:00:22 (0.5376 s / it)
[06:52:18.621937] Averaged stats: lr: 0.001486  loss: 0.8034 (0.8052)
[06:52:23.147003] {"train_lr": 0.0014595918367346934, "train_loss": 0.8051515044200988, "epoch": 24}
[06:52:23.147270] [06:52:23.147354] Training epoch 24 for 0:00:27
[06:52:23.147405] [06:52:23.151874] log_dir: ./exp/debug/cifar100-LT/debug
[06:52:24.820602] Epoch: [25]  [ 0/42]  eta: 0:01:10  lr: 0.001500  loss: 0.7909 (0.7909)  time: 1.6680  data: 1.1735  max mem: 9341
[06:52:35.005658] Epoch: [25]  [20/42]  eta: 0:00:12  lr: 0.001523  loss: 0.8051 (0.8017)  time: 0.5092  data: 0.0001  max mem: 9341
[06:52:45.180584] Epoch: [25]  [40/42]  eta: 0:00:01  lr: 0.001546  loss: 0.7999 (0.8032)  time: 0.5087  data: 0.0001  max mem: 9341
[06:52:45.686869] Epoch: [25]  [41/42]  eta: 0:00:00  lr: 0.001546  loss: 0.7999 (0.8035)  time: 0.5088  data: 0.0001  max mem: 9341
[06:52:45.868038] Epoch: [25] Total time: 0:00:22 (0.5409 s / it)
[06:52:45.883621] Averaged stats: lr: 0.001546  loss: 0.7999 (0.8004)
[06:52:50.456951] {"train_lr": 0.001519591836734695, "train_loss": 0.8004035385591644, "epoch": 25}
[06:52:50.457287] [06:52:50.457369] Training epoch 25 for 0:00:27
[06:52:50.457420] [06:52:50.461775] log_dir: ./exp/debug/cifar100-LT/debug
[06:52:52.059521] Epoch: [26]  [ 0/42]  eta: 0:01:07  lr: 0.001560  loss: 0.8080 (0.8080)  time: 1.5969  data: 1.0812  max mem: 9341
[06:53:02.220664] Epoch: [26]  [20/42]  eta: 0:00:12  lr: 0.001583  loss: 0.8014 (0.8012)  time: 0.5080  data: 0.0001  max mem: 9341
[06:53:12.374401] Epoch: [26]  [40/42]  eta: 0:00:01  lr: 0.001606  loss: 0.7931 (0.7993)  time: 0.5076  data: 0.0001  max mem: 9341
[06:53:12.882006] Epoch: [26]  [41/42]  eta: 0:00:00  lr: 0.001606  loss: 0.7966 (0.7995)  time: 0.5077  data: 0.0001  max mem: 9341
[06:53:13.040374] Epoch: [26] Total time: 0:00:22 (0.5376 s / it)
[06:53:13.055307] Averaged stats: lr: 0.001606  loss: 0.7966 (0.7970)
[06:53:17.762745] {"train_lr": 0.0015795918367346937, "train_loss": 0.797012675021376, "epoch": 26}
[06:53:17.763030] [06:53:17.763112] Training epoch 26 for 0:00:27
[06:53:17.763163] [06:53:17.767688] log_dir: ./exp/debug/cifar100-LT/debug
[06:53:19.348711] Epoch: [27]  [ 0/42]  eta: 0:01:06  lr: 0.001620  loss: 0.8279 (0.8279)  time: 1.5801  data: 1.0610  max mem: 9341
[06:53:29.511623] Epoch: [27]  [20/42]  eta: 0:00:12  lr: 0.001643  loss: 0.8039 (0.8020)  time: 0.5081  data: 0.0001  max mem: 9341
[06:53:39.654164] Epoch: [27]  [40/42]  eta: 0:00:01  lr: 0.001666  loss: 0.7863 (0.7967)  time: 0.5071  data: 0.0001  max mem: 9341
[06:53:40.161237] Epoch: [27]  [41/42]  eta: 0:00:00  lr: 0.001666  loss: 0.7863 (0.7977)  time: 0.5072  data: 0.0001  max mem: 9341
[06:53:40.330671] Epoch: [27] Total time: 0:00:22 (0.5372 s / it)
[06:53:40.341597] Averaged stats: lr: 0.001666  loss: 0.7863 (0.7998)
[06:53:44.869252] {"train_lr": 0.001639591836734693, "train_loss": 0.7998103382331985, "epoch": 27}
[06:53:44.869578] [06:53:44.869659] Training epoch 27 for 0:00:27
[06:53:44.869710] [06:53:44.874103] log_dir: ./exp/debug/cifar100-LT/debug
[06:53:46.510663] Epoch: [28]  [ 0/42]  eta: 0:01:08  lr: 0.001680  loss: 0.8156 (0.8156)  time: 1.6358  data: 1.1245  max mem: 9341
[06:53:56.680227] Epoch: [28]  [20/42]  eta: 0:00:12  lr: 0.001703  loss: 0.8169 (0.8096)  time: 0.5084  data: 0.0001  max mem: 9341
[06:54:06.825866] Epoch: [28]  [40/42]  eta: 0:00:01  lr: 0.001726  loss: 0.8185 (0.8169)  time: 0.5072  data: 0.0001  max mem: 9341
[06:54:07.331865] Epoch: [28]  [41/42]  eta: 0:00:00  lr: 0.001726  loss: 0.8250 (0.8174)  time: 0.5072  data: 0.0001  max mem: 9341
[06:54:07.497816] Epoch: [28] Total time: 0:00:22 (0.5387 s / it)
[06:54:07.510646] Averaged stats: lr: 0.001726  loss: 0.8250 (0.8123)
[06:54:12.006725] {"train_lr": 0.001699591836734695, "train_loss": 0.8123066854618844, "epoch": 28}
[06:54:12.007000] [06:54:12.007082] Training epoch 28 for 0:00:27
[06:54:12.007150] [06:54:12.011569] log_dir: ./exp/debug/cifar100-LT/debug
[06:54:13.699692] Epoch: [29]  [ 0/42]  eta: 0:01:10  lr: 0.001740  loss: 0.7741 (0.7741)  time: 1.6873  data: 1.1816  max mem: 9341
[06:54:23.882986] Epoch: [29]  [20/42]  eta: 0:00:12  lr: 0.001763  loss: 0.8307 (0.8311)  time: 0.5091  data: 0.0001  max mem: 9341
[06:54:34.037885] Epoch: [29]  [40/42]  eta: 0:00:01  lr: 0.001786  loss: 0.8271 (0.8307)  time: 0.5077  data: 0.0001  max mem: 9341
[06:54:34.543347] Epoch: [29]  [41/42]  eta: 0:00:00  lr: 0.001786  loss: 0.8251 (0.8298)  time: 0.5077  data: 0.0001  max mem: 9341
[06:54:34.712818] Epoch: [29] Total time: 0:00:22 (0.5405 s / it)
[06:54:34.715196] Averaged stats: lr: 0.001786  loss: 0.8251 (0.8287)
[06:54:39.319178] {"train_lr": 0.0017595918367346926, "train_loss": 0.828711045285066, "epoch": 29}
[06:54:39.319544] [06:54:39.319625] Training epoch 29 for 0:00:27
[06:54:39.319676] [06:54:39.324048] log_dir: ./exp/debug/cifar100-LT/debug
[06:54:40.813832] Epoch: [30]  [ 0/42]  eta: 0:01:02  lr: 0.001800  loss: 0.8146 (0.8146)  time: 1.4886  data: 0.9742  max mem: 9341
[06:54:50.970856] Epoch: [30]  [20/42]  eta: 0:00:12  lr: 0.001823  loss: 0.7968 (0.8069)  time: 0.5078  data: 0.0001  max mem: 9341
[06:55:01.111235] Epoch: [30]  [40/42]  eta: 0:00:01  lr: 0.001846  loss: 0.8117 (0.8119)  time: 0.5070  data: 0.0001  max mem: 9341
[06:55:01.617037] Epoch: [30]  [41/42]  eta: 0:00:00  lr: 0.001846  loss: 0.8117 (0.8118)  time: 0.5070  data: 0.0001  max mem: 9341
[06:55:01.775117] Epoch: [30] Total time: 0:00:22 (0.5345 s / it)
[06:55:01.782570] Averaged stats: lr: 0.001846  loss: 0.8117 (0.8202)
[06:55:06.265054] {"train_lr": 0.0018195918367346926, "train_loss": 0.8202490579514277, "epoch": 30}
[06:55:06.265395] [06:55:06.265476] Training epoch 30 for 0:00:26
[06:55:06.265526] [06:55:06.269916] log_dir: ./exp/debug/cifar100-LT/debug
[06:55:07.921348] Epoch: [31]  [ 0/42]  eta: 0:01:09  lr: 0.001860  loss: 0.8657 (0.8657)  time: 1.6502  data: 1.1279  max mem: 9341
[06:55:18.087030] Epoch: [31]  [20/42]  eta: 0:00:12  lr: 0.001883  loss: 0.8167 (0.8233)  time: 0.5082  data: 0.0001  max mem: 9341
[06:55:28.232399] Epoch: [31]  [40/42]  eta: 0:00:01  lr: 0.001906  loss: 0.8140 (0.8182)  time: 0.5072  data: 0.0001  max mem: 9341
[06:55:28.739114] Epoch: [31]  [41/42]  eta: 0:00:00  lr: 0.001906  loss: 0.8134 (0.8177)  time: 0.5072  data: 0.0001  max mem: 9341
[06:55:28.905592] Epoch: [31] Total time: 0:00:22 (0.5389 s / it)
[06:55:28.909180] Averaged stats: lr: 0.001906  loss: 0.8134 (0.8193)
[06:55:33.561881] {"train_lr": 0.0018795918367346949, "train_loss": 0.8193000012210437, "epoch": 31}
[06:55:33.562244] [06:55:33.562345] Training epoch 31 for 0:00:27
[06:55:33.562398] [06:55:33.566778] log_dir: ./exp/debug/cifar100-LT/debug
[06:55:35.098889] Epoch: [32]  [ 0/42]  eta: 0:01:04  lr: 0.001920  loss: 0.7830 (0.7830)  time: 1.5310  data: 1.0210  max mem: 9341
[06:55:45.254608] Epoch: [32]  [20/42]  eta: 0:00:12  lr: 0.001943  loss: 0.8103 (0.8044)  time: 0.5077  data: 0.0001  max mem: 9341
[06:55:55.394932] Epoch: [32]  [40/42]  eta: 0:00:01  lr: 0.001966  loss: 0.7998 (0.8017)  time: 0.5070  data: 0.0001  max mem: 9341
[06:55:55.901071] Epoch: [32]  [41/42]  eta: 0:00:00  lr: 0.001966  loss: 0.7998 (0.8008)  time: 0.5070  data: 0.0001  max mem: 9341
[06:55:56.066140] Epoch: [32] Total time: 0:00:22 (0.5357 s / it)
[06:55:56.067763] Averaged stats: lr: 0.001966  loss: 0.7998 (0.8084)
[06:56:00.577060] {"train_lr": 0.0019395918367346916, "train_loss": 0.8083525768348149, "epoch": 32}
[06:56:00.577442] [06:56:00.577523] Training epoch 32 for 0:00:27
[06:56:00.577573] [06:56:00.581952] log_dir: ./exp/debug/cifar100-LT/debug
[06:56:02.050980] Epoch: [33]  [ 0/42]  eta: 0:01:01  lr: 0.001980  loss: 0.7838 (0.7838)  time: 1.4676  data: 0.9437  max mem: 9341
[06:56:12.212455] Epoch: [33]  [20/42]  eta: 0:00:12  lr: 0.002003  loss: 0.7967 (0.8012)  time: 0.5080  data: 0.0001  max mem: 9341
[06:56:22.359850] Epoch: [33]  [40/42]  eta: 0:00:01  lr: 0.002026  loss: 0.8036 (0.8045)  time: 0.5073  data: 0.0001  max mem: 9341
[06:56:22.865593] Epoch: [33]  [41/42]  eta: 0:00:00  lr: 0.002026  loss: 0.8060 (0.8049)  time: 0.5073  data: 0.0001  max mem: 9341
[06:56:23.021659] Epoch: [33] Total time: 0:00:22 (0.5343 s / it)
[06:56:23.024680] Averaged stats: lr: 0.002026  loss: 0.8060 (0.8055)
[06:56:27.561146] {"train_lr": 0.001999591836734692, "train_loss": 0.8054540111195474, "epoch": 33}
[06:56:27.561506] [06:56:27.561607] Training epoch 33 for 0:00:26
[06:56:27.561660] [06:56:27.565989] log_dir: ./exp/debug/cifar100-LT/debug
[06:56:29.145719] Epoch: [34]  [ 0/42]  eta: 0:01:06  lr: 0.002040  loss: 0.8061 (0.8061)  time: 1.5786  data: 1.0689  max mem: 9341
[06:56:39.323536] Epoch: [34]  [20/42]  eta: 0:00:12  lr: 0.002063  loss: 0.8032 (0.8023)  time: 0.5088  data: 0.0001  max mem: 9341
[06:56:49.493899] Epoch: [34]  [40/42]  eta: 0:00:01  lr: 0.002086  loss: 0.7923 (0.8002)  time: 0.5085  data: 0.0001  max mem: 9341
[06:56:50.000345] Epoch: [34]  [41/42]  eta: 0:00:00  lr: 0.002086  loss: 0.7923 (0.7998)  time: 0.5086  data: 0.0001  max mem: 9341
[06:56:50.163710] Epoch: [34] Total time: 0:00:22 (0.5380 s / it)
[06:56:50.175196] Averaged stats: lr: 0.002086  loss: 0.7923 (0.7988)
[06:56:54.760830] {"train_lr": 0.0020595918367346945, "train_loss": 0.7988490327483132, "epoch": 34}
[06:56:54.761159] [06:56:54.761247] Training epoch 34 for 0:00:27
[06:56:54.761300] [06:56:54.765759] log_dir: ./exp/debug/cifar100-LT/debug
[06:56:56.216602] Epoch: [35]  [ 0/42]  eta: 0:01:00  lr: 0.002100  loss: 0.7972 (0.7972)  time: 1.4496  data: 0.9397  max mem: 9341
[06:57:06.411836] Epoch: [35]  [20/42]  eta: 0:00:12  lr: 0.002123  loss: 0.7862 (0.7922)  time: 0.5097  data: 0.0001  max mem: 9341
[06:57:16.584962] Epoch: [35]  [40/42]  eta: 0:00:01  lr: 0.002146  loss: 0.7745 (0.7869)  time: 0.5086  data: 0.0001  max mem: 9341
[06:57:17.091652] Epoch: [35]  [41/42]  eta: 0:00:00  lr: 0.002146  loss: 0.7745 (0.7873)  time: 0.5087  data: 0.0001  max mem: 9341
[06:57:17.249141] Epoch: [35] Total time: 0:00:22 (0.5353 s / it)
[06:57:17.258106] Averaged stats: lr: 0.002146  loss: 0.7745 (0.7925)
[06:57:21.855855] {"train_lr": 0.002119591836734692, "train_loss": 0.7925045788288116, "epoch": 35}
[06:57:21.856277] [06:57:21.856369] Training epoch 35 for 0:00:27
[06:57:21.856454] [06:57:21.861323] log_dir: ./exp/debug/cifar100-LT/debug
[06:57:23.357395] Epoch: [36]  [ 0/42]  eta: 0:01:02  lr: 0.002160  loss: 0.7836 (0.7836)  time: 1.4951  data: 0.9778  max mem: 9341
[06:57:33.521472] Epoch: [36]  [20/42]  eta: 0:00:12  lr: 0.002183  loss: 0.7876 (0.7862)  time: 0.5081  data: 0.0002  max mem: 9341
[06:57:43.654512] Epoch: [36]  [40/42]  eta: 0:00:01  lr: 0.002206  loss: 0.7861 (0.7854)  time: 0.5066  data: 0.0001  max mem: 9341
[06:57:44.159401] Epoch: [36]  [41/42]  eta: 0:00:00  lr: 0.002206  loss: 0.7861 (0.7852)  time: 0.5066  data: 0.0001  max mem: 9341
[06:57:44.324997] Epoch: [36] Total time: 0:00:22 (0.5348 s / it)
[06:57:44.343563] Averaged stats: lr: 0.002206  loss: 0.7861 (0.7896)
[06:57:48.936583] {"train_lr": 0.0021795918367346918, "train_loss": 0.7895665292938551, "epoch": 36}
[06:57:48.936931] [06:57:48.937016] Training epoch 36 for 0:00:27
[06:57:48.937068] [06:57:48.941430] log_dir: ./exp/debug/cifar100-LT/debug
[06:57:50.618954] Epoch: [37]  [ 0/42]  eta: 0:01:10  lr: 0.002220  loss: 0.7790 (0.7790)  time: 1.6764  data: 1.1810  max mem: 9341
[06:58:00.785170] Epoch: [37]  [20/42]  eta: 0:00:12  lr: 0.002243  loss: 0.7938 (0.7858)  time: 0.5083  data: 0.0001  max mem: 9341
[06:58:10.933240] Epoch: [37]  [40/42]  eta: 0:00:01  lr: 0.002266  loss: 0.7856 (0.7852)  time: 0.5074  data: 0.0001  max mem: 9341
[06:58:11.439751] Epoch: [37]  [41/42]  eta: 0:00:00  lr: 0.002266  loss: 0.7841 (0.7849)  time: 0.5075  data: 0.0001  max mem: 9341
[06:58:11.611647] Epoch: [37] Total time: 0:00:22 (0.5398 s / it)
[06:58:11.614040] Averaged stats: lr: 0.002266  loss: 0.7841 (0.7882)
[06:58:16.237019] {"train_lr": 0.0022395918367346945, "train_loss": 0.7882029595119613, "epoch": 37}
[06:58:16.237369] [06:58:16.237448] Training epoch 37 for 0:00:27
[06:58:16.237499] [06:58:16.241883] log_dir: ./exp/debug/cifar100-LT/debug
[06:58:17.724541] Epoch: [38]  [ 0/42]  eta: 0:01:02  lr: 0.002280  loss: 0.8112 (0.8112)  time: 1.4814  data: 0.9734  max mem: 9341
[06:58:27.884065] Epoch: [38]  [20/42]  eta: 0:00:12  lr: 0.002303  loss: 0.7829 (0.7864)  time: 0.5079  data: 0.0001  max mem: 9341
[06:58:38.052206] Epoch: [38]  [40/42]  eta: 0:00:01  lr: 0.002326  loss: 0.7910 (0.7894)  time: 0.5084  data: 0.0001  max mem: 9341
[06:58:38.558093] Epoch: [38]  [41/42]  eta: 0:00:00  lr: 0.002326  loss: 0.7913 (0.7899)  time: 0.5084  data: 0.0001  max mem: 9341
[06:58:38.731415] Epoch: [38] Total time: 0:00:22 (0.5355 s / it)
[06:58:38.732236] Averaged stats: lr: 0.002326  loss: 0.7913 (0.7862)
[06:58:43.293415] {"train_lr": 0.0022995918367346956, "train_loss": 0.7861944039662679, "epoch": 38}
[06:58:43.293678] [06:58:43.293760] Training epoch 38 for 0:00:27
[06:58:43.293811] [06:58:43.298277] log_dir: ./exp/debug/cifar100-LT/debug
[06:58:44.758775] Epoch: [39]  [ 0/42]  eta: 0:01:01  lr: 0.002340  loss: 0.7526 (0.7526)  time: 1.4596  data: 0.9430  max mem: 9341
[06:58:54.946948] Epoch: [39]  [20/42]  eta: 0:00:12  lr: 0.002363  loss: 0.7879 (0.7835)  time: 0.5094  data: 0.0001  max mem: 9341
[06:59:05.119827] Epoch: [39]  [40/42]  eta: 0:00:01  lr: 0.002386  loss: 0.7918 (0.7922)  time: 0.5086  data: 0.0001  max mem: 9341
[06:59:05.627002] Epoch: [39]  [41/42]  eta: 0:00:00  lr: 0.002386  loss: 0.7941 (0.7931)  time: 0.5087  data: 0.0001  max mem: 9341
[06:59:05.789662] Epoch: [39] Total time: 0:00:22 (0.5355 s / it)
[06:59:05.800673] Averaged stats: lr: 0.002386  loss: 0.7941 (0.7864)
[06:59:10.392306] {"train_lr": 0.002359591836734693, "train_loss": 0.786363498086021, "epoch": 39}
[06:59:10.392697] [06:59:10.392776] Training epoch 39 for 0:00:27
[06:59:10.392828] [06:59:10.397120] log_dir: ./exp/debug/cifar100-LT/debug
[06:59:12.116631] Epoch: [40]  [ 0/42]  eta: 0:01:12  lr: 0.002400  loss: 0.7759 (0.7759)  time: 1.7184  data: 1.2102  max mem: 9341
[06:59:22.281916] Epoch: [40]  [20/42]  eta: 0:00:12  lr: 0.002400  loss: 0.7947 (0.7944)  time: 0.5082  data: 0.0001  max mem: 9341
[06:59:32.433889] Epoch: [40]  [40/42]  eta: 0:00:01  lr: 0.002400  loss: 0.7817 (0.7849)  time: 0.5076  data: 0.0001  max mem: 9341
[06:59:32.939869] Epoch: [40]  [41/42]  eta: 0:00:00  lr: 0.002400  loss: 0.7817 (0.7843)  time: 0.5076  data: 0.0001  max mem: 9341
[06:59:33.101491] Epoch: [40] Total time: 0:00:22 (0.5406 s / it)
[06:59:33.109681] Averaged stats: lr: 0.002400  loss: 0.7817 (0.7833)
[06:59:37.684544] {"train_lr": 0.0023999980161736683, "train_loss": 0.7833425164932296, "epoch": 40}
[06:59:37.684896] [06:59:37.684981] Training epoch 40 for 0:00:27
[06:59:37.685032] [06:59:37.689357] log_dir: ./exp/debug/cifar100-LT/debug
[06:59:39.121003] Epoch: [41]  [ 0/42]  eta: 0:01:00  lr: 0.002400  loss: 0.8075 (0.8075)  time: 1.4303  data: 0.9175  max mem: 9341
[06:59:49.285866] Epoch: [41]  [20/42]  eta: 0:00:12  lr: 0.002400  loss: 0.7871 (0.7866)  time: 0.5082  data: 0.0001  max mem: 9341
[06:59:59.419987] Epoch: [41]  [40/42]  eta: 0:00:01  lr: 0.002400  loss: 0.7730 (0.7821)  time: 0.5067  data: 0.0001  max mem: 9341
[06:59:59.924530] Epoch: [41]  [41/42]  eta: 0:00:00  lr: 0.002400  loss: 0.7730 (0.7825)  time: 0.5066  data: 0.0001  max mem: 9341
[07:00:00.086861] Epoch: [41] Total time: 0:00:22 (0.5333 s / it)
[07:00:00.093457] Averaged stats: lr: 0.002400  loss: 0.7730 (0.7869)
[07:00:04.619066] {"train_lr": 0.0023999810684543037, "train_loss": 0.7869043765323502, "epoch": 41}
[07:00:04.619444] [07:00:04.619525] Training epoch 41 for 0:00:26
[07:00:04.619576] [07:00:04.624009] log_dir: ./exp/debug/cifar100-LT/debug
[07:00:06.160055] Epoch: [42]  [ 0/42]  eta: 0:01:04  lr: 0.002400  loss: 0.7705 (0.7705)  time: 1.5351  data: 1.0186  max mem: 9341
[07:00:16.318710] Epoch: [42]  [20/42]  eta: 0:00:12  lr: 0.002400  loss: 0.7895 (0.7929)  time: 0.5079  data: 0.0001  max mem: 9341
[07:00:26.467683] Epoch: [42]  [40/42]  eta: 0:00:01  lr: 0.002400  loss: 0.8281 (0.8083)  time: 0.5074  data: 0.0001  max mem: 9341
[07:00:26.974058] Epoch: [42]  [41/42]  eta: 0:00:00  lr: 0.002400  loss: 0.8281 (0.8088)  time: 0.5074  data: 0.0001  max mem: 9341
[07:00:27.139355] Epoch: [42] Total time: 0:00:22 (0.5361 s / it)
[07:00:27.150815] Averaged stats: lr: 0.002400  loss: 0.8281 (0.8082)
[07:00:31.764543] {"train_lr": 0.002399943616369344, "train_loss": 0.8082203201594806, "epoch": 42}
[07:00:31.764893] [07:00:31.764971] Training epoch 42 for 0:00:27
[07:00:31.765021] [07:00:31.769307] log_dir: ./exp/debug/cifar100-LT/debug
[07:00:33.485778] Epoch: [43]  [ 0/42]  eta: 0:01:12  lr: 0.002400  loss: 0.8788 (0.8788)  time: 1.7155  data: 1.2080  max mem: 9341
[07:00:43.652218] Epoch: [43]  [20/42]  eta: 0:00:12  lr: 0.002400  loss: 0.8119 (0.8163)  time: 0.5083  data: 0.0001  max mem: 9341
[07:00:53.791881] Epoch: [43]  [40/42]  eta: 0:00:01  lr: 0.002400  loss: 0.8012 (0.8086)  time: 0.5069  data: 0.0001  max mem: 9341
[07:00:54.297126] Epoch: [43]  [41/42]  eta: 0:00:00  lr: 0.002400  loss: 0.8017 (0.8088)  time: 0.5070  data: 0.0001  max mem: 9341
[07:00:54.470113] Epoch: [43] Total time: 0:00:22 (0.5405 s / it)
[07:00:54.471007] Averaged stats: lr: 0.002400  loss: 0.8017 (0.8077)
[07:00:59.017908] {"train_lr": 0.0023998856605587328, "train_loss": 0.807717502826736, "epoch": 43}
[07:00:59.018182] [07:00:59.018275] Training epoch 43 for 0:00:27
[07:00:59.018328] [07:00:59.022757] log_dir: ./exp/debug/cifar100-LT/debug
[07:01:00.456428] Epoch: [44]  [ 0/42]  eta: 0:01:00  lr: 0.002400  loss: 0.8023 (0.8023)  time: 1.4328  data: 0.9165  max mem: 9341
[07:01:10.638156] Epoch: [44]  [20/42]  eta: 0:00:12  lr: 0.002400  loss: 0.7952 (0.8007)  time: 0.5090  data: 0.0001  max mem: 9341
[07:01:20.787906] Epoch: [44]  [40/42]  eta: 0:00:01  lr: 0.002400  loss: 0.7825 (0.7974)  time: 0.5074  data: 0.0001  max mem: 9341
[07:01:21.292849] Epoch: [44]  [41/42]  eta: 0:00:00  lr: 0.002400  loss: 0.7825 (0.7946)  time: 0.5074  data: 0.0001  max mem: 9341
[07:01:21.455478] Epoch: [44] Total time: 0:00:22 (0.5341 s / it)
[07:01:21.471423] Averaged stats: lr: 0.002400  loss: 0.7825 (0.7922)
[07:01:26.076632] {"train_lr": 0.0023998072020127827, "train_loss": 0.7922481876753625, "epoch": 44}
[07:01:26.076870] [07:01:26.076948] Training epoch 44 for 0:00:27
[07:01:26.076998] [07:01:26.081272] log_dir: ./exp/debug/cifar100-LT/debug
[07:01:27.664348] Epoch: [45]  [ 0/42]  eta: 0:01:06  lr: 0.002400  loss: 0.8408 (0.8408)  time: 1.5823  data: 1.0867  max mem: 9341
[07:01:37.829314] Epoch: [45]  [20/42]  eta: 0:00:12  lr: 0.002400  loss: 0.7977 (0.8027)  time: 0.5082  data: 0.0001  max mem: 9341
[07:01:47.979122] Epoch: [45]  [40/42]  eta: 0:00:01  lr: 0.002400  loss: 0.7889 (0.7962)  time: 0.5075  data: 0.0001  max mem: 9341
[07:01:48.484991] Epoch: [45]  [41/42]  eta: 0:00:00  lr: 0.002400  loss: 0.7889 (0.7953)  time: 0.5074  data: 0.0001  max mem: 9341
[07:01:48.648145] Epoch: [45] Total time: 0:00:22 (0.5373 s / it)
[07:01:48.656413] Averaged stats: lr: 0.002400  loss: 0.7889 (0.7921)
[07:01:53.252848] {"train_lr": 0.00239970824207213, "train_loss": 0.7921485127437682, "epoch": 45}
[07:01:53.253115] [07:01:53.253195] Training epoch 45 for 0:00:27
[07:01:53.253246] [07:01:53.257569] log_dir: ./exp/debug/cifar100-LT/debug
[07:01:54.954977] Epoch: [46]  [ 0/42]  eta: 0:01:11  lr: 0.002400  loss: 0.8125 (0.8125)  time: 1.6967  data: 1.1840  max mem: 9341
[07:02:05.140999] Epoch: [46]  [20/42]  eta: 0:00:12  lr: 0.002400  loss: 0.7803 (0.7817)  time: 0.5093  data: 0.0001  max mem: 9341
[07:02:15.307330] Epoch: [46]  [40/42]  eta: 0:00:01  lr: 0.002400  loss: 0.7914 (0.7861)  time: 0.5083  data: 0.0001  max mem: 9341
[07:02:15.814381] Epoch: [46]  [41/42]  eta: 0:00:00  lr: 0.002400  loss: 0.7914 (0.7868)  time: 0.5083  data: 0.0001  max mem: 9341
[07:02:15.993528] Epoch: [46] Total time: 0:00:22 (0.5413 s / it)
[07:02:16.001027] Averaged stats: lr: 0.002400  loss: 0.7914 (0.7894)
[07:02:20.578112] {"train_lr": 0.0023995887824277275, "train_loss": 0.7894082647703943, "epoch": 46}
[07:02:20.578442] [07:02:20.578542] Training epoch 46 for 0:00:27
[07:02:20.578595] [07:02:20.583015] log_dir: ./exp/debug/cifar100-LT/debug
[07:02:22.121559] Epoch: [47]  [ 0/42]  eta: 0:01:04  lr: 0.002399  loss: 0.7687 (0.7687)  time: 1.5375  data: 1.0304  max mem: 9341
[07:02:32.287182] Epoch: [47]  [20/42]  eta: 0:00:12  lr: 0.002399  loss: 0.7881 (0.7936)  time: 0.5082  data: 0.0001  max mem: 9341
[07:02:42.433999] Epoch: [47]  [40/42]  eta: 0:00:01  lr: 0.002399  loss: 0.7820 (0.7927)  time: 0.5073  data: 0.0001  max mem: 9341
[07:02:42.938796] Epoch: [47]  [41/42]  eta: 0:00:00  lr: 0.002399  loss: 0.7820 (0.7919)  time: 0.5072  data: 0.0001  max mem: 9341
[07:02:43.132276] Epoch: [47] Total time: 0:00:22 (0.5369 s / it)
[07:02:43.133008] Averaged stats: lr: 0.002399  loss: 0.7820 (0.7880)
[07:02:47.714210] {"train_lr": 0.0023994488251208113, "train_loss": 0.7880294106545902, "epoch": 47}
[07:02:47.714560] [07:02:47.714643] Training epoch 47 for 0:00:27
[07:02:47.714693] [07:02:47.719025] log_dir: ./exp/debug/cifar100-LT/debug
[07:02:49.199522] Epoch: [48]  [ 0/42]  eta: 0:01:02  lr: 0.002399  loss: 0.8113 (0.8113)  time: 1.4790  data: 0.9729  max mem: 9341
[07:02:59.356183] Epoch: [48]  [20/42]  eta: 0:00:12  lr: 0.002399  loss: 0.7787 (0.7803)  time: 0.5078  data: 0.0002  max mem: 9341
[07:03:09.507457] Epoch: [48]  [40/42]  eta: 0:00:01  lr: 0.002399  loss: 0.7904 (0.7854)  time: 0.5075  data: 0.0001  max mem: 9341
[07:03:10.013231] Epoch: [48]  [41/42]  eta: 0:00:00  lr: 0.002399  loss: 0.7904 (0.7854)  time: 0.5075  data: 0.0001  max mem: 9341
[07:03:10.191818] Epoch: [48] Total time: 0:00:22 (0.5351 s / it)
[07:03:10.196869] Averaged stats: lr: 0.002399  loss: 0.7904 (0.7875)
[07:03:14.786726] {"train_lr": 0.002399288372542867, "train_loss": 0.787537827023438, "epoch": 48}
[07:03:14.787087] [07:03:14.787182] Training epoch 48 for 0:00:27
[07:03:14.787234] [07:03:14.791589] log_dir: ./exp/debug/cifar100-LT/debug
[07:03:16.273503] Epoch: [49]  [ 0/42]  eta: 0:01:02  lr: 0.002399  loss: 0.7789 (0.7789)  time: 1.4807  data: 0.9641  max mem: 9341
[07:03:26.440576] Epoch: [49]  [20/42]  eta: 0:00:12  lr: 0.002399  loss: 0.7733 (0.7807)  time: 0.5083  data: 0.0002  max mem: 9341
[07:03:36.589204] Epoch: [49]  [40/42]  eta: 0:00:01  lr: 0.002399  loss: 0.7841 (0.7834)  time: 0.5074  data: 0.0001  max mem: 9341
[07:03:37.094224] Epoch: [49]  [41/42]  eta: 0:00:00  lr: 0.002399  loss: 0.7851 (0.7839)  time: 0.5072  data: 0.0001  max mem: 9341
[07:03:37.261911] Epoch: [49] Total time: 0:00:22 (0.5350 s / it)
[07:03:37.265783] Averaged stats: lr: 0.002399  loss: 0.7851 (0.7846)
[07:03:41.886485] {"train_lr": 0.0023991074274355858, "train_loss": 0.7846263948650587, "epoch": 49}
[07:03:41.886765] [07:03:41.886844] Training epoch 49 for 0:00:27
[07:03:41.886895] [07:03:41.891184] log_dir: ./exp/debug/cifar100-LT/debug
[07:03:43.437828] Epoch: [50]  [ 0/42]  eta: 0:01:04  lr: 0.002399  loss: 0.8129 (0.8129)  time: 1.5454  data: 1.0412  max mem: 9341
[07:03:53.615741] Epoch: [50]  [20/42]  eta: 0:00:12  lr: 0.002399  loss: 0.7881 (0.7969)  time: 0.5088  data: 0.0001  max mem: 9341
[07:04:03.770208] Epoch: [50]  [40/42]  eta: 0:00:01  lr: 0.002399  loss: 0.8062 (0.8026)  time: 0.5077  data: 0.0001  max mem: 9341
[07:04:04.276033] Epoch: [50]  [41/42]  eta: 0:00:00  lr: 0.002399  loss: 0.7998 (0.8016)  time: 0.5077  data: 0.0001  max mem: 9341
[07:04:04.441595] Epoch: [50] Total time: 0:00:22 (0.5369 s / it)
[07:04:04.451307] Averaged stats: lr: 0.002399  loss: 0.7998 (0.7973)
[07:04:09.020723] {"train_lr": 0.0023989059928908175, "train_loss": 0.7973339068038123, "epoch": 50}
[07:04:09.021057] [07:04:09.021139] Training epoch 50 for 0:00:27
[07:04:09.021191] [07:04:09.025620] log_dir: ./exp/debug/cifar100-LT/debug
[07:04:10.602241] Epoch: [51]  [ 0/42]  eta: 0:01:06  lr: 0.002399  loss: 0.7944 (0.7944)  time: 1.5754  data: 1.0625  max mem: 9341
[07:04:20.762581] Epoch: [51]  [20/42]  eta: 0:00:12  lr: 0.002399  loss: 0.7864 (0.7832)  time: 0.5080  data: 0.0001  max mem: 9341
[07:04:30.914985] Epoch: [51]  [40/42]  eta: 0:00:01  lr: 0.002399  loss: 0.7784 (0.7829)  time: 0.5076  data: 0.0001  max mem: 9341
[07:04:31.420768] Epoch: [51]  [41/42]  eta: 0:00:00  lr: 0.002399  loss: 0.7773 (0.7819)  time: 0.5075  data: 0.0001  max mem: 9341
[07:04:31.577304] Epoch: [51] Total time: 0:00:22 (0.5369 s / it)
[07:04:31.594840] Averaged stats: lr: 0.002399  loss: 0.7773 (0.7857)
[07:04:36.121682] {"train_lr": 0.0023986840723505235, "train_loss": 0.7856500915118626, "epoch": 51}
[07:04:36.122024] [07:04:36.122104] Training epoch 51 for 0:00:27
[07:04:36.122154] [07:04:36.126407] log_dir: ./exp/debug/cifar100-LT/debug
[07:04:37.685850] Epoch: [52]  [ 0/42]  eta: 0:01:05  lr: 0.002399  loss: 0.7521 (0.7521)  time: 1.5582  data: 1.0498  max mem: 9341
[07:04:47.855986] Epoch: [52]  [20/42]  eta: 0:00:12  lr: 0.002398  loss: 0.7738 (0.7747)  time: 0.5085  data: 0.0001  max mem: 9341
[07:04:57.998878] Epoch: [52]  [40/42]  eta: 0:00:01  lr: 0.002398  loss: 0.7638 (0.7705)  time: 0.5071  data: 0.0001  max mem: 9341
[07:04:58.504971] Epoch: [52]  [41/42]  eta: 0:00:00  lr: 0.002398  loss: 0.7648 (0.7713)  time: 0.5071  data: 0.0001  max mem: 9341
[07:04:58.673628] Epoch: [52] Total time: 0:00:22 (0.5368 s / it)
[07:04:58.683854] Averaged stats: lr: 0.002398  loss: 0.7648 (0.7798)
[07:05:03.178971] {"train_lr": 0.0023984416696067163, "train_loss": 0.7798439863891828, "epoch": 52}
[07:05:03.179263] [07:05:03.179349] Training epoch 52 for 0:00:27
[07:05:03.179457] [07:05:03.183809] log_dir: ./exp/debug/cifar100-LT/debug
[07:05:04.741147] Epoch: [53]  [ 0/42]  eta: 0:01:05  lr: 0.002398  loss: 0.7988 (0.7988)  time: 1.5564  data: 1.0450  max mem: 9341
[07:05:14.896819] Epoch: [53]  [20/42]  eta: 0:00:12  lr: 0.002398  loss: 0.7989 (0.7852)  time: 0.5077  data: 0.0001  max mem: 9341
[07:05:25.043109] Epoch: [53]  [40/42]  eta: 0:00:01  lr: 0.002398  loss: 0.7916 (0.7842)  time: 0.5073  data: 0.0001  max mem: 9341
[07:05:25.547929] Epoch: [53]  [41/42]  eta: 0:00:00  lr: 0.002398  loss: 0.7916 (0.7841)  time: 0.5072  data: 0.0001  max mem: 9341
[07:05:25.707711] Epoch: [53] Total time: 0:00:22 (0.5363 s / it)
[07:05:25.710169] Averaged stats: lr: 0.002398  loss: 0.7916 (0.7825)
[07:05:30.290049] {"train_lr": 0.0023981787888013883, "train_loss": 0.7824734101692835, "epoch": 53}
[07:05:30.290385] [07:05:30.290464] Training epoch 53 for 0:00:27
[07:05:30.290516] [07:05:30.294980] log_dir: ./exp/debug/cifar100-LT/debug
[07:05:31.903613] Epoch: [54]  [ 0/42]  eta: 0:01:07  lr: 0.002398  loss: 0.7919 (0.7919)  time: 1.6077  data: 1.0959  max mem: 9341
[07:05:42.068538] Epoch: [54]  [20/42]  eta: 0:00:12  lr: 0.002398  loss: 0.7831 (0.7803)  time: 0.5082  data: 0.0001  max mem: 9341
[07:05:52.219253] Epoch: [54]  [40/42]  eta: 0:00:01  lr: 0.002398  loss: 0.7765 (0.7791)  time: 0.5075  data: 0.0001  max mem: 9341
[07:05:52.725937] Epoch: [54]  [41/42]  eta: 0:00:00  lr: 0.002398  loss: 0.7765 (0.7800)  time: 0.5075  data: 0.0001  max mem: 9341
[07:05:52.887567] Epoch: [54] Total time: 0:00:22 (0.5379 s / it)
[07:05:52.895720] Averaged stats: lr: 0.002398  loss: 0.7765 (0.7787)
[07:05:57.395237] {"train_lr": 0.002397895434426448, "train_loss": 0.7786784828418777, "epoch": 54}
[07:05:57.395630] [07:05:57.395716] Training epoch 54 for 0:00:27
[07:05:57.395768] [07:05:57.400258] log_dir: ./exp/debug/cifar100-LT/debug
[07:05:59.091931] Epoch: [55]  [ 0/42]  eta: 0:01:11  lr: 0.002398  loss: 0.7834 (0.7834)  time: 1.6906  data: 1.1840  max mem: 9341
[07:06:09.249170] Epoch: [55]  [20/42]  eta: 0:00:12  lr: 0.002398  loss: 0.7946 (0.7904)  time: 0.5078  data: 0.0001  max mem: 9341
[07:06:19.398702] Epoch: [55]  [40/42]  eta: 0:00:01  lr: 0.002397  loss: 0.7977 (0.7935)  time: 0.5074  data: 0.0001  max mem: 9341
[07:06:19.904353] Epoch: [55]  [41/42]  eta: 0:00:00  lr: 0.002397  loss: 0.7977 (0.7945)  time: 0.5075  data: 0.0001  max mem: 9341
[07:06:20.082707] Epoch: [55] Total time: 0:00:22 (0.5401 s / it)
[07:06:20.087771] Averaged stats: lr: 0.002397  loss: 0.7977 (0.7878)
[07:06:24.587484] {"train_lr": 0.002397591611323643, "train_loss": 0.787765004095577, "epoch": 55}
[07:06:24.587825] [07:06:24.587910] Training epoch 55 for 0:00:27
[07:06:24.587959] [07:06:24.592764] log_dir: ./exp/debug/cifar100-LT/debug
[07:06:26.093645] Epoch: [56]  [ 0/42]  eta: 0:01:02  lr: 0.002397  loss: 0.7703 (0.7703)  time: 1.4996  data: 0.9984  max mem: 9341
[07:06:36.263586] Epoch: [56]  [20/42]  eta: 0:00:12  lr: 0.002397  loss: 0.7808 (0.7826)  time: 0.5085  data: 0.0001  max mem: 9341
[07:06:46.410289] Epoch: [56]  [40/42]  eta: 0:00:01  lr: 0.002397  loss: 0.7818 (0.7825)  time: 0.5073  data: 0.0001  max mem: 9341
[07:06:46.915831] Epoch: [56]  [41/42]  eta: 0:00:00  lr: 0.002397  loss: 0.7818 (0.7818)  time: 0.5073  data: 0.0001  max mem: 9341
[07:06:47.079557] Epoch: [56] Total time: 0:00:22 (0.5354 s / it)
[07:06:47.086951] Averaged stats: lr: 0.002397  loss: 0.7818 (0.7836)
[07:06:51.723070] {"train_lr": 0.002397267324684461, "train_loss": 0.7835658973171598, "epoch": 56}
[07:06:51.723400] [07:06:51.723485] Training epoch 56 for 0:00:27
[07:06:51.723537] [07:06:51.727876] log_dir: ./exp/debug/cifar100-LT/debug
[07:06:53.298341] Epoch: [57]  [ 0/42]  eta: 0:01:05  lr: 0.002397  loss: 0.7546 (0.7546)  time: 1.5697  data: 1.0733  max mem: 9341
[07:07:03.477322] Epoch: [57]  [20/42]  eta: 0:00:12  lr: 0.002397  loss: 0.7891 (0.7854)  time: 0.5089  data: 0.0001  max mem: 9341
[07:07:13.638463] Epoch: [57]  [40/42]  eta: 0:00:01  lr: 0.002397  loss: 0.7877 (0.7854)  time: 0.5080  data: 0.0001  max mem: 9341
[07:07:14.143943] Epoch: [57]  [41/42]  eta: 0:00:00  lr: 0.002397  loss: 0.7877 (0.7860)  time: 0.5081  data: 0.0001  max mem: 9341
[07:07:14.316130] Epoch: [57] Total time: 0:00:22 (0.5378 s / it)
[07:07:14.317906] Averaged stats: lr: 0.002397  loss: 0.7877 (0.7820)
[07:07:18.910077] {"train_lr": 0.002396922580050078, "train_loss": 0.7819572048527854, "epoch": 57}
[07:07:18.910451] [07:07:18.910533] Training epoch 57 for 0:00:27
[07:07:18.910585] [07:07:18.914964] log_dir: ./exp/debug/cifar100-LT/debug
[07:07:20.400898] Epoch: [58]  [ 0/42]  eta: 0:01:02  lr: 0.002397  loss: 0.7605 (0.7605)  time: 1.4846  data: 0.9746  max mem: 9341
[07:07:30.619751] Epoch: [58]  [20/42]  eta: 0:00:12  lr: 0.002397  loss: 0.7844 (0.7790)  time: 0.5109  data: 0.0001  max mem: 9341
[07:07:40.779652] Epoch: [58]  [40/42]  eta: 0:00:01  lr: 0.002396  loss: 0.7980 (0.7875)  time: 0.5080  data: 0.0001  max mem: 9341
[07:07:41.284332] Epoch: [58]  [41/42]  eta: 0:00:00  lr: 0.002396  loss: 0.7948 (0.7873)  time: 0.5080  data: 0.0001  max mem: 9341
[07:07:41.452619] Epoch: [58] Total time: 0:00:22 (0.5366 s / it)
[07:07:41.457374] Averaged stats: lr: 0.002396  loss: 0.7948 (0.7791)
[07:07:45.962999] {"train_lr": 0.002396557383311222, "train_loss": 0.7791292607074692, "epoch": 58}
[07:07:45.963316] [07:07:45.963415] Training epoch 58 for 0:00:27
[07:07:45.963469] [07:07:45.967893] log_dir: ./exp/debug/cifar100-LT/debug
[07:07:47.527362] Epoch: [59]  [ 0/42]  eta: 0:01:05  lr: 0.002396  loss: 0.8567 (0.8567)  time: 1.5585  data: 1.0474  max mem: 9341
[07:07:57.685196] Epoch: [59]  [20/42]  eta: 0:00:12  lr: 0.002396  loss: 0.7904 (0.7846)  time: 0.5078  data: 0.0001  max mem: 9341
[07:08:07.831586] Epoch: [59]  [40/42]  eta: 0:00:01  lr: 0.002396  loss: 0.7724 (0.7784)  time: 0.5073  data: 0.0001  max mem: 9341
[07:08:08.336753] Epoch: [59]  [41/42]  eta: 0:00:00  lr: 0.002396  loss: 0.7658 (0.7774)  time: 0.5072  data: 0.0001  max mem: 9341
[07:08:08.510063] Epoch: [59] Total time: 0:00:22 (0.5367 s / it)
[07:08:08.516948] Averaged stats: lr: 0.002396  loss: 0.7658 (0.7787)
[07:08:13.228074] {"train_lr": 0.0023961717407080993, "train_loss": 0.7786710194888569, "epoch": 59}
[07:08:13.228458] [07:08:13.228540] Training epoch 59 for 0:00:27
[07:08:13.228591] [07:08:13.233357] log_dir: ./exp/debug/cifar100-LT/debug
[07:08:14.822907] Epoch: [60]  [ 0/42]  eta: 0:01:06  lr: 0.002396  loss: 0.7537 (0.7537)  time: 1.5883  data: 1.0747  max mem: 9341
[07:08:24.993199] Epoch: [60]  [20/42]  eta: 0:00:12  lr: 0.002396  loss: 0.7839 (0.7887)  time: 0.5085  data: 0.0001  max mem: 9341
[07:08:35.124823] Epoch: [60]  [40/42]  eta: 0:00:01  lr: 0.002396  loss: 0.7688 (0.7824)  time: 0.5065  data: 0.0001  max mem: 9341
[07:08:35.628594] Epoch: [60]  [41/42]  eta: 0:00:00  lr: 0.002396  loss: 0.7688 (0.7834)  time: 0.5065  data: 0.0001  max mem: 9341
[07:08:35.788119] Epoch: [60] Total time: 0:00:22 (0.5370 s / it)
[07:08:35.794042] Averaged stats: lr: 0.002396  loss: 0.7688 (0.7769)
[07:08:40.403167] {"train_lr": 0.0023957656588302783, "train_loss": 0.7768958711198398, "epoch": 60}
[07:08:40.403513] [07:08:40.403597] Training epoch 60 for 0:00:27
[07:08:40.403649] [07:08:40.408048] log_dir: ./exp/debug/cifar100-LT/debug
[07:08:42.043872] Epoch: [61]  [ 0/42]  eta: 0:01:08  lr: 0.002395  loss: 0.7916 (0.7916)  time: 1.6348  data: 1.1306  max mem: 9341
[07:08:52.207907] Epoch: [61]  [20/42]  eta: 0:00:12  lr: 0.002395  loss: 0.7590 (0.7656)  time: 0.5082  data: 0.0001  max mem: 9341
[07:09:02.360472] Epoch: [61]  [40/42]  eta: 0:00:01  lr: 0.002395  loss: 0.7798 (0.7780)  time: 0.5076  data: 0.0001  max mem: 9341
[07:09:02.866843] Epoch: [61]  [41/42]  eta: 0:00:00  lr: 0.002395  loss: 0.7798 (0.7783)  time: 0.5076  data: 0.0001  max mem: 9341
[07:09:03.028625] Epoch: [61] Total time: 0:00:22 (0.5386 s / it)
[07:09:03.034479] Averaged stats: lr: 0.002395  loss: 0.7798 (0.7800)
[07:09:07.628051] {"train_lr": 0.0023953391446165794, "train_loss": 0.779952085798695, "epoch": 61}
[07:09:07.628488] [07:09:07.628576] Training epoch 61 for 0:00:27
[07:09:07.628627] [07:09:07.633397] log_dir: ./exp/debug/cifar100-LT/debug
[07:09:09.076336] Epoch: [62]  [ 0/42]  eta: 0:01:00  lr: 0.002395  loss: 0.8091 (0.8091)  time: 1.4416  data: 0.9179  max mem: 9341
[07:09:19.256737] Epoch: [62]  [20/42]  eta: 0:00:12  lr: 0.002395  loss: 0.7942 (0.8022)  time: 0.5090  data: 0.0001  max mem: 9341
[07:09:29.427255] Epoch: [62]  [40/42]  eta: 0:00:01  lr: 0.002395  loss: 0.8340 (0.8189)  time: 0.5085  data: 0.0001  max mem: 9341
[07:09:29.934179] Epoch: [62]  [41/42]  eta: 0:00:00  lr: 0.002395  loss: 0.8272 (0.8186)  time: 0.5087  data: 0.0001  max mem: 9341
[07:09:30.095198] Epoch: [62] Total time: 0:00:22 (0.5348 s / it)
[07:09:30.106343] Averaged stats: lr: 0.002395  loss: 0.8272 (0.8229)
[07:09:34.732980] {"train_lr": 0.0023948922053549475, "train_loss": 0.8229160418822652, "epoch": 62}
[07:09:34.733333] [07:09:34.733415] Training epoch 62 for 0:00:27
[07:09:34.733466] [07:09:34.737842] log_dir: ./exp/debug/cifar100-LT/debug
[07:09:36.259619] Epoch: [63]  [ 0/42]  eta: 0:01:03  lr: 0.002395  loss: 0.7994 (0.7994)  time: 1.5206  data: 1.0063  max mem: 9341
[07:09:46.421068] Epoch: [63]  [20/42]  eta: 0:00:12  lr: 0.002394  loss: 0.7896 (0.7910)  time: 0.5080  data: 0.0001  max mem: 9341
[07:09:56.561023] Epoch: [63]  [40/42]  eta: 0:00:01  lr: 0.002394  loss: 0.8031 (0.7930)  time: 0.5070  data: 0.0001  max mem: 9341
[07:09:57.067116] Epoch: [63]  [41/42]  eta: 0:00:00  lr: 0.002394  loss: 0.8038 (0.7937)  time: 0.5070  data: 0.0001  max mem: 9341
[07:09:57.225408] Epoch: [63] Total time: 0:00:22 (0.5354 s / it)
[07:09:57.231201] Averaged stats: lr: 0.002394  loss: 0.8038 (0.7999)
[07:10:01.698427] {"train_lr": 0.0023944248486823514, "train_loss": 0.7999439626222565, "epoch": 63}
[07:10:01.698793] [07:10:01.698872] Training epoch 63 for 0:00:26
[07:10:01.698923] [07:10:01.703396] log_dir: ./exp/debug/cifar100-LT/debug
[07:10:03.434773] Epoch: [64]  [ 0/42]  eta: 0:01:12  lr: 0.002394  loss: 0.7336 (0.7336)  time: 1.7301  data: 1.2343  max mem: 9341
[07:10:13.601824] Epoch: [64]  [20/42]  eta: 0:00:12  lr: 0.002394  loss: 0.7899 (0.7879)  time: 0.5083  data: 0.0001  max mem: 9341
[07:10:23.749806] Epoch: [64]  [40/42]  eta: 0:00:01  lr: 0.002394  loss: 0.8056 (0.7961)  time: 0.5073  data: 0.0001  max mem: 9341
[07:10:24.256977] Epoch: [64]  [41/42]  eta: 0:00:00  lr: 0.002394  loss: 0.8056 (0.7967)  time: 0.5075  data: 0.0001  max mem: 9341
[07:10:24.423283] Epoch: [64] Total time: 0:00:22 (0.5409 s / it)
[07:10:24.426434] Averaged stats: lr: 0.002394  loss: 0.8056 (0.7963)
[07:10:29.113962] {"train_lr": 0.002393937082584618, "train_loss": 0.7963474162277722, "epoch": 64}
[07:10:29.114339] [07:10:29.114425] Training epoch 64 for 0:00:27
[07:10:29.114478] [07:10:29.118944] log_dir: ./exp/debug/cifar100-LT/debug
[07:10:30.689553] Epoch: [65]  [ 0/42]  eta: 0:01:05  lr: 0.002394  loss: 0.8318 (0.8318)  time: 1.5695  data: 1.0487  max mem: 9341
[07:10:40.859303] Epoch: [65]  [20/42]  eta: 0:00:12  lr: 0.002393  loss: 0.8073 (0.8101)  time: 0.5084  data: 0.0001  max mem: 9341
[07:10:51.015892] Epoch: [65]  [40/42]  eta: 0:00:01  lr: 0.002393  loss: 0.7983 (0.8031)  time: 0.5078  data: 0.0001  max mem: 9341
[07:10:51.521903] Epoch: [65]  [41/42]  eta: 0:00:00  lr: 0.002393  loss: 0.7983 (0.8027)  time: 0.5078  data: 0.0001  max mem: 9341
[07:10:51.684968] Epoch: [65] Total time: 0:00:22 (0.5373 s / it)
[07:10:51.689385] Averaged stats: lr: 0.002393  loss: 0.7983 (0.7982)
[07:10:56.265777] {"train_lr": 0.002393428915396328, "train_loss": 0.7982045050178256, "epoch": 65}
[07:10:56.266054] [07:10:56.266138] Training epoch 65 for 0:00:27
[07:10:56.266191] [07:10:56.270567] log_dir: ./exp/debug/cifar100-LT/debug
[07:10:57.743042] Epoch: [66]  [ 0/42]  eta: 0:01:01  lr: 0.002393  loss: 0.8002 (0.8002)  time: 1.4713  data: 0.9685  max mem: 9341
[07:11:07.934673] Epoch: [66]  [20/42]  eta: 0:00:12  lr: 0.002393  loss: 0.8009 (0.7966)  time: 0.5095  data: 0.0001  max mem: 9341
[07:11:18.103730] Epoch: [66]  [40/42]  eta: 0:00:01  lr: 0.002393  loss: 0.7773 (0.7874)  time: 0.5084  data: 0.0001  max mem: 9341
[07:11:18.610480] Epoch: [66]  [41/42]  eta: 0:00:00  lr: 0.002393  loss: 0.7773 (0.7883)  time: 0.5085  data: 0.0001  max mem: 9341
[07:11:18.771491] Epoch: [66] Total time: 0:00:22 (0.5357 s / it)
[07:11:18.783196] Averaged stats: lr: 0.002393  loss: 0.7773 (0.7908)
[07:11:23.396998] {"train_lr": 0.002392900355800657, "train_loss": 0.7907605582759494, "epoch": 66}
[07:11:23.397406] [07:11:23.397507] Training epoch 66 for 0:00:27
[07:11:23.397561] [07:11:23.402419] log_dir: ./exp/debug/cifar100-LT/debug
[07:11:24.960220] Epoch: [67]  [ 0/42]  eta: 0:01:05  lr: 0.002393  loss: 0.7910 (0.7910)  time: 1.5566  data: 1.0410  max mem: 9341
[07:11:35.124678] Epoch: [67]  [20/42]  eta: 0:00:12  lr: 0.002392  loss: 0.7858 (0.7805)  time: 0.5082  data: 0.0001  max mem: 9341
[07:11:45.279087] Epoch: [67]  [40/42]  eta: 0:00:01  lr: 0.002392  loss: 0.7764 (0.7814)  time: 0.5077  data: 0.0001  max mem: 9341
[07:11:45.786052] Epoch: [67]  [41/42]  eta: 0:00:00  lr: 0.002392  loss: 0.7727 (0.7802)  time: 0.5078  data: 0.0001  max mem: 9341
[07:11:45.953221] Epoch: [67] Total time: 0:00:22 (0.5369 s / it)
[07:11:45.963400] Averaged stats: lr: 0.002392  loss: 0.7727 (0.7839)
[07:11:50.455915] {"train_lr": 0.0023923514128292287, "train_loss": 0.7838797139979544, "epoch": 67}
[07:11:50.456315] [07:11:50.456415] Training epoch 67 for 0:00:27
[07:11:50.456483] [07:11:50.460831] log_dir: ./exp/debug/cifar100-LT/debug
[07:11:52.147112] Epoch: [68]  [ 0/42]  eta: 0:01:10  lr: 0.002392  loss: 0.8088 (0.8088)  time: 1.6853  data: 1.1865  max mem: 9341
[07:12:02.311015] Epoch: [68]  [20/42]  eta: 0:00:12  lr: 0.002392  loss: 0.7807 (0.7798)  time: 0.5081  data: 0.0001  max mem: 9341
[07:12:12.446972] Epoch: [68]  [40/42]  eta: 0:00:01  lr: 0.002392  loss: 0.7784 (0.7795)  time: 0.5068  data: 0.0001  max mem: 9341
[07:12:12.951821] Epoch: [68]  [41/42]  eta: 0:00:00  lr: 0.002392  loss: 0.7839 (0.7796)  time: 0.5067  data: 0.0001  max mem: 9341
[07:12:13.122038] Epoch: [68] Total time: 0:00:22 (0.5396 s / it)
[07:12:13.132988] Averaged stats: lr: 0.002392  loss: 0.7839 (0.7790)
[07:12:17.624126] {"train_lr": 0.002391782095861968, "train_loss": 0.7790054082870483, "epoch": 68}
[07:12:17.624477] [07:12:17.624562] Training epoch 68 for 0:00:27
[07:12:17.624612] [07:12:17.628911] log_dir: ./exp/debug/cifar100-LT/debug
[07:12:19.178152] Epoch: [69]  [ 0/42]  eta: 0:01:05  lr: 0.002391  loss: 0.7536 (0.7536)  time: 1.5484  data: 1.0317  max mem: 9341
[07:12:29.366391] Epoch: [69]  [20/42]  eta: 0:00:12  lr: 0.002391  loss: 0.7557 (0.7651)  time: 0.5094  data: 0.0001  max mem: 9341
[07:12:39.532887] Epoch: [69]  [40/42]  eta: 0:00:01  lr: 0.002391  loss: 0.7693 (0.7660)  time: 0.5083  data: 0.0001  max mem: 9341
[07:12:40.038534] Epoch: [69]  [41/42]  eta: 0:00:00  lr: 0.002391  loss: 0.7704 (0.7664)  time: 0.5082  data: 0.0001  max mem: 9341
[07:12:40.215357] Epoch: [69] Total time: 0:00:22 (0.5378 s / it)
[07:12:40.216236] Averaged stats: lr: 0.002391  loss: 0.7704 (0.7733)
[07:12:44.822963] {"train_lr": 0.0023911924146269217, "train_loss": 0.7732888630458287, "epoch": 69}
[07:12:44.823213] [07:12:44.823296] Training epoch 69 for 0:00:27
[07:12:44.823348] [07:12:44.827541] log_dir: ./exp/debug/cifar100-LT/debug
[07:12:46.282299] Epoch: [70]  [ 0/42]  eta: 0:01:01  lr: 0.002391  loss: 0.7678 (0.7678)  time: 1.4538  data: 0.9593  max mem: 9341
[07:12:56.447069] Epoch: [70]  [20/42]  eta: 0:00:12  lr: 0.002391  loss: 0.7636 (0.7702)  time: 0.5082  data: 0.0001  max mem: 9341
[07:13:06.591557] Epoch: [70]  [40/42]  eta: 0:00:01  lr: 0.002390  loss: 0.7683 (0.7708)  time: 0.5072  data: 0.0001  max mem: 9341
[07:13:07.095477] Epoch: [70]  [41/42]  eta: 0:00:00  lr: 0.002390  loss: 0.7683 (0.7701)  time: 0.5071  data: 0.0001  max mem: 9341
[07:13:07.259304] Epoch: [70] Total time: 0:00:22 (0.5341 s / it)
[07:13:07.278520] Averaged stats: lr: 0.002390  loss: 0.7683 (0.7718)
[07:13:11.782075] {"train_lr": 0.0023905823792001253, "train_loss": 0.7717520175945192, "epoch": 70}
[07:13:11.782446] [07:13:11.782538] Training epoch 70 for 0:00:26
[07:13:11.782590] [07:13:11.787334] log_dir: ./exp/debug/cifar100-LT/debug
[07:13:13.300687] Epoch: [71]  [ 0/42]  eta: 0:01:03  lr: 0.002390  loss: 0.7661 (0.7661)  time: 1.5120  data: 0.9919  max mem: 9341
[07:13:23.464827] Epoch: [71]  [20/42]  eta: 0:00:12  lr: 0.002390  loss: 0.7624 (0.7618)  time: 0.5082  data: 0.0001  max mem: 9341
[07:13:33.619562] Epoch: [71]  [40/42]  eta: 0:00:01  lr: 0.002390  loss: 0.7847 (0.7726)  time: 0.5077  data: 0.0001  max mem: 9341
[07:13:34.127011] Epoch: [71]  [41/42]  eta: 0:00:00  lr: 0.002390  loss: 0.7854 (0.7729)  time: 0.5078  data: 0.0001  max mem: 9341
[07:13:34.288868] Epoch: [71] Total time: 0:00:22 (0.5357 s / it)
[07:13:34.299791] Averaged stats: lr: 0.002390  loss: 0.7854 (0.7692)
[07:13:38.832177] {"train_lr": 0.0023899520000053957, "train_loss": 0.7692064217158726, "epoch": 71}
[07:13:38.832528] [07:13:38.832624] Training epoch 71 for 0:00:27
[07:13:38.832677] [07:13:38.836993] log_dir: ./exp/debug/cifar100-LT/debug
[07:13:40.481406] Epoch: [72]  [ 0/42]  eta: 0:01:09  lr: 0.002390  loss: 0.7504 (0.7504)  time: 1.6432  data: 1.1260  max mem: 9341
[07:13:50.642229] Epoch: [72]  [20/42]  eta: 0:00:12  lr: 0.002389  loss: 0.7658 (0.7697)  time: 0.5080  data: 0.0001  max mem: 9341
[07:14:00.791562] Epoch: [72]  [40/42]  eta: 0:00:01  lr: 0.002389  loss: 0.7805 (0.7726)  time: 0.5074  data: 0.0001  max mem: 9341
[07:14:01.296650] Epoch: [72]  [41/42]  eta: 0:00:00  lr: 0.002389  loss: 0.7805 (0.7727)  time: 0.5074  data: 0.0001  max mem: 9341
[07:14:01.471494] Epoch: [72] Total time: 0:00:22 (0.5389 s / it)
[07:14:01.476985] Averaged stats: lr: 0.002389  loss: 0.7805 (0.7803)
[07:14:05.992982] {"train_lr": 0.002389301287814175, "train_loss": 0.780332825368359, "epoch": 72}
[07:14:05.993329] [07:14:05.993410] Training epoch 72 for 0:00:27
[07:14:05.993462] [07:14:05.997747] log_dir: ./exp/debug/cifar100-LT/debug
[07:14:07.468199] Epoch: [73]  [ 0/42]  eta: 0:01:01  lr: 0.002389  loss: 0.7778 (0.7778)  time: 1.4692  data: 0.9625  max mem: 9341
[07:14:17.637308] Epoch: [73]  [20/42]  eta: 0:00:12  lr: 0.002389  loss: 0.7959 (0.7973)  time: 0.5084  data: 0.0001  max mem: 9341
[07:14:27.791412] Epoch: [73]  [40/42]  eta: 0:00:01  lr: 0.002388  loss: 0.8175 (0.8094)  time: 0.5077  data: 0.0001  max mem: 9341
[07:14:28.296578] Epoch: [73]  [41/42]  eta: 0:00:00  lr: 0.002388  loss: 0.8175 (0.8105)  time: 0.5076  data: 0.0001  max mem: 9341
[07:14:28.469040] Epoch: [73] Total time: 0:00:22 (0.5350 s / it)
[07:14:28.478083] Averaged stats: lr: 0.002388  loss: 0.8175 (0.8082)
[07:14:32.989894] {"train_lr": 0.0023886302537453395, "train_loss": 0.8082252013541403, "epoch": 73}
[07:14:32.990137] [07:14:32.990215] Training epoch 73 for 0:00:26
[07:14:32.990266] [07:14:32.994616] log_dir: ./exp/debug/cifar100-LT/debug
[07:14:34.618330] Epoch: [74]  [ 0/42]  eta: 0:01:08  lr: 0.002388  loss: 0.8063 (0.8063)  time: 1.6225  data: 1.1212  max mem: 9341
[07:14:44.788745] Epoch: [74]  [20/42]  eta: 0:00:12  lr: 0.002388  loss: 0.8029 (0.8059)  time: 0.5085  data: 0.0001  max mem: 9341
[07:14:54.942119] Epoch: [74]  [40/42]  eta: 0:00:01  lr: 0.002388  loss: 0.7897 (0.8002)  time: 0.5076  data: 0.0001  max mem: 9341
[07:14:55.448254] Epoch: [74]  [41/42]  eta: 0:00:00  lr: 0.002388  loss: 0.7879 (0.7989)  time: 0.5076  data: 0.0001  max mem: 9341
[07:14:55.607857] Epoch: [74] Total time: 0:00:22 (0.5384 s / it)
[07:14:55.632234] Averaged stats: lr: 0.002388  loss: 0.7879 (0.8011)
[07:15:00.342888] {"train_lr": 0.00238793890926501, "train_loss": 0.8011019013467289, "epoch": 74}
[07:15:00.343230] [07:15:00.343318] Training epoch 74 for 0:00:27
[07:15:00.343370] [07:15:00.348030] log_dir: ./exp/debug/cifar100-LT/debug
[07:15:01.893411] Epoch: [75]  [ 0/42]  eta: 0:01:04  lr: 0.002387  loss: 0.7846 (0.7846)  time: 1.5441  data: 1.0283  max mem: 9341
[07:15:12.062725] Epoch: [75]  [20/42]  eta: 0:00:12  lr: 0.002387  loss: 0.7831 (0.7814)  time: 0.5084  data: 0.0001  max mem: 9341
[07:15:22.219207] Epoch: [75]  [40/42]  eta: 0:00:01  lr: 0.002387  loss: 0.7740 (0.7801)  time: 0.5078  data: 0.0001  max mem: 9341
[07:15:22.723773] Epoch: [75]  [41/42]  eta: 0:00:00  lr: 0.002387  loss: 0.7765 (0.7800)  time: 0.5077  data: 0.0001  max mem: 9341
[07:15:22.887681] Epoch: [75] Total time: 0:00:22 (0.5367 s / it)
[07:15:22.900601] Averaged stats: lr: 0.002387  loss: 0.7765 (0.7850)
[07:15:27.416549] {"train_lr": 0.0023872272661863555, "train_loss": 0.7849741649060022, "epoch": 75}
[07:15:27.416816] [07:15:27.416896] Training epoch 75 for 0:00:27
[07:15:27.416946] [07:15:27.421269] log_dir: ./exp/debug/cifar100-LT/debug
[07:15:28.952519] Epoch: [76]  [ 0/42]  eta: 0:01:04  lr: 0.002387  loss: 0.7782 (0.7782)  time: 1.5300  data: 1.0242  max mem: 9341
[07:15:39.113355] Epoch: [76]  [20/42]  eta: 0:00:12  lr: 0.002386  loss: 0.7981 (0.7987)  time: 0.5080  data: 0.0001  max mem: 9341
[07:15:49.248407] Epoch: [76]  [40/42]  eta: 0:00:01  lr: 0.002386  loss: 0.7879 (0.7951)  time: 0.5067  data: 0.0001  max mem: 9341
[07:15:49.752882] Epoch: [76]  [41/42]  eta: 0:00:00  lr: 0.002386  loss: 0.7885 (0.7956)  time: 0.5067  data: 0.0001  max mem: 9341
[07:15:49.915083] Epoch: [76] Total time: 0:00:22 (0.5356 s / it)
[07:15:49.927670] Averaged stats: lr: 0.002386  loss: 0.7885 (0.7955)
[07:15:54.439941] {"train_lr": 0.0023864953366693944, "train_loss": 0.795544184034779, "epoch": 76}
[07:15:54.440333] [07:15:54.440435] Training epoch 76 for 0:00:27
[07:15:54.440487] [07:15:54.444801] log_dir: ./exp/debug/cifar100-LT/debug
[07:15:56.046715] Epoch: [77]  [ 0/42]  eta: 0:01:07  lr: 0.002386  loss: 0.7967 (0.7967)  time: 1.6006  data: 1.0873  max mem: 9341
[07:16:06.214673] Epoch: [77]  [20/42]  eta: 0:00:12  lr: 0.002386  loss: 0.7848 (0.7864)  time: 0.5084  data: 0.0001  max mem: 9341
[07:16:16.383154] Epoch: [77]  [40/42]  eta: 0:00:01  lr: 0.002385  loss: 0.7746 (0.7822)  time: 0.5084  data: 0.0001  max mem: 9341
[07:16:16.890813] Epoch: [77]  [41/42]  eta: 0:00:00  lr: 0.002385  loss: 0.7746 (0.7836)  time: 0.5084  data: 0.0001  max mem: 9341
[07:16:17.066034] Epoch: [77] Total time: 0:00:22 (0.5386 s / it)
[07:16:17.068358] Averaged stats: lr: 0.002385  loss: 0.7746 (0.7852)
[07:16:21.624145] {"train_lr": 0.002385743133220782, "train_loss": 0.7851574736691657, "epoch": 77}
[07:16:21.624601] [07:16:21.624692] Training epoch 77 for 0:00:27
[07:16:21.624744] [07:16:21.629826] log_dir: ./exp/debug/cifar100-LT/debug
[07:16:23.269615] Epoch: [78]  [ 0/42]  eta: 0:01:08  lr: 0.002385  loss: 0.8057 (0.8057)  time: 1.6391  data: 1.1389  max mem: 9341
[07:16:33.427122] Epoch: [78]  [20/42]  eta: 0:00:12  lr: 0.002385  loss: 0.7935 (0.7856)  time: 0.5078  data: 0.0001  max mem: 9341
[07:16:43.578053] Epoch: [78]  [40/42]  eta: 0:00:01  lr: 0.002385  loss: 0.7706 (0.7795)  time: 0.5075  data: 0.0001  max mem: 9341
[07:16:44.083319] Epoch: [78]  [41/42]  eta: 0:00:00  lr: 0.002385  loss: 0.7706 (0.7797)  time: 0.5075  data: 0.0001  max mem: 9341
[07:16:44.248934] Epoch: [78] Total time: 0:00:22 (0.5385 s / it)
[07:16:44.263302] Averaged stats: lr: 0.002385  loss: 0.7706 (0.7831)
[07:16:48.784423] {"train_lr": 0.0023849706686935985, "train_loss": 0.7831405426065127, "epoch": 78}
[07:16:48.784839] [07:16:48.784923] Training epoch 78 for 0:00:27
[07:16:48.784973] [07:16:48.789422] log_dir: ./exp/debug/cifar100-LT/debug
[07:16:50.549927] Epoch: [79]  [ 0/42]  eta: 0:01:13  lr: 0.002384  loss: 0.8170 (0.8170)  time: 1.7591  data: 1.2527  max mem: 9341
[07:17:00.739082] Epoch: [79]  [20/42]  eta: 0:00:12  lr: 0.002384  loss: 0.7839 (0.7832)  time: 0.5094  data: 0.0001  max mem: 9341
[07:17:10.884008] Epoch: [79]  [40/42]  eta: 0:00:01  lr: 0.002384  loss: 0.7851 (0.7833)  time: 0.5072  data: 0.0001  max mem: 9341
[07:17:11.391132] Epoch: [79]  [41/42]  eta: 0:00:00  lr: 0.002384  loss: 0.7851 (0.7823)  time: 0.5073  data: 0.0001  max mem: 9341
[07:17:11.556268] Epoch: [79] Total time: 0:00:22 (0.5421 s / it)
[07:17:11.563001] Averaged stats: lr: 0.002384  loss: 0.7851 (0.7805)
[07:17:16.231017] {"train_lr": 0.002384177956287135, "train_loss": 0.7804931207072168, "epoch": 79}
[07:17:16.231283] [07:17:16.231365] Training epoch 79 for 0:00:27
[07:17:16.231428] [07:17:16.235754] log_dir: ./exp/debug/cifar100-LT/debug
[07:17:17.974372] Epoch: [80]  [ 0/42]  eta: 0:01:12  lr: 0.002384  loss: 0.7912 (0.7912)  time: 1.7378  data: 1.2416  max mem: 9341
[07:17:28.139642] Epoch: [80]  [20/42]  eta: 0:00:12  lr: 0.002383  loss: 0.7843 (0.7809)  time: 0.5082  data: 0.0001  max mem: 9341
[07:17:38.283431] Epoch: [80]  [40/42]  eta: 0:00:01  lr: 0.002383  loss: 0.7618 (0.7762)  time: 0.5071  data: 0.0001  max mem: 9341
[07:17:38.790931] Epoch: [80]  [41/42]  eta: 0:00:00  lr: 0.002383  loss: 0.7698 (0.7770)  time: 0.5072  data: 0.0001  max mem: 9341
[07:17:38.956801] Epoch: [80] Total time: 0:00:22 (0.5410 s / it)
[07:17:38.965592] Averaged stats: lr: 0.002383  loss: 0.7698 (0.7762)
[07:17:43.598895] {"train_lr": 0.0023833650095466527, "train_loss": 0.7761840327155023, "epoch": 80}
[07:17:43.599256] [07:17:43.599339] Training epoch 80 for 0:00:27
[07:17:43.599390] [07:17:43.603706] log_dir: ./exp/debug/cifar100-LT/debug
[07:17:45.209661] Epoch: [81]  [ 0/42]  eta: 0:01:07  lr: 0.002383  loss: 0.7408 (0.7408)  time: 1.6048  data: 1.0846  max mem: 9341
[07:17:55.364681] Epoch: [81]  [20/42]  eta: 0:00:12  lr: 0.002382  loss: 0.7785 (0.7756)  time: 0.5077  data: 0.0001  max mem: 9341
[07:18:05.513056] Epoch: [81]  [40/42]  eta: 0:00:01  lr: 0.002382  loss: 0.7591 (0.7693)  time: 0.5074  data: 0.0001  max mem: 9341
[07:18:06.019768] Epoch: [81]  [41/42]  eta: 0:00:00  lr: 0.002382  loss: 0.7591 (0.7690)  time: 0.5075  data: 0.0001  max mem: 9341
[07:18:06.194914] Epoch: [81] Total time: 0:00:22 (0.5379 s / it)
[07:18:06.202613] Averaged stats: lr: 0.002382  loss: 0.7591 (0.7726)
[07:18:10.722288] {"train_lr": 0.0023825318423631747, "train_loss": 0.7725670135446957, "epoch": 81}
[07:18:10.722642] [07:18:10.722724] Training epoch 81 for 0:00:27
[07:18:10.722775] [07:18:10.727236] log_dir: ./exp/debug/cifar100-LT/debug
[07:18:12.169364] Epoch: [82]  [ 0/42]  eta: 0:01:00  lr: 0.002382  loss: 0.7911 (0.7911)  time: 1.4409  data: 0.9235  max mem: 9341
[07:18:22.334830] Epoch: [82]  [20/42]  eta: 0:00:12  lr: 0.002382  loss: 0.7723 (0.7723)  time: 0.5082  data: 0.0001  max mem: 9341
[07:18:32.484917] Epoch: [82]  [40/42]  eta: 0:00:01  lr: 0.002381  loss: 0.7622 (0.7685)  time: 0.5075  data: 0.0001  max mem: 9341
[07:18:32.990164] Epoch: [82]  [41/42]  eta: 0:00:00  lr: 0.002381  loss: 0.7622 (0.7692)  time: 0.5075  data: 0.0001  max mem: 9341
[07:18:33.152872] Epoch: [82] Total time: 0:00:22 (0.5339 s / it)
[07:18:33.165245] Averaged stats: lr: 0.002381  loss: 0.7622 (0.7725)
[07:18:37.670244] {"train_lr": 0.0023816784689732256, "train_loss": 0.7724779954268819, "epoch": 82}
[07:18:37.670605] [07:18:37.670689] Training epoch 82 for 0:00:26
[07:18:37.670740] [07:18:37.675020] log_dir: ./exp/debug/cifar100-LT/debug
[07:18:39.332661] Epoch: [83]  [ 0/42]  eta: 0:01:09  lr: 0.002381  loss: 0.8092 (0.8092)  time: 1.6565  data: 1.1348  max mem: 9341
[07:18:49.493944] Epoch: [83]  [20/42]  eta: 0:00:12  lr: 0.002381  loss: 0.7726 (0.7762)  time: 0.5080  data: 0.0001  max mem: 9341
[07:18:59.645082] Epoch: [83]  [40/42]  eta: 0:00:01  lr: 0.002380  loss: 0.7745 (0.7744)  time: 0.5075  data: 0.0001  max mem: 9341
[07:19:00.150892] Epoch: [83]  [41/42]  eta: 0:00:00  lr: 0.002380  loss: 0.7781 (0.7747)  time: 0.5075  data: 0.0001  max mem: 9341
[07:19:00.315345] Epoch: [83] Total time: 0:00:22 (0.5391 s / it)
[07:19:00.320651] Averaged stats: lr: 0.002380  loss: 0.7781 (0.7689)
[07:19:04.948312] {"train_lr": 0.0023808049039586035, "train_loss": 0.7688804527833348, "epoch": 83}
[07:19:04.948588] [07:19:04.948667] Training epoch 83 for 0:00:27
[07:19:04.948717] [07:19:04.953010] log_dir: ./exp/debug/cifar100-LT/debug
[07:19:06.562206] Epoch: [84]  [ 0/42]  eta: 0:01:07  lr: 0.002380  loss: 0.8007 (0.8007)  time: 1.6084  data: 1.0993  max mem: 9341
[07:19:16.726704] Epoch: [84]  [20/42]  eta: 0:00:12  lr: 0.002380  loss: 0.7578 (0.7634)  time: 0.5082  data: 0.0001  max mem: 9341
[07:19:26.872709] Epoch: [84]  [40/42]  eta: 0:00:01  lr: 0.002380  loss: 0.7748 (0.7676)  time: 0.5073  data: 0.0001  max mem: 9341
[07:19:27.378746] Epoch: [84]  [41/42]  eta: 0:00:00  lr: 0.002380  loss: 0.7748 (0.7676)  time: 0.5073  data: 0.0001  max mem: 9341
[07:19:27.549215] Epoch: [84] Total time: 0:00:22 (0.5380 s / it)
[07:19:27.557226] Averaged stats: lr: 0.002380  loss: 0.7748 (0.7648)
[07:19:32.073501] {"train_lr": 0.002379911162246124, "train_loss": 0.7647664004138538, "epoch": 84}
[07:19:32.073803] [07:19:32.073882] Training epoch 84 for 0:00:27
[07:19:32.073933] [07:19:32.078199] log_dir: ./exp/debug/cifar100-LT/debug
[07:19:33.474686] Epoch: [85]  [ 0/42]  eta: 0:00:58  lr: 0.002379  loss: 0.7302 (0.7302)  time: 1.3956  data: 0.8929  max mem: 9341
[07:19:43.629246] Epoch: [85]  [20/42]  eta: 0:00:12  lr: 0.002379  loss: 0.7693 (0.7731)  time: 0.5077  data: 0.0002  max mem: 9341
[07:19:53.754602] Epoch: [85]  [40/42]  eta: 0:00:01  lr: 0.002379  loss: 0.7625 (0.7669)  time: 0.5062  data: 0.0001  max mem: 9341
[07:19:54.259438] Epoch: [85]  [41/42]  eta: 0:00:00  lr: 0.002379  loss: 0.7586 (0.7667)  time: 0.5062  data: 0.0001  max mem: 9341
[07:19:54.425384] Epoch: [85] Total time: 0:00:22 (0.5321 s / it)
[07:19:54.426929] Averaged stats: lr: 0.002379  loss: 0.7586 (0.7676)
[07:19:58.963733] {"train_lr": 0.002378997259107371, "train_loss": 0.7675640026018733, "epoch": 85}
[07:19:58.964150] [07:19:58.964242] Training epoch 85 for 0:00:26
[07:19:58.964297] [07:19:58.969108] log_dir: ./exp/debug/cifar100-LT/debug
[07:20:00.423409] Epoch: [86]  [ 0/42]  eta: 0:01:01  lr: 0.002378  loss: 0.7323 (0.7323)  time: 1.4535  data: 0.9453  max mem: 9341
[07:20:10.650502] Epoch: [86]  [20/42]  eta: 0:00:12  lr: 0.002378  loss: 0.7830 (0.7834)  time: 0.5113  data: 0.0001  max mem: 9341
[07:20:20.801291] Epoch: [86]  [40/42]  eta: 0:00:01  lr: 0.002378  loss: 0.7582 (0.7694)  time: 0.5075  data: 0.0001  max mem: 9341
[07:20:21.307827] Epoch: [86]  [41/42]  eta: 0:00:00  lr: 0.002378  loss: 0.7582 (0.7691)  time: 0.5075  data: 0.0001  max mem: 9341
[07:20:21.484661] Epoch: [86] Total time: 0:00:22 (0.5361 s / it)
[07:20:21.492484] Averaged stats: lr: 0.002378  loss: 0.7582 (0.7637)
[07:20:26.033676] {"train_lr": 0.0023780632101584203, "train_loss": 0.763708682287307, "epoch": 86}
[07:20:26.034032] [07:20:26.034117] Training epoch 86 for 0:00:27
[07:20:26.034168] [07:20:26.038682] log_dir: ./exp/debug/cifar100-LT/debug
[07:20:27.582516] Epoch: [87]  [ 0/42]  eta: 0:01:04  lr: 0.002377  loss: 0.7416 (0.7416)  time: 1.5425  data: 1.0357  max mem: 9341
[07:20:37.747538] Epoch: [87]  [20/42]  eta: 0:00:12  lr: 0.002377  loss: 0.7536 (0.7609)  time: 0.5082  data: 0.0001  max mem: 9341
[07:20:47.896677] Epoch: [87]  [40/42]  eta: 0:00:01  lr: 0.002377  loss: 0.7771 (0.7698)  time: 0.5074  data: 0.0001  max mem: 9341
[07:20:48.402260] Epoch: [87]  [41/42]  eta: 0:00:00  lr: 0.002377  loss: 0.7771 (0.7704)  time: 0.5074  data: 0.0001  max mem: 9341
[07:20:48.577227] Epoch: [87] Total time: 0:00:22 (0.5366 s / it)
[07:20:48.578146] Averaged stats: lr: 0.002377  loss: 0.7771 (0.7630)
[07:20:53.086469] {"train_lr": 0.002377109031359596, "train_loss": 0.7629807829147294, "epoch": 87}
[07:20:53.086793] [07:20:53.086874] Training epoch 87 for 0:00:27
[07:20:53.086924] [07:20:53.091264] log_dir: ./exp/debug/cifar100-LT/debug
[07:20:54.741007] Epoch: [88]  [ 0/42]  eta: 0:01:09  lr: 0.002376  loss: 0.7729 (0.7729)  time: 1.6490  data: 1.1432  max mem: 9341
[07:21:04.901376] Epoch: [88]  [20/42]  eta: 0:00:12  lr: 0.002376  loss: 0.7803 (0.7839)  time: 0.5080  data: 0.0001  max mem: 9341
[07:21:15.051258] Epoch: [88]  [40/42]  eta: 0:00:01  lr: 0.002376  loss: 0.7569 (0.7734)  time: 0.5075  data: 0.0001  max mem: 9341
[07:21:15.555816] Epoch: [88]  [41/42]  eta: 0:00:00  lr: 0.002376  loss: 0.7569 (0.7732)  time: 0.5074  data: 0.0001  max mem: 9341
[07:21:15.727201] Epoch: [88] Total time: 0:00:22 (0.5389 s / it)
[07:21:15.732656] Averaged stats: lr: 0.002376  loss: 0.7569 (0.7722)
[07:21:20.282787] {"train_lr": 0.00237613473901518, "train_loss": 0.7721830311985243, "epoch": 88}
[07:21:20.283188] [07:21:20.283276] Training epoch 88 for 0:00:27
[07:21:20.283327] [07:21:20.288220] log_dir: ./exp/debug/cifar100-LT/debug
[07:21:21.784103] Epoch: [89]  [ 0/42]  eta: 0:01:02  lr: 0.002375  loss: 0.7865 (0.7865)  time: 1.4946  data: 0.9865  max mem: 9341
[07:21:31.943479] Epoch: [89]  [20/42]  eta: 0:00:12  lr: 0.002375  loss: 0.7740 (0.7778)  time: 0.5079  data: 0.0002  max mem: 9341
[07:21:42.091872] Epoch: [89]  [40/42]  eta: 0:00:01  lr: 0.002375  loss: 0.7577 (0.7683)  time: 0.5074  data: 0.0001  max mem: 9341
[07:21:42.597682] Epoch: [89]  [41/42]  eta: 0:00:00  lr: 0.002375  loss: 0.7601 (0.7682)  time: 0.5073  data: 0.0001  max mem: 9341
[07:21:42.764992] Epoch: [89] Total time: 0:00:22 (0.5352 s / it)
[07:21:42.778780] Averaged stats: lr: 0.002375  loss: 0.7601 (0.7628)
[07:21:47.280030] {"train_lr": 0.0023751403497731427, "train_loss": 0.7628321257375535, "epoch": 89}
[07:21:47.280450] [07:21:47.280537] Training epoch 89 for 0:00:26
[07:21:47.280589] [07:21:47.285020] log_dir: ./exp/debug/cifar100-LT/debug
[07:21:48.803258] Epoch: [90]  [ 0/42]  eta: 0:01:03  lr: 0.002374  loss: 0.8116 (0.8116)  time: 1.5171  data: 1.0022  max mem: 9341
[07:21:58.968140] Epoch: [90]  [20/42]  eta: 0:00:12  lr: 0.002374  loss: 0.7381 (0.7513)  time: 0.5082  data: 0.0001  max mem: 9341
[07:22:09.120642] Epoch: [90]  [40/42]  eta: 0:00:01  lr: 0.002374  loss: 0.7513 (0.7524)  time: 0.5076  data: 0.0001  max mem: 9341
[07:22:09.628162] Epoch: [90]  [41/42]  eta: 0:00:00  lr: 0.002374  loss: 0.7513 (0.7535)  time: 0.5076  data: 0.0001  max mem: 9341
[07:22:09.795737] Epoch: [90] Total time: 0:00:22 (0.5360 s / it)
[07:22:09.798400] Averaged stats: lr: 0.002374  loss: 0.7513 (0.7570)
[07:22:14.320836] {"train_lr": 0.0023741258806248447, "train_loss": 0.7570094824546859, "epoch": 90}
[07:22:14.321190] [07:22:14.321277] Training epoch 90 for 0:00:27
[07:22:14.321330] [07:22:14.325989] log_dir: ./exp/debug/cifar100-LT/debug
[07:22:16.093230] Epoch: [91]  [ 0/42]  eta: 0:01:14  lr: 0.002373  loss: 0.7227 (0.7227)  time: 1.7664  data: 1.2712  max mem: 9341
[07:22:26.255847] Epoch: [91]  [20/42]  eta: 0:00:12  lr: 0.002373  loss: 0.7444 (0.7451)  time: 0.5081  data: 0.0001  max mem: 9341
[07:22:36.405447] Epoch: [91]  [40/42]  eta: 0:00:01  lr: 0.002373  loss: 0.7397 (0.7478)  time: 0.5074  data: 0.0001  max mem: 9341
[07:22:36.912589] Epoch: [91]  [41/42]  eta: 0:00:00  lr: 0.002373  loss: 0.7397 (0.7481)  time: 0.5074  data: 0.0001  max mem: 9341
[07:22:37.069753] Epoch: [91] Total time: 0:00:22 (0.5415 s / it)
[07:22:37.075722] Averaged stats: lr: 0.002373  loss: 0.7397 (0.7541)
[07:22:41.745835] {"train_lr": 0.0023730913489047748, "train_loss": 0.7541440558575448, "epoch": 91}
[07:22:41.746234] [07:22:41.746321] Training epoch 91 for 0:00:27
[07:22:41.746372] [07:22:41.751284] log_dir: ./exp/debug/cifar100-LT/debug
[07:22:43.213954] Epoch: [92]  [ 0/42]  eta: 0:01:01  lr: 0.002372  loss: 0.7185 (0.7185)  time: 1.4614  data: 0.9585  max mem: 9341
[07:22:53.379883] Epoch: [92]  [20/42]  eta: 0:00:12  lr: 0.002372  loss: 0.7546 (0.7594)  time: 0.5083  data: 0.0001  max mem: 9341
[07:23:03.522087] Epoch: [92]  [40/42]  eta: 0:00:01  lr: 0.002372  loss: 0.7470 (0.7541)  time: 0.5071  data: 0.0001  max mem: 9341
[07:23:04.027999] Epoch: [92]  [41/42]  eta: 0:00:00  lr: 0.002372  loss: 0.7469 (0.7527)  time: 0.5071  data: 0.0001  max mem: 9341
[07:23:04.193564] Epoch: [92] Total time: 0:00:22 (0.5343 s / it)
[07:23:04.200465] Averaged stats: lr: 0.002372  loss: 0.7469 (0.7526)
[07:23:08.858538] {"train_lr": 0.0023720367722902223, "train_loss": 0.7526194314871516, "epoch": 92}
[07:23:08.858988] [07:23:08.859081] Training epoch 92 for 0:00:27
[07:23:08.859133] [07:23:08.864270] log_dir: ./exp/debug/cifar100-LT/debug
[07:23:10.333693] Epoch: [93]  [ 0/42]  eta: 0:01:01  lr: 0.002371  loss: 0.7026 (0.7026)  time: 1.4681  data: 0.9547  max mem: 9341
[07:23:20.495508] Epoch: [93]  [20/42]  eta: 0:00:12  lr: 0.002371  loss: 0.7563 (0.7571)  time: 0.5080  data: 0.0002  max mem: 9341
[07:23:30.644733] Epoch: [93]  [40/42]  eta: 0:00:01  lr: 0.002370  loss: 0.7669 (0.7592)  time: 0.5074  data: 0.0001  max mem: 9341
[07:23:31.149213] Epoch: [93]  [41/42]  eta: 0:00:00  lr: 0.002370  loss: 0.7669 (0.7602)  time: 0.5073  data: 0.0001  max mem: 9341
[07:23:31.316794] Epoch: [93] Total time: 0:00:22 (0.5346 s / it)
[07:23:31.321321] Averaged stats: lr: 0.002370  loss: 0.7669 (0.7519)
[07:23:35.898174] {"train_lr": 0.002370962168800989, "train_loss": 0.7518687432720548, "epoch": 93}
[07:23:35.898444] [07:23:35.898525] Training epoch 93 for 0:00:27
[07:23:35.898576] [07:23:35.902926] log_dir: ./exp/debug/cifar100-LT/debug
[07:23:37.648918] Epoch: [94]  [ 0/42]  eta: 0:01:13  lr: 0.002370  loss: 0.7952 (0.7952)  time: 1.7452  data: 1.2428  max mem: 9341
[07:23:47.835429] Epoch: [94]  [20/42]  eta: 0:00:12  lr: 0.002370  loss: 0.7592 (0.7657)  time: 0.5093  data: 0.0001  max mem: 9341
[07:23:58.010673] Epoch: [94]  [40/42]  eta: 0:00:01  lr: 0.002369  loss: 0.7573 (0.7654)  time: 0.5087  data: 0.0001  max mem: 9341
[07:23:58.517036] Epoch: [94]  [41/42]  eta: 0:00:00  lr: 0.002369  loss: 0.7573 (0.7653)  time: 0.5087  data: 0.0001  max mem: 9341
[07:23:58.675899] Epoch: [94] Total time: 0:00:22 (0.5422 s / it)
[07:23:58.680809] Averaged stats: lr: 0.002369  loss: 0.7573 (0.7637)
[07:24:03.250278] {"train_lr": 0.002369867556799084, "train_loss": 0.7637183740735054, "epoch": 94}
[07:24:03.250592] [07:24:03.250673] Training epoch 94 for 0:00:27
[07:24:03.250724] [07:24:03.255042] log_dir: ./exp/debug/cifar100-LT/debug
[07:24:04.739022] Epoch: [95]  [ 0/42]  eta: 0:01:02  lr: 0.002369  loss: 0.7477 (0.7477)  time: 1.4829  data: 0.9790  max mem: 9341
[07:24:14.906792] Epoch: [95]  [20/42]  eta: 0:00:12  lr: 0.002369  loss: 0.7553 (0.7593)  time: 0.5083  data: 0.0001  max mem: 9341
[07:24:25.060612] Epoch: [95]  [40/42]  eta: 0:00:01  lr: 0.002368  loss: 0.7640 (0.7616)  time: 0.5076  data: 0.0001  max mem: 9341
[07:24:25.567371] Epoch: [95]  [41/42]  eta: 0:00:00  lr: 0.002368  loss: 0.7640 (0.7624)  time: 0.5076  data: 0.0001  max mem: 9341
[07:24:25.754100] Epoch: [95] Total time: 0:00:22 (0.5357 s / it)
[07:24:25.759114] Averaged stats: lr: 0.002368  loss: 0.7640 (0.7626)
[07:24:30.412783] {"train_lr": 0.0023687529549884087, "train_loss": 0.7625661989053091, "epoch": 95}
[07:24:30.413133] [07:24:30.413212] Training epoch 95 for 0:00:27
[07:24:30.413262] [07:24:30.417584] log_dir: ./exp/debug/cifar100-LT/debug
[07:24:31.930582] Epoch: [96]  [ 0/42]  eta: 0:01:03  lr: 0.002368  loss: 0.7490 (0.7490)  time: 1.5117  data: 1.0066  max mem: 9341
[07:24:42.119389] Epoch: [96]  [20/42]  eta: 0:00:12  lr: 0.002368  loss: 0.7660 (0.7721)  time: 0.5094  data: 0.0001  max mem: 9341
[07:24:52.281125] Epoch: [96]  [40/42]  eta: 0:00:01  lr: 0.002367  loss: 0.7628 (0.7743)  time: 0.5080  data: 0.0001  max mem: 9341
[07:24:52.785947] Epoch: [96]  [41/42]  eta: 0:00:00  lr: 0.002367  loss: 0.7765 (0.7753)  time: 0.5080  data: 0.0001  max mem: 9341
[07:24:52.962158] Epoch: [96] Total time: 0:00:22 (0.5368 s / it)
[07:24:52.971362] Averaged stats: lr: 0.002367  loss: 0.7765 (0.7775)
[07:24:57.473977] {"train_lr": 0.002367618382414432, "train_loss": 0.7774696477821895, "epoch": 96}
[07:24:57.474353] [07:24:57.474435] Training epoch 96 for 0:00:27
[07:24:57.474489] [07:24:57.478920] log_dir: ./exp/debug/cifar100-LT/debug
[07:24:58.993591] Epoch: [97]  [ 0/42]  eta: 0:01:03  lr: 0.002367  loss: 0.7306 (0.7306)  time: 1.5132  data: 0.9991  max mem: 9341
[07:25:09.153183] Epoch: [97]  [20/42]  eta: 0:00:12  lr: 0.002366  loss: 0.7915 (0.7831)  time: 0.5079  data: 0.0001  max mem: 9341
[07:25:19.293299] Epoch: [97]  [40/42]  eta: 0:00:01  lr: 0.002366  loss: 0.7759 (0.7761)  time: 0.5070  data: 0.0001  max mem: 9341
[07:25:19.797334] Epoch: [97]  [41/42]  eta: 0:00:00  lr: 0.002366  loss: 0.7759 (0.7746)  time: 0.5069  data: 0.0001  max mem: 9341
[07:25:19.944590] Epoch: [97] Total time: 0:00:22 (0.5349 s / it)
[07:25:19.968522] Averaged stats: lr: 0.002366  loss: 0.7759 (0.7759)
[07:25:24.502755] {"train_lr": 0.0023664638584638686, "train_loss": 0.7759359890506381, "epoch": 97}
[07:25:24.503086] [07:25:24.503171] Training epoch 97 for 0:00:27
[07:25:24.503241] [07:25:24.508141] log_dir: ./exp/debug/cifar100-LT/debug
[07:25:25.977389] Epoch: [98]  [ 0/42]  eta: 0:01:01  lr: 0.002366  loss: 0.7567 (0.7567)  time: 1.4683  data: 0.9575  max mem: 9341
[07:25:36.150311] Epoch: [98]  [20/42]  eta: 0:00:12  lr: 0.002365  loss: 0.7524 (0.7521)  time: 0.5086  data: 0.0001  max mem: 9341
[07:25:46.321599] Epoch: [98]  [40/42]  eta: 0:00:01  lr: 0.002365  loss: 0.7478 (0.7549)  time: 0.5085  data: 0.0001  max mem: 9341
[07:25:46.828209] Epoch: [98]  [41/42]  eta: 0:00:00  lr: 0.002365  loss: 0.7492 (0.7565)  time: 0.5084  data: 0.0001  max mem: 9341
[07:25:46.982074] Epoch: [98] Total time: 0:00:22 (0.5351 s / it)
[07:25:46.990314] Averaged stats: lr: 0.002365  loss: 0.7492 (0.7648)
[07:25:51.517386] {"train_lr": 0.0023652894028643482, "train_loss": 0.7647858868752208, "epoch": 98}
[07:25:51.517698] [07:25:51.517782] Training epoch 98 for 0:00:27
[07:25:51.517835] [07:25:51.522146] log_dir: ./exp/debug/cifar100-LT/debug
[07:25:53.049667] Epoch: [99]  [ 0/42]  eta: 0:01:04  lr: 0.002364  loss: 0.7504 (0.7504)  time: 1.5263  data: 1.0080  max mem: 9341
[07:26:03.215457] Epoch: [99]  [20/42]  eta: 0:00:12  lr: 0.002364  loss: 0.7603 (0.7620)  time: 0.5082  data: 0.0001  max mem: 9341
[07:26:13.370483] Epoch: [99]  [40/42]  eta: 0:00:01  lr: 0.002364  loss: 0.7627 (0.7683)  time: 0.5077  data: 0.0001  max mem: 9341
[07:26:13.876934] Epoch: [99]  [41/42]  eta: 0:00:00  lr: 0.002364  loss: 0.7627 (0.7676)  time: 0.5077  data: 0.0001  max mem: 9341
[07:26:14.035371] Epoch: [99] Total time: 0:00:22 (0.5360 s / it)
[07:26:14.041259] Averaged stats: lr: 0.002364  loss: 0.7627 (0.7674)
[07:26:18.591408] {"train_lr": 0.002364095035684075, "train_loss": 0.7674210036084765, "epoch": 99}
[07:26:18.591747] [07:26:18.591829] Training epoch 99 for 0:00:27
[07:26:18.591882] [07:26:18.596272] log_dir: ./exp/debug/cifar100-LT/debug
[07:26:20.098246] Epoch: [100]  [ 0/42]  eta: 0:01:03  lr: 0.002363  loss: 0.7293 (0.7293)  time: 1.5012  data: 1.0047  max mem: 9341
[07:26:30.256659] Epoch: [100]  [20/42]  eta: 0:00:12  lr: 0.002363  loss: 0.7580 (0.7545)  time: 0.5079  data: 0.0001  max mem: 9341
[07:26:40.400212] Epoch: [100]  [40/42]  eta: 0:00:01  lr: 0.002362  loss: 0.7581 (0.7595)  time: 0.5071  data: 0.0001  max mem: 9341
[07:26:40.905432] Epoch: [100]  [41/42]  eta: 0:00:00  lr: 0.002362  loss: 0.7581 (0.7602)  time: 0.5071  data: 0.0001  max mem: 9341
[07:26:41.067098] Epoch: [100] Total time: 0:00:22 (0.5350 s / it)
[07:26:41.075994] Averaged stats: lr: 0.002362  loss: 0.7581 (0.7631)
[07:26:45.571314] {"train_lr": 0.002362880777331487, "train_loss": 0.763139667965117, "epoch": 100}
[07:26:45.571661] [07:26:45.571742] Training epoch 100 for 0:00:26
[07:26:45.571792] [07:26:45.576242] log_dir: ./exp/debug/cifar100-LT/debug
[07:26:47.289076] Epoch: [101]  [ 0/42]  eta: 0:01:11  lr: 0.002362  loss: 0.7950 (0.7950)  time: 1.7121  data: 1.2080  max mem: 9341
[07:26:57.480261] Epoch: [101]  [20/42]  eta: 0:00:12  lr: 0.002362  loss: 0.7592 (0.7694)  time: 0.5095  data: 0.0001  max mem: 9341
[07:27:07.650740] Epoch: [101]  [40/42]  eta: 0:00:01  lr: 0.002361  loss: 0.7757 (0.7699)  time: 0.5085  data: 0.0001  max mem: 9341
[07:27:08.157301] Epoch: [101]  [41/42]  eta: 0:00:00  lr: 0.002361  loss: 0.7757 (0.7690)  time: 0.5085  data: 0.0001  max mem: 9341
[07:27:08.321270] Epoch: [101] Total time: 0:00:22 (0.5415 s / it)
[07:27:08.327637] Averaged stats: lr: 0.002361  loss: 0.7757 (0.7608)
[07:27:12.839625] {"train_lr": 0.0023616466485549157, "train_loss": 0.7607633904332206, "epoch": 101}
[07:27:12.840017] [07:27:12.840130] Training epoch 101 for 0:00:27
[07:27:12.840187] [07:27:12.844634] log_dir: ./exp/debug/cifar100-LT/debug
[07:27:14.341322] Epoch: [102]  [ 0/42]  eta: 0:01:02  lr: 0.002361  loss: 0.7494 (0.7494)  time: 1.4957  data: 0.9774  max mem: 9341
[07:27:24.530532] Epoch: [102]  [20/42]  eta: 0:00:12  lr: 0.002360  loss: 0.7487 (0.7530)  time: 0.5094  data: 0.0001  max mem: 9341
[07:27:34.682927] Epoch: [102]  [40/42]  eta: 0:00:01  lr: 0.002360  loss: 0.7521 (0.7565)  time: 0.5076  data: 0.0001  max mem: 9341
[07:27:35.188705] Epoch: [102]  [41/42]  eta: 0:00:00  lr: 0.002360  loss: 0.7516 (0.7562)  time: 0.5075  data: 0.0001  max mem: 9341
[07:27:35.353879] Epoch: [102] Total time: 0:00:22 (0.5359 s / it)
[07:27:35.369780] Averaged stats: lr: 0.002360  loss: 0.7516 (0.7582)
[07:27:40.094650] {"train_lr": 0.00236039267044221, "train_loss": 0.7582425109687305, "epoch": 102}
[07:27:40.095095] [07:27:40.095188] Training epoch 102 for 0:00:27
[07:27:40.095241] [07:27:40.100279] log_dir: ./exp/debug/cifar100-LT/debug
[07:27:41.745462] Epoch: [103]  [ 0/42]  eta: 0:01:09  lr: 0.002360  loss: 0.7363 (0.7363)  time: 1.6439  data: 1.1363  max mem: 9341
[07:27:51.916001] Epoch: [103]  [20/42]  eta: 0:00:12  lr: 0.002359  loss: 0.7566 (0.7571)  time: 0.5085  data: 0.0001  max mem: 9341
[07:28:02.063619] Epoch: [103]  [40/42]  eta: 0:00:01  lr: 0.002359  loss: 0.7450 (0.7524)  time: 0.5073  data: 0.0001  max mem: 9341
[07:28:02.569958] Epoch: [103]  [41/42]  eta: 0:00:00  lr: 0.002359  loss: 0.7450 (0.7520)  time: 0.5074  data: 0.0001  max mem: 9341
[07:28:02.735842] Epoch: [103] Total time: 0:00:22 (0.5389 s / it)
[07:28:02.737847] Averaged stats: lr: 0.002359  loss: 0.7450 (0.7491)
[07:28:07.259252] {"train_lr": 0.0023591188644203993, "train_loss": 0.7491282153697241, "epoch": 103}
[07:28:07.259717] [07:28:07.259808] Training epoch 103 for 0:00:27
[07:28:07.259860] [07:28:07.264889] log_dir: ./exp/debug/cifar100-LT/debug
[07:28:09.024248] Epoch: [104]  [ 0/42]  eta: 0:01:13  lr: 0.002358  loss: 0.7247 (0.7247)  time: 1.7586  data: 1.2625  max mem: 9341
[07:28:19.191903] Epoch: [104]  [20/42]  eta: 0:00:12  lr: 0.002358  loss: 0.7481 (0.7436)  time: 0.5083  data: 0.0001  max mem: 9341
[07:28:29.344250] Epoch: [104]  [40/42]  eta: 0:00:01  lr: 0.002357  loss: 0.7371 (0.7439)  time: 0.5076  data: 0.0001  max mem: 9341
[07:28:29.850060] Epoch: [104]  [41/42]  eta: 0:00:00  lr: 0.002357  loss: 0.7315 (0.7436)  time: 0.5076  data: 0.0001  max mem: 9341
[07:28:30.019747] Epoch: [104] Total time: 0:00:22 (0.5418 s / it)
[07:28:30.021528] Averaged stats: lr: 0.002357  loss: 0.7315 (0.7476)
[07:28:34.625408] {"train_lr": 0.0023578252522553113, "train_loss": 0.7475613721069836, "epoch": 104}
[07:28:34.625680] [07:28:34.625765] Training epoch 104 for 0:00:27
[07:28:34.625816] [07:28:34.630061] log_dir: ./exp/debug/cifar100-LT/debug
[07:28:36.056777] Epoch: [105]  [ 0/42]  eta: 0:00:59  lr: 0.002357  loss: 0.7609 (0.7609)  time: 1.4258  data: 0.9057  max mem: 9341
[07:28:46.219230] Epoch: [105]  [20/42]  eta: 0:00:12  lr: 0.002356  loss: 0.7523 (0.7570)  time: 0.5081  data: 0.0001  max mem: 9341
[07:28:56.364009] Epoch: [105]  [40/42]  eta: 0:00:01  lr: 0.002356  loss: 0.7562 (0.7580)  time: 0.5072  data: 0.0001  max mem: 9341
[07:28:56.871192] Epoch: [105]  [41/42]  eta: 0:00:00  lr: 0.002356  loss: 0.7562 (0.7583)  time: 0.5072  data: 0.0001  max mem: 9341
[07:28:57.051981] Epoch: [105] Total time: 0:00:22 (0.5339 s / it)
[07:28:57.055535] Averaged stats: lr: 0.002356  loss: 0.7562 (0.7503)
[07:29:01.533461] {"train_lr": 0.0023565118560512177, "train_loss": 0.7503074985884485, "epoch": 105}
[07:29:01.533777] [07:29:01.533857] Training epoch 105 for 0:00:26
[07:29:01.533908] [07:29:01.538262] log_dir: ./exp/debug/cifar100-LT/debug
[07:29:03.011891] Epoch: [106]  [ 0/42]  eta: 0:01:01  lr: 0.002356  loss: 0.7363 (0.7363)  time: 1.4728  data: 0.9674  max mem: 9341
[07:29:13.181967] Epoch: [106]  [20/42]  eta: 0:00:12  lr: 0.002355  loss: 0.7400 (0.7509)  time: 0.5085  data: 0.0001  max mem: 9341
[07:29:23.330101] Epoch: [106]  [40/42]  eta: 0:00:01  lr: 0.002355  loss: 0.7327 (0.7434)  time: 0.5074  data: 0.0001  max mem: 9341
[07:29:23.835381] Epoch: [106]  [41/42]  eta: 0:00:00  lr: 0.002355  loss: 0.7346 (0.7446)  time: 0.5073  data: 0.0001  max mem: 9341
[07:29:23.991026] Epoch: [106] Total time: 0:00:22 (0.5346 s / it)
[07:29:23.995605] Averaged stats: lr: 0.002355  loss: 0.7346 (0.7436)
[07:29:28.552715] {"train_lr": 0.002355178698250422, "train_loss": 0.7435675582715443, "epoch": 106}
[07:29:28.552973] [07:29:28.553055] Training epoch 106 for 0:00:27
[07:29:28.553106] [07:29:28.557401] log_dir: ./exp/debug/cifar100-LT/debug
[07:29:30.164645] Epoch: [107]  [ 0/42]  eta: 0:01:07  lr: 0.002354  loss: 0.6764 (0.6764)  time: 1.6064  data: 1.0934  max mem: 9341
[07:29:40.356921] Epoch: [107]  [20/42]  eta: 0:00:12  lr: 0.002354  loss: 0.7330 (0.7319)  time: 0.5096  data: 0.0001  max mem: 9341
[07:29:50.495588] Epoch: [107]  [40/42]  eta: 0:00:01  lr: 0.002353  loss: 0.7537 (0.7430)  time: 0.5069  data: 0.0001  max mem: 9341
[07:29:51.000922] Epoch: [107]  [41/42]  eta: 0:00:00  lr: 0.002353  loss: 0.7530 (0.7428)  time: 0.5069  data: 0.0001  max mem: 9341
[07:29:51.160405] Epoch: [107] Total time: 0:00:22 (0.5382 s / it)
[07:29:51.172388] Averaged stats: lr: 0.002353  loss: 0.7530 (0.7403)
[07:29:55.708024] {"train_lr": 0.002353825801632923, "train_loss": 0.7402577109280086, "epoch": 107}
[07:29:55.708459] [07:29:55.708540] Training epoch 107 for 0:00:27
[07:29:55.708591] [07:29:55.712812] log_dir: ./exp/debug/cifar100-LT/debug
[07:29:57.354472] Epoch: [108]  [ 0/42]  eta: 0:01:08  lr: 0.002353  loss: 0.7530 (0.7530)  time: 1.6406  data: 1.1288  max mem: 9341
[07:30:07.504874] Epoch: [108]  [20/42]  eta: 0:00:12  lr: 0.002352  loss: 0.7300 (0.7278)  time: 0.5075  data: 0.0001  max mem: 9341
[07:30:17.641945] Epoch: [108]  [40/42]  eta: 0:00:01  lr: 0.002352  loss: 0.7311 (0.7303)  time: 0.5068  data: 0.0001  max mem: 9341
[07:30:18.146566] Epoch: [108]  [41/42]  eta: 0:00:00  lr: 0.002352  loss: 0.7311 (0.7293)  time: 0.5069  data: 0.0001  max mem: 9341
[07:30:18.316054] Epoch: [108] Total time: 0:00:22 (0.5382 s / it)
[07:30:18.328722] Averaged stats: lr: 0.002352  loss: 0.7311 (0.7326)
[07:30:23.056591] {"train_lr": 0.0023524531893159843, "train_loss": 0.7325680692281041, "epoch": 108}
[07:30:23.056852] [07:30:23.056933] Training epoch 108 for 0:00:27
[07:30:23.056983] [07:30:23.061305] log_dir: ./exp/debug/cifar100-LT/debug
[07:30:24.619018] Epoch: [109]  [ 0/42]  eta: 0:01:05  lr: 0.002352  loss: 0.7414 (0.7414)  time: 1.5569  data: 1.0423  max mem: 9341
[07:30:34.780024] Epoch: [109]  [20/42]  eta: 0:00:12  lr: 0.002351  loss: 0.7279 (0.7339)  time: 0.5080  data: 0.0001  max mem: 9341
[07:30:44.925896] Epoch: [109]  [40/42]  eta: 0:00:01  lr: 0.002350  loss: 0.7137 (0.7274)  time: 0.5072  data: 0.0001  max mem: 9341
[07:30:45.430768] Epoch: [109]  [41/42]  eta: 0:00:00  lr: 0.002350  loss: 0.7137 (0.7281)  time: 0.5072  data: 0.0001  max mem: 9341
[07:30:45.593078] Epoch: [109] Total time: 0:00:22 (0.5365 s / it)
[07:30:45.603418] Averaged stats: lr: 0.002350  loss: 0.7137 (0.7292)
[07:30:50.205703] {"train_lr": 0.002351060884753771, "train_loss": 0.729171477612995, "epoch": 109}
[07:30:50.206011] [07:30:50.206093] Training epoch 109 for 0:00:27
[07:30:50.206204] [07:30:50.210761] log_dir: ./exp/debug/cifar100-LT/debug
[07:30:51.646442] Epoch: [110]  [ 0/42]  eta: 0:01:00  lr: 0.002350  loss: 0.7826 (0.7826)  time: 1.4347  data: 0.9225  max mem: 9341
[07:31:01.815068] Epoch: [110]  [20/42]  eta: 0:00:12  lr: 0.002350  loss: 0.7346 (0.7350)  time: 0.5084  data: 0.0001  max mem: 9341
[07:31:11.968847] Epoch: [110]  [40/42]  eta: 0:00:01  lr: 0.002349  loss: 0.7491 (0.7410)  time: 0.5076  data: 0.0001  max mem: 9341
[07:31:12.476136] Epoch: [110]  [41/42]  eta: 0:00:00  lr: 0.002349  loss: 0.7468 (0.7406)  time: 0.5077  data: 0.0001  max mem: 9341
[07:31:12.641878] Epoch: [110] Total time: 0:00:22 (0.5340 s / it)
[07:31:12.645594] Averaged stats: lr: 0.002349  loss: 0.7468 (0.7332)
[07:31:17.171738] {"train_lr": 0.0023496489117369205, "train_loss": 0.733152374625206, "epoch": 110}
[07:31:17.172223] [07:31:17.172325] Training epoch 110 for 0:00:26
[07:31:17.172381] [07:31:17.176955] log_dir: ./exp/debug/cifar100-LT/debug
[07:31:18.890414] Epoch: [111]  [ 0/42]  eta: 0:01:11  lr: 0.002349  loss: 0.7703 (0.7703)  time: 1.7124  data: 1.2005  max mem: 9341
[07:31:29.049000] Epoch: [111]  [20/42]  eta: 0:00:12  lr: 0.002348  loss: 0.7349 (0.7492)  time: 0.5079  data: 0.0002  max mem: 9341
[07:31:39.200532] Epoch: [111]  [40/42]  eta: 0:00:01  lr: 0.002348  loss: 0.7550 (0.7534)  time: 0.5075  data: 0.0001  max mem: 9341
[07:31:39.706313] Epoch: [111]  [41/42]  eta: 0:00:00  lr: 0.002348  loss: 0.7550 (0.7535)  time: 0.5075  data: 0.0001  max mem: 9341
[07:31:39.872071] Epoch: [111] Total time: 0:00:22 (0.5404 s / it)
[07:31:39.873054] Averaged stats: lr: 0.002348  loss: 0.7550 (0.7456)
[07:31:44.385746] {"train_lr": 0.002348217294392156, "train_loss": 0.7456276377751714, "epoch": 111}
[07:31:44.386185] [07:31:44.386269] Training epoch 111 for 0:00:27
[07:31:44.386320] [07:31:44.390543] log_dir: ./exp/debug/cifar100-LT/debug
[07:31:45.892662] Epoch: [112]  [ 0/42]  eta: 0:01:03  lr: 0.002347  loss: 0.7188 (0.7188)  time: 1.5008  data: 1.0011  max mem: 9341
[07:31:56.060820] Epoch: [112]  [20/42]  eta: 0:00:12  lr: 0.002347  loss: 0.7395 (0.7384)  time: 0.5084  data: 0.0001  max mem: 9341
[07:32:06.203678] Epoch: [112]  [40/42]  eta: 0:00:01  lr: 0.002346  loss: 0.7380 (0.7398)  time: 0.5071  data: 0.0001  max mem: 9341
[07:32:06.711261] Epoch: [112]  [41/42]  eta: 0:00:00  lr: 0.002346  loss: 0.7375 (0.7390)  time: 0.5073  data: 0.0001  max mem: 9341
[07:32:06.879078] Epoch: [112] Total time: 0:00:22 (0.5354 s / it)
[07:32:06.885020] Averaged stats: lr: 0.002346  loss: 0.7375 (0.7395)
[07:32:11.445985] {"train_lr": 0.0023467660571818718, "train_loss": 0.7395080845980417, "epoch": 112}
[07:32:11.446358] [07:32:11.446443] Training epoch 112 for 0:00:27
[07:32:11.446494] [07:32:11.451280] log_dir: ./exp/debug/cifar100-LT/debug
[07:32:13.164365] Epoch: [113]  [ 0/42]  eta: 0:01:11  lr: 0.002346  loss: 0.7522 (0.7522)  time: 1.7116  data: 1.2058  max mem: 9341
[07:32:23.351144] Epoch: [113]  [20/42]  eta: 0:00:12  lr: 0.002345  loss: 0.7424 (0.7375)  time: 0.5093  data: 0.0001  max mem: 9341
[07:32:33.515087] Epoch: [113]  [40/42]  eta: 0:00:01  lr: 0.002345  loss: 0.7475 (0.7425)  time: 0.5081  data: 0.0001  max mem: 9341
[07:32:34.020923] Epoch: [113]  [41/42]  eta: 0:00:00  lr: 0.002345  loss: 0.7475 (0.7423)  time: 0.5082  data: 0.0001  max mem: 9341
[07:32:34.197106] Epoch: [113] Total time: 0:00:22 (0.5416 s / it)
[07:32:34.197891] Averaged stats: lr: 0.002345  loss: 0.7475 (0.7426)
[07:32:38.901355] {"train_lr": 0.002345295224903706, "train_loss": 0.7425742106778281, "epoch": 113}
[07:32:38.901617] [07:32:38.901701] Training epoch 113 for 0:00:27
[07:32:38.901752] [07:32:38.906188] log_dir: ./exp/debug/cifar100-LT/debug
[07:32:40.387877] Epoch: [114]  [ 0/42]  eta: 0:01:02  lr: 0.002344  loss: 0.7654 (0.7654)  time: 1.4804  data: 0.9817  max mem: 9341
[07:32:50.558907] Epoch: [114]  [20/42]  eta: 0:00:12  lr: 0.002344  loss: 0.7448 (0.7456)  time: 0.5085  data: 0.0001  max mem: 9341
[07:33:00.707828] Epoch: [114]  [40/42]  eta: 0:00:01  lr: 0.002343  loss: 0.7318 (0.7408)  time: 0.5074  data: 0.0001  max mem: 9341
[07:33:01.214555] Epoch: [114]  [41/42]  eta: 0:00:00  lr: 0.002343  loss: 0.7321 (0.7414)  time: 0.5074  data: 0.0001  max mem: 9341
[07:33:01.384410] Epoch: [114] Total time: 0:00:22 (0.5352 s / it)
[07:33:01.398925] Averaged stats: lr: 0.002343  loss: 0.7321 (0.7409)
[07:33:05.997919] {"train_lr": 0.0023438048226901245, "train_loss": 0.7409050340453783, "epoch": 114}
[07:33:05.998265] [07:33:05.998352] Training epoch 114 for 0:00:27
[07:33:05.998404] [07:33:06.002801] log_dir: ./exp/debug/cifar100-LT/debug
[07:33:07.504562] Epoch: [115]  [ 0/42]  eta: 0:01:03  lr: 0.002343  loss: 0.6672 (0.6672)  time: 1.5006  data: 0.9944  max mem: 9341
[07:33:17.674915] Epoch: [115]  [20/42]  eta: 0:00:12  lr: 0.002342  loss: 0.7266 (0.7172)  time: 0.5085  data: 0.0001  max mem: 9341
[07:33:27.824478] Epoch: [115]  [40/42]  eta: 0:00:01  lr: 0.002342  loss: 0.7209 (0.7175)  time: 0.5074  data: 0.0001  max mem: 9341
[07:33:28.328856] Epoch: [115]  [41/42]  eta: 0:00:00  lr: 0.002342  loss: 0.7179 (0.7166)  time: 0.5073  data: 0.0001  max mem: 9341
[07:33:28.484123] Epoch: [115] Total time: 0:00:22 (0.5353 s / it)
[07:33:28.496509] Averaged stats: lr: 0.002342  loss: 0.7179 (0.7246)
[07:33:33.018682] {"train_lr": 0.0023422948760079844, "train_loss": 0.7245819611208779, "epoch": 115}
[07:33:33.019057] [07:33:33.019140] Training epoch 115 for 0:00:27
[07:33:33.019191] [07:33:33.023385] log_dir: ./exp/debug/cifar100-LT/debug
[07:33:34.583248] Epoch: [116]  [ 0/42]  eta: 0:01:05  lr: 0.002341  loss: 0.7232 (0.7232)  time: 1.5589  data: 1.0561  max mem: 9341
[07:33:44.747373] Epoch: [116]  [20/42]  eta: 0:00:12  lr: 0.002341  loss: 0.7204 (0.7197)  time: 0.5082  data: 0.0001  max mem: 9341
[07:33:54.895766] Epoch: [116]  [40/42]  eta: 0:00:01  lr: 0.002340  loss: 0.7257 (0.7200)  time: 0.5074  data: 0.0001  max mem: 9341
[07:33:55.402793] Epoch: [116]  [41/42]  eta: 0:00:00  lr: 0.002340  loss: 0.7257 (0.7201)  time: 0.5075  data: 0.0001  max mem: 9341
[07:33:55.562661] Epoch: [116] Total time: 0:00:22 (0.5366 s / it)
[07:33:55.575787] Averaged stats: lr: 0.002340  loss: 0.7257 (0.7270)
[07:34:00.189103] {"train_lr": 0.002340765410658114, "train_loss": 0.726980793334189, "epoch": 116}
[07:34:00.189444] [07:34:00.189523] Training epoch 116 for 0:00:27
[07:34:00.189573] [07:34:00.193885] log_dir: ./exp/debug/cifar100-LT/debug
[07:34:01.747399] Epoch: [117]  [ 0/42]  eta: 0:01:05  lr: 0.002340  loss: 0.7503 (0.7503)  time: 1.5524  data: 1.0428  max mem: 9341
[07:34:11.974170] Epoch: [117]  [20/42]  eta: 0:00:12  lr: 0.002339  loss: 0.7336 (0.7291)  time: 0.5113  data: 0.0001  max mem: 9341
[07:34:22.118092] Epoch: [117]  [40/42]  eta: 0:00:01  lr: 0.002339  loss: 0.6995 (0.7172)  time: 0.5072  data: 0.0001  max mem: 9341
[07:34:22.624387] Epoch: [117]  [41/42]  eta: 0:00:00  lr: 0.002339  loss: 0.7113 (0.7181)  time: 0.5072  data: 0.0001  max mem: 9341
[07:34:22.785200] Epoch: [117] Total time: 0:00:22 (0.5379 s / it)
[07:34:22.814581] Averaged stats: lr: 0.002339  loss: 0.7113 (0.7157)
[07:34:27.398254] {"train_lr": 0.0023392164527748565, "train_loss": 0.7157318439512026, "epoch": 117}
[07:34:27.398634] [07:34:27.398718] Training epoch 117 for 0:00:27
[07:34:27.398771] [07:34:27.403320] log_dir: ./exp/debug/cifar100-LT/debug
[07:34:29.155047] Epoch: [118]  [ 0/42]  eta: 0:01:13  lr: 0.002338  loss: 0.7169 (0.7169)  time: 1.7507  data: 1.2592  max mem: 9341
[07:34:39.326179] Epoch: [118]  [20/42]  eta: 0:00:12  lr: 0.002338  loss: 0.6994 (0.7007)  time: 0.5084  data: 0.0001  max mem: 9341
[07:34:49.469346] Epoch: [118]  [40/42]  eta: 0:00:01  lr: 0.002337  loss: 0.7144 (0.7124)  time: 0.5071  data: 0.0001  max mem: 9341
[07:34:49.975845] Epoch: [118]  [41/42]  eta: 0:00:00  lr: 0.002337  loss: 0.7144 (0.7120)  time: 0.5073  data: 0.0001  max mem: 9341
[07:34:50.142343] Epoch: [118] Total time: 0:00:22 (0.5414 s / it)
[07:34:50.147749] Averaged stats: lr: 0.002337  loss: 0.7144 (0.7116)
[07:34:54.659052] {"train_lr": 0.0023376480288256236, "train_loss": 0.7115886097862607, "epoch": 118}
[07:34:54.659408] [07:34:54.659491] Training epoch 118 for 0:00:27
[07:34:54.659541] [07:34:54.663918] log_dir: ./exp/debug/cifar100-LT/debug
[07:34:56.082224] Epoch: [119]  [ 0/42]  eta: 0:00:59  lr: 0.002337  loss: 0.7205 (0.7205)  time: 1.4170  data: 0.9221  max mem: 9341
[07:35:06.271637] Epoch: [119]  [20/42]  eta: 0:00:12  lr: 0.002336  loss: 0.7159 (0.7202)  time: 0.5094  data: 0.0001  max mem: 9341
[07:35:16.442300] Epoch: [119]  [40/42]  eta: 0:00:01  lr: 0.002335  loss: 0.7195 (0.7211)  time: 0.5085  data: 0.0001  max mem: 9341
[07:35:16.948883] Epoch: [119]  [41/42]  eta: 0:00:00  lr: 0.002335  loss: 0.7169 (0.7199)  time: 0.5085  data: 0.0001  max mem: 9341
[07:35:17.094936] Epoch: [119] Total time: 0:00:22 (0.5341 s / it)
[07:35:17.122371] Averaged stats: lr: 0.002335  loss: 0.7169 (0.7180)
[07:35:21.693604] {"train_lr": 0.0023360601656104533, "train_loss": 0.7180032566899345, "epoch": 119}
[07:35:21.693961] [07:35:21.694059] Training epoch 119 for 0:00:27
[07:35:21.694111] [07:35:21.698490] log_dir: ./exp/debug/cifar100-LT/debug
[07:35:23.511999] Epoch: [120]  [ 0/42]  eta: 0:01:16  lr: 0.002335  loss: 0.7127 (0.7127)  time: 1.8124  data: 1.3099  max mem: 9341
[07:35:33.674940] Epoch: [120]  [20/42]  eta: 0:00:12  lr: 0.002334  loss: 0.7252 (0.7304)  time: 0.5081  data: 0.0001  max mem: 9341
[07:35:43.821437] Epoch: [120]  [40/42]  eta: 0:00:01  lr: 0.002334  loss: 0.7138 (0.7223)  time: 0.5073  data: 0.0001  max mem: 9341
[07:35:44.327396] Epoch: [120]  [41/42]  eta: 0:00:00  lr: 0.002334  loss: 0.7181 (0.7222)  time: 0.5073  data: 0.0001  max mem: 9341
[07:35:44.501760] Epoch: [120] Total time: 0:00:22 (0.5429 s / it)
[07:35:44.504174] Averaged stats: lr: 0.002334  loss: 0.7181 (0.7193)
[07:35:49.154898] {"train_lr": 0.0023344528902615514, "train_loss": 0.7193263400168646, "epoch": 120}
[07:35:49.155276] [07:35:49.155377] Training epoch 120 for 0:00:27
[07:35:49.155428] [07:35:49.159808] log_dir: ./exp/debug/cifar100-LT/debug
[07:35:50.870934] Epoch: [121]  [ 0/42]  eta: 0:01:11  lr: 0.002333  loss: 0.6807 (0.6807)  time: 1.7101  data: 1.2058  max mem: 9341
[07:36:01.058866] Epoch: [121]  [20/42]  eta: 0:00:12  lr: 0.002333  loss: 0.7120 (0.7122)  time: 0.5093  data: 0.0001  max mem: 9341
[07:36:11.212482] Epoch: [121]  [40/42]  eta: 0:00:01  lr: 0.002332  loss: 0.7333 (0.7230)  time: 0.5076  data: 0.0001  max mem: 9341
[07:36:11.717999] Epoch: [121]  [41/42]  eta: 0:00:00  lr: 0.002332  loss: 0.7379 (0.7234)  time: 0.5076  data: 0.0001  max mem: 9341
[07:36:11.885859] Epoch: [121] Total time: 0:00:22 (0.5411 s / it)
[07:36:11.895705] Averaged stats: lr: 0.002332  loss: 0.7379 (0.7177)
[07:36:16.505555] {"train_lr": 0.0023328262302428206, "train_loss": 0.7177176943847111, "epoch": 121}
[07:36:16.505901] [07:36:16.505982] Training epoch 121 for 0:00:27
[07:36:16.506033] [07:36:16.510380] log_dir: ./exp/debug/cifar100-LT/debug
[07:36:18.020212] Epoch: [122]  [ 0/42]  eta: 0:01:03  lr: 0.002332  loss: 0.7142 (0.7142)  time: 1.5084  data: 0.9991  max mem: 9341
[07:36:28.185180] Epoch: [122]  [20/42]  eta: 0:00:12  lr: 0.002331  loss: 0.7001 (0.7104)  time: 0.5082  data: 0.0001  max mem: 9341
[07:36:38.328364] Epoch: [122]  [40/42]  eta: 0:00:01  lr: 0.002330  loss: 0.7101 (0.7148)  time: 0.5071  data: 0.0001  max mem: 9341
[07:36:38.834492] Epoch: [122]  [41/42]  eta: 0:00:00  lr: 0.002330  loss: 0.7081 (0.7145)  time: 0.5071  data: 0.0001  max mem: 9341
[07:36:38.996309] Epoch: [122] Total time: 0:00:22 (0.5354 s / it)
[07:36:39.006521] Averaged stats: lr: 0.002330  loss: 0.7081 (0.7203)
[07:36:43.558959] {"train_lr": 0.0023311802133493854, "train_loss": 0.7203146336334092, "epoch": 122}
[07:36:43.559311] [07:36:43.559398] Training epoch 122 for 0:00:27
[07:36:43.559451] [07:36:43.564035] log_dir: ./exp/debug/cifar100-LT/debug
[07:36:45.141276] Epoch: [123]  [ 0/42]  eta: 0:01:06  lr: 0.002330  loss: 0.7317 (0.7317)  time: 1.5760  data: 1.0717  max mem: 9341
[07:36:55.306319] Epoch: [123]  [20/42]  eta: 0:00:12  lr: 0.002329  loss: 0.7127 (0.7126)  time: 0.5082  data: 0.0001  max mem: 9341
[07:37:05.457912] Epoch: [123]  [40/42]  eta: 0:00:01  lr: 0.002329  loss: 0.6980 (0.7075)  time: 0.5075  data: 0.0001  max mem: 9341
[07:37:05.964467] Epoch: [123]  [41/42]  eta: 0:00:00  lr: 0.002329  loss: 0.6980 (0.7080)  time: 0.5076  data: 0.0001  max mem: 9341
[07:37:06.135298] Epoch: [123] Total time: 0:00:22 (0.5374 s / it)
[07:37:06.138961] Averaged stats: lr: 0.002329  loss: 0.6980 (0.7123)
[07:37:10.716474] {"train_lr": 0.002329514867707135, "train_loss": 0.7122550031968525, "epoch": 123}
[07:37:10.716826] [07:37:10.716905] Training epoch 123 for 0:00:27
[07:37:10.716956] [07:37:10.721134] log_dir: ./exp/debug/cifar100-LT/debug
[07:37:12.344585] Epoch: [124]  [ 0/42]  eta: 0:01:08  lr: 0.002328  loss: 0.6299 (0.6299)  time: 1.6224  data: 1.1159  max mem: 9341
[07:37:22.508126] Epoch: [124]  [20/42]  eta: 0:00:12  lr: 0.002328  loss: 0.6915 (0.7009)  time: 0.5081  data: 0.0001  max mem: 9341
[07:37:32.656996] Epoch: [124]  [40/42]  eta: 0:00:01  lr: 0.002327  loss: 0.7043 (0.7096)  time: 0.5074  data: 0.0001  max mem: 9341
[07:37:33.164842] Epoch: [124]  [41/42]  eta: 0:00:00  lr: 0.002327  loss: 0.7043 (0.7089)  time: 0.5075  data: 0.0001  max mem: 9341
[07:37:33.328716] Epoch: [124] Total time: 0:00:22 (0.5383 s / it)
[07:37:33.337503] Averaged stats: lr: 0.002327  loss: 0.7043 (0.7077)
[07:37:38.037797] {"train_lr": 0.002327830221772236, "train_loss": 0.7077320682860556, "epoch": 124}
[07:37:38.038027] [07:37:38.038110] Training epoch 124 for 0:00:27
[07:37:38.038161] [07:37:38.042551] log_dir: ./exp/debug/cifar100-LT/debug
[07:37:39.673739] Epoch: [125]  [ 0/42]  eta: 0:01:08  lr: 0.002327  loss: 0.7052 (0.7052)  time: 1.6300  data: 1.1333  max mem: 9341
[07:37:49.829017] Epoch: [125]  [20/42]  eta: 0:00:12  lr: 0.002326  loss: 0.7304 (0.7254)  time: 0.5077  data: 0.0001  max mem: 9341
[07:37:59.970640] Epoch: [125]  [40/42]  eta: 0:00:01  lr: 0.002325  loss: 0.7375 (0.7305)  time: 0.5070  data: 0.0001  max mem: 9341
[07:38:00.475805] Epoch: [125]  [41/42]  eta: 0:00:00  lr: 0.002325  loss: 0.7375 (0.7289)  time: 0.5070  data: 0.0001  max mem: 9341
[07:38:00.645573] Epoch: [125] Total time: 0:00:22 (0.5382 s / it)
[07:38:00.654076] Averaged stats: lr: 0.002325  loss: 0.7375 (0.7303)
[07:38:05.075354] {"train_lr": 0.0023261263043306358, "train_loss": 0.7302891040841738, "epoch": 125}
[07:38:05.075713] [07:38:05.075795] Training epoch 125 for 0:00:27
[07:38:05.075847] [07:38:05.080305] log_dir: ./exp/debug/cifar100-LT/debug
[07:38:06.609641] Epoch: [126]  [ 0/42]  eta: 0:01:04  lr: 0.002325  loss: 0.7527 (0.7527)  time: 1.5281  data: 1.0181  max mem: 9341
[07:38:16.769328] Epoch: [126]  [20/42]  eta: 0:00:12  lr: 0.002324  loss: 0.7175 (0.7214)  time: 0.5079  data: 0.0001  max mem: 9341
[07:38:26.909953] Epoch: [126]  [40/42]  eta: 0:00:01  lr: 0.002324  loss: 0.7022 (0.7183)  time: 0.5070  data: 0.0001  max mem: 9341
[07:38:27.415741] Epoch: [126]  [41/42]  eta: 0:00:00  lr: 0.002324  loss: 0.7200 (0.7188)  time: 0.5069  data: 0.0001  max mem: 9341
[07:38:27.578657] Epoch: [126] Total time: 0:00:22 (0.5357 s / it)
[07:38:27.593817] Averaged stats: lr: 0.002324  loss: 0.7200 (0.7207)
[07:38:32.110370] {"train_lr": 0.002324403144497581, "train_loss": 0.7206711754912422, "epoch": 126}
[07:38:32.110700] [07:38:32.110787] Training epoch 126 for 0:00:27
[07:38:32.110840] [07:38:32.115352] log_dir: ./exp/debug/cifar100-LT/debug
[07:38:33.689740] Epoch: [127]  [ 0/42]  eta: 0:01:06  lr: 0.002323  loss: 0.6410 (0.6410)  time: 1.5733  data: 1.0645  max mem: 9341
[07:38:43.852021] Epoch: [127]  [20/42]  eta: 0:00:12  lr: 0.002323  loss: 0.7311 (0.7299)  time: 0.5081  data: 0.0001  max mem: 9341
[07:38:54.005567] Epoch: [127]  [40/42]  eta: 0:00:01  lr: 0.002322  loss: 0.7169 (0.7237)  time: 0.5076  data: 0.0001  max mem: 9341
[07:38:54.512392] Epoch: [127]  [41/42]  eta: 0:00:00  lr: 0.002322  loss: 0.7176 (0.7254)  time: 0.5077  data: 0.0001  max mem: 9341
[07:38:54.675263] Epoch: [127] Total time: 0:00:22 (0.5371 s / it)
[07:38:54.688737] Averaged stats: lr: 0.002322  loss: 0.7176 (0.7235)
[07:38:59.302701] {"train_lr": 0.002322660771717124, "train_loss": 0.723504598296824, "epoch": 127}
[07:38:59.303039] [07:38:59.303119] Training epoch 127 for 0:00:27
[07:38:59.303170] [07:38:59.307663] log_dir: ./exp/debug/cifar100-LT/debug
[07:39:00.989395] Epoch: [128]  [ 0/42]  eta: 0:01:10  lr: 0.002321  loss: 0.6573 (0.6573)  time: 1.6809  data: 1.1724  max mem: 9341
[07:39:11.155618] Epoch: [128]  [20/42]  eta: 0:00:12  lr: 0.002321  loss: 0.7201 (0.7232)  time: 0.5083  data: 0.0001  max mem: 9341
[07:39:21.304986] Epoch: [128]  [40/42]  eta: 0:00:01  lr: 0.002320  loss: 0.7227 (0.7222)  time: 0.5074  data: 0.0001  max mem: 9341
[07:39:21.810248] Epoch: [128]  [41/42]  eta: 0:00:00  lr: 0.002320  loss: 0.7227 (0.7214)  time: 0.5075  data: 0.0001  max mem: 9341
[07:39:21.976201] Epoch: [128] Total time: 0:00:22 (0.5397 s / it)
[07:39:21.982563] Averaged stats: lr: 0.002320  loss: 0.7227 (0.7235)
[07:39:26.562720] {"train_lr": 0.002320899215761602, "train_loss": 0.7235068891729627, "epoch": 128}
[07:39:26.563085] [07:39:26.563170] Training epoch 128 for 0:00:27
[07:39:26.563222] [07:39:26.567636] log_dir: ./exp/debug/cifar100-LT/debug
[07:39:28.181287] Epoch: [129]  [ 0/42]  eta: 0:01:07  lr: 0.002320  loss: 0.6372 (0.6372)  time: 1.6127  data: 1.1029  max mem: 9341
[07:39:38.345630] Epoch: [129]  [20/42]  eta: 0:00:12  lr: 0.002319  loss: 0.7296 (0.7264)  time: 0.5082  data: 0.0001  max mem: 9341
[07:39:48.501889] Epoch: [129]  [40/42]  eta: 0:00:01  lr: 0.002318  loss: 0.7215 (0.7231)  time: 0.5078  data: 0.0001  max mem: 9341
[07:39:49.007648] Epoch: [129]  [41/42]  eta: 0:00:00  lr: 0.002318  loss: 0.7148 (0.7221)  time: 0.5078  data: 0.0001  max mem: 9341
[07:39:49.166939] Epoch: [129] Total time: 0:00:22 (0.5381 s / it)
[07:39:49.181513] Averaged stats: lr: 0.002318  loss: 0.7148 (0.7232)
[07:39:53.717823] {"train_lr": 0.0023191185067311486, "train_loss": 0.7232109120204335, "epoch": 129}
[07:39:53.718242] [07:39:53.718339] Training epoch 129 for 0:00:27
[07:39:53.718391] [07:39:53.723544] log_dir: ./exp/debug/cifar100-LT/debug
[07:39:55.180394] Epoch: [130]  [ 0/42]  eta: 0:01:01  lr: 0.002318  loss: 0.7504 (0.7504)  time: 1.4559  data: 0.9575  max mem: 9341
[07:40:05.352969] Epoch: [130]  [20/42]  eta: 0:00:12  lr: 0.002317  loss: 0.7106 (0.7153)  time: 0.5086  data: 0.0001  max mem: 9341
[07:40:15.501474] Epoch: [130]  [40/42]  eta: 0:00:01  lr: 0.002317  loss: 0.7153 (0.7133)  time: 0.5074  data: 0.0001  max mem: 9341
[07:40:16.009815] Epoch: [130]  [41/42]  eta: 0:00:00  lr: 0.002317  loss: 0.7189 (0.7134)  time: 0.5075  data: 0.0001  max mem: 9341
[07:40:16.176896] Epoch: [130] Total time: 0:00:22 (0.5346 s / it)
[07:40:16.182926] Averaged stats: lr: 0.002317  loss: 0.7189 (0.7158)
[07:40:20.730882] {"train_lr": 0.00231731867505317, "train_loss": 0.7158088623767808, "epoch": 130}
[07:40:20.731285] [07:40:20.731376] Training epoch 130 for 0:00:27
[07:40:20.731446] [07:40:20.735871] log_dir: ./exp/debug/cifar100-LT/debug
[07:40:22.232672] Epoch: [131]  [ 0/42]  eta: 0:01:02  lr: 0.002316  loss: 0.6784 (0.6784)  time: 1.4954  data: 0.9928  max mem: 9341
[07:40:32.397117] Epoch: [131]  [20/42]  eta: 0:00:12  lr: 0.002315  loss: 0.7249 (0.7233)  time: 0.5082  data: 0.0001  max mem: 9341
[07:40:42.549864] Epoch: [131]  [40/42]  eta: 0:00:01  lr: 0.002315  loss: 0.7023 (0.7152)  time: 0.5076  data: 0.0001  max mem: 9341
[07:40:43.054405] Epoch: [131]  [41/42]  eta: 0:00:00  lr: 0.002315  loss: 0.7183 (0.7157)  time: 0.5075  data: 0.0001  max mem: 9341
[07:40:43.239407] Epoch: [131] Total time: 0:00:22 (0.5358 s / it)
[07:40:43.240129] Averaged stats: lr: 0.002315  loss: 0.7183 (0.7126)
[07:40:47.846530] {"train_lr": 0.0023154997514818225, "train_loss": 0.7125605440565518, "epoch": 131}
[07:40:47.846925] [07:40:47.847026] Training epoch 131 for 0:00:27
[07:40:47.847079] [07:40:47.851341] log_dir: ./exp/debug/cifar100-LT/debug
[07:40:49.351142] Epoch: [132]  [ 0/42]  eta: 0:01:02  lr: 0.002314  loss: 0.7426 (0.7426)  time: 1.4984  data: 0.9902  max mem: 9341
[07:40:59.517156] Epoch: [132]  [20/42]  eta: 0:00:12  lr: 0.002314  loss: 0.6961 (0.7125)  time: 0.5083  data: 0.0001  max mem: 9341
[07:41:09.670661] Epoch: [132]  [40/42]  eta: 0:00:01  lr: 0.002313  loss: 0.7007 (0.7071)  time: 0.5076  data: 0.0001  max mem: 9341
[07:41:10.178584] Epoch: [132]  [41/42]  eta: 0:00:00  lr: 0.002313  loss: 0.7007 (0.7071)  time: 0.5077  data: 0.0001  max mem: 9341
[07:41:10.352999] Epoch: [132] Total time: 0:00:22 (0.5358 s / it)
[07:41:10.353992] Averaged stats: lr: 0.002313  loss: 0.7007 (0.7076)
[07:41:14.895730] {"train_lr": 0.002313661767097489, "train_loss": 0.7075726450199172, "epoch": 132}
[07:41:14.896122] [07:41:14.896225] Training epoch 132 for 0:00:27
[07:41:14.896281] [07:41:14.901404] log_dir: ./exp/debug/cifar100-LT/debug
[07:41:16.375122] Epoch: [133]  [ 0/42]  eta: 0:01:01  lr: 0.002312  loss: 0.7303 (0.7303)  time: 1.4728  data: 0.9643  max mem: 9341
[07:41:26.535689] Epoch: [133]  [20/42]  eta: 0:00:12  lr: 0.002312  loss: 0.7126 (0.7120)  time: 0.5080  data: 0.0002  max mem: 9341
[07:41:36.681850] Epoch: [133]  [40/42]  eta: 0:00:01  lr: 0.002311  loss: 0.7102 (0.7095)  time: 0.5073  data: 0.0001  max mem: 9341
[07:41:37.186512] Epoch: [133]  [41/42]  eta: 0:00:00  lr: 0.002311  loss: 0.7123 (0.7109)  time: 0.5073  data: 0.0001  max mem: 9341
[07:41:37.347757] Epoch: [133] Total time: 0:00:22 (0.5344 s / it)
[07:41:37.359059] Averaged stats: lr: 0.002311  loss: 0.7123 (0.7080)
[07:41:41.911929] {"train_lr": 0.002311804753306251, "train_loss": 0.7080217191860789, "epoch": 133}
[07:41:41.912442] [07:41:41.912537] Training epoch 133 for 0:00:27
[07:41:41.912591] [07:41:41.917894] log_dir: ./exp/debug/cifar100-LT/debug
[07:41:43.570768] Epoch: [134]  [ 0/42]  eta: 0:01:09  lr: 0.002311  loss: 0.7258 (0.7258)  time: 1.6521  data: 1.1448  max mem: 9341
[07:41:53.736589] Epoch: [134]  [20/42]  eta: 0:00:12  lr: 0.002310  loss: 0.7145 (0.7149)  time: 0.5082  data: 0.0001  max mem: 9341
[07:42:03.887509] Epoch: [134]  [40/42]  eta: 0:00:01  lr: 0.002309  loss: 0.6896 (0.7042)  time: 0.5075  data: 0.0001  max mem: 9341
[07:42:04.393798] Epoch: [134]  [41/42]  eta: 0:00:00  lr: 0.002309  loss: 0.6896 (0.7034)  time: 0.5075  data: 0.0001  max mem: 9341
[07:42:04.564772] Epoch: [134] Total time: 0:00:22 (0.5392 s / it)
[07:42:04.574542] Averaged stats: lr: 0.002309  loss: 0.6896 (0.7029)
[07:42:09.036684] {"train_lr": 0.0023099287418393553, "train_loss": 0.702868535050324, "epoch": 134}
[07:42:09.037057] [07:42:09.037163] Training epoch 134 for 0:00:27
[07:42:09.037215] [07:42:09.041610] log_dir: ./exp/debug/cifar100-LT/debug
[07:42:10.543271] Epoch: [135]  [ 0/42]  eta: 0:01:03  lr: 0.002309  loss: 0.6933 (0.6933)  time: 1.5006  data: 0.9818  max mem: 9341
[07:42:20.732936] Epoch: [135]  [20/42]  eta: 0:00:12  lr: 0.002308  loss: 0.6954 (0.6982)  time: 0.5094  data: 0.0001  max mem: 9341
[07:42:30.902971] Epoch: [135]  [40/42]  eta: 0:00:01  lr: 0.002307  loss: 0.7121 (0.7024)  time: 0.5085  data: 0.0001  max mem: 9341
[07:42:31.408473] Epoch: [135]  [41/42]  eta: 0:00:00  lr: 0.002307  loss: 0.7121 (0.7029)  time: 0.5084  data: 0.0001  max mem: 9341
[07:42:31.582428] Epoch: [135] Total time: 0:00:22 (0.5367 s / it)
[07:42:31.583225] Averaged stats: lr: 0.002307  loss: 0.7121 (0.6998)
[07:42:36.230708] {"train_lr": 0.002308033764752661, "train_loss": 0.699824659597306, "epoch": 135}
[07:42:36.231020] [07:42:36.231102] Training epoch 135 for 0:00:27
[07:42:36.231152] [07:42:36.235425] log_dir: ./exp/debug/cifar100-LT/debug
[07:42:37.731288] Epoch: [136]  [ 0/42]  eta: 0:01:02  lr: 0.002307  loss: 0.6516 (0.6516)  time: 1.4950  data: 0.9941  max mem: 9341
[07:42:47.896196] Epoch: [136]  [20/42]  eta: 0:00:12  lr: 0.002306  loss: 0.6977 (0.6993)  time: 0.5082  data: 0.0001  max mem: 9341
[07:42:58.045920] Epoch: [136]  [40/42]  eta: 0:00:01  lr: 0.002305  loss: 0.6908 (0.6971)  time: 0.5074  data: 0.0001  max mem: 9341
[07:42:58.550910] Epoch: [136]  [41/42]  eta: 0:00:00  lr: 0.002305  loss: 0.6908 (0.6972)  time: 0.5075  data: 0.0001  max mem: 9341
[07:42:58.727275] Epoch: [136] Total time: 0:00:22 (0.5355 s / it)
[07:42:58.728303] Averaged stats: lr: 0.002305  loss: 0.6908 (0.7043)
[07:43:03.349089] {"train_lr": 0.002306119854426094, "train_loss": 0.7042826222521918, "epoch": 136}
[07:43:03.349434] [07:43:03.349523] Training epoch 136 for 0:00:27
[07:43:03.349575] [07:43:03.354597] log_dir: ./exp/debug/cifar100-LT/debug
[07:43:05.053638] Epoch: [137]  [ 0/42]  eta: 0:01:11  lr: 0.002305  loss: 0.6879 (0.6879)  time: 1.6983  data: 1.1935  max mem: 9341
[07:43:15.214529] Epoch: [137]  [20/42]  eta: 0:00:12  lr: 0.002304  loss: 0.7010 (0.7015)  time: 0.5080  data: 0.0001  max mem: 9341
[07:43:25.362855] Epoch: [137]  [40/42]  eta: 0:00:01  lr: 0.002303  loss: 0.6954 (0.7018)  time: 0.5074  data: 0.0001  max mem: 9341
[07:43:25.868864] Epoch: [137]  [41/42]  eta: 0:00:00  lr: 0.002303  loss: 0.6959 (0.7016)  time: 0.5074  data: 0.0001  max mem: 9341
[07:43:26.044498] Epoch: [137] Total time: 0:00:22 (0.5402 s / it)
[07:43:26.052523] Averaged stats: lr: 0.002303  loss: 0.6959 (0.6991)
[07:43:30.662099] {"train_lr": 0.0023041870435631037, "train_loss": 0.699080245480651, "epoch": 137}
[07:43:30.662368] [07:43:30.662455] Training epoch 137 for 0:00:27
[07:43:30.662507] [07:43:30.666874] log_dir: ./exp/debug/cifar100-LT/debug
[07:43:32.410419] Epoch: [138]  [ 0/42]  eta: 0:01:13  lr: 0.002303  loss: 0.7002 (0.7002)  time: 1.7428  data: 1.2471  max mem: 9341
[07:43:42.578475] Epoch: [138]  [20/42]  eta: 0:00:12  lr: 0.002302  loss: 0.6963 (0.6986)  time: 0.5084  data: 0.0001  max mem: 9341
[07:43:52.732464] Epoch: [138]  [40/42]  eta: 0:00:01  lr: 0.002301  loss: 0.7072 (0.7041)  time: 0.5077  data: 0.0001  max mem: 9341
[07:43:53.238808] Epoch: [138]  [41/42]  eta: 0:00:00  lr: 0.002301  loss: 0.7127 (0.7045)  time: 0.5077  data: 0.0001  max mem: 9341
[07:43:53.404521] Epoch: [138] Total time: 0:00:22 (0.5414 s / it)
[07:43:53.408283] Averaged stats: lr: 0.002301  loss: 0.7127 (0.7000)
[07:43:58.021724] {"train_lr": 0.0023022353651900923, "train_loss": 0.7000209848795619, "epoch": 138}
[07:43:58.022043] [07:43:58.022124] Training epoch 138 for 0:00:27
[07:43:58.022174] [07:43:58.026448] log_dir: ./exp/debug/cifar100-LT/debug
[07:43:59.702228] Epoch: [139]  [ 0/42]  eta: 0:01:10  lr: 0.002301  loss: 0.7589 (0.7589)  time: 1.6749  data: 1.1606  max mem: 9341
[07:44:09.864047] Epoch: [139]  [20/42]  eta: 0:00:12  lr: 0.002300  loss: 0.7063 (0.7102)  time: 0.5080  data: 0.0001  max mem: 9341
[07:44:20.017312] Epoch: [139]  [40/42]  eta: 0:00:01  lr: 0.002299  loss: 0.7322 (0.7181)  time: 0.5076  data: 0.0001  max mem: 9341
[07:44:20.523423] Epoch: [139]  [41/42]  eta: 0:00:00  lr: 0.002299  loss: 0.7322 (0.7179)  time: 0.5076  data: 0.0001  max mem: 9341
[07:44:20.694672] Epoch: [139] Total time: 0:00:22 (0.5397 s / it)
[07:44:20.706755] Averaged stats: lr: 0.002299  loss: 0.7322 (0.7209)
[07:44:25.297826] {"train_lr": 0.0023002648526558624, "train_loss": 0.7209281932030406, "epoch": 139}
[07:44:25.298164] [07:44:25.298243] Training epoch 139 for 0:00:27
[07:44:25.298293] [07:44:25.302668] log_dir: ./exp/debug/cifar100-LT/debug
[07:44:26.801102] Epoch: [140]  [ 0/42]  eta: 0:01:02  lr: 0.002299  loss: 0.6911 (0.6911)  time: 1.4971  data: 0.9812  max mem: 9341
[07:44:37.006888] Epoch: [140]  [20/42]  eta: 0:00:12  lr: 0.002298  loss: 0.7033 (0.7013)  time: 0.5102  data: 0.0001  max mem: 9341
[07:44:47.172048] Epoch: [140]  [40/42]  eta: 0:00:01  lr: 0.002297  loss: 0.7047 (0.7036)  time: 0.5082  data: 0.0001  max mem: 9341
[07:44:47.677570] Epoch: [140]  [41/42]  eta: 0:00:00  lr: 0.002297  loss: 0.7176 (0.7041)  time: 0.5082  data: 0.0001  max mem: 9341
[07:44:47.857156] Epoch: [140] Total time: 0:00:22 (0.5370 s / it)
[07:44:47.864503] Averaged stats: lr: 0.002297  loss: 0.7176 (0.7094)
[07:44:52.554750] {"train_lr": 0.002298275539631034, "train_loss": 0.7093956498872667, "epoch": 140}
[07:44:52.555058] [07:44:52.555139] Training epoch 140 for 0:00:27
[07:44:52.555249] [07:44:52.559547] log_dir: ./exp/debug/cifar100-LT/debug
[07:44:54.197232] Epoch: [141]  [ 0/42]  eta: 0:01:08  lr: 0.002297  loss: 0.7796 (0.7796)  time: 1.6363  data: 1.1355  max mem: 9341
[07:45:04.359687] Epoch: [141]  [20/42]  eta: 0:00:12  lr: 0.002296  loss: 0.7018 (0.7093)  time: 0.5081  data: 0.0002  max mem: 9341
[07:45:14.502497] Epoch: [141]  [40/42]  eta: 0:00:01  lr: 0.002295  loss: 0.6997 (0.7029)  time: 0.5071  data: 0.0001  max mem: 9341
[07:45:15.008974] Epoch: [141]  [41/42]  eta: 0:00:00  lr: 0.002295  loss: 0.6966 (0.7022)  time: 0.5072  data: 0.0001  max mem: 9341
[07:45:15.186157] Epoch: [141] Total time: 0:00:22 (0.5387 s / it)
[07:45:15.194285] Averaged stats: lr: 0.002295  loss: 0.6966 (0.6998)
[07:45:19.815113] {"train_lr": 0.00229626746010748, "train_loss": 0.699821167758533, "epoch": 141}
[07:45:19.815477] [07:45:19.815560] Training epoch 141 for 0:00:27
[07:45:19.815609] [07:45:19.820046] log_dir: ./exp/debug/cifar100-LT/debug
[07:45:21.320467] Epoch: [142]  [ 0/42]  eta: 0:01:02  lr: 0.002295  loss: 0.7434 (0.7434)  time: 1.4994  data: 0.9946  max mem: 9341
[07:45:31.479202] Epoch: [142]  [20/42]  eta: 0:00:12  lr: 0.002294  loss: 0.7027 (0.6944)  time: 0.5079  data: 0.0001  max mem: 9341
[07:45:41.630613] Epoch: [142]  [40/42]  eta: 0:00:01  lr: 0.002293  loss: 0.7036 (0.7019)  time: 0.5075  data: 0.0001  max mem: 9341
[07:45:42.135137] Epoch: [142]  [41/42]  eta: 0:00:00  lr: 0.002293  loss: 0.7036 (0.7025)  time: 0.5075  data: 0.0001  max mem: 9341
[07:45:42.304837] Epoch: [142] Total time: 0:00:22 (0.5353 s / it)
[07:45:42.309062] Averaged stats: lr: 0.002293  loss: 0.7036 (0.6975)
[07:45:46.917042] {"train_lr": 0.002294240648397739, "train_loss": 0.6975169557900656, "epoch": 142}
[07:45:46.917425] [07:45:46.917527] Training epoch 142 for 0:00:27
[07:45:46.917581] [07:45:46.922052] log_dir: ./exp/debug/cifar100-LT/debug
[07:45:48.435356] Epoch: [143]  [ 0/42]  eta: 0:01:03  lr: 0.002293  loss: 0.7195 (0.7195)  time: 1.5121  data: 0.9971  max mem: 9341
[07:45:58.594554] Epoch: [143]  [20/42]  eta: 0:00:12  lr: 0.002292  loss: 0.6805 (0.6891)  time: 0.5079  data: 0.0001  max mem: 9341
[07:46:08.736945] Epoch: [143]  [40/42]  eta: 0:00:01  lr: 0.002291  loss: 0.6838 (0.6911)  time: 0.5071  data: 0.0001  max mem: 9341
[07:46:09.240621] Epoch: [143]  [41/42]  eta: 0:00:00  lr: 0.002291  loss: 0.6838 (0.6911)  time: 0.5070  data: 0.0001  max mem: 9341
[07:46:09.410265] Epoch: [143] Total time: 0:00:22 (0.5354 s / it)
[07:46:09.413614] Averaged stats: lr: 0.002291  loss: 0.6838 (0.6914)
[07:46:14.105281] {"train_lr": 0.0022921951391344244, "train_loss": 0.6913858588252749, "epoch": 143}
[07:46:14.105578] [07:46:14.105666] Training epoch 143 for 0:00:27
[07:46:14.105735] [07:46:14.110470] log_dir: ./exp/debug/cifar100-LT/debug
[07:46:15.811545] Epoch: [144]  [ 0/42]  eta: 0:01:11  lr: 0.002291  loss: 0.6594 (0.6594)  time: 1.7004  data: 1.1936  max mem: 9341
[07:46:26.039414] Epoch: [144]  [20/42]  eta: 0:00:12  lr: 0.002290  loss: 0.6907 (0.6895)  time: 0.5113  data: 0.0001  max mem: 9341
[07:46:36.210259] Epoch: [144]  [40/42]  eta: 0:00:01  lr: 0.002289  loss: 0.6950 (0.6921)  time: 0.5085  data: 0.0001  max mem: 9341
[07:46:36.714724] Epoch: [144]  [41/42]  eta: 0:00:00  lr: 0.002289  loss: 0.6988 (0.6929)  time: 0.5084  data: 0.0001  max mem: 9341
[07:46:36.879163] Epoch: [144] Total time: 0:00:22 (0.5421 s / it)
[07:46:36.882858] Averaged stats: lr: 0.002289  loss: 0.6988 (0.6878)
[07:46:41.520030] {"train_lr": 0.0022901309672696555, "train_loss": 0.6877921503924188, "epoch": 144}
[07:46:41.520377] [07:46:41.520497] Training epoch 144 for 0:00:27
[07:46:41.520549] [07:46:41.525076] log_dir: ./exp/debug/cifar100-LT/debug
[07:46:43.093217] Epoch: [145]  [ 0/42]  eta: 0:01:05  lr: 0.002289  loss: 0.7123 (0.7123)  time: 1.5668  data: 1.0611  max mem: 9341
[07:46:53.255696] Epoch: [145]  [20/42]  eta: 0:00:12  lr: 0.002288  loss: 0.6787 (0.6756)  time: 0.5081  data: 0.0001  max mem: 9341
[07:47:03.406326] Epoch: [145]  [40/42]  eta: 0:00:01  lr: 0.002287  loss: 0.6996 (0.6873)  time: 0.5075  data: 0.0001  max mem: 9341
[07:47:03.910573] Epoch: [145]  [41/42]  eta: 0:00:00  lr: 0.002287  loss: 0.6996 (0.6882)  time: 0.5074  data: 0.0001  max mem: 9341
[07:47:04.076809] Epoch: [145] Total time: 0:00:22 (0.5369 s / it)
[07:47:04.090240] Averaged stats: lr: 0.002287  loss: 0.6996 (0.6847)
[07:47:08.652286] {"train_lr": 0.002288048168074427, "train_loss": 0.6846689698951585, "epoch": 145}
[07:47:08.652555] [07:47:08.652638] Training epoch 145 for 0:00:27
[07:47:08.652689] [07:47:08.657061] log_dir: ./exp/debug/cifar100-LT/debug
[07:47:10.247110] Epoch: [146]  [ 0/42]  eta: 0:01:06  lr: 0.002287  loss: 0.7364 (0.7364)  time: 1.5893  data: 1.0783  max mem: 9341
[07:47:20.413878] Epoch: [146]  [20/42]  eta: 0:00:12  lr: 0.002286  loss: 0.6905 (0.6913)  time: 0.5083  data: 0.0001  max mem: 9341
[07:47:30.569470] Epoch: [146]  [40/42]  eta: 0:00:01  lr: 0.002285  loss: 0.6879 (0.6901)  time: 0.5077  data: 0.0001  max mem: 9341
[07:47:31.076479] Epoch: [146]  [41/42]  eta: 0:00:00  lr: 0.002285  loss: 0.6871 (0.6896)  time: 0.5078  data: 0.0001  max mem: 9341
[07:47:31.252391] Epoch: [146] Total time: 0:00:22 (0.5380 s / it)
[07:47:31.254502] Averaged stats: lr: 0.002285  loss: 0.6871 (0.6898)
[07:47:35.896181] {"train_lr": 0.0022859467771380363, "train_loss": 0.6898120368520418, "epoch": 146}
[07:47:35.896561] [07:47:35.896645] Training epoch 146 for 0:00:27
[07:47:35.896696] [07:47:35.900958] log_dir: ./exp/debug/cifar100-LT/debug
[07:47:37.340329] Epoch: [147]  [ 0/42]  eta: 0:01:00  lr: 0.002285  loss: 0.6524 (0.6524)  time: 1.4384  data: 0.9308  max mem: 9341
[07:47:47.509283] Epoch: [147]  [20/42]  eta: 0:00:12  lr: 0.002284  loss: 0.6858 (0.6842)  time: 0.5084  data: 0.0001  max mem: 9341
[07:47:57.659908] Epoch: [147]  [40/42]  eta: 0:00:01  lr: 0.002283  loss: 0.6759 (0.6810)  time: 0.5075  data: 0.0001  max mem: 9341
[07:47:58.165199] Epoch: [147]  [41/42]  eta: 0:00:00  lr: 0.002283  loss: 0.6759 (0.6816)  time: 0.5075  data: 0.0001  max mem: 9341
[07:47:58.343438] Epoch: [147] Total time: 0:00:22 (0.5343 s / it)
[07:47:58.353944] Averaged stats: lr: 0.002283  loss: 0.6759 (0.6837)
[07:48:02.954589] {"train_lr": 0.0022838268303674494, "train_loss": 0.683701488943327, "epoch": 147}
[07:48:02.954893] [07:48:02.954974] Training epoch 147 for 0:00:27
[07:48:02.955043] [07:48:02.959261] log_dir: ./exp/debug/cifar100-LT/debug
[07:48:04.594172] Epoch: [148]  [ 0/42]  eta: 0:01:08  lr: 0.002282  loss: 0.6981 (0.6981)  time: 1.6339  data: 1.1268  max mem: 9341
[07:48:14.779256] Epoch: [148]  [20/42]  eta: 0:00:12  lr: 0.002282  loss: 0.6770 (0.6819)  time: 0.5092  data: 0.0001  max mem: 9341
[07:48:24.924719] Epoch: [148]  [40/42]  eta: 0:00:01  lr: 0.002281  loss: 0.6968 (0.6895)  time: 0.5072  data: 0.0001  max mem: 9341
[07:48:25.431024] Epoch: [148]  [41/42]  eta: 0:00:00  lr: 0.002281  loss: 0.6971 (0.6909)  time: 0.5073  data: 0.0001  max mem: 9341
[07:48:25.599332] Epoch: [148] Total time: 0:00:22 (0.5390 s / it)
[07:48:25.600660] Averaged stats: lr: 0.002281  loss: 0.6971 (0.6853)
[07:48:30.179460] {"train_lr": 0.002281688363986715, "train_loss": 0.6853348697934832, "epoch": 148}
[07:48:30.179785] [07:48:30.179866] Training epoch 148 for 0:00:27
[07:48:30.179917] [07:48:30.184335] log_dir: ./exp/debug/cifar100-LT/debug
[07:48:31.894065] Epoch: [149]  [ 0/42]  eta: 0:01:11  lr: 0.002280  loss: 0.6886 (0.6886)  time: 1.7090  data: 1.2123  max mem: 9341
[07:48:42.051656] Epoch: [149]  [20/42]  eta: 0:00:12  lr: 0.002279  loss: 0.6768 (0.6829)  time: 0.5078  data: 0.0001  max mem: 9341
[07:48:52.197191] Epoch: [149]  [40/42]  eta: 0:00:01  lr: 0.002279  loss: 0.6938 (0.6836)  time: 0.5072  data: 0.0001  max mem: 9341
[07:48:52.700419] Epoch: [149]  [41/42]  eta: 0:00:00  lr: 0.002279  loss: 0.6938 (0.6834)  time: 0.5071  data: 0.0001  max mem: 9341
[07:48:52.862669] Epoch: [149] Total time: 0:00:22 (0.5400 s / it)
[07:48:52.863436] Averaged stats: lr: 0.002279  loss: 0.6938 (0.6927)
[07:48:57.466747] {"train_lr": 0.002279531414536319, "train_loss": 0.6927154397680646, "epoch": 149}
[07:48:57.467151] [07:48:57.467243] Training epoch 149 for 0:00:27
[07:48:57.467307] [07:48:57.472117] log_dir: ./exp/debug/cifar100-LT/debug
[07:48:59.051113] Epoch: [150]  [ 0/42]  eta: 0:01:06  lr: 0.002278  loss: 0.7078 (0.7078)  time: 1.5781  data: 1.0641  max mem: 9341
[07:49:09.240753] Epoch: [150]  [20/42]  eta: 0:00:12  lr: 0.002277  loss: 0.6926 (0.6993)  time: 0.5094  data: 0.0001  max mem: 9341
[07:49:19.412626] Epoch: [150]  [40/42]  eta: 0:00:01  lr: 0.002276  loss: 0.7068 (0.7004)  time: 0.5085  data: 0.0001  max mem: 9341
[07:49:19.919972] Epoch: [150]  [41/42]  eta: 0:00:00  lr: 0.002276  loss: 0.7068 (0.6997)  time: 0.5086  data: 0.0001  max mem: 9341
[07:49:20.082337] Epoch: [150] Total time: 0:00:22 (0.5383 s / it)
[07:49:20.085091] Averaged stats: lr: 0.002276  loss: 0.7068 (0.7047)
[07:49:24.595936] {"train_lr": 0.0022773560188725775, "train_loss": 0.7047289546046939, "epoch": 150}
[07:49:24.596271] [07:49:24.596362] Training epoch 150 for 0:00:27
[07:49:24.596433] [07:49:24.600779] log_dir: ./exp/debug/cifar100-LT/debug
[07:49:26.285140] Epoch: [151]  [ 0/42]  eta: 0:01:10  lr: 0.002276  loss: 0.6259 (0.6259)  time: 1.6836  data: 1.1704  max mem: 9341
[07:49:36.449572] Epoch: [151]  [20/42]  eta: 0:00:12  lr: 0.002275  loss: 0.7144 (0.6868)  time: 0.5082  data: 0.0001  max mem: 9341
[07:49:46.603223] Epoch: [151]  [40/42]  eta: 0:00:01  lr: 0.002274  loss: 0.7027 (0.6973)  time: 0.5076  data: 0.0001  max mem: 9341
[07:49:47.110135] Epoch: [151]  [41/42]  eta: 0:00:00  lr: 0.002274  loss: 0.7027 (0.6978)  time: 0.5077  data: 0.0001  max mem: 9341
[07:49:47.289365] Epoch: [151] Total time: 0:00:22 (0.5402 s / it)
[07:49:47.290127] Averaged stats: lr: 0.002274  loss: 0.7027 (0.6967)
[07:49:51.889104] {"train_lr": 0.0022751622141669965, "train_loss": 0.6967343748325393, "epoch": 151}
[07:49:51.889562] [07:49:51.889642] Training epoch 151 for 0:00:27
[07:49:51.889692] [07:49:51.893924] log_dir: ./exp/debug/cifar100-LT/debug
[07:49:53.414073] Epoch: [152]  [ 0/42]  eta: 0:01:03  lr: 0.002274  loss: 0.6893 (0.6893)  time: 1.5191  data: 0.9956  max mem: 9341
[07:50:03.572443] Epoch: [152]  [20/42]  eta: 0:00:12  lr: 0.002273  loss: 0.6856 (0.6926)  time: 0.5079  data: 0.0001  max mem: 9341
[07:50:13.703205] Epoch: [152]  [40/42]  eta: 0:00:01  lr: 0.002272  loss: 0.6946 (0.6948)  time: 0.5065  data: 0.0001  max mem: 9341
[07:50:14.208896] Epoch: [152]  [41/42]  eta: 0:00:00  lr: 0.002272  loss: 0.6909 (0.6946)  time: 0.5065  data: 0.0001  max mem: 9341
[07:50:14.377282] Epoch: [152] Total time: 0:00:22 (0.5353 s / it)
[07:50:14.381814] Averaged stats: lr: 0.002272  loss: 0.6909 (0.6953)
[07:50:18.933733] {"train_lr": 0.00227295003790565, "train_loss": 0.6952549286541485, "epoch": 152}
[07:50:18.934070] [07:50:18.934160] Training epoch 152 for 0:00:27
[07:50:18.934212] [07:50:18.938841] log_dir: ./exp/debug/cifar100-LT/debug
[07:50:20.577371] Epoch: [153]  [ 0/42]  eta: 0:01:08  lr: 0.002271  loss: 0.6738 (0.6738)  time: 1.6378  data: 1.1229  max mem: 9341
[07:50:30.754290] Epoch: [153]  [20/42]  eta: 0:00:12  lr: 0.002271  loss: 0.7031 (0.7019)  time: 0.5088  data: 0.0001  max mem: 9341
[07:50:40.893328] Epoch: [153]  [40/42]  eta: 0:00:01  lr: 0.002270  loss: 0.6937 (0.6972)  time: 0.5069  data: 0.0001  max mem: 9341
[07:50:41.398098] Epoch: [153]  [41/42]  eta: 0:00:00  lr: 0.002270  loss: 0.6937 (0.6974)  time: 0.5068  data: 0.0001  max mem: 9341
[07:50:41.570878] Epoch: [153] Total time: 0:00:22 (0.5389 s / it)
[07:50:41.581084] Averaged stats: lr: 0.002270  loss: 0.6937 (0.6949)
[07:50:46.052847] {"train_lr": 0.0022707195278885276, "train_loss": 0.6949161543023019, "epoch": 153}
[07:50:46.053178] [07:50:46.053278] Training epoch 153 for 0:00:27
[07:50:46.053329] [07:50:46.058261] log_dir: ./exp/debug/cifar100-LT/debug
[07:50:47.523303] Epoch: [154]  [ 0/42]  eta: 0:01:01  lr: 0.002269  loss: 0.6344 (0.6344)  time: 1.4641  data: 0.9460  max mem: 9341
[07:50:57.687033] Epoch: [154]  [20/42]  eta: 0:00:12  lr: 0.002268  loss: 0.6843 (0.6823)  time: 0.5081  data: 0.0001  max mem: 9341
[07:51:07.831410] Epoch: [154]  [40/42]  eta: 0:00:01  lr: 0.002267  loss: 0.6935 (0.6890)  time: 0.5072  data: 0.0001  max mem: 9341
[07:51:08.337354] Epoch: [154]  [41/42]  eta: 0:00:00  lr: 0.002267  loss: 0.6931 (0.6886)  time: 0.5072  data: 0.0001  max mem: 9341
[07:51:08.504661] Epoch: [154] Total time: 0:00:22 (0.5344 s / it)
[07:51:08.515019] Averaged stats: lr: 0.002267  loss: 0.6931 (0.6930)
[07:51:13.071252] {"train_lr": 0.0022684707222288864, "train_loss": 0.692956092102187, "epoch": 154}
[07:51:13.071568] [07:51:13.071650] Training epoch 154 for 0:00:27
[07:51:13.071700] [07:51:13.076027] log_dir: ./exp/debug/cifar100-LT/debug
[07:51:14.626402] Epoch: [155]  [ 0/42]  eta: 0:01:05  lr: 0.002267  loss: 0.7508 (0.7508)  time: 1.5495  data: 1.0300  max mem: 9341
[07:51:24.811804] Epoch: [155]  [20/42]  eta: 0:00:12  lr: 0.002266  loss: 0.6953 (0.6969)  time: 0.5092  data: 0.0001  max mem: 9341
[07:51:34.985920] Epoch: [155]  [40/42]  eta: 0:00:01  lr: 0.002265  loss: 0.6916 (0.6965)  time: 0.5087  data: 0.0001  max mem: 9341
[07:51:35.491874] Epoch: [155]  [41/42]  eta: 0:00:00  lr: 0.002265  loss: 0.6877 (0.6963)  time: 0.5086  data: 0.0001  max mem: 9341
[07:51:35.656846] Epoch: [155] Total time: 0:00:22 (0.5376 s / it)
[07:51:35.665473] Averaged stats: lr: 0.002265  loss: 0.6877 (0.6911)
[07:51:40.221296] {"train_lr": 0.0022662036593526177, "train_loss": 0.6910984445185888, "epoch": 155}
[07:51:40.221679] [07:51:40.221764] Training epoch 155 for 0:00:27
[07:51:40.221815] [07:51:40.226163] log_dir: ./exp/debug/cifar100-LT/debug
[07:51:41.805417] Epoch: [156]  [ 0/42]  eta: 0:01:06  lr: 0.002265  loss: 0.6523 (0.6523)  time: 1.5780  data: 1.0670  max mem: 9341
[07:51:51.978855] Epoch: [156]  [20/42]  eta: 0:00:12  lr: 0.002264  loss: 0.6856 (0.6871)  time: 0.5086  data: 0.0001  max mem: 9341
[07:52:02.132362] Epoch: [156]  [40/42]  eta: 0:00:01  lr: 0.002263  loss: 0.7014 (0.6925)  time: 0.5076  data: 0.0001  max mem: 9341
[07:52:02.639001] Epoch: [156]  [41/42]  eta: 0:00:00  lr: 0.002263  loss: 0.7014 (0.6934)  time: 0.5076  data: 0.0001  max mem: 9341
[07:52:02.818792] Epoch: [156] Total time: 0:00:22 (0.5379 s / it)
[07:52:02.822294] Averaged stats: lr: 0.002263  loss: 0.7014 (0.6881)
[07:52:07.429717] {"train_lr": 0.002263918377997565, "train_loss": 0.688122384250164, "epoch": 156}
[07:52:07.430060] [07:52:07.430141] Training epoch 156 for 0:00:27
[07:52:07.430192] [07:52:07.434468] log_dir: ./exp/debug/cifar100-LT/debug
[07:52:08.945910] Epoch: [157]  [ 0/42]  eta: 0:01:03  lr: 0.002262  loss: 0.6387 (0.6387)  time: 1.5101  data: 0.9887  max mem: 9341
[07:52:19.130134] Epoch: [157]  [20/42]  eta: 0:00:12  lr: 0.002261  loss: 0.6832 (0.6823)  time: 0.5092  data: 0.0002  max mem: 9341
[07:52:29.279282] Epoch: [157]  [40/42]  eta: 0:00:01  lr: 0.002261  loss: 0.6936 (0.6863)  time: 0.5074  data: 0.0001  max mem: 9341
[07:52:29.784906] Epoch: [157]  [41/42]  eta: 0:00:00  lr: 0.002261  loss: 0.6936 (0.6855)  time: 0.5073  data: 0.0001  max mem: 9341
[07:52:29.954383] Epoch: [157] Total time: 0:00:22 (0.5362 s / it)
[07:52:29.955617] Averaged stats: lr: 0.002261  loss: 0.6936 (0.6830)
[07:52:34.534695] {"train_lr": 0.002261614917212882, "train_loss": 0.6829854966629119, "epoch": 157}
[07:52:34.535016] [07:52:34.535098] Training epoch 157 for 0:00:27
[07:52:34.535150] [07:52:34.539476] log_dir: ./exp/debug/cifar100-LT/debug
[07:52:36.035030] Epoch: [158]  [ 0/42]  eta: 0:01:02  lr: 0.002260  loss: 0.7077 (0.7077)  time: 1.4934  data: 0.9637  max mem: 9341
[07:52:46.201832] Epoch: [158]  [20/42]  eta: 0:00:12  lr: 0.002259  loss: 0.6811 (0.6795)  time: 0.5083  data: 0.0001  max mem: 9341
[07:52:56.353516] Epoch: [158]  [40/42]  eta: 0:00:01  lr: 0.002258  loss: 0.6714 (0.6789)  time: 0.5075  data: 0.0001  max mem: 9341
[07:52:56.859438] Epoch: [158]  [41/42]  eta: 0:00:00  lr: 0.002258  loss: 0.6630 (0.6770)  time: 0.5076  data: 0.0001  max mem: 9341
[07:52:57.020957] Epoch: [158] Total time: 0:00:22 (0.5353 s / it)
[07:52:57.028674] Averaged stats: lr: 0.002258  loss: 0.6630 (0.6827)
[07:53:01.584975] {"train_lr": 0.0022592933163583582, "train_loss": 0.6826985421634856, "epoch": 158}
[07:53:01.585300] [07:53:01.585385] Training epoch 158 for 0:00:27
[07:53:01.585435] [07:53:01.589750] log_dir: ./exp/debug/cifar100-LT/debug
[07:53:03.215053] Epoch: [159]  [ 0/42]  eta: 0:01:08  lr: 0.002258  loss: 0.7069 (0.7069)  time: 1.6245  data: 1.1111  max mem: 9341
[07:53:13.372947] Epoch: [159]  [20/42]  eta: 0:00:12  lr: 0.002257  loss: 0.6740 (0.6743)  time: 0.5078  data: 0.0001  max mem: 9341
[07:53:23.518297] Epoch: [159]  [40/42]  eta: 0:00:01  lr: 0.002256  loss: 0.6811 (0.6813)  time: 0.5072  data: 0.0001  max mem: 9341
[07:53:24.025789] Epoch: [159]  [41/42]  eta: 0:00:00  lr: 0.002256  loss: 0.6811 (0.6822)  time: 0.5073  data: 0.0001  max mem: 9341
[07:53:24.208719] Epoch: [159] Total time: 0:00:22 (0.5385 s / it)
[07:53:24.209948] Averaged stats: lr: 0.002256  loss: 0.6811 (0.6825)
[07:53:28.783837] {"train_lr": 0.0022569536151037487, "train_loss": 0.682537449612504, "epoch": 159}
[07:53:28.784220] [07:53:28.784306] Training epoch 159 for 0:00:27
[07:53:28.784360] [07:53:28.788736] log_dir: ./exp/debug/cifar100-LT/debug
[07:53:30.279538] Epoch: [160]  [ 0/42]  eta: 0:01:02  lr: 0.002255  loss: 0.6816 (0.6816)  time: 1.4895  data: 0.9698  max mem: 9341
[07:53:40.444372] Epoch: [160]  [20/42]  eta: 0:00:12  lr: 0.002254  loss: 0.6870 (0.6793)  time: 0.5082  data: 0.0001  max mem: 9341
[07:53:50.589391] Epoch: [160]  [40/42]  eta: 0:00:01  lr: 0.002254  loss: 0.6821 (0.6829)  time: 0.5072  data: 0.0001  max mem: 9341
[07:53:51.094652] Epoch: [160]  [41/42]  eta: 0:00:00  lr: 0.002254  loss: 0.6821 (0.6824)  time: 0.5072  data: 0.0001  max mem: 9341
[07:53:51.261922] Epoch: [160] Total time: 0:00:22 (0.5351 s / it)
[07:53:51.263965] Averaged stats: lr: 0.002254  loss: 0.6821 (0.6830)
[07:53:55.745912] {"train_lr": 0.002254595853428091, "train_loss": 0.6829731521152315, "epoch": 160}
[07:53:55.746258] [07:53:55.746339] Training epoch 160 for 0:00:26
[07:53:55.746390] [07:53:55.750722] log_dir: ./exp/debug/cifar100-LT/debug
[07:53:57.169406] Epoch: [161]  [ 0/42]  eta: 0:00:59  lr: 0.002253  loss: 0.6566 (0.6566)  time: 1.4175  data: 0.9134  max mem: 9341
[07:54:07.335507] Epoch: [161]  [20/42]  eta: 0:00:12  lr: 0.002252  loss: 0.6756 (0.6715)  time: 0.5082  data: 0.0001  max mem: 9341
[07:54:17.482662] Epoch: [161]  [40/42]  eta: 0:00:01  lr: 0.002251  loss: 0.6546 (0.6646)  time: 0.5073  data: 0.0001  max mem: 9341
[07:54:17.989831] Epoch: [161]  [41/42]  eta: 0:00:00  lr: 0.002251  loss: 0.6538 (0.6636)  time: 0.5075  data: 0.0001  max mem: 9341
[07:54:18.156993] Epoch: [161] Total time: 0:00:22 (0.5335 s / it)
[07:54:18.165226] Averaged stats: lr: 0.002251  loss: 0.6538 (0.6696)
[07:54:22.773535] {"train_lr": 0.002252220071619026, "train_loss": 0.6695584647712254, "epoch": 161}
[07:54:22.773917] [07:54:22.774003] Training epoch 161 for 0:00:27
[07:54:22.774053] [07:54:22.778740] log_dir: ./exp/debug/cifar100-LT/debug
[07:54:24.497223] Epoch: [162]  [ 0/42]  eta: 0:01:12  lr: 0.002251  loss: 0.6777 (0.6777)  time: 1.7174  data: 1.2161  max mem: 9341
[07:54:34.665925] Epoch: [162]  [20/42]  eta: 0:00:12  lr: 0.002250  loss: 0.6739 (0.6767)  time: 0.5084  data: 0.0001  max mem: 9341
[07:54:44.814026] Epoch: [162]  [40/42]  eta: 0:00:01  lr: 0.002249  loss: 0.6622 (0.6741)  time: 0.5074  data: 0.0001  max mem: 9341
[07:54:45.320354] Epoch: [162]  [41/42]  eta: 0:00:00  lr: 0.002249  loss: 0.6622 (0.6738)  time: 0.5074  data: 0.0001  max mem: 9341
[07:54:45.480780] Epoch: [162] Total time: 0:00:22 (0.5405 s / it)
[07:54:45.486106] Averaged stats: lr: 0.002249  loss: 0.6622 (0.6723)
[07:54:50.097618] {"train_lr": 0.002249826310272114, "train_loss": 0.6722925766592934, "epoch": 162}
[07:54:50.098004] [07:54:50.098096] Training epoch 162 for 0:00:27
[07:54:50.098149] [07:54:50.103172] log_dir: ./exp/debug/cifar100-LT/debug
[07:54:51.739240] Epoch: [163]  [ 0/42]  eta: 0:01:08  lr: 0.002248  loss: 0.7096 (0.7096)  time: 1.6353  data: 1.1194  max mem: 9341
[07:55:01.910690] Epoch: [163]  [20/42]  eta: 0:00:12  lr: 0.002247  loss: 0.6633 (0.6747)  time: 0.5085  data: 0.0001  max mem: 9341
[07:55:12.065003] Epoch: [163]  [40/42]  eta: 0:00:01  lr: 0.002246  loss: 0.6809 (0.6813)  time: 0.5077  data: 0.0001  max mem: 9341
[07:55:12.570907] Epoch: [163]  [41/42]  eta: 0:00:00  lr: 0.002246  loss: 0.6809 (0.6810)  time: 0.5076  data: 0.0001  max mem: 9341
[07:55:12.745527] Epoch: [163] Total time: 0:00:22 (0.5391 s / it)
[07:55:12.749082] Averaged stats: lr: 0.002246  loss: 0.6809 (0.6773)
[07:55:17.342931] {"train_lr": 0.0022474146102901245, "train_loss": 0.6772734309945788, "epoch": 163}
[07:55:17.343270] [07:55:17.343369] Training epoch 163 for 0:00:27
[07:55:17.343419] [07:55:17.349357] log_dir: ./exp/debug/cifar100-LT/debug
[07:55:19.061301] Epoch: [164]  [ 0/42]  eta: 0:01:11  lr: 0.002246  loss: 0.6357 (0.6357)  time: 1.7109  data: 1.2061  max mem: 9341
[07:55:29.231839] Epoch: [164]  [20/42]  eta: 0:00:12  lr: 0.002245  loss: 0.6707 (0.6709)  time: 0.5085  data: 0.0001  max mem: 9341
[07:55:39.385733] Epoch: [164]  [40/42]  eta: 0:00:01  lr: 0.002244  loss: 0.6694 (0.6700)  time: 0.5077  data: 0.0001  max mem: 9341
[07:55:39.890890] Epoch: [164]  [41/42]  eta: 0:00:00  lr: 0.002244  loss: 0.6694 (0.6707)  time: 0.5076  data: 0.0001  max mem: 9341
[07:55:40.055044] Epoch: [164] Total time: 0:00:22 (0.5406 s / it)
[07:55:40.076682] Averaged stats: lr: 0.002244  loss: 0.6694 (0.6706)
[07:55:44.588446] {"train_lr": 0.0022449850128823646, "train_loss": 0.6706483101560956, "epoch": 164}
[07:55:44.588711] [07:55:44.588791] Training epoch 164 for 0:00:27
[07:55:44.588843] [07:55:44.593108] log_dir: ./exp/debug/cifar100-LT/debug
[07:55:46.053628] Epoch: [165]  [ 0/42]  eta: 0:01:01  lr: 0.002243  loss: 0.7026 (0.7026)  time: 1.4595  data: 0.9513  max mem: 9341
[07:55:56.232491] Epoch: [165]  [20/42]  eta: 0:00:12  lr: 0.002242  loss: 0.6563 (0.6635)  time: 0.5089  data: 0.0001  max mem: 9341
[07:56:06.379533] Epoch: [165]  [40/42]  eta: 0:00:01  lr: 0.002241  loss: 0.6744 (0.6662)  time: 0.5073  data: 0.0001  max mem: 9341
[07:56:06.887292] Epoch: [165]  [41/42]  eta: 0:00:00  lr: 0.002241  loss: 0.6744 (0.6653)  time: 0.5074  data: 0.0001  max mem: 9341
[07:56:07.059829] Epoch: [165] Total time: 0:00:22 (0.5349 s / it)
[07:56:07.070317] Averaged stats: lr: 0.002241  loss: 0.6744 (0.6648)
[07:56:11.668031] {"train_lr": 0.002242537559563942, "train_loss": 0.6648446871411233, "epoch": 165}
[07:56:11.668382] [07:56:11.668496] Training epoch 165 for 0:00:27
[07:56:11.668546] [07:56:11.672821] log_dir: ./exp/debug/cifar100-LT/debug
[07:56:13.326994] Epoch: [166]  [ 0/42]  eta: 0:01:09  lr: 0.002241  loss: 0.6554 (0.6554)  time: 1.6535  data: 1.1439  max mem: 9341
[07:56:23.478322] Epoch: [166]  [20/42]  eta: 0:00:12  lr: 0.002240  loss: 0.6672 (0.6731)  time: 0.5075  data: 0.0001  max mem: 9341
[07:56:33.619533] Epoch: [166]  [40/42]  eta: 0:00:01  lr: 0.002239  loss: 0.6719 (0.6718)  time: 0.5070  data: 0.0001  max mem: 9341
[07:56:34.125309] Epoch: [166]  [41/42]  eta: 0:00:00  lr: 0.002239  loss: 0.6719 (0.6716)  time: 0.5070  data: 0.0001  max mem: 9341
[07:56:34.288922] Epoch: [166] Total time: 0:00:22 (0.5385 s / it)
[07:56:34.297640] Averaged stats: lr: 0.002239  loss: 0.6719 (0.6709)
[07:56:38.835134] {"train_lr": 0.002240072292155091, "train_loss": 0.6708825855028062, "epoch": 166}
[07:56:38.835455] [07:56:38.835539] Training epoch 166 for 0:00:27
[07:56:38.835590] [07:56:38.839999] log_dir: ./exp/debug/cifar100-LT/debug
[07:56:40.402035] Epoch: [167]  [ 0/42]  eta: 0:01:05  lr: 0.002238  loss: 0.6795 (0.6795)  time: 1.5612  data: 1.0516  max mem: 9341
[07:56:50.565953] Epoch: [167]  [20/42]  eta: 0:00:12  lr: 0.002237  loss: 0.6579 (0.6665)  time: 0.5082  data: 0.0001  max mem: 9341
[07:57:00.710511] Epoch: [167]  [40/42]  eta: 0:00:01  lr: 0.002237  loss: 0.6667 (0.6670)  time: 0.5072  data: 0.0001  max mem: 9341
[07:57:01.217509] Epoch: [167]  [41/42]  eta: 0:00:00  lr: 0.002237  loss: 0.6667 (0.6675)  time: 0.5073  data: 0.0001  max mem: 9341
[07:57:01.386223] Epoch: [167] Total time: 0:00:22 (0.5368 s / it)
[07:57:01.395057] Averaged stats: lr: 0.002237  loss: 0.6667 (0.6681)
[07:57:05.971778] {"train_lr": 0.002237589252780419, "train_loss": 0.6681498168479829, "epoch": 167}
[07:57:05.972108] [07:57:05.972194] Training epoch 167 for 0:00:27
[07:57:05.972306] [07:57:05.976690] log_dir: ./exp/debug/cifar100-LT/debug
[07:57:07.474727] Epoch: [168]  [ 0/42]  eta: 0:01:02  lr: 0.002236  loss: 0.7044 (0.7044)  time: 1.4970  data: 0.9892  max mem: 9341
[07:57:17.645245] Epoch: [168]  [20/42]  eta: 0:00:12  lr: 0.002235  loss: 0.6676 (0.6786)  time: 0.5085  data: 0.0001  max mem: 9341
[07:57:27.798853] Epoch: [168]  [40/42]  eta: 0:00:01  lr: 0.002234  loss: 0.6620 (0.6697)  time: 0.5076  data: 0.0001  max mem: 9341
[07:57:28.303701] Epoch: [168]  [41/42]  eta: 0:00:00  lr: 0.002234  loss: 0.6667 (0.6701)  time: 0.5077  data: 0.0001  max mem: 9341
[07:57:28.464620] Epoch: [168] Total time: 0:00:22 (0.5354 s / it)
[07:57:28.477585] Averaged stats: lr: 0.002234  loss: 0.6667 (0.6664)
[07:57:33.176879] {"train_lr": 0.002235088483868225, "train_loss": 0.6663608189140048, "epoch": 168}
[07:57:33.177184] [07:57:33.177265] Training epoch 168 for 0:00:27
[07:57:33.177315] [07:57:33.181747] log_dir: ./exp/debug/cifar100-LT/debug
[07:57:34.702651] Epoch: [169]  [ 0/42]  eta: 0:01:03  lr: 0.002233  loss: 0.6529 (0.6529)  time: 1.5201  data: 0.9974  max mem: 9341
[07:57:44.877758] Epoch: [169]  [20/42]  eta: 0:00:12  lr: 0.002232  loss: 0.6761 (0.6757)  time: 0.5087  data: 0.0001  max mem: 9341
[07:57:55.045398] Epoch: [169]  [40/42]  eta: 0:00:01  lr: 0.002231  loss: 0.6566 (0.6720)  time: 0.5083  data: 0.0001  max mem: 9341
[07:57:55.550774] Epoch: [169]  [41/42]  eta: 0:00:00  lr: 0.002231  loss: 0.6506 (0.6711)  time: 0.5084  data: 0.0001  max mem: 9341
[07:57:55.712173] Epoch: [169] Total time: 0:00:22 (0.5364 s / it)
[07:57:55.717106] Averaged stats: lr: 0.002231  loss: 0.6506 (0.6650)
[07:58:00.272686] {"train_lr": 0.0022325700281497496, "train_loss": 0.6649573345979055, "epoch": 169}
[07:58:00.273031] [07:58:00.273111] Training epoch 169 for 0:00:27
[07:58:00.273162] [07:58:00.277519] log_dir: ./exp/debug/cifar100-LT/debug
[07:58:01.780358] Epoch: [170]  [ 0/42]  eta: 0:01:03  lr: 0.002231  loss: 0.6355 (0.6355)  time: 1.5016  data: 0.9955  max mem: 9341
[07:58:11.951052] Epoch: [170]  [20/42]  eta: 0:00:12  lr: 0.002230  loss: 0.6783 (0.6729)  time: 0.5085  data: 0.0001  max mem: 9341
[07:58:22.097660] Epoch: [170]  [40/42]  eta: 0:00:01  lr: 0.002229  loss: 0.6542 (0.6651)  time: 0.5073  data: 0.0001  max mem: 9341
[07:58:22.603483] Epoch: [170]  [41/42]  eta: 0:00:00  lr: 0.002229  loss: 0.6542 (0.6650)  time: 0.5072  data: 0.0001  max mem: 9341
[07:58:22.764478] Epoch: [170] Total time: 0:00:22 (0.5354 s / it)
[07:58:22.767773] Averaged stats: lr: 0.002229  loss: 0.6542 (0.6613)
[07:58:27.302512] {"train_lr": 0.0022300339286584494, "train_loss": 0.6612977012991905, "epoch": 170}
[07:58:27.302917] [07:58:27.303005] Training epoch 170 for 0:00:27
[07:58:27.303057] [07:58:27.307497] log_dir: ./exp/debug/cifar100-LT/debug
[07:58:29.073641] Epoch: [171]  [ 0/42]  eta: 0:01:14  lr: 0.002228  loss: 0.6394 (0.6394)  time: 1.7651  data: 1.2680  max mem: 9341
[07:58:39.234605] Epoch: [171]  [20/42]  eta: 0:00:12  lr: 0.002227  loss: 0.6634 (0.6662)  time: 0.5080  data: 0.0001  max mem: 9341
[07:58:49.364129] Epoch: [171]  [40/42]  eta: 0:00:01  lr: 0.002226  loss: 0.6599 (0.6654)  time: 0.5064  data: 0.0001  max mem: 9341
[07:58:49.868334] Epoch: [171]  [41/42]  eta: 0:00:00  lr: 0.002226  loss: 0.6599 (0.6652)  time: 0.5065  data: 0.0001  max mem: 9341
[07:58:50.021087] Epoch: [171] Total time: 0:00:22 (0.5408 s / it)
[07:58:50.031433] Averaged stats: lr: 0.002226  loss: 0.6599 (0.6624)
[07:58:54.525677] {"train_lr": 0.0022274802287292677, "train_loss": 0.6623847722297623, "epoch": 171}
[07:58:54.525938] [07:58:54.526016] Training epoch 171 for 0:00:27
[07:58:54.526066] [07:58:54.530320] log_dir: ./exp/debug/cifar100-LT/debug
[07:58:56.096954] Epoch: [172]  [ 0/42]  eta: 0:01:05  lr: 0.002226  loss: 0.6563 (0.6563)  time: 1.5654  data: 1.0480  max mem: 9341
[07:59:06.263537] Epoch: [172]  [20/42]  eta: 0:00:12  lr: 0.002225  loss: 0.6622 (0.6628)  time: 0.5083  data: 0.0001  max mem: 9341
[07:59:16.409670] Epoch: [172]  [40/42]  eta: 0:00:01  lr: 0.002224  loss: 0.6630 (0.6660)  time: 0.5073  data: 0.0001  max mem: 9341
[07:59:16.916051] Epoch: [172]  [41/42]  eta: 0:00:00  lr: 0.002224  loss: 0.6629 (0.6657)  time: 0.5073  data: 0.0001  max mem: 9341
[07:59:17.079023] Epoch: [172] Total time: 0:00:22 (0.5369 s / it)
[07:59:17.086326] Averaged stats: lr: 0.002224  loss: 0.6629 (0.6600)
[07:59:21.603055] {"train_lr": 0.0022249089719978935, "train_loss": 0.6599620307485262, "epoch": 172}
[07:59:21.603474] [07:59:21.603575] Training epoch 172 for 0:00:27
[07:59:21.603627] [07:59:21.608048] log_dir: ./exp/debug/cifar100-LT/debug
[07:59:23.309177] Epoch: [173]  [ 0/42]  eta: 0:01:11  lr: 0.002223  loss: 0.6616 (0.6616)  time: 1.7000  data: 1.1850  max mem: 9341
[07:59:33.475726] Epoch: [173]  [20/42]  eta: 0:00:12  lr: 0.002222  loss: 0.6633 (0.6591)  time: 0.5083  data: 0.0001  max mem: 9341
[07:59:43.623244] Epoch: [173]  [40/42]  eta: 0:00:01  lr: 0.002221  loss: 0.6469 (0.6581)  time: 0.5073  data: 0.0001  max mem: 9341
[07:59:44.128927] Epoch: [173]  [41/42]  eta: 0:00:00  lr: 0.002221  loss: 0.6469 (0.6585)  time: 0.5073  data: 0.0001  max mem: 9341
[07:59:44.304690] Epoch: [173] Total time: 0:00:22 (0.5404 s / it)
[07:59:44.305718] Averaged stats: lr: 0.002221  loss: 0.6469 (0.6617)
[07:59:48.812896] {"train_lr": 0.002222320202400005, "train_loss": 0.6617126507418496, "epoch": 173}
[07:59:48.813215] [07:59:48.813293] Training epoch 173 for 0:00:27
[07:59:48.813343] [07:59:48.817591] log_dir: ./exp/debug/cifar100-LT/debug
[07:59:50.334935] Epoch: [174]  [ 0/42]  eta: 0:01:03  lr: 0.002221  loss: 0.6847 (0.6847)  time: 1.5164  data: 1.0214  max mem: 9341
[08:00:00.498754] Epoch: [174]  [20/42]  eta: 0:00:12  lr: 0.002220  loss: 0.6503 (0.6578)  time: 0.5081  data: 0.0001  max mem: 9341
[08:00:10.641389] Epoch: [174]  [40/42]  eta: 0:00:01  lr: 0.002219  loss: 0.6648 (0.6609)  time: 0.5071  data: 0.0001  max mem: 9341
[08:00:11.146411] Epoch: [174]  [41/42]  eta: 0:00:00  lr: 0.002219  loss: 0.6648 (0.6609)  time: 0.5072  data: 0.0001  max mem: 9341
[08:00:11.320364] Epoch: [174] Total time: 0:00:22 (0.5358 s / it)
[08:00:11.321149] Averaged stats: lr: 0.002219  loss: 0.6648 (0.6575)
[08:00:15.845166] {"train_lr": 0.0022197139641705376, "train_loss": 0.6575181317471323, "epoch": 174}
[08:00:15.845503] [08:00:15.845587] Training epoch 174 for 0:00:27
[08:00:15.845639] [08:00:15.850044] log_dir: ./exp/debug/cifar100-LT/debug
[08:00:17.551187] Epoch: [175]  [ 0/42]  eta: 0:01:11  lr: 0.002218  loss: 0.6762 (0.6762)  time: 1.7004  data: 1.1766  max mem: 9341
[08:00:27.716565] Epoch: [175]  [20/42]  eta: 0:00:12  lr: 0.002217  loss: 0.6556 (0.6531)  time: 0.5082  data: 0.0001  max mem: 9341
[08:00:37.872182] Epoch: [175]  [40/42]  eta: 0:00:01  lr: 0.002216  loss: 0.6534 (0.6534)  time: 0.5077  data: 0.0001  max mem: 9341
[08:00:38.378808] Epoch: [175]  [41/42]  eta: 0:00:00  lr: 0.002216  loss: 0.6534 (0.6530)  time: 0.5078  data: 0.0001  max mem: 9341
[08:00:38.554367] Epoch: [175] Total time: 0:00:22 (0.5406 s / it)
[08:00:38.556409] Averaged stats: lr: 0.002216  loss: 0.6534 (0.6567)
[08:00:43.142295] {"train_lr": 0.002217090301842909, "train_loss": 0.6567285678216389, "epoch": 175}
[08:00:43.142649] [08:00:43.142730] Training epoch 175 for 0:00:27
[08:00:43.142780] [08:00:43.147050] log_dir: ./exp/debug/cifar100-LT/debug
[08:00:44.585963] Epoch: [176]  [ 0/42]  eta: 0:01:00  lr: 0.002215  loss: 0.6411 (0.6411)  time: 1.4375  data: 0.9387  max mem: 9341
[08:00:54.744268] Epoch: [176]  [20/42]  eta: 0:00:12  lr: 0.002214  loss: 0.6616 (0.6582)  time: 0.5079  data: 0.0001  max mem: 9341
[08:01:04.887912] Epoch: [176]  [40/42]  eta: 0:00:01  lr: 0.002213  loss: 0.6602 (0.6580)  time: 0.5071  data: 0.0001  max mem: 9341
[08:01:05.393400] Epoch: [176]  [41/42]  eta: 0:00:00  lr: 0.002213  loss: 0.6602 (0.6570)  time: 0.5071  data: 0.0001  max mem: 9341
[08:01:05.562243] Epoch: [176] Total time: 0:00:22 (0.5337 s / it)
[08:01:05.562943] Averaged stats: lr: 0.002213  loss: 0.6602 (0.6572)
[08:01:10.067232] {"train_lr": 0.0022144492602482655, "train_loss": 0.657170526328541, "epoch": 176}
[08:01:10.067589] [08:01:10.067676] Training epoch 176 for 0:00:26
[08:01:10.067728] [08:01:10.072281] log_dir: ./exp/debug/cifar100-LT/debug
[08:01:11.781413] Epoch: [177]  [ 0/42]  eta: 0:01:11  lr: 0.002213  loss: 0.6471 (0.6471)  time: 1.7084  data: 1.2025  max mem: 9341
[08:01:21.939988] Epoch: [177]  [20/42]  eta: 0:00:12  lr: 0.002212  loss: 0.6722 (0.6675)  time: 0.5079  data: 0.0001  max mem: 9341
[08:01:32.092523] Epoch: [177]  [40/42]  eta: 0:00:01  lr: 0.002211  loss: 0.6609 (0.6658)  time: 0.5076  data: 0.0001  max mem: 9341
[08:01:32.596562] Epoch: [177]  [41/42]  eta: 0:00:00  lr: 0.002211  loss: 0.6609 (0.6656)  time: 0.5075  data: 0.0001  max mem: 9341
[08:01:32.758898] Epoch: [177] Total time: 0:00:22 (0.5402 s / it)
[08:01:32.768047] Averaged stats: lr: 0.002211  loss: 0.6609 (0.6673)
[08:01:37.348346] {"train_lr": 0.002211790884514731, "train_loss": 0.667312757954711, "epoch": 177}
[08:01:37.348639] [08:01:37.348718] Training epoch 177 for 0:00:27
[08:01:37.348786] [08:01:37.353085] log_dir: ./exp/debug/cifar100-LT/debug
[08:01:38.965410] Epoch: [178]  [ 0/42]  eta: 0:01:07  lr: 0.002210  loss: 0.6014 (0.6014)  time: 1.6116  data: 1.1079  max mem: 9341
[08:01:49.123601] Epoch: [178]  [20/42]  eta: 0:00:12  lr: 0.002209  loss: 0.6587 (0.6529)  time: 0.5079  data: 0.0001  max mem: 9341
[08:01:59.257920] Epoch: [178]  [40/42]  eta: 0:00:01  lr: 0.002208  loss: 0.6623 (0.6572)  time: 0.5067  data: 0.0001  max mem: 9341
[08:01:59.764930] Epoch: [178]  [41/42]  eta: 0:00:00  lr: 0.002208  loss: 0.6623 (0.6566)  time: 0.5067  data: 0.0001  max mem: 9341
[08:01:59.939999] Epoch: [178] Total time: 0:00:22 (0.5378 s / it)
[08:01:59.941131] Averaged stats: lr: 0.002208  loss: 0.6623 (0.6587)
[08:02:04.441194] {"train_lr": 0.0022091152200666006, "train_loss": 0.6586703381368092, "epoch": 178}
[08:02:04.441520] [08:02:04.441602] Training epoch 178 for 0:00:27
[08:02:04.441653] [08:02:04.446359] log_dir: ./exp/debug/cifar100-LT/debug
[08:02:05.955252] Epoch: [179]  [ 0/42]  eta: 0:01:03  lr: 0.002207  loss: 0.6538 (0.6538)  time: 1.5081  data: 1.0010  max mem: 9341
[08:02:16.116995] Epoch: [179]  [20/42]  eta: 0:00:12  lr: 0.002206  loss: 0.6381 (0.6538)  time: 0.5080  data: 0.0001  max mem: 9341
[08:02:26.267881] Epoch: [179]  [40/42]  eta: 0:00:01  lr: 0.002205  loss: 0.6491 (0.6533)  time: 0.5075  data: 0.0001  max mem: 9341
[08:02:26.773066] Epoch: [179]  [41/42]  eta: 0:00:00  lr: 0.002205  loss: 0.6508 (0.6537)  time: 0.5075  data: 0.0001  max mem: 9341
[08:02:26.931607] Epoch: [179] Total time: 0:00:22 (0.5354 s / it)
[08:02:26.938547] Averaged stats: lr: 0.002205  loss: 0.6508 (0.6564)
[08:02:31.692055] {"train_lr": 0.002206422312623609, "train_loss": 0.6563700820718493, "epoch": 179}
[08:02:31.692415] [08:02:31.692496] Training epoch 179 for 0:00:27
[08:02:31.692549] [08:02:31.697273] log_dir: ./exp/debug/cifar100-LT/debug
[08:02:33.424042] Epoch: [180]  [ 0/42]  eta: 0:01:12  lr: 0.002205  loss: 0.6620 (0.6620)  time: 1.7256  data: 1.2204  max mem: 9341
[08:02:43.589551] Epoch: [180]  [20/42]  eta: 0:00:12  lr: 0.002204  loss: 0.6455 (0.6469)  time: 0.5082  data: 0.0001  max mem: 9341
[08:02:53.741822] Epoch: [180]  [40/42]  eta: 0:00:01  lr: 0.002203  loss: 0.6602 (0.6538)  time: 0.5076  data: 0.0001  max mem: 9341
[08:02:54.247604] Epoch: [180]  [41/42]  eta: 0:00:00  lr: 0.002203  loss: 0.6602 (0.6542)  time: 0.5076  data: 0.0001  max mem: 9341
[08:02:54.422892] Epoch: [180] Total time: 0:00:22 (0.5411 s / it)
[08:02:54.424418] Averaged stats: lr: 0.002203  loss: 0.6602 (0.6524)
[08:02:59.079270] {"train_lr": 0.002203712208200103, "train_loss": 0.6523960708152681, "epoch": 180}
[08:02:59.079577] [08:02:59.079662] Training epoch 180 for 0:00:27
[08:02:59.079715] [08:02:59.084024] log_dir: ./exp/debug/cifar100-LT/debug
[08:03:00.735247] Epoch: [181]  [ 0/42]  eta: 0:01:09  lr: 0.002202  loss: 0.6998 (0.6998)  time: 1.6505  data: 1.1347  max mem: 9341
[08:03:10.899853] Epoch: [181]  [20/42]  eta: 0:00:12  lr: 0.002201  loss: 0.6578 (0.6657)  time: 0.5082  data: 0.0001  max mem: 9341
[08:03:21.058483] Epoch: [181]  [40/42]  eta: 0:00:01  lr: 0.002200  loss: 0.6657 (0.6655)  time: 0.5079  data: 0.0001  max mem: 9341
[08:03:21.565485] Epoch: [181]  [41/42]  eta: 0:00:00  lr: 0.002200  loss: 0.6740 (0.6667)  time: 0.5078  data: 0.0001  max mem: 9341
[08:03:21.728340] Epoch: [181] Total time: 0:00:22 (0.5391 s / it)
[08:03:21.738580] Averaged stats: lr: 0.002200  loss: 0.6740 (0.6632)
[08:03:26.263656] {"train_lr": 0.0022009849531042965, "train_loss": 0.6632038259080478, "epoch": 181}
[08:03:26.264011] [08:03:26.264137] Training epoch 181 for 0:00:27
[08:03:26.264194] [08:03:26.268536] log_dir: ./exp/debug/cifar100-LT/debug
[08:03:27.990687] Epoch: [182]  [ 0/42]  eta: 0:01:12  lr: 0.002199  loss: 0.6702 (0.6702)  time: 1.7212  data: 1.2272  max mem: 9341
[08:03:38.155538] Epoch: [182]  [20/42]  eta: 0:00:12  lr: 0.002198  loss: 0.6589 (0.6631)  time: 0.5082  data: 0.0001  max mem: 9341
[08:03:48.312565] Epoch: [182]  [40/42]  eta: 0:00:01  lr: 0.002197  loss: 0.6628 (0.6650)  time: 0.5078  data: 0.0001  max mem: 9341
[08:03:48.819230] Epoch: [182]  [41/42]  eta: 0:00:00  lr: 0.002197  loss: 0.6631 (0.6656)  time: 0.5079  data: 0.0001  max mem: 9341
[08:03:48.996500] Epoch: [182] Total time: 0:00:22 (0.5411 s / it)
[08:03:49.001012] Averaged stats: lr: 0.002197  loss: 0.6631 (0.6696)
[08:03:53.537989] {"train_lr": 0.0021982405939374554, "train_loss": 0.6695522584375881, "epoch": 182}
[08:03:53.538339] [08:03:53.538441] Training epoch 182 for 0:00:27
[08:03:53.538493] [08:03:53.542870] log_dir: ./exp/debug/cifar100-LT/debug
[08:03:55.223568] Epoch: [183]  [ 0/42]  eta: 0:01:10  lr: 0.002196  loss: 0.6600 (0.6600)  time: 1.6800  data: 1.1687  max mem: 9341
[08:04:05.409278] Epoch: [183]  [20/42]  eta: 0:00:12  lr: 0.002195  loss: 0.6737 (0.6647)  time: 0.5092  data: 0.0001  max mem: 9341
[08:04:15.576908] Epoch: [183]  [40/42]  eta: 0:00:01  lr: 0.002194  loss: 0.6674 (0.6663)  time: 0.5083  data: 0.0001  max mem: 9341
[08:04:16.083742] Epoch: [183]  [41/42]  eta: 0:00:00  lr: 0.002194  loss: 0.6670 (0.6661)  time: 0.5084  data: 0.0001  max mem: 9341
[08:04:16.263810] Epoch: [183] Total time: 0:00:22 (0.5410 s / it)
[08:04:16.267918] Averaged stats: lr: 0.002194  loss: 0.6670 (0.6580)
[08:04:20.821774] {"train_lr": 0.0021954791775931014, "train_loss": 0.6579608565994671, "epoch": 183}
[08:04:20.822183] [08:04:20.822262] Training epoch 183 for 0:00:27
[08:04:20.822313] [08:04:20.826701] log_dir: ./exp/debug/cifar100-LT/debug
[08:04:22.335854] Epoch: [184]  [ 0/42]  eta: 0:01:03  lr: 0.002194  loss: 0.6537 (0.6537)  time: 1.5080  data: 0.9918  max mem: 9341
[08:04:32.500483] Epoch: [184]  [20/42]  eta: 0:00:12  lr: 0.002193  loss: 0.6439 (0.6494)  time: 0.5082  data: 0.0001  max mem: 9341
[08:04:42.646152] Epoch: [184]  [40/42]  eta: 0:00:01  lr: 0.002191  loss: 0.6607 (0.6536)  time: 0.5072  data: 0.0001  max mem: 9341
[08:04:43.151828] Epoch: [184]  [41/42]  eta: 0:00:00  lr: 0.002191  loss: 0.6529 (0.6536)  time: 0.5072  data: 0.0001  max mem: 9341
[08:04:43.321762] Epoch: [184] Total time: 0:00:22 (0.5356 s / it)
[08:04:43.328048] Averaged stats: lr: 0.002191  loss: 0.6529 (0.6509)
[08:04:47.849926] {"train_lr": 0.002192700751256222, "train_loss": 0.6508825307800656, "epoch": 184}
[08:04:47.850289] [08:04:47.850375] Training epoch 184 for 0:00:27
[08:04:47.850426] [08:04:47.854847] log_dir: ./exp/debug/cifar100-LT/debug
[08:04:49.456072] Epoch: [185]  [ 0/42]  eta: 0:01:07  lr: 0.002191  loss: 0.6531 (0.6531)  time: 1.6005  data: 1.0867  max mem: 9341
[08:04:59.640436] Epoch: [185]  [20/42]  eta: 0:00:12  lr: 0.002190  loss: 0.6348 (0.6376)  time: 0.5092  data: 0.0001  max mem: 9341
[08:05:09.809951] Epoch: [185]  [40/42]  eta: 0:00:01  lr: 0.002189  loss: 0.6514 (0.6438)  time: 0.5084  data: 0.0001  max mem: 9341
[08:05:10.317047] Epoch: [185]  [41/42]  eta: 0:00:00  lr: 0.002189  loss: 0.6446 (0.6432)  time: 0.5085  data: 0.0001  max mem: 9341
[08:05:10.501485] Epoch: [185] Total time: 0:00:22 (0.5392 s / it)
[08:05:10.507440] Averaged stats: lr: 0.002189  loss: 0.6446 (0.6451)
[08:05:15.008796] {"train_lr": 0.002189905362402459, "train_loss": 0.64512880501293, "epoch": 185}
[08:05:15.009045] [08:05:15.009124] Training epoch 185 for 0:00:27
[08:05:15.009172] [08:05:15.013607] log_dir: ./exp/debug/cifar100-LT/debug
[08:05:16.663743] Epoch: [186]  [ 0/42]  eta: 0:01:09  lr: 0.002188  loss: 0.6112 (0.6112)  time: 1.6494  data: 1.1326  max mem: 9341
[08:05:26.840656] Epoch: [186]  [20/42]  eta: 0:00:12  lr: 0.002187  loss: 0.6505 (0.6474)  time: 0.5088  data: 0.0001  max mem: 9341
[08:05:36.992777] Epoch: [186]  [40/42]  eta: 0:00:01  lr: 0.002186  loss: 0.6386 (0.6430)  time: 0.5076  data: 0.0001  max mem: 9341
[08:05:37.497103] Epoch: [186]  [41/42]  eta: 0:00:00  lr: 0.002186  loss: 0.6386 (0.6436)  time: 0.5075  data: 0.0001  max mem: 9341
[08:05:37.666695] Epoch: [186] Total time: 0:00:22 (0.5394 s / it)
[08:05:37.677511] Averaged stats: lr: 0.002186  loss: 0.6386 (0.6391)
[08:05:42.286601] {"train_lr": 0.0021870930587972967, "train_loss": 0.6391352290908495, "epoch": 186}
[08:05:42.286861] [08:05:42.286946] Training epoch 186 for 0:00:27
[08:05:42.286998] [08:05:42.291306] log_dir: ./exp/debug/cifar100-LT/debug
[08:05:43.734601] Epoch: [187]  [ 0/42]  eta: 0:01:00  lr: 0.002185  loss: 0.5945 (0.5945)  time: 1.4424  data: 0.9239  max mem: 9341
[08:05:53.914842] Epoch: [187]  [20/42]  eta: 0:00:12  lr: 0.002184  loss: 0.6340 (0.6337)  time: 0.5090  data: 0.0001  max mem: 9341
[08:06:04.083656] Epoch: [187]  [40/42]  eta: 0:00:01  lr: 0.002183  loss: 0.6423 (0.6413)  time: 0.5084  data: 0.0001  max mem: 9341
[08:06:04.589612] Epoch: [187]  [41/42]  eta: 0:00:00  lr: 0.002183  loss: 0.6423 (0.6419)  time: 0.5084  data: 0.0001  max mem: 9341
[08:06:04.746804] Epoch: [187] Total time: 0:00:22 (0.5347 s / it)
[08:06:04.765992] Averaged stats: lr: 0.002183  loss: 0.6423 (0.6410)
[08:06:09.414780] {"train_lr": 0.002184263888495237, "train_loss": 0.6410388900410562, "epoch": 187}
[08:06:09.415092] [08:06:09.415171] Training epoch 187 for 0:00:27
[08:06:09.415222] [08:06:09.419461] log_dir: ./exp/debug/cifar100-LT/debug
[08:06:10.876479] Epoch: [188]  [ 0/42]  eta: 0:01:01  lr: 0.002182  loss: 0.6350 (0.6350)  time: 1.4560  data: 0.9436  max mem: 9341
[08:06:21.056746] Epoch: [188]  [20/42]  eta: 0:00:12  lr: 0.002181  loss: 0.6444 (0.6406)  time: 0.5090  data: 0.0001  max mem: 9341
[08:06:31.214700] Epoch: [188]  [40/42]  eta: 0:00:01  lr: 0.002180  loss: 0.6403 (0.6406)  time: 0.5078  data: 0.0001  max mem: 9341
[08:06:31.720818] Epoch: [188]  [41/42]  eta: 0:00:00  lr: 0.002180  loss: 0.6403 (0.6410)  time: 0.5079  data: 0.0001  max mem: 9341
[08:06:31.885284] Epoch: [188] Total time: 0:00:22 (0.5349 s / it)
[08:06:31.891135] Averaged stats: lr: 0.002180  loss: 0.6403 (0.6388)
[08:06:36.561802] {"train_lr": 0.002181417899839, "train_loss": 0.6387926052723613, "epoch": 188}
[08:06:36.562127] [08:06:36.562210] Training epoch 188 for 0:00:27
[08:06:36.562262] [08:06:36.566604] log_dir: ./exp/debug/cifar100-LT/debug
[08:06:38.245107] Epoch: [189]  [ 0/42]  eta: 0:01:10  lr: 0.002179  loss: 0.6608 (0.6608)  time: 1.6778  data: 1.1721  max mem: 9341
[08:06:48.409515] Epoch: [189]  [20/42]  eta: 0:00:12  lr: 0.002178  loss: 0.6339 (0.6345)  time: 0.5082  data: 0.0001  max mem: 9341
[08:06:58.568010] Epoch: [189]  [40/42]  eta: 0:00:01  lr: 0.002177  loss: 0.6485 (0.6419)  time: 0.5079  data: 0.0001  max mem: 9341
[08:06:59.071813] Epoch: [189]  [41/42]  eta: 0:00:00  lr: 0.002177  loss: 0.6485 (0.6426)  time: 0.5077  data: 0.0001  max mem: 9341
[08:06:59.241362] Epoch: [189] Total time: 0:00:22 (0.5399 s / it)
[08:06:59.252395] Averaged stats: lr: 0.002177  loss: 0.6485 (0.6434)
[08:07:03.980828] {"train_lr": 0.002178555141458683, "train_loss": 0.643428650640306, "epoch": 189}
[08:07:03.981127] [08:07:03.981208] Training epoch 189 for 0:00:27
[08:07:03.981259] [08:07:03.985779] log_dir: ./exp/debug/cifar100-LT/debug
[08:07:05.575059] Epoch: [190]  [ 0/42]  eta: 0:01:06  lr: 0.002177  loss: 0.6816 (0.6816)  time: 1.5884  data: 1.0782  max mem: 9341
[08:07:15.755619] Epoch: [190]  [20/42]  eta: 0:00:12  lr: 0.002176  loss: 0.6420 (0.6416)  time: 0.5090  data: 0.0001  max mem: 9341
[08:07:25.923923] Epoch: [190]  [40/42]  eta: 0:00:01  lr: 0.002174  loss: 0.6505 (0.6494)  time: 0.5084  data: 0.0001  max mem: 9341
[08:07:26.430193] Epoch: [190]  [41/42]  eta: 0:00:00  lr: 0.002174  loss: 0.6505 (0.6481)  time: 0.5084  data: 0.0001  max mem: 9341
[08:07:26.593125] Epoch: [190] Total time: 0:00:22 (0.5383 s / it)
[08:07:26.593855] Averaged stats: lr: 0.002174  loss: 0.6505 (0.6447)
[08:07:31.174318] {"train_lr": 0.002175675662270921, "train_loss": 0.6447161773131007, "epoch": 190}
[08:07:31.174608] [08:07:31.174692] Training epoch 190 for 0:00:27
[08:07:31.174742] [08:07:31.179000] log_dir: ./exp/debug/cifar100-LT/debug
[08:07:32.724956] Epoch: [191]  [ 0/42]  eta: 0:01:04  lr: 0.002174  loss: 0.6142 (0.6142)  time: 1.5449  data: 1.0333  max mem: 9341
[08:07:42.892589] Epoch: [191]  [20/42]  eta: 0:00:12  lr: 0.002173  loss: 0.6362 (0.6337)  time: 0.5083  data: 0.0001  max mem: 9341
[08:07:53.031255] Epoch: [191]  [40/42]  eta: 0:00:01  lr: 0.002172  loss: 0.6444 (0.6413)  time: 0.5069  data: 0.0001  max mem: 9341
[08:07:53.537197] Epoch: [191]  [41/42]  eta: 0:00:00  lr: 0.002172  loss: 0.6444 (0.6404)  time: 0.5069  data: 0.0001  max mem: 9341
[08:07:53.712463] Epoch: [191] Total time: 0:00:22 (0.5365 s / it)
[08:07:53.718369] Averaged stats: lr: 0.002172  loss: 0.6444 (0.6417)
[08:07:58.221308] {"train_lr": 0.002172779511478072, "train_loss": 0.6416603876721292, "epoch": 191}
[08:07:58.221684] [08:07:58.221773] Training epoch 191 for 0:00:27
[08:07:58.221824] [08:07:58.226755] log_dir: ./exp/debug/cifar100-LT/debug
[08:07:59.801894] Epoch: [192]  [ 0/42]  eta: 0:01:06  lr: 0.002171  loss: 0.6499 (0.6499)  time: 1.5743  data: 1.0639  max mem: 9341
[08:08:09.965722] Epoch: [192]  [20/42]  eta: 0:00:12  lr: 0.002170  loss: 0.6257 (0.6241)  time: 0.5081  data: 0.0001  max mem: 9341
[08:08:20.107123] Epoch: [192]  [40/42]  eta: 0:00:01  lr: 0.002169  loss: 0.6253 (0.6273)  time: 0.5070  data: 0.0001  max mem: 9341
[08:08:20.612664] Epoch: [192]  [41/42]  eta: 0:00:00  lr: 0.002169  loss: 0.6155 (0.6261)  time: 0.5069  data: 0.0001  max mem: 9341
[08:08:20.786100] Epoch: [192] Total time: 0:00:22 (0.5371 s / it)
[08:08:20.787034] Averaged stats: lr: 0.002169  loss: 0.6155 (0.6343)
[08:08:25.286293] {"train_lr": 0.0021698667385673643, "train_loss": 0.634316681041604, "epoch": 192}
[08:08:25.286625] [08:08:25.286722] Training epoch 192 for 0:00:27
[08:08:25.286776] [08:08:25.291215] log_dir: ./exp/debug/cifar100-LT/debug
[08:08:26.977390] Epoch: [193]  [ 0/42]  eta: 0:01:10  lr: 0.002168  loss: 0.6066 (0.6066)  time: 1.6855  data: 1.1650  max mem: 9341
[08:08:37.240591] Epoch: [193]  [20/42]  eta: 0:00:12  lr: 0.002167  loss: 0.6309 (0.6372)  time: 0.5131  data: 0.0001  max mem: 9341
[08:08:47.415613] Epoch: [193]  [40/42]  eta: 0:00:01  lr: 0.002166  loss: 0.6275 (0.6327)  time: 0.5087  data: 0.0001  max mem: 9341
[08:08:47.923177] Epoch: [193]  [41/42]  eta: 0:00:00  lr: 0.002166  loss: 0.6324 (0.6335)  time: 0.5088  data: 0.0001  max mem: 9341
[08:08:48.081729] Epoch: [193] Total time: 0:00:22 (0.5426 s / it)
[08:08:48.090592] Averaged stats: lr: 0.002166  loss: 0.6324 (0.6367)
[08:08:52.589525] {"train_lr": 0.002166937393310047, "train_loss": 0.636717959174088, "epoch": 193}
[08:08:52.589904] [08:08:52.589994] Training epoch 193 for 0:00:27
[08:08:52.590046] [08:08:52.594986] log_dir: ./exp/debug/cifar100-LT/debug
[08:08:54.055137] Epoch: [194]  [ 0/42]  eta: 0:01:01  lr: 0.002165  loss: 0.6483 (0.6483)  time: 1.4591  data: 0.9397  max mem: 9341
[08:09:04.216690] Epoch: [194]  [20/42]  eta: 0:00:12  lr: 0.002164  loss: 0.6295 (0.6267)  time: 0.5080  data: 0.0001  max mem: 9341
[08:09:14.361454] Epoch: [194]  [40/42]  eta: 0:00:01  lr: 0.002163  loss: 0.6435 (0.6330)  time: 0.5072  data: 0.0001  max mem: 9341
[08:09:14.868471] Epoch: [194]  [41/42]  eta: 0:00:00  lr: 0.002163  loss: 0.6410 (0.6331)  time: 0.5072  data: 0.0001  max mem: 9341
[08:09:15.031834] Epoch: [194] Total time: 0:00:22 (0.5342 s / it)
[08:09:15.042273] Averaged stats: lr: 0.002163  loss: 0.6410 (0.6409)
[08:09:19.573514] {"train_lr": 0.0021639915257605446, "train_loss": 0.6408524133619808, "epoch": 194}
[08:09:19.573925] [08:09:19.574013] Training epoch 194 for 0:00:26
[08:09:19.574065] [08:09:19.578485] log_dir: ./exp/debug/cifar100-LT/debug
[08:09:21.154860] Epoch: [195]  [ 0/42]  eta: 0:01:06  lr: 0.002162  loss: 0.7104 (0.7104)  time: 1.5743  data: 1.0707  max mem: 9341
[08:09:31.333078] Epoch: [195]  [20/42]  eta: 0:00:12  lr: 0.002161  loss: 0.6370 (0.6406)  time: 0.5089  data: 0.0001  max mem: 9341
[08:09:41.485945] Epoch: [195]  [40/42]  eta: 0:00:01  lr: 0.002160  loss: 0.6281 (0.6396)  time: 0.5076  data: 0.0001  max mem: 9341
[08:09:41.991064] Epoch: [195]  [41/42]  eta: 0:00:00  lr: 0.002160  loss: 0.6277 (0.6393)  time: 0.5075  data: 0.0001  max mem: 9341
[08:09:42.163628] Epoch: [195] Total time: 0:00:22 (0.5377 s / it)
[08:09:42.177014] Averaged stats: lr: 0.002160  loss: 0.6277 (0.6359)
[08:09:46.644300] {"train_lr": 0.002161029186255614, "train_loss": 0.6358832138634863, "epoch": 195}
[08:09:46.644593] [08:09:46.644671] Training epoch 195 for 0:00:27
[08:09:46.644721] [08:09:46.648959] log_dir: ./exp/debug/cifar100-LT/debug
[08:09:48.214072] Epoch: [196]  [ 0/42]  eta: 0:01:05  lr: 0.002159  loss: 0.6391 (0.6391)  time: 1.5641  data: 1.0536  max mem: 9341
[08:09:58.371907] Epoch: [196]  [20/42]  eta: 0:00:12  lr: 0.002158  loss: 0.6385 (0.6378)  time: 0.5078  data: 0.0001  max mem: 9341
[08:10:08.508276] Epoch: [196]  [40/42]  eta: 0:00:01  lr: 0.002157  loss: 0.6277 (0.6326)  time: 0.5068  data: 0.0001  max mem: 9341
[08:10:09.014007] Epoch: [196]  [41/42]  eta: 0:00:00  lr: 0.002157  loss: 0.6277 (0.6322)  time: 0.5067  data: 0.0001  max mem: 9341
[08:10:09.190640] Epoch: [196] Total time: 0:00:22 (0.5367 s / it)
[08:10:09.191346] Averaged stats: lr: 0.002157  loss: 0.6277 (0.6335)
[08:10:13.799522] {"train_lr": 0.0021580504254134583, "train_loss": 0.6334748938679695, "epoch": 196}
[08:10:13.799879] [08:10:13.799964] Training epoch 196 for 0:00:27
[08:10:13.800015] [08:10:13.805024] log_dir: ./exp/debug/cifar100-LT/debug
[08:10:15.322437] Epoch: [197]  [ 0/42]  eta: 0:01:03  lr: 0.002156  loss: 0.6114 (0.6114)  time: 1.5162  data: 1.0027  max mem: 9341
[08:10:25.508394] Epoch: [197]  [20/42]  eta: 0:00:12  lr: 0.002155  loss: 0.6248 (0.6286)  time: 0.5092  data: 0.0001  max mem: 9341
[08:10:35.680935] Epoch: [197]  [40/42]  eta: 0:00:01  lr: 0.002154  loss: 0.6375 (0.6312)  time: 0.5086  data: 0.0001  max mem: 9341
[08:10:36.187035] Epoch: [197]  [41/42]  eta: 0:00:00  lr: 0.002154  loss: 0.6375 (0.6323)  time: 0.5086  data: 0.0001  max mem: 9341
[08:10:36.358557] Epoch: [197] Total time: 0:00:22 (0.5370 s / it)
[08:10:36.362098] Averaged stats: lr: 0.002154  loss: 0.6375 (0.6281)
[08:10:40.932532] {"train_lr": 0.002155055294132885, "train_loss": 0.6281065603806859, "epoch": 197}
[08:10:40.932789] [08:10:40.932868] Training epoch 197 for 0:00:27
[08:10:40.932918] [08:10:40.937321] log_dir: ./exp/debug/cifar100-LT/debug
[08:10:42.527598] Epoch: [198]  [ 0/42]  eta: 0:01:06  lr: 0.002153  loss: 0.6389 (0.6389)  time: 1.5894  data: 1.0927  max mem: 9341
[08:10:52.693168] Epoch: [198]  [20/42]  eta: 0:00:12  lr: 0.002152  loss: 0.6400 (0.6429)  time: 0.5082  data: 0.0001  max mem: 9341
[08:11:02.848300] Epoch: [198]  [40/42]  eta: 0:00:01  lr: 0.002151  loss: 0.6231 (0.6335)  time: 0.5077  data: 0.0001  max mem: 9341
[08:11:03.355916] Epoch: [198]  [41/42]  eta: 0:00:00  lr: 0.002151  loss: 0.6231 (0.6328)  time: 0.5078  data: 0.0001  max mem: 9341
[08:11:03.516409] Epoch: [198] Total time: 0:00:22 (0.5376 s / it)
[08:11:03.523291] Averaged stats: lr: 0.002151  loss: 0.6231 (0.6379)
[08:11:08.160296] {"train_lr": 0.002152043843592421, "train_loss": 0.6378558991210801, "epoch": 198}
[08:11:08.160644] [08:11:08.160734] Training epoch 198 for 0:00:27
[08:11:08.160806] [08:11:08.165267] log_dir: ./exp/debug/cifar100-LT/debug
[08:11:09.738591] Epoch: [199]  [ 0/42]  eta: 0:01:06  lr: 0.002150  loss: 0.6099 (0.6099)  time: 1.5724  data: 1.0565  max mem: 9341
[08:11:19.895114] Epoch: [199]  [20/42]  eta: 0:00:12  lr: 0.002149  loss: 0.6105 (0.6204)  time: 0.5078  data: 0.0001  max mem: 9341
[08:11:30.039222] Epoch: [199]  [40/42]  eta: 0:00:01  lr: 0.002148  loss: 0.6449 (0.6300)  time: 0.5072  data: 0.0001  max mem: 9341
[08:11:30.545958] Epoch: [199]  [41/42]  eta: 0:00:00  lr: 0.002148  loss: 0.6498 (0.6310)  time: 0.5072  data: 0.0001  max mem: 9341
[08:11:30.719841] Epoch: [199] Total time: 0:00:22 (0.5370 s / it)
[08:11:30.725887] Averaged stats: lr: 0.002148  loss: 0.6498 (0.6303)
[08:11:35.241788] {"train_lr": 0.0021490161252494485, "train_loss": 0.6303071890558515, "epoch": 199}
[08:11:35.242189] [08:11:35.242276] Training epoch 199 for 0:00:27
[08:11:35.242328] [08:11:35.247256] log_dir: ./exp/debug/cifar100-LT/debug
[08:11:36.871166] Epoch: [200]  [ 0/42]  eta: 0:01:08  lr: 0.002147  loss: 0.6451 (0.6451)  time: 1.6231  data: 1.1081  max mem: 9341
[08:11:47.088175] Epoch: [200]  [20/42]  eta: 0:00:12  lr: 0.002146  loss: 0.6202 (0.6296)  time: 0.5108  data: 0.0001  max mem: 9341
[08:11:57.239263] Epoch: [200]  [40/42]  eta: 0:00:01  lr: 0.002145  loss: 0.6308 (0.6310)  time: 0.5075  data: 0.0001  max mem: 9341
[08:11:57.745296] Epoch: [200]  [41/42]  eta: 0:00:00  lr: 0.002145  loss: 0.6383 (0.6319)  time: 0.5075  data: 0.0001  max mem: 9341
[08:11:57.900236] Epoch: [200] Total time: 0:00:22 (0.5394 s / it)
[08:11:57.912236] Averaged stats: lr: 0.002145  loss: 0.6383 (0.6332)
[08:12:02.429766] {"train_lr": 0.002145972190839321, "train_loss": 0.6331843680569104, "epoch": 200}
[08:12:02.430105] [08:12:02.430186] Training epoch 200 for 0:00:27
[08:12:02.430237] [08:12:02.434568] log_dir: ./exp/debug/cifar100-LT/debug
[08:12:04.025265] Epoch: [201]  [ 0/42]  eta: 0:01:06  lr: 0.002144  loss: 0.6723 (0.6723)  time: 1.5896  data: 1.0740  max mem: 9341
[08:12:14.192103] Epoch: [201]  [20/42]  eta: 0:00:12  lr: 0.002143  loss: 0.6303 (0.6354)  time: 0.5083  data: 0.0002  max mem: 9341
[08:12:24.337539] Epoch: [201]  [40/42]  eta: 0:00:01  lr: 0.002142  loss: 0.6365 (0.6345)  time: 0.5072  data: 0.0001  max mem: 9341
[08:12:24.842924] Epoch: [201]  [41/42]  eta: 0:00:00  lr: 0.002142  loss: 0.6360 (0.6338)  time: 0.5072  data: 0.0001  max mem: 9341
[08:12:25.015045] Epoch: [201] Total time: 0:00:22 (0.5376 s / it)
[08:12:25.021681] Averaged stats: lr: 0.002142  loss: 0.6360 (0.6328)
[08:12:29.655847] {"train_lr": 0.0021429120923744755, "train_loss": 0.6327966047184808, "epoch": 201}
[08:12:29.656192] [08:12:29.656278] Training epoch 201 for 0:00:27
[08:12:29.656333] [08:12:29.660808] log_dir: ./exp/debug/cifar100-LT/debug
[08:12:31.331801] Epoch: [202]  [ 0/42]  eta: 0:01:10  lr: 0.002141  loss: 0.6281 (0.6281)  time: 1.6690  data: 1.1625  max mem: 9341
[08:12:41.494619] Epoch: [202]  [20/42]  eta: 0:00:12  lr: 0.002140  loss: 0.6341 (0.6324)  time: 0.5081  data: 0.0001  max mem: 9341
[08:12:51.644272] Epoch: [202]  [40/42]  eta: 0:00:01  lr: 0.002138  loss: 0.6161 (0.6292)  time: 0.5074  data: 0.0001  max mem: 9341
[08:12:52.149758] Epoch: [202]  [41/42]  eta: 0:00:00  lr: 0.002138  loss: 0.6161 (0.6303)  time: 0.5074  data: 0.0001  max mem: 9341
[08:12:52.338105] Epoch: [202] Total time: 0:00:22 (0.5399 s / it)
[08:12:52.339304] Averaged stats: lr: 0.002138  loss: 0.6161 (0.6266)
[08:12:56.853883] {"train_lr": 0.002139835882143557, "train_loss": 0.6266306447131293, "epoch": 202}
[08:12:56.854130] [08:12:56.854213] Training epoch 202 for 0:00:27
[08:12:56.854263] [08:12:56.858743] log_dir: ./exp/debug/cifar100-LT/debug
[08:12:58.353425] Epoch: [203]  [ 0/42]  eta: 0:01:02  lr: 0.002138  loss: 0.6052 (0.6052)  time: 1.4937  data: 0.9881  max mem: 9341
[08:13:08.518227] Epoch: [203]  [20/42]  eta: 0:00:12  lr: 0.002137  loss: 0.6329 (0.6345)  time: 0.5082  data: 0.0001  max mem: 9341
[08:13:18.675243] Epoch: [203]  [40/42]  eta: 0:00:01  lr: 0.002135  loss: 0.6355 (0.6366)  time: 0.5078  data: 0.0001  max mem: 9341
[08:13:19.180260] Epoch: [203]  [41/42]  eta: 0:00:00  lr: 0.002135  loss: 0.6374 (0.6367)  time: 0.5078  data: 0.0001  max mem: 9341
[08:13:19.351414] Epoch: [203] Total time: 0:00:22 (0.5355 s / it)
[08:13:19.360431] Averaged stats: lr: 0.002135  loss: 0.6374 (0.6357)
[08:13:23.913947] {"train_lr": 0.0021367436127105095, "train_loss": 0.6357277284065882, "epoch": 203}
[08:13:23.914289] [08:13:23.914372] Training epoch 203 for 0:00:27
[08:13:23.914425] [08:13:23.918721] log_dir: ./exp/debug/cifar100-LT/debug
[08:13:25.516158] Epoch: [204]  [ 0/42]  eta: 0:01:07  lr: 0.002135  loss: 0.6802 (0.6802)  time: 1.5967  data: 1.0734  max mem: 9341
[08:13:35.680391] Epoch: [204]  [20/42]  eta: 0:00:12  lr: 0.002133  loss: 0.6307 (0.6381)  time: 0.5082  data: 0.0001  max mem: 9341
[08:13:45.823263] Epoch: [204]  [40/42]  eta: 0:00:01  lr: 0.002132  loss: 0.6141 (0.6286)  time: 0.5071  data: 0.0001  max mem: 9341
[08:13:46.328309] Epoch: [204]  [41/42]  eta: 0:00:00  lr: 0.002132  loss: 0.6164 (0.6284)  time: 0.5071  data: 0.0001  max mem: 9341
[08:13:46.510837] Epoch: [204] Total time: 0:00:22 (0.5379 s / it)
[08:13:46.515193] Averaged stats: lr: 0.002132  loss: 0.6164 (0.6278)
[08:13:51.049393] {"train_lr": 0.00213363533691368, "train_loss": 0.6277766362542198, "epoch": 204}
[08:13:51.049643] [08:13:51.049724] Training epoch 204 for 0:00:27
[08:13:51.049776] [08:13:51.054335] log_dir: ./exp/debug/cifar100-LT/debug
[08:13:52.533291] Epoch: [205]  [ 0/42]  eta: 0:01:02  lr: 0.002132  loss: 0.5866 (0.5866)  time: 1.4776  data: 0.9688  max mem: 9341
[08:14:02.694716] Epoch: [205]  [20/42]  eta: 0:00:12  lr: 0.002130  loss: 0.6289 (0.6286)  time: 0.5080  data: 0.0001  max mem: 9341
[08:14:12.847192] Epoch: [205]  [40/42]  eta: 0:00:01  lr: 0.002129  loss: 0.6142 (0.6237)  time: 0.5076  data: 0.0001  max mem: 9341
[08:14:13.354300] Epoch: [205]  [41/42]  eta: 0:00:00  lr: 0.002129  loss: 0.6175 (0.6244)  time: 0.5077  data: 0.0001  max mem: 9341
[08:14:13.523028] Epoch: [205] Total time: 0:00:22 (0.5350 s / it)
[08:14:13.531114] Averaged stats: lr: 0.002129  loss: 0.6175 (0.6250)
[08:14:18.006386] {"train_lr": 0.0021305111078649313, "train_loss": 0.6249899764855703, "epoch": 205}
[08:14:18.006759] [08:14:18.006843] Training epoch 205 for 0:00:26
[08:14:18.006896] [08:14:18.011259] log_dir: ./exp/debug/cifar100-LT/debug
[08:14:19.642192] Epoch: [206]  [ 0/42]  eta: 0:01:08  lr: 0.002128  loss: 0.5813 (0.5813)  time: 1.6299  data: 1.1365  max mem: 9341
[08:14:29.793426] Epoch: [206]  [20/42]  eta: 0:00:12  lr: 0.002127  loss: 0.6037 (0.6078)  time: 0.5075  data: 0.0001  max mem: 9341
[08:14:39.937833] Epoch: [206]  [40/42]  eta: 0:00:01  lr: 0.002126  loss: 0.6117 (0.6131)  time: 0.5072  data: 0.0001  max mem: 9341
[08:14:40.443885] Epoch: [206]  [41/42]  eta: 0:00:00  lr: 0.002126  loss: 0.6092 (0.6127)  time: 0.5072  data: 0.0001  max mem: 9341
[08:14:40.627692] Epoch: [206] Total time: 0:00:22 (0.5385 s / it)
[08:14:40.628612] Averaged stats: lr: 0.002126  loss: 0.6092 (0.6187)
[08:14:45.218617] {"train_lr": 0.002127370978948719, "train_loss": 0.6186891428771473, "epoch": 206}
[08:14:45.218899] [08:14:45.218980] Training epoch 206 for 0:00:27
[08:14:45.219096] [08:14:45.223596] log_dir: ./exp/debug/cifar100-LT/debug
[08:14:46.944821] Epoch: [207]  [ 0/42]  eta: 0:01:12  lr: 0.002125  loss: 0.6127 (0.6127)  time: 1.7204  data: 1.2232  max mem: 9341
[08:14:57.113555] Epoch: [207]  [20/42]  eta: 0:00:12  lr: 0.002124  loss: 0.6038 (0.6119)  time: 0.5084  data: 0.0001  max mem: 9341
[08:15:07.257513] Epoch: [207]  [40/42]  eta: 0:00:01  lr: 0.002123  loss: 0.6079 (0.6124)  time: 0.5072  data: 0.0001  max mem: 9341
[08:15:07.763406] Epoch: [207]  [41/42]  eta: 0:00:00  lr: 0.002123  loss: 0.6079 (0.6119)  time: 0.5072  data: 0.0001  max mem: 9341
[08:15:07.936423] Epoch: [207] Total time: 0:00:22 (0.5408 s / it)
[08:15:07.938438] Averaged stats: lr: 0.002123  loss: 0.6079 (0.6184)
[08:15:12.558054] {"train_lr": 0.002124215003821178, "train_loss": 0.6184147000312805, "epoch": 207}
[08:15:12.558307] [08:15:12.558387] Training epoch 207 for 0:00:27
[08:15:12.558437] [08:15:12.562756] log_dir: ./exp/debug/cifar100-LT/debug
[08:15:14.185799] Epoch: [208]  [ 0/42]  eta: 0:01:08  lr: 0.002122  loss: 0.6113 (0.6113)  time: 1.6222  data: 1.1050  max mem: 9341
[08:15:24.432120] Epoch: [208]  [20/42]  eta: 0:00:12  lr: 0.002121  loss: 0.6073 (0.6170)  time: 0.5123  data: 0.0001  max mem: 9341
[08:15:34.582083] Epoch: [208]  [40/42]  eta: 0:00:01  lr: 0.002120  loss: 0.6170 (0.6193)  time: 0.5075  data: 0.0001  max mem: 9341
[08:15:35.089398] Epoch: [208]  [41/42]  eta: 0:00:00  lr: 0.002120  loss: 0.6161 (0.6190)  time: 0.5076  data: 0.0001  max mem: 9341
[08:15:35.262254] Epoch: [208] Total time: 0:00:22 (0.5405 s / it)
[08:15:35.264048] Averaged stats: lr: 0.002120  loss: 0.6161 (0.6199)
[08:15:39.912620] {"train_lr": 0.0021210432364092207, "train_loss": 0.6199010113875071, "epoch": 208}
[08:15:39.912987] [08:15:39.913067] Training epoch 208 for 0:00:27
[08:15:39.913118] [08:15:39.917460] log_dir: ./exp/debug/cifar100-LT/debug
[08:15:41.384916] Epoch: [209]  [ 0/42]  eta: 0:01:01  lr: 0.002119  loss: 0.7233 (0.7233)  time: 1.4654  data: 0.9601  max mem: 9341
[08:15:51.559350] Epoch: [209]  [20/42]  eta: 0:00:12  lr: 0.002118  loss: 0.6151 (0.6201)  time: 0.5087  data: 0.0002  max mem: 9341
[08:16:01.712660] Epoch: [209]  [40/42]  eta: 0:00:01  lr: 0.002116  loss: 0.6187 (0.6211)  time: 0.5076  data: 0.0001  max mem: 9341
[08:16:02.219267] Epoch: [209]  [41/42]  eta: 0:00:00  lr: 0.002116  loss: 0.6180 (0.6211)  time: 0.5077  data: 0.0001  max mem: 9341
[08:16:02.386673] Epoch: [209] Total time: 0:00:22 (0.5350 s / it)
[08:16:02.396190] Averaged stats: lr: 0.002116  loss: 0.6180 (0.6171)
[08:16:06.948898] {"train_lr": 0.002117855730909595, "train_loss": 0.61708324189697, "epoch": 209}
[08:16:06.949277] [08:16:06.949360] Training epoch 209 for 0:00:27
[08:16:06.949414] [08:16:06.953841] log_dir: ./exp/debug/cifar100-LT/debug
[08:16:08.553911] Epoch: [210]  [ 0/42]  eta: 0:01:07  lr: 0.002116  loss: 0.5998 (0.5998)  time: 1.5988  data: 1.0899  max mem: 9341
[08:16:18.728937] Epoch: [210]  [20/42]  eta: 0:00:12  lr: 0.002114  loss: 0.6126 (0.6151)  time: 0.5087  data: 0.0001  max mem: 9341
[08:16:28.889626] Epoch: [210]  [40/42]  eta: 0:00:01  lr: 0.002113  loss: 0.6220 (0.6166)  time: 0.5080  data: 0.0001  max mem: 9341
[08:16:29.395246] Epoch: [210]  [41/42]  eta: 0:00:00  lr: 0.002113  loss: 0.6220 (0.6170)  time: 0.5080  data: 0.0001  max mem: 9341
[08:16:29.559529] Epoch: [210] Total time: 0:00:22 (0.5382 s / it)
[08:16:29.565293] Averaged stats: lr: 0.002113  loss: 0.6220 (0.6210)
[08:16:34.061190] {"train_lr": 0.0021146525417879776, "train_loss": 0.6209517429981913, "epoch": 210}
[08:16:34.061536] [08:16:34.061620] Training epoch 210 for 0:00:27
[08:16:34.061672] [08:16:34.066019] log_dir: ./exp/debug/cifar100-LT/debug
[08:16:35.762188] Epoch: [211]  [ 0/42]  eta: 0:01:11  lr: 0.002112  loss: 0.6151 (0.6151)  time: 1.6953  data: 1.1804  max mem: 9341
[08:16:45.923339] Epoch: [211]  [20/42]  eta: 0:00:12  lr: 0.002111  loss: 0.6116 (0.6152)  time: 0.5080  data: 0.0001  max mem: 9341
[08:16:56.062769] Epoch: [211]  [40/42]  eta: 0:00:01  lr: 0.002110  loss: 0.6171 (0.6120)  time: 0.5069  data: 0.0001  max mem: 9341
[08:16:56.568182] Epoch: [211]  [41/42]  eta: 0:00:00  lr: 0.002110  loss: 0.6108 (0.6109)  time: 0.5069  data: 0.0001  max mem: 9341
[08:16:56.738320] Epoch: [211] Total time: 0:00:22 (0.5398 s / it)
[08:16:56.741966] Averaged stats: lr: 0.002110  loss: 0.6108 (0.6191)
[08:17:01.248514] {"train_lr": 0.002111433723778035, "train_loss": 0.6190744315584501, "epoch": 211}
[08:17:01.248946] [08:17:01.249038] Training epoch 211 for 0:00:27
[08:17:01.249091] [08:17:01.253675] log_dir: ./exp/debug/cifar100-LT/debug
[08:17:02.846889] Epoch: [212]  [ 0/42]  eta: 0:01:06  lr: 0.002109  loss: 0.5882 (0.5882)  time: 1.5920  data: 1.0967  max mem: 9341
[08:17:13.033440] Epoch: [212]  [20/42]  eta: 0:00:12  lr: 0.002108  loss: 0.6025 (0.6069)  time: 0.5093  data: 0.0001  max mem: 9341
[08:17:23.205019] Epoch: [212]  [40/42]  eta: 0:00:01  lr: 0.002107  loss: 0.6093 (0.6095)  time: 0.5085  data: 0.0001  max mem: 9341
[08:17:23.710526] Epoch: [212]  [41/42]  eta: 0:00:00  lr: 0.002107  loss: 0.6042 (0.6086)  time: 0.5085  data: 0.0001  max mem: 9341
[08:17:23.882696] Epoch: [212] Total time: 0:00:22 (0.5388 s / it)
[08:17:23.887421] Averaged stats: lr: 0.002107  loss: 0.6042 (0.6093)
[08:17:28.538888] {"train_lr": 0.0021081993318804843, "train_loss": 0.609342699604375, "epoch": 212}
[08:17:28.539158] [08:17:28.539242] Training epoch 212 for 0:00:27
[08:17:28.539293] [08:17:28.543631] log_dir: ./exp/debug/cifar100-LT/debug
[08:17:30.213014] Epoch: [213]  [ 0/42]  eta: 0:01:10  lr: 0.002106  loss: 0.5772 (0.5772)  time: 1.6685  data: 1.1613  max mem: 9341
[08:17:40.359513] Epoch: [213]  [20/42]  eta: 0:00:12  lr: 0.002105  loss: 0.6085 (0.6028)  time: 0.5073  data: 0.0001  max mem: 9341
[08:17:50.489874] Epoch: [213]  [40/42]  eta: 0:00:01  lr: 0.002104  loss: 0.6188 (0.6104)  time: 0.5065  data: 0.0001  max mem: 9341
[08:17:50.996680] Epoch: [213]  [41/42]  eta: 0:00:00  lr: 0.002104  loss: 0.6188 (0.6110)  time: 0.5065  data: 0.0001  max mem: 9341
[08:17:51.151793] Epoch: [213] Total time: 0:00:22 (0.5383 s / it)
[08:17:51.164057] Averaged stats: lr: 0.002104  loss: 0.6188 (0.6113)
[08:17:55.749179] {"train_lr": 0.002104949421362156, "train_loss": 0.6113199839989344, "epoch": 213}
[08:17:55.749577] [08:17:55.749673] Training epoch 213 for 0:00:27
[08:17:55.749724] [08:17:55.754121] log_dir: ./exp/debug/cifar100-LT/debug
[08:17:57.405531] Epoch: [214]  [ 0/42]  eta: 0:01:09  lr: 0.002103  loss: 0.5925 (0.5925)  time: 1.6506  data: 1.1354  max mem: 9341
[08:18:07.560577] Epoch: [214]  [20/42]  eta: 0:00:12  lr: 0.002102  loss: 0.6101 (0.6077)  time: 0.5077  data: 0.0001  max mem: 9341
[08:18:17.696338] Epoch: [214]  [40/42]  eta: 0:00:01  lr: 0.002100  loss: 0.6108 (0.6105)  time: 0.5067  data: 0.0001  max mem: 9341
[08:18:18.201726] Epoch: [214]  [41/42]  eta: 0:00:00  lr: 0.002100  loss: 0.6108 (0.6102)  time: 0.5067  data: 0.0001  max mem: 9341
[08:18:18.361512] Epoch: [214] Total time: 0:00:22 (0.5383 s / it)
[08:18:18.369800] Averaged stats: lr: 0.002100  loss: 0.6108 (0.6082)
[08:18:22.931482] {"train_lr": 0.0021016840477550624, "train_loss": 0.6082454777899242, "epoch": 214}
[08:18:22.931871] [08:18:22.931953] Training epoch 214 for 0:00:27
[08:18:22.932004] [08:18:22.936547] log_dir: ./exp/debug/cifar100-LT/debug
[08:18:24.588556] Epoch: [215]  [ 0/42]  eta: 0:01:09  lr: 0.002099  loss: 0.5730 (0.5730)  time: 1.6508  data: 1.1515  max mem: 9341
[08:18:34.756636] Epoch: [215]  [20/42]  eta: 0:00:12  lr: 0.002098  loss: 0.6041 (0.6040)  time: 0.5084  data: 0.0001  max mem: 9341
[08:18:44.913466] Epoch: [215]  [40/42]  eta: 0:00:01  lr: 0.002097  loss: 0.6006 (0.6033)  time: 0.5078  data: 0.0001  max mem: 9341
[08:18:45.419027] Epoch: [215]  [41/42]  eta: 0:00:00  lr: 0.002097  loss: 0.6026 (0.6039)  time: 0.5078  data: 0.0001  max mem: 9341
[08:18:45.585672] Epoch: [215] Total time: 0:00:22 (0.5393 s / it)
[08:18:45.596478] Averaged stats: lr: 0.002097  loss: 0.6026 (0.6047)
[08:18:50.135270] {"train_lr": 0.0020984032668554214, "train_loss": 0.6046547194321951, "epoch": 215}
[08:18:50.135624] [08:18:50.135705] Training epoch 215 for 0:00:27
[08:18:50.135756] [08:18:50.140124] log_dir: ./exp/debug/cifar100-LT/debug
[08:18:51.596268] Epoch: [216]  [ 0/42]  eta: 0:01:01  lr: 0.002096  loss: 0.6149 (0.6149)  time: 1.4551  data: 0.9394  max mem: 9341
[08:19:01.760091] Epoch: [216]  [20/42]  eta: 0:00:12  lr: 0.002095  loss: 0.6022 (0.5998)  time: 0.5081  data: 0.0001  max mem: 9341
[08:19:11.894845] Epoch: [216]  [40/42]  eta: 0:00:01  lr: 0.002094  loss: 0.6076 (0.6055)  time: 0.5067  data: 0.0001  max mem: 9341
[08:19:12.399730] Epoch: [216]  [41/42]  eta: 0:00:00  lr: 0.002094  loss: 0.6073 (0.6052)  time: 0.5067  data: 0.0001  max mem: 9341
[08:19:12.550524] Epoch: [216] Total time: 0:00:22 (0.5336 s / it)
[08:19:12.564645] Averaged stats: lr: 0.002094  loss: 0.6073 (0.6077)
[08:19:17.068387] {"train_lr": 0.0020951071347227266, "train_loss": 0.6076618954539299, "epoch": 216}
[08:19:17.068808] [08:19:17.068909] Training epoch 216 for 0:00:26
[08:19:17.068963] [08:19:17.073369] log_dir: ./exp/debug/cifar100-LT/debug
[08:19:18.491874] Epoch: [217]  [ 0/42]  eta: 0:00:59  lr: 0.002093  loss: 0.5953 (0.5953)  time: 1.4172  data: 0.9172  max mem: 9341
[08:19:28.651818] Epoch: [217]  [20/42]  eta: 0:00:12  lr: 0.002092  loss: 0.6049 (0.6063)  time: 0.5079  data: 0.0002  max mem: 9341
[08:19:38.786593] Epoch: [217]  [40/42]  eta: 0:00:01  lr: 0.002090  loss: 0.6036 (0.6057)  time: 0.5067  data: 0.0001  max mem: 9341
[08:19:39.292190] Epoch: [217]  [41/42]  eta: 0:00:00  lr: 0.002090  loss: 0.6088 (0.6065)  time: 0.5067  data: 0.0001  max mem: 9341
[08:19:39.459794] Epoch: [217] Total time: 0:00:22 (0.5330 s / it)
[08:19:39.468865] Averaged stats: lr: 0.002090  loss: 0.6088 (0.6064)
[08:19:44.020225] {"train_lr": 0.002091795707678785, "train_loss": 0.606423689850739, "epoch": 217}
[08:19:44.020614] [08:19:44.020695] Training epoch 217 for 0:00:26
[08:19:44.020746] [08:19:44.025196] log_dir: ./exp/debug/cifar100-LT/debug
[08:19:45.626347] Epoch: [218]  [ 0/42]  eta: 0:01:07  lr: 0.002090  loss: 0.6135 (0.6135)  time: 1.5999  data: 1.1028  max mem: 9341
[08:19:55.779063] Epoch: [218]  [20/42]  eta: 0:00:12  lr: 0.002088  loss: 0.6117 (0.6116)  time: 0.5076  data: 0.0001  max mem: 9341
[08:20:05.910920] Epoch: [218]  [40/42]  eta: 0:00:01  lr: 0.002087  loss: 0.6005 (0.6072)  time: 0.5065  data: 0.0001  max mem: 9341
[08:20:06.415292] Epoch: [218]  [41/42]  eta: 0:00:00  lr: 0.002087  loss: 0.6005 (0.6070)  time: 0.5065  data: 0.0001  max mem: 9341
[08:20:06.599505] Epoch: [218] Total time: 0:00:22 (0.5375 s / it)
[08:20:06.602793] Averaged stats: lr: 0.002087  loss: 0.6005 (0.6054)
[08:20:11.094379] {"train_lr": 0.0020884690423067457, "train_loss": 0.605354497830073, "epoch": 218}
[08:20:11.094714] [08:20:11.094799] Training epoch 218 for 0:00:27
[08:20:11.094911] [08:20:11.099398] log_dir: ./exp/debug/cifar100-LT/debug
[08:20:12.788941] Epoch: [219]  [ 0/42]  eta: 0:01:10  lr: 0.002086  loss: 0.5862 (0.5862)  time: 1.6884  data: 1.1785  max mem: 9341
[08:20:22.947859] Epoch: [219]  [20/42]  eta: 0:00:12  lr: 0.002085  loss: 0.6066 (0.6018)  time: 0.5079  data: 0.0001  max mem: 9341
[08:20:33.081909] Epoch: [219]  [40/42]  eta: 0:00:01  lr: 0.002084  loss: 0.6041 (0.6051)  time: 0.5066  data: 0.0001  max mem: 9341
[08:20:33.587109] Epoch: [219]  [41/42]  eta: 0:00:00  lr: 0.002084  loss: 0.6045 (0.6050)  time: 0.5067  data: 0.0001  max mem: 9341
[08:20:33.752833] Epoch: [219] Total time: 0:00:22 (0.5394 s / it)
[08:20:33.753674] Averaged stats: lr: 0.002084  loss: 0.6045 (0.6035)
[08:20:38.351997] {"train_lr": 0.0020851271954501474, "train_loss": 0.6035339058864684, "epoch": 219}
[08:20:38.352360] [08:20:38.352468] Training epoch 219 for 0:00:27
[08:20:38.352578] [08:20:38.357144] log_dir: ./exp/debug/cifar100-LT/debug
[08:20:39.900945] Epoch: [220]  [ 0/42]  eta: 0:01:04  lr: 0.002083  loss: 0.6560 (0.6560)  time: 1.5429  data: 1.0360  max mem: 9341
[08:20:50.057514] Epoch: [220]  [20/42]  eta: 0:00:12  lr: 0.002082  loss: 0.5971 (0.6023)  time: 0.5078  data: 0.0001  max mem: 9341
[08:21:00.196084] Epoch: [220]  [40/42]  eta: 0:00:01  lr: 0.002080  loss: 0.6060 (0.6062)  time: 0.5069  data: 0.0001  max mem: 9341
[08:21:00.700209] Epoch: [220]  [41/42]  eta: 0:00:00  lr: 0.002080  loss: 0.6063 (0.6074)  time: 0.5067  data: 0.0001  max mem: 9341
[08:21:00.867529] Epoch: [220] Total time: 0:00:22 (0.5360 s / it)
[08:21:00.875684] Averaged stats: lr: 0.002080  loss: 0.6063 (0.6067)
[08:21:05.465998] {"train_lr": 0.0020817702242119217, "train_loss": 0.6066699006727764, "epoch": 220}
[08:21:05.466382] [08:21:05.466464] Training epoch 220 for 0:00:27
[08:21:05.466516] [08:21:05.471034] log_dir: ./exp/debug/cifar100-LT/debug
[08:21:06.919205] Epoch: [221]  [ 0/42]  eta: 0:01:00  lr: 0.002080  loss: 0.5803 (0.5803)  time: 1.4458  data: 0.9357  max mem: 9341
[08:21:17.099730] Epoch: [221]  [20/42]  eta: 0:00:12  lr: 0.002078  loss: 0.6157 (0.6200)  time: 0.5090  data: 0.0002  max mem: 9341
[08:21:27.273088] Epoch: [221]  [40/42]  eta: 0:00:01  lr: 0.002077  loss: 0.6089 (0.6151)  time: 0.5086  data: 0.0001  max mem: 9341
[08:21:27.779390] Epoch: [221]  [41/42]  eta: 0:00:00  lr: 0.002077  loss: 0.6089 (0.6146)  time: 0.5086  data: 0.0001  max mem: 9341
[08:21:27.951205] Epoch: [221] Total time: 0:00:22 (0.5352 s / it)
[08:21:27.951928] Averaged stats: lr: 0.002077  loss: 0.6089 (0.6130)
[08:21:32.511262] {"train_lr": 0.0020783981859534583, "train_loss": 0.6129643285558337, "epoch": 221}
[08:21:32.511590] [08:21:32.511669] Training epoch 221 for 0:00:27
[08:21:32.511720] [08:21:32.516019] log_dir: ./exp/debug/cifar100-LT/debug
[08:21:34.166596] Epoch: [222]  [ 0/42]  eta: 0:01:09  lr: 0.002076  loss: 0.6097 (0.6097)  time: 1.6498  data: 1.1390  max mem: 9341
[08:21:44.334420] Epoch: [222]  [20/42]  eta: 0:00:12  lr: 0.002075  loss: 0.6135 (0.6115)  time: 0.5083  data: 0.0001  max mem: 9341
[08:21:54.485955] Epoch: [222]  [40/42]  eta: 0:00:01  lr: 0.002074  loss: 0.6007 (0.6082)  time: 0.5075  data: 0.0001  max mem: 9341
[08:21:54.992034] Epoch: [222]  [41/42]  eta: 0:00:00  lr: 0.002074  loss: 0.6028 (0.6088)  time: 0.5076  data: 0.0001  max mem: 9341
[08:21:55.158486] Epoch: [222] Total time: 0:00:22 (0.5391 s / it)
[08:21:55.165599] Averaged stats: lr: 0.002074  loss: 0.6028 (0.6105)
[08:21:59.678625] {"train_lr": 0.0020750111382935752, "train_loss": 0.610493836303552, "epoch": 222}
[08:21:59.678977] [08:21:59.679064] Training epoch 222 for 0:00:27
[08:21:59.679117] [08:21:59.683476] log_dir: ./exp/debug/cifar100-LT/debug
[08:22:01.242423] Epoch: [223]  [ 0/42]  eta: 0:01:05  lr: 0.002073  loss: 0.6095 (0.6095)  time: 1.5581  data: 1.0441  max mem: 9341
[08:22:11.424420] Epoch: [223]  [20/42]  eta: 0:00:12  lr: 0.002071  loss: 0.6160 (0.6149)  time: 0.5090  data: 0.0001  max mem: 9341
[08:22:21.595388] Epoch: [223]  [40/42]  eta: 0:00:01  lr: 0.002070  loss: 0.5930 (0.6091)  time: 0.5085  data: 0.0001  max mem: 9341
[08:22:22.100264] Epoch: [223]  [41/42]  eta: 0:00:00  lr: 0.002070  loss: 0.5930 (0.6098)  time: 0.5084  data: 0.0001  max mem: 9341
[08:22:22.269682] Epoch: [223] Total time: 0:00:22 (0.5378 s / it)
[08:22:22.270461] Averaged stats: lr: 0.002070  loss: 0.5930 (0.6083)
[08:22:26.882509] {"train_lr": 0.002071609139107577, "train_loss": 0.6083058218161265, "epoch": 223}
[08:22:26.882838] [08:22:26.882919] Training epoch 223 for 0:00:27
[08:22:26.882994] [08:22:26.887279] log_dir: ./exp/debug/cifar100-LT/debug
[08:22:28.504020] Epoch: [224]  [ 0/42]  eta: 0:01:07  lr: 0.002069  loss: 0.6529 (0.6529)  time: 1.6157  data: 1.1015  max mem: 9341
[08:22:38.655690] Epoch: [224]  [20/42]  eta: 0:00:12  lr: 0.002068  loss: 0.6014 (0.6028)  time: 0.5075  data: 0.0001  max mem: 9341
[08:22:48.793288] Epoch: [224]  [40/42]  eta: 0:00:01  lr: 0.002067  loss: 0.5926 (0.6002)  time: 0.5068  data: 0.0001  max mem: 9341
[08:22:49.298739] Epoch: [224]  [41/42]  eta: 0:00:00  lr: 0.002067  loss: 0.5926 (0.6004)  time: 0.5069  data: 0.0001  max mem: 9341
[08:22:49.466078] Epoch: [224] Total time: 0:00:22 (0.5376 s / it)
[08:22:49.489006] Averaged stats: lr: 0.002067  loss: 0.5926 (0.6026)
[08:22:54.232367] {"train_lr": 0.0020681922465262416, "train_loss": 0.6025844507274174, "epoch": 224}
[08:22:54.232819] [08:22:54.232910] Training epoch 224 for 0:00:27
[08:22:54.232962] [08:22:54.238026] log_dir: ./exp/debug/cifar100-LT/debug
[08:22:55.732035] Epoch: [225]  [ 0/42]  eta: 0:01:02  lr: 0.002066  loss: 0.5709 (0.5709)  time: 1.4927  data: 0.9799  max mem: 9341
[08:23:05.894986] Epoch: [225]  [20/42]  eta: 0:00:12  lr: 0.002065  loss: 0.5806 (0.5922)  time: 0.5081  data: 0.0001  max mem: 9341
[08:23:16.046094] Epoch: [225]  [40/42]  eta: 0:00:01  lr: 0.002063  loss: 0.6058 (0.5983)  time: 0.5075  data: 0.0001  max mem: 9341
[08:23:16.551173] Epoch: [225]  [41/42]  eta: 0:00:00  lr: 0.002063  loss: 0.6031 (0.5984)  time: 0.5075  data: 0.0001  max mem: 9341
[08:23:16.735226] Epoch: [225] Total time: 0:00:22 (0.5356 s / it)
[08:23:16.738430] Averaged stats: lr: 0.002063  loss: 0.6031 (0.5976)
[08:23:21.318866] {"train_lr": 0.0020647605189348405, "train_loss": 0.5976216094124884, "epoch": 225}
[08:23:21.319234] [08:23:21.319319] Training epoch 225 for 0:00:27
[08:23:21.319387] [08:23:21.323884] log_dir: ./exp/debug/cifar100-LT/debug
[08:23:22.783052] Epoch: [226]  [ 0/42]  eta: 0:01:01  lr: 0.002062  loss: 0.5646 (0.5646)  time: 1.4576  data: 0.9451  max mem: 9341
[08:23:32.967934] Epoch: [226]  [20/42]  eta: 0:00:12  lr: 0.002061  loss: 0.5887 (0.5928)  time: 0.5092  data: 0.0001  max mem: 9341
[08:23:43.142771] Epoch: [226]  [40/42]  eta: 0:00:01  lr: 0.002060  loss: 0.6019 (0.5976)  time: 0.5087  data: 0.0001  max mem: 9341
[08:23:43.648547] Epoch: [226]  [41/42]  eta: 0:00:00  lr: 0.002060  loss: 0.5973 (0.5975)  time: 0.5087  data: 0.0001  max mem: 9341
[08:23:43.816756] Epoch: [226] Total time: 0:00:22 (0.5355 s / it)
[08:23:43.818630] Averaged stats: lr: 0.002060  loss: 0.5973 (0.5979)
[08:23:48.313914] {"train_lr": 0.0020613140149721245, "train_loss": 0.5978629656490826, "epoch": 226}
[08:23:48.314265] [08:23:48.314348] Training epoch 226 for 0:00:26
[08:23:48.314399] [08:23:48.318752] log_dir: ./exp/debug/cifar100-LT/debug
[08:23:49.982616] Epoch: [227]  [ 0/42]  eta: 0:01:09  lr: 0.002059  loss: 0.6188 (0.6188)  time: 1.6628  data: 1.1524  max mem: 9341
[08:24:00.147405] Epoch: [227]  [20/42]  eta: 0:00:12  lr: 0.002058  loss: 0.5901 (0.5917)  time: 0.5082  data: 0.0001  max mem: 9341
[08:24:10.294632] Epoch: [227]  [40/42]  eta: 0:00:01  lr: 0.002056  loss: 0.5936 (0.5935)  time: 0.5073  data: 0.0001  max mem: 9341
[08:24:10.802352] Epoch: [227]  [41/42]  eta: 0:00:00  lr: 0.002056  loss: 0.5936 (0.5939)  time: 0.5074  data: 0.0001  max mem: 9341
[08:24:10.986765] Epoch: [227] Total time: 0:00:22 (0.5397 s / it)
[08:24:10.987680] Averaged stats: lr: 0.002056  loss: 0.5936 (0.5953)
[08:24:15.570399] {"train_lr": 0.002057852793529336, "train_loss": 0.5953400282277947, "epoch": 227}
[08:24:15.570773] [08:24:15.570852] Training epoch 227 for 0:00:27
[08:24:15.570903] [08:24:15.575162] log_dir: ./exp/debug/cifar100-LT/debug
[08:24:17.156567] Epoch: [228]  [ 0/42]  eta: 0:01:06  lr: 0.002056  loss: 0.5939 (0.5939)  time: 1.5801  data: 1.0822  max mem: 9341
[08:24:27.317234] Epoch: [228]  [20/42]  eta: 0:00:12  lr: 0.002054  loss: 0.5825 (0.5875)  time: 0.5080  data: 0.0001  max mem: 9341
[08:24:37.464134] Epoch: [228]  [40/42]  eta: 0:00:01  lr: 0.002053  loss: 0.6037 (0.5955)  time: 0.5073  data: 0.0001  max mem: 9341
[08:24:37.970203] Epoch: [228]  [41/42]  eta: 0:00:00  lr: 0.002053  loss: 0.6082 (0.5959)  time: 0.5073  data: 0.0001  max mem: 9341
[08:24:38.153365] Epoch: [228] Total time: 0:00:22 (0.5376 s / it)
[08:24:38.154048] Averaged stats: lr: 0.002053  loss: 0.6082 (0.5989)
[08:24:42.721561] {"train_lr": 0.002054376913749207, "train_loss": 0.5988852885507402, "epoch": 228}
[08:24:42.721919] [08:24:42.722007] Training epoch 228 for 0:00:27
[08:24:42.722058] [08:24:42.726661] log_dir: ./exp/debug/cifar100-LT/debug
[08:24:44.280586] Epoch: [229]  [ 0/42]  eta: 0:01:05  lr: 0.002052  loss: 0.5491 (0.5491)  time: 1.5527  data: 1.0455  max mem: 9341
[08:24:54.472990] Epoch: [229]  [20/42]  eta: 0:00:12  lr: 0.002051  loss: 0.6079 (0.6040)  time: 0.5096  data: 0.0001  max mem: 9341
[08:25:04.644073] Epoch: [229]  [40/42]  eta: 0:00:01  lr: 0.002049  loss: 0.6126 (0.6073)  time: 0.5085  data: 0.0001  max mem: 9341
[08:25:05.149817] Epoch: [229]  [41/42]  eta: 0:00:00  lr: 0.002049  loss: 0.6126 (0.6082)  time: 0.5085  data: 0.0001  max mem: 9341
[08:25:05.321026] Epoch: [229] Total time: 0:00:22 (0.5380 s / it)
[08:25:05.322031] Averaged stats: lr: 0.002049  loss: 0.6126 (0.6109)
[08:25:09.881369] {"train_lr": 0.002050886435024926, "train_loss": 0.6109254367294765, "epoch": 229}
[08:25:09.881714] [08:25:09.881793] Training epoch 229 for 0:00:27
[08:25:09.881844] [08:25:09.886086] log_dir: ./exp/debug/cifar100-LT/debug
[08:25:11.511986] Epoch: [230]  [ 0/42]  eta: 0:01:08  lr: 0.002049  loss: 0.6584 (0.6584)  time: 1.6248  data: 1.1090  max mem: 9341
[08:25:21.683445] Epoch: [230]  [20/42]  eta: 0:00:12  lr: 0.002047  loss: 0.6054 (0.6083)  time: 0.5085  data: 0.0001  max mem: 9341
[08:25:31.849652] Epoch: [230]  [40/42]  eta: 0:00:01  lr: 0.002046  loss: 0.5940 (0.6047)  time: 0.5083  data: 0.0001  max mem: 9341
[08:25:32.353624] Epoch: [230]  [41/42]  eta: 0:00:00  lr: 0.002046  loss: 0.5953 (0.6046)  time: 0.5082  data: 0.0001  max mem: 9341
[08:25:32.504994] Epoch: [230] Total time: 0:00:22 (0.5385 s / it)
[08:25:32.538328] Averaged stats: lr: 0.002046  loss: 0.5953 (0.6078)
[08:25:37.134790] {"train_lr": 0.0020473814169991453, "train_loss": 0.6077557678023974, "epoch": 230}
[08:25:37.135164] [08:25:37.135247] Training epoch 230 for 0:00:27
[08:25:37.135297] [08:25:37.139745] log_dir: ./exp/debug/cifar100-LT/debug
[08:25:38.720151] Epoch: [231]  [ 0/42]  eta: 0:01:06  lr: 0.002045  loss: 0.5465 (0.5465)  time: 1.5790  data: 1.0703  max mem: 9341
[08:25:48.888711] Epoch: [231]  [20/42]  eta: 0:00:12  lr: 0.002044  loss: 0.6119 (0.6101)  time: 0.5084  data: 0.0001  max mem: 9341
[08:25:59.032444] Epoch: [231]  [40/42]  eta: 0:00:01  lr: 0.002042  loss: 0.6136 (0.6101)  time: 0.5071  data: 0.0001  max mem: 9341
[08:25:59.539213] Epoch: [231]  [41/42]  eta: 0:00:00  lr: 0.002042  loss: 0.6136 (0.6096)  time: 0.5072  data: 0.0001  max mem: 9341
[08:25:59.713166] Epoch: [231] Total time: 0:00:22 (0.5375 s / it)
[08:25:59.716928] Averaged stats: lr: 0.002042  loss: 0.6136 (0.6031)
[08:26:04.416575] {"train_lr": 0.0020438619195629563, "train_loss": 0.6031376144715718, "epoch": 231}
[08:26:04.416960] [08:26:04.417042] Training epoch 231 for 0:00:27
[08:26:04.417094] [08:26:04.421396] log_dir: ./exp/debug/cifar100-LT/debug
[08:26:06.026098] Epoch: [232]  [ 0/42]  eta: 0:01:07  lr: 0.002041  loss: 0.6305 (0.6305)  time: 1.6036  data: 1.0903  max mem: 9341
[08:26:16.193620] Epoch: [232]  [20/42]  eta: 0:00:12  lr: 0.002040  loss: 0.5940 (0.5975)  time: 0.5083  data: 0.0001  max mem: 9341
[08:26:26.338990] Epoch: [232]  [40/42]  eta: 0:00:01  lr: 0.002039  loss: 0.5866 (0.5951)  time: 0.5072  data: 0.0001  max mem: 9341
[08:26:26.844902] Epoch: [232]  [41/42]  eta: 0:00:00  lr: 0.002039  loss: 0.5866 (0.5947)  time: 0.5072  data: 0.0001  max mem: 9341
[08:26:27.019594] Epoch: [232] Total time: 0:00:22 (0.5381 s / it)
[08:26:27.020299] Averaged stats: lr: 0.002039  loss: 0.5866 (0.5972)
[08:26:31.641720] {"train_lr": 0.002040328002854854, "train_loss": 0.5971825953040805, "epoch": 232}
[08:26:31.642087] [08:26:31.642169] Training epoch 232 for 0:00:27
[08:26:31.642224] [08:26:31.647007] log_dir: ./exp/debug/cifar100-LT/debug
[08:26:33.225498] Epoch: [233]  [ 0/42]  eta: 0:01:06  lr: 0.002038  loss: 0.6113 (0.6113)  time: 1.5770  data: 1.0688  max mem: 9341
[08:26:43.382679] Epoch: [233]  [20/42]  eta: 0:00:12  lr: 0.002037  loss: 0.6015 (0.6049)  time: 0.5078  data: 0.0001  max mem: 9341
[08:26:53.525687] Epoch: [233]  [40/42]  eta: 0:00:01  lr: 0.002035  loss: 0.5991 (0.6042)  time: 0.5071  data: 0.0001  max mem: 9341
[08:26:54.031352] Epoch: [233]  [41/42]  eta: 0:00:00  lr: 0.002035  loss: 0.5991 (0.6033)  time: 0.5072  data: 0.0001  max mem: 9341
[08:26:54.195186] Epoch: [233] Total time: 0:00:22 (0.5369 s / it)
[08:26:54.200725] Averaged stats: lr: 0.002035  loss: 0.5991 (0.6013)
[08:26:58.700907] {"train_lr": 0.0020367797272597254, "train_loss": 0.6013094814760345, "epoch": 233}
[08:26:58.701255] [08:26:58.701336] Training epoch 233 for 0:00:27
[08:26:58.701387] [08:26:58.705702] log_dir: ./exp/debug/cifar100-LT/debug
[08:27:00.274471] Epoch: [234]  [ 0/42]  eta: 0:01:05  lr: 0.002034  loss: 0.5724 (0.5724)  time: 1.5675  data: 1.0567  max mem: 9341
[08:27:10.456943] Epoch: [234]  [20/42]  eta: 0:00:12  lr: 0.002033  loss: 0.6094 (0.6003)  time: 0.5091  data: 0.0001  max mem: 9341
[08:27:20.627835] Epoch: [234]  [40/42]  eta: 0:00:01  lr: 0.002032  loss: 0.5918 (0.5976)  time: 0.5085  data: 0.0001  max mem: 9341
[08:27:21.133074] Epoch: [234]  [41/42]  eta: 0:00:00  lr: 0.002032  loss: 0.5774 (0.5969)  time: 0.5085  data: 0.0001  max mem: 9341
[08:27:21.292163] Epoch: [234] Total time: 0:00:22 (0.5378 s / it)
[08:27:21.306281] Averaged stats: lr: 0.002032  loss: 0.5774 (0.5987)
[08:27:26.018567] {"train_lr": 0.0020332171534078155, "train_loss": 0.5987202078104019, "epoch": 234}
[08:27:26.018865] [08:27:26.018944] Training epoch 234 for 0:00:27
[08:27:26.018994] [08:27:26.023360] log_dir: ./exp/debug/cifar100-LT/debug
[08:27:27.683269] Epoch: [235]  [ 0/42]  eta: 0:01:09  lr: 0.002031  loss: 0.6236 (0.6236)  time: 1.6592  data: 1.1597  max mem: 9341
[08:27:37.854743] Epoch: [235]  [20/42]  eta: 0:00:12  lr: 0.002029  loss: 0.6047 (0.6012)  time: 0.5085  data: 0.0001  max mem: 9341
[08:27:48.000755] Epoch: [235]  [40/42]  eta: 0:00:01  lr: 0.002028  loss: 0.5867 (0.5947)  time: 0.5073  data: 0.0001  max mem: 9341
[08:27:48.505750] Epoch: [235]  [41/42]  eta: 0:00:00  lr: 0.002028  loss: 0.5822 (0.5943)  time: 0.5072  data: 0.0001  max mem: 9341
[08:27:48.667805] Epoch: [235] Total time: 0:00:22 (0.5392 s / it)
[08:27:48.670982] Averaged stats: lr: 0.002028  loss: 0.5822 (0.5959)
[08:27:53.293484] {"train_lr": 0.0020296403421736758, "train_loss": 0.5958847488675799, "epoch": 235}
[08:27:53.293834] [08:27:53.293916] Training epoch 235 for 0:00:27
[08:27:53.293966] [08:27:53.298372] log_dir: ./exp/debug/cifar100-LT/debug
[08:27:54.873254] Epoch: [236]  [ 0/42]  eta: 0:01:06  lr: 0.002027  loss: 0.5854 (0.5854)  time: 1.5737  data: 1.0598  max mem: 9341
[08:28:05.036112] Epoch: [236]  [20/42]  eta: 0:00:12  lr: 0.002026  loss: 0.5985 (0.5964)  time: 0.5081  data: 0.0001  max mem: 9341
[08:28:15.188298] Epoch: [236]  [40/42]  eta: 0:00:01  lr: 0.002024  loss: 0.5961 (0.5964)  time: 0.5076  data: 0.0001  max mem: 9341
[08:28:15.693944] Epoch: [236]  [41/42]  eta: 0:00:00  lr: 0.002024  loss: 0.5961 (0.5948)  time: 0.5076  data: 0.0001  max mem: 9341
[08:28:15.853762] Epoch: [236] Total time: 0:00:22 (0.5370 s / it)
[08:28:15.872480] Averaged stats: lr: 0.002024  loss: 0.5961 (0.5910)
[08:28:20.450521] {"train_lr": 0.0020260493546751406, "train_loss": 0.5910053458951768, "epoch": 236}
[08:28:20.450854] [08:28:20.450938] Training epoch 236 for 0:00:27
[08:28:20.450989] [08:28:20.455285] log_dir: ./exp/debug/cifar100-LT/debug
[08:28:22.044193] Epoch: [237]  [ 0/42]  eta: 0:01:06  lr: 0.002024  loss: 0.6082 (0.6082)  time: 1.5881  data: 1.0674  max mem: 9341
[08:28:32.209112] Epoch: [237]  [20/42]  eta: 0:00:12  lr: 0.002022  loss: 0.5885 (0.5858)  time: 0.5082  data: 0.0001  max mem: 9341
[08:28:42.361231] Epoch: [237]  [40/42]  eta: 0:00:01  lr: 0.002021  loss: 0.5880 (0.5856)  time: 0.5076  data: 0.0001  max mem: 9341
[08:28:42.867970] Epoch: [237]  [41/42]  eta: 0:00:00  lr: 0.002021  loss: 0.5880 (0.5864)  time: 0.5075  data: 0.0001  max mem: 9341
[08:28:43.041184] Epoch: [237] Total time: 0:00:22 (0.5378 s / it)
[08:28:43.042656] Averaged stats: lr: 0.002021  loss: 0.5880 (0.5909)
[08:28:47.608957] {"train_lr": 0.002022444252272287, "train_loss": 0.5909285843372345, "epoch": 237}
[08:28:47.609302] [08:28:47.609382] Training epoch 237 for 0:00:27
[08:28:47.609432] [08:28:47.613756] log_dir: ./exp/debug/cifar100-LT/debug
[08:28:49.140737] Epoch: [238]  [ 0/42]  eta: 0:01:04  lr: 0.002020  loss: 0.6041 (0.6041)  time: 1.5257  data: 1.0215  max mem: 9341
[08:28:59.304307] Epoch: [238]  [20/42]  eta: 0:00:12  lr: 0.002019  loss: 0.5852 (0.5965)  time: 0.5081  data: 0.0001  max mem: 9341
[08:29:09.451355] Epoch: [238]  [40/42]  eta: 0:00:01  lr: 0.002017  loss: 0.5819 (0.5905)  time: 0.5073  data: 0.0001  max mem: 9341
[08:29:09.956822] Epoch: [238]  [41/42]  eta: 0:00:00  lr: 0.002017  loss: 0.5819 (0.5907)  time: 0.5073  data: 0.0001  max mem: 9341
[08:29:10.119202] Epoch: [238] Total time: 0:00:22 (0.5358 s / it)
[08:29:10.129436] Averaged stats: lr: 0.002017  loss: 0.5819 (0.5919)
[08:29:14.666534] {"train_lr": 0.00201882509656636, "train_loss": 0.591903496001448, "epoch": 238}
[08:29:14.666877] [08:29:14.666958] Training epoch 238 for 0:00:27
[08:29:14.667009] [08:29:14.671348] log_dir: ./exp/debug/cifar100-LT/debug
[08:29:16.338057] Epoch: [239]  [ 0/42]  eta: 0:01:09  lr: 0.002016  loss: 0.5218 (0.5218)  time: 1.6653  data: 1.1602  max mem: 9341
[08:29:26.499775] Epoch: [239]  [20/42]  eta: 0:00:12  lr: 0.002015  loss: 0.5729 (0.5792)  time: 0.5080  data: 0.0001  max mem: 9341
[08:29:36.647506] Epoch: [239]  [40/42]  eta: 0:00:01  lr: 0.002014  loss: 0.5833 (0.5841)  time: 0.5073  data: 0.0001  max mem: 9341
[08:29:37.152948] Epoch: [239]  [41/42]  eta: 0:00:00  lr: 0.002014  loss: 0.5848 (0.5852)  time: 0.5072  data: 0.0001  max mem: 9341
[08:29:37.321912] Epoch: [239] Total time: 0:00:22 (0.5393 s / it)
[08:29:37.322571] Averaged stats: lr: 0.002014  loss: 0.5848 (0.5883)
[08:29:41.865309] {"train_lr": 0.002015191949398749, "train_loss": 0.5882903273616519, "epoch": 239}
[08:29:41.865623] [08:29:41.865706] Training epoch 239 for 0:00:27
[08:29:41.865757] [08:29:41.870561] log_dir: ./exp/debug/cifar100-LT/debug
[08:29:43.337263] Epoch: [240]  [ 0/42]  eta: 0:01:01  lr: 0.002013  loss: 0.5600 (0.5600)  time: 1.4655  data: 0.9651  max mem: 9341
[08:29:53.498665] Epoch: [240]  [20/42]  eta: 0:00:12  lr: 0.002011  loss: 0.5806 (0.5892)  time: 0.5080  data: 0.0001  max mem: 9341
[08:30:03.633815] Epoch: [240]  [40/42]  eta: 0:00:01  lr: 0.002010  loss: 0.5841 (0.5881)  time: 0.5067  data: 0.0001  max mem: 9341
[08:30:04.137462] Epoch: [240]  [41/42]  eta: 0:00:00  lr: 0.002010  loss: 0.5857 (0.5884)  time: 0.5067  data: 0.0001  max mem: 9341
[08:30:04.289370] Epoch: [240] Total time: 0:00:22 (0.5338 s / it)
[08:30:04.308937] Averaged stats: lr: 0.002010  loss: 0.5857 (0.5858)
[08:30:08.934565] {"train_lr": 0.002011544872849913, "train_loss": 0.5857704679171244, "epoch": 240}
[08:30:08.934835] [08:30:08.934916] Training epoch 240 for 0:00:27
[08:30:08.934968] [08:30:08.939304] log_dir: ./exp/debug/cifar100-LT/debug
[08:30:10.513926] Epoch: [241]  [ 0/42]  eta: 0:01:06  lr: 0.002009  loss: 0.6059 (0.6059)  time: 1.5733  data: 1.0802  max mem: 9341
[08:30:20.675735] Epoch: [241]  [20/42]  eta: 0:00:12  lr: 0.002008  loss: 0.5995 (0.5956)  time: 0.5080  data: 0.0001  max mem: 9341
[08:30:30.827857] Epoch: [241]  [40/42]  eta: 0:00:01  lr: 0.002006  loss: 0.5847 (0.5951)  time: 0.5076  data: 0.0001  max mem: 9341
[08:30:31.333217] Epoch: [241]  [41/42]  eta: 0:00:00  lr: 0.002006  loss: 0.5847 (0.5950)  time: 0.5075  data: 0.0001  max mem: 9341
[08:30:31.495699] Epoch: [241] Total time: 0:00:22 (0.5371 s / it)
[08:30:31.503781] Averaged stats: lr: 0.002006  loss: 0.5847 (0.5884)
[08:30:36.074570] {"train_lr": 0.0020078839292383306, "train_loss": 0.5884101667574474, "epoch": 241}
[08:30:36.075023] [08:30:36.075116] Training epoch 241 for 0:00:27
[08:30:36.075170] [08:30:36.080338] log_dir: ./exp/debug/cifar100-LT/debug
[08:30:37.676104] Epoch: [242]  [ 0/42]  eta: 0:01:06  lr: 0.002005  loss: 0.5553 (0.5553)  time: 1.5944  data: 1.0865  max mem: 9341
[08:30:47.837743] Epoch: [242]  [20/42]  eta: 0:00:12  lr: 0.002004  loss: 0.5763 (0.5782)  time: 0.5080  data: 0.0001  max mem: 9341
[08:30:57.987164] Epoch: [242]  [40/42]  eta: 0:00:01  lr: 0.002003  loss: 0.5913 (0.5842)  time: 0.5074  data: 0.0001  max mem: 9341
[08:30:58.491881] Epoch: [242]  [41/42]  eta: 0:00:00  lr: 0.002003  loss: 0.5913 (0.5847)  time: 0.5074  data: 0.0001  max mem: 9341
[08:30:58.660094] Epoch: [242] Total time: 0:00:22 (0.5376 s / it)
[08:30:58.665742] Averaged stats: lr: 0.002003  loss: 0.5913 (0.5882)
[08:31:03.257666] {"train_lr": 0.0020042091811194227, "train_loss": 0.5881932597784769, "epoch": 242}
[08:31:03.258074] [08:31:03.258178] Training epoch 242 for 0:00:27
[08:31:03.258229] [08:31:03.262748] log_dir: ./exp/debug/cifar100-LT/debug
[08:31:04.948111] Epoch: [243]  [ 0/42]  eta: 0:01:10  lr: 0.002002  loss: 0.5572 (0.5572)  time: 1.6842  data: 1.1899  max mem: 9341
[08:31:15.131216] Epoch: [243]  [20/42]  eta: 0:00:12  lr: 0.002000  loss: 0.5832 (0.5951)  time: 0.5091  data: 0.0001  max mem: 9341
[08:31:25.304719] Epoch: [243]  [40/42]  eta: 0:00:01  lr: 0.001999  loss: 0.5869 (0.5931)  time: 0.5086  data: 0.0001  max mem: 9341
[08:31:25.810420] Epoch: [243]  [41/42]  eta: 0:00:00  lr: 0.001999  loss: 0.5869 (0.5921)  time: 0.5086  data: 0.0001  max mem: 9341
[08:31:25.984959] Epoch: [243] Total time: 0:00:22 (0.5410 s / it)
[08:31:25.986696] Averaged stats: lr: 0.001999  loss: 0.5869 (0.5869)
[08:31:30.581558] {"train_lr": 0.0020005206912845014, "train_loss": 0.5868899389391854, "epoch": 243}
[08:31:30.581954] [08:31:30.582054] Training epoch 243 for 0:00:27
[08:31:30.582105] [08:31:30.586311] log_dir: ./exp/debug/cifar100-LT/debug
[08:31:32.027410] Epoch: [244]  [ 0/42]  eta: 0:01:00  lr: 0.001998  loss: 0.5972 (0.5972)  time: 1.4401  data: 0.9310  max mem: 9341
[08:31:42.190441] Epoch: [244]  [20/42]  eta: 0:00:12  lr: 0.001997  loss: 0.5871 (0.5862)  time: 0.5081  data: 0.0001  max mem: 9341
[08:31:52.339948] Epoch: [244]  [40/42]  eta: 0:00:01  lr: 0.001995  loss: 0.5704 (0.5815)  time: 0.5074  data: 0.0001  max mem: 9341
[08:31:52.846294] Epoch: [244]  [41/42]  eta: 0:00:00  lr: 0.001995  loss: 0.5704 (0.5819)  time: 0.5074  data: 0.0001  max mem: 9341
[08:31:53.014583] Epoch: [244] Total time: 0:00:22 (0.5340 s / it)
[08:31:53.017650] Averaged stats: lr: 0.001995  loss: 0.5704 (0.5840)
[08:31:57.526906] {"train_lr": 0.001996818522759674, "train_loss": 0.584045848321347, "epoch": 244}
[08:31:57.527199] [08:31:57.527289] Training epoch 244 for 0:00:26
[08:31:57.527355] [08:31:57.532477] log_dir: ./exp/debug/cifar100-LT/debug
[08:31:59.112863] Epoch: [245]  [ 0/42]  eta: 0:01:06  lr: 0.001994  loss: 0.5704 (0.5704)  time: 1.5794  data: 1.0585  max mem: 9341
[08:32:09.300679] Epoch: [245]  [20/42]  eta: 0:00:12  lr: 0.001993  loss: 0.5922 (0.5872)  time: 0.5093  data: 0.0001  max mem: 9341
[08:32:19.474698] Epoch: [245]  [40/42]  eta: 0:00:01  lr: 0.001991  loss: 0.5818 (0.5881)  time: 0.5087  data: 0.0001  max mem: 9341
[08:32:19.981315] Epoch: [245]  [41/42]  eta: 0:00:00  lr: 0.001991  loss: 0.5818 (0.5889)  time: 0.5086  data: 0.0001  max mem: 9341
[08:32:20.156854] Epoch: [245] Total time: 0:00:22 (0.5387 s / it)
[08:32:20.158351] Averaged stats: lr: 0.001991  loss: 0.5818 (0.5849)
[08:32:24.831086] {"train_lr": 0.001993102738804791, "train_loss": 0.5848889219618979, "epoch": 245}
[08:32:24.831462] [08:32:24.831543] Training epoch 245 for 0:00:27
[08:32:24.831594] [08:32:24.836036] log_dir: ./exp/debug/cifar100-LT/debug
[08:32:26.563968] Epoch: [246]  [ 0/42]  eta: 0:01:12  lr: 0.001991  loss: 0.6414 (0.6414)  time: 1.7272  data: 1.2326  max mem: 9341
[08:32:36.731785] Epoch: [246]  [20/42]  eta: 0:00:12  lr: 0.001989  loss: 0.5780 (0.5815)  time: 0.5083  data: 0.0001  max mem: 9341
[08:32:46.882392] Epoch: [246]  [40/42]  eta: 0:00:01  lr: 0.001988  loss: 0.5989 (0.5868)  time: 0.5075  data: 0.0001  max mem: 9341
[08:32:47.389511] Epoch: [246]  [41/42]  eta: 0:00:00  lr: 0.001988  loss: 0.5901 (0.5868)  time: 0.5076  data: 0.0001  max mem: 9341
[08:32:47.547127] Epoch: [246] Total time: 0:00:22 (0.5407 s / it)
[08:32:47.558854] Averaged stats: lr: 0.001988  loss: 0.5901 (0.5834)
[08:32:52.186763] {"train_lr": 0.0019893734029123454, "train_loss": 0.5833540718470301, "epoch": 246}
[08:32:52.187146] [08:32:52.187235] Training epoch 246 for 0:00:27
[08:32:52.187289] [08:32:52.192178] log_dir: ./exp/debug/cifar100-LT/debug
[08:32:53.744749] Epoch: [247]  [ 0/42]  eta: 0:01:05  lr: 0.001987  loss: 0.5733 (0.5733)  time: 1.5511  data: 1.0477  max mem: 9341
[08:33:03.911278] Epoch: [247]  [20/42]  eta: 0:00:12  lr: 0.001985  loss: 0.5713 (0.5827)  time: 0.5083  data: 0.0002  max mem: 9341
[08:33:14.061857] Epoch: [247]  [40/42]  eta: 0:00:01  lr: 0.001984  loss: 0.5753 (0.5776)  time: 0.5075  data: 0.0001  max mem: 9341
[08:33:14.566829] Epoch: [247]  [41/42]  eta: 0:00:00  lr: 0.001984  loss: 0.5723 (0.5773)  time: 0.5074  data: 0.0001  max mem: 9341
[08:33:14.728854] Epoch: [247] Total time: 0:00:22 (0.5366 s / it)
[08:33:14.750030] Averaged stats: lr: 0.001984  loss: 0.5723 (0.5790)
[08:33:19.255696] {"train_lr": 0.001985630578806401, "train_loss": 0.5790274356092725, "epoch": 247}
[08:33:19.256215] [08:33:19.256316] Training epoch 247 for 0:00:27
[08:33:19.256373] [08:33:19.261412] log_dir: ./exp/debug/cifar100-LT/debug
[08:33:20.836113] Epoch: [248]  [ 0/42]  eta: 0:01:06  lr: 0.001983  loss: 0.6084 (0.6084)  time: 1.5739  data: 1.0657  max mem: 9341
[08:33:31.004287] Epoch: [248]  [20/42]  eta: 0:00:12  lr: 0.001982  loss: 0.5811 (0.5818)  time: 0.5084  data: 0.0001  max mem: 9341
[08:33:41.156651] Epoch: [248]  [40/42]  eta: 0:00:01  lr: 0.001980  loss: 0.5813 (0.5801)  time: 0.5076  data: 0.0001  max mem: 9341
[08:33:41.663839] Epoch: [248]  [41/42]  eta: 0:00:00  lr: 0.001980  loss: 0.5752 (0.5800)  time: 0.5076  data: 0.0001  max mem: 9341
[08:33:41.828988] Epoch: [248] Total time: 0:00:22 (0.5373 s / it)
[08:33:41.837206] Averaged stats: lr: 0.001980  loss: 0.5752 (0.5790)
[08:33:46.307011] {"train_lr": 0.0019818743304414943, "train_loss": 0.5790161754758585, "epoch": 248}
[08:33:46.307379] [08:33:46.307461] Training epoch 248 for 0:00:27
[08:33:46.307511] [08:33:46.311846] log_dir: ./exp/debug/cifar100-LT/debug
[08:33:47.952051] Epoch: [249]  [ 0/42]  eta: 0:01:08  lr: 0.001979  loss: 0.6037 (0.6037)  time: 1.6392  data: 1.1209  max mem: 9341
[08:33:58.121488] Epoch: [249]  [20/42]  eta: 0:00:12  lr: 0.001978  loss: 0.5686 (0.5770)  time: 0.5084  data: 0.0001  max mem: 9341
[08:34:08.280093] Epoch: [249]  [40/42]  eta: 0:00:01  lr: 0.001976  loss: 0.5907 (0.5816)  time: 0.5079  data: 0.0001  max mem: 9341
[08:34:08.785899] Epoch: [249]  [41/42]  eta: 0:00:00  lr: 0.001976  loss: 0.5873 (0.5816)  time: 0.5079  data: 0.0001  max mem: 9341
[08:34:08.959469] Epoch: [249] Total time: 0:00:22 (0.5392 s / it)
[08:34:08.966972] Averaged stats: lr: 0.001976  loss: 0.5873 (0.5784)
[08:34:13.577002] {"train_lr": 0.0019781047220015432, "train_loss": 0.5784442158682006, "epoch": 249}
[08:34:13.577469] [08:34:13.577568] Training epoch 249 for 0:00:27
[08:34:13.577623] [08:34:13.582669] log_dir: ./exp/debug/cifar100-LT/debug
[08:34:15.185583] Epoch: [250]  [ 0/42]  eta: 0:01:07  lr: 0.001976  loss: 0.6179 (0.6179)  time: 1.6018  data: 1.0837  max mem: 9341
[08:34:25.353098] Epoch: [250]  [20/42]  eta: 0:00:12  lr: 0.001974  loss: 0.5799 (0.5804)  time: 0.5083  data: 0.0001  max mem: 9341
[08:34:35.504790] Epoch: [250]  [40/42]  eta: 0:00:01  lr: 0.001973  loss: 0.5707 (0.5792)  time: 0.5075  data: 0.0001  max mem: 9341
[08:34:36.012141] Epoch: [250]  [41/42]  eta: 0:00:00  lr: 0.001973  loss: 0.5696 (0.5789)  time: 0.5076  data: 0.0001  max mem: 9341
[08:34:36.167221] Epoch: [250] Total time: 0:00:22 (0.5377 s / it)
[08:34:36.177308] Averaged stats: lr: 0.001973  loss: 0.5696 (0.5761)
[08:34:40.701821] {"train_lr": 0.0019743218178987594, "train_loss": 0.5760675122340521, "epoch": 250}
[08:34:40.702227] [08:34:40.702317] Training epoch 250 for 0:00:27
[08:34:40.702371] [08:34:40.707368] log_dir: ./exp/debug/cifar100-LT/debug
[08:34:42.263653] Epoch: [251]  [ 0/42]  eta: 0:01:05  lr: 0.001972  loss: 0.6454 (0.6454)  time: 1.5550  data: 1.0492  max mem: 9341
[08:34:52.449726] Epoch: [251]  [20/42]  eta: 0:00:12  lr: 0.001970  loss: 0.5648 (0.5724)  time: 0.5093  data: 0.0001  max mem: 9341
[08:35:02.628565] Epoch: [251]  [40/42]  eta: 0:00:01  lr: 0.001969  loss: 0.5900 (0.5795)  time: 0.5089  data: 0.0001  max mem: 9341
[08:35:03.134519] Epoch: [251]  [41/42]  eta: 0:00:00  lr: 0.001969  loss: 0.5826 (0.5795)  time: 0.5089  data: 0.0001  max mem: 9341
[08:35:03.300711] Epoch: [251] Total time: 0:00:22 (0.5379 s / it)
[08:35:03.302674] Averaged stats: lr: 0.001969  loss: 0.5826 (0.5794)
[08:35:07.817607] {"train_lr": 0.001970525682772532, "train_loss": 0.5793987258913971, "epoch": 251}
[08:35:07.817999] [08:35:07.818083] Training epoch 251 for 0:00:27
[08:35:07.818135] [08:35:07.822812] log_dir: ./exp/debug/cifar100-LT/debug
[08:35:09.300719] Epoch: [252]  [ 0/42]  eta: 0:01:02  lr: 0.001968  loss: 0.5757 (0.5757)  time: 1.4767  data: 0.9702  max mem: 9341
[08:35:19.463958] Epoch: [252]  [20/42]  eta: 0:00:12  lr: 0.001967  loss: 0.5714 (0.5776)  time: 0.5081  data: 0.0001  max mem: 9341
[08:35:29.616312] Epoch: [252]  [40/42]  eta: 0:00:01  lr: 0.001965  loss: 0.5653 (0.5724)  time: 0.5076  data: 0.0001  max mem: 9341
[08:35:30.122331] Epoch: [252]  [41/42]  eta: 0:00:00  lr: 0.001965  loss: 0.5600 (0.5715)  time: 0.5076  data: 0.0001  max mem: 9341
[08:35:30.285125] Epoch: [252] Total time: 0:00:22 (0.5348 s / it)
[08:35:30.309775] Averaged stats: lr: 0.001965  loss: 0.5600 (0.5743)
[08:35:34.905537] {"train_lr": 0.001966716381488339, "train_loss": 0.574309702962637, "epoch": 252}
[08:35:34.905984] [08:35:34.906082] Training epoch 252 for 0:00:27
[08:35:34.906135] [08:35:34.911391] log_dir: ./exp/debug/cifar100-LT/debug
[08:35:36.626035] Epoch: [253]  [ 0/42]  eta: 0:01:11  lr: 0.001964  loss: 0.6205 (0.6205)  time: 1.7139  data: 1.2030  max mem: 9341
[08:35:46.797052] Epoch: [253]  [20/42]  eta: 0:00:12  lr: 0.001963  loss: 0.5711 (0.5711)  time: 0.5085  data: 0.0001  max mem: 9341
[08:35:56.946375] Epoch: [253]  [40/42]  eta: 0:00:01  lr: 0.001961  loss: 0.5642 (0.5689)  time: 0.5074  data: 0.0001  max mem: 9341
[08:35:57.452155] Epoch: [253]  [41/42]  eta: 0:00:00  lr: 0.001961  loss: 0.5605 (0.5681)  time: 0.5073  data: 0.0001  max mem: 9341
[08:35:57.623923] Epoch: [253] Total time: 0:00:22 (0.5408 s / it)
[08:35:57.630267] Averaged stats: lr: 0.001961  loss: 0.5605 (0.5708)
[08:36:02.172669] {"train_lr": 0.001962893979136627, "train_loss": 0.5707782691433316, "epoch": 253}
[08:36:02.173043] [08:36:02.173124] Training epoch 253 for 0:00:27
[08:36:02.173175] [08:36:02.177571] log_dir: ./exp/debug/cifar100-LT/debug
[08:36:03.744836] Epoch: [254]  [ 0/42]  eta: 0:01:05  lr: 0.001960  loss: 0.5524 (0.5524)  time: 1.5660  data: 1.0498  max mem: 9341
[08:36:13.910539] Epoch: [254]  [20/42]  eta: 0:00:12  lr: 0.001959  loss: 0.5840 (0.5752)  time: 0.5082  data: 0.0001  max mem: 9341
[08:36:24.057365] Epoch: [254]  [40/42]  eta: 0:00:01  lr: 0.001957  loss: 0.5571 (0.5720)  time: 0.5073  data: 0.0001  max mem: 9341
[08:36:24.562598] Epoch: [254]  [41/42]  eta: 0:00:00  lr: 0.001957  loss: 0.5571 (0.5720)  time: 0.5073  data: 0.0001  max mem: 9341
[08:36:24.728562] Epoch: [254] Total time: 0:00:22 (0.5369 s / it)
[08:36:24.738002] Averaged stats: lr: 0.001957  loss: 0.5571 (0.5720)
[08:36:29.339881] {"train_lr": 0.001959058541031712, "train_loss": 0.5719530209898949, "epoch": 254}
[08:36:29.340271] [08:36:29.340357] Training epoch 254 for 0:00:27
[08:36:29.340410] [08:36:29.344684] log_dir: ./exp/debug/cifar100-LT/debug
[08:36:31.008985] Epoch: [255]  [ 0/42]  eta: 0:01:09  lr: 0.001956  loss: 0.5185 (0.5185)  time: 1.6635  data: 1.1695  max mem: 9341
[08:36:41.166445] Epoch: [255]  [20/42]  eta: 0:00:12  lr: 0.001955  loss: 0.5741 (0.5711)  time: 0.5078  data: 0.0001  max mem: 9341
[08:36:51.307809] Epoch: [255]  [40/42]  eta: 0:00:01  lr: 0.001954  loss: 0.5695 (0.5739)  time: 0.5070  data: 0.0001  max mem: 9341
[08:36:51.812954] Epoch: [255]  [41/42]  eta: 0:00:00  lr: 0.001954  loss: 0.5695 (0.5710)  time: 0.5070  data: 0.0001  max mem: 9341
[08:36:51.974182] Epoch: [255] Total time: 0:00:22 (0.5388 s / it)
[08:36:51.980132] Averaged stats: lr: 0.001954  loss: 0.5695 (0.5674)
[08:36:56.558486] {"train_lr": 0.001955210132710644, "train_loss": 0.5673794439505964, "epoch": 255}
[08:36:56.558834] [08:36:56.558915] Training epoch 255 for 0:00:27
[08:36:56.558966] [08:36:56.563364] log_dir: ./exp/debug/cifar100-LT/debug
[08:36:58.044984] Epoch: [256]  [ 0/42]  eta: 0:01:02  lr: 0.001953  loss: 0.5582 (0.5582)  time: 1.4799  data: 0.9770  max mem: 9341
[08:37:08.221735] Epoch: [256]  [20/42]  eta: 0:00:12  lr: 0.001951  loss: 0.5623 (0.5685)  time: 0.5088  data: 0.0001  max mem: 9341
[08:37:18.393453] Epoch: [256]  [40/42]  eta: 0:00:01  lr: 0.001950  loss: 0.5669 (0.5681)  time: 0.5085  data: 0.0001  max mem: 9341
[08:37:18.898380] Epoch: [256]  [41/42]  eta: 0:00:00  lr: 0.001950  loss: 0.5669 (0.5680)  time: 0.5084  data: 0.0001  max mem: 9341
[08:37:19.082633] Epoch: [256] Total time: 0:00:22 (0.5362 s / it)
[08:37:19.084054] Averaged stats: lr: 0.001950  loss: 0.5669 (0.5696)
[08:37:23.800708] {"train_lr": 0.0019513488199320989, "train_loss": 0.5696470387989566, "epoch": 256}
[08:37:23.801087] [08:37:23.801176] Training epoch 256 for 0:00:27
[08:37:23.801245] [08:37:23.806023] log_dir: ./exp/debug/cifar100-LT/debug
[08:37:25.538192] Epoch: [257]  [ 0/42]  eta: 0:01:12  lr: 0.001949  loss: 0.5514 (0.5514)  time: 1.7314  data: 1.2114  max mem: 9341
[08:37:35.725721] Epoch: [257]  [20/42]  eta: 0:00:12  lr: 0.001947  loss: 0.5710 (0.5720)  time: 0.5093  data: 0.0001  max mem: 9341
[08:37:45.892493] Epoch: [257]  [40/42]  eta: 0:00:01  lr: 0.001946  loss: 0.5669 (0.5668)  time: 0.5083  data: 0.0001  max mem: 9341
[08:37:46.399105] Epoch: [257]  [41/42]  eta: 0:00:00  lr: 0.001946  loss: 0.5669 (0.5665)  time: 0.5083  data: 0.0001  max mem: 9341
[08:37:46.565096] Epoch: [257] Total time: 0:00:22 (0.5419 s / it)
[08:37:46.573423] Averaged stats: lr: 0.001946  loss: 0.5669 (0.5696)
[08:37:51.079303] {"train_lr": 0.001947474668675268, "train_loss": 0.5696195126289413, "epoch": 257}
[08:37:51.079655] [08:37:51.079739] Training epoch 257 for 0:00:27
[08:37:51.079791] [08:37:51.084231] log_dir: ./exp/debug/cifar100-LT/debug
[08:37:52.615638] Epoch: [258]  [ 0/42]  eta: 0:01:04  lr: 0.001945  loss: 0.5369 (0.5369)  time: 1.5301  data: 1.0234  max mem: 9341
[08:38:02.801615] Epoch: [258]  [20/42]  eta: 0:00:12  lr: 0.001943  loss: 0.5609 (0.5604)  time: 0.5093  data: 0.0001  max mem: 9341
[08:38:12.955593] Epoch: [258]  [40/42]  eta: 0:00:01  lr: 0.001942  loss: 0.5719 (0.5642)  time: 0.5076  data: 0.0001  max mem: 9341
[08:38:13.461545] Epoch: [258]  [41/42]  eta: 0:00:00  lr: 0.001942  loss: 0.5719 (0.5644)  time: 0.5077  data: 0.0001  max mem: 9341
[08:38:13.639070] Epoch: [258] Total time: 0:00:22 (0.5370 s / it)
[08:38:13.641459] Averaged stats: lr: 0.001942  loss: 0.5719 (0.5689)
[08:38:18.286282] {"train_lr": 0.0019435877451387, "train_loss": 0.5688884542101905, "epoch": 258}
[08:38:18.286634] [08:38:18.286720] Training epoch 258 for 0:00:27
[08:38:18.286772] [08:38:18.291221] log_dir: ./exp/debug/cifar100-LT/debug
[08:38:19.874219] Epoch: [259]  [ 0/42]  eta: 0:01:06  lr: 0.001941  loss: 0.5754 (0.5754)  time: 1.5815  data: 1.0809  max mem: 9341
[08:38:30.038311] Epoch: [259]  [20/42]  eta: 0:00:12  lr: 0.001939  loss: 0.5642 (0.5697)  time: 0.5082  data: 0.0001  max mem: 9341
[08:38:40.189800] Epoch: [259]  [40/42]  eta: 0:00:01  lr: 0.001938  loss: 0.5622 (0.5681)  time: 0.5075  data: 0.0001  max mem: 9341
[08:38:40.695773] Epoch: [259]  [41/42]  eta: 0:00:00  lr: 0.001938  loss: 0.5622 (0.5683)  time: 0.5076  data: 0.0001  max mem: 9341
[08:38:40.863382] Epoch: [259] Total time: 0:00:22 (0.5374 s / it)
[08:38:40.869727] Averaged stats: lr: 0.001938  loss: 0.5622 (0.5689)
[08:38:45.441247] {"train_lr": 0.001939688115739198, "train_loss": 0.5689460766457376, "epoch": 259}
[08:38:45.441568] [08:38:45.441648] Training epoch 259 for 0:00:27
[08:38:45.441718] [08:38:45.446035] log_dir: ./exp/debug/cifar100-LT/debug
[08:38:46.948353] Epoch: [260]  [ 0/42]  eta: 0:01:03  lr: 0.001937  loss: 0.5553 (0.5553)  time: 1.5014  data: 0.9781  max mem: 9341
[08:38:57.131832] Epoch: [260]  [20/42]  eta: 0:00:12  lr: 0.001936  loss: 0.5546 (0.5586)  time: 0.5091  data: 0.0001  max mem: 9341
[08:39:07.297072] Epoch: [260]  [40/42]  eta: 0:00:01  lr: 0.001934  loss: 0.5687 (0.5624)  time: 0.5082  data: 0.0001  max mem: 9341
[08:39:07.803283] Epoch: [260]  [41/42]  eta: 0:00:00  lr: 0.001934  loss: 0.5600 (0.5611)  time: 0.5082  data: 0.0001  max mem: 9341
[08:39:07.972105] Epoch: [260] Total time: 0:00:22 (0.5363 s / it)
[08:39:07.975997] Averaged stats: lr: 0.001934  loss: 0.5600 (0.5738)
[08:39:12.566534] {"train_lr": 0.0019357758471106638, "train_loss": 0.5737709379976704, "epoch": 260}
[08:39:12.566799] [08:39:12.566876] Training epoch 260 for 0:00:27
[08:39:12.566927] [08:39:12.571188] log_dir: ./exp/debug/cifar100-LT/debug
[08:39:14.070173] Epoch: [261]  [ 0/42]  eta: 0:01:02  lr: 0.001933  loss: 0.5664 (0.5664)  time: 1.4980  data: 0.9907  max mem: 9341
[08:39:24.256014] Epoch: [261]  [20/42]  eta: 0:00:12  lr: 0.001932  loss: 0.5676 (0.5711)  time: 0.5092  data: 0.0001  max mem: 9341
[08:39:34.427560] Epoch: [261]  [40/42]  eta: 0:00:01  lr: 0.001930  loss: 0.5868 (0.5756)  time: 0.5085  data: 0.0001  max mem: 9341
[08:39:34.934774] Epoch: [261]  [41/42]  eta: 0:00:00  lr: 0.001930  loss: 0.5835 (0.5755)  time: 0.5085  data: 0.0001  max mem: 9341
[08:39:35.095802] Epoch: [261] Total time: 0:00:22 (0.5363 s / it)
[08:39:35.096525] Averaged stats: lr: 0.001930  loss: 0.5835 (0.5778)
[08:39:39.568596] {"train_lr": 0.0019318510061029782, "train_loss": 0.5778090297466233, "epoch": 261}
[08:39:39.568963] [08:39:39.569062] Training epoch 261 for 0:00:27
[08:39:39.569112] [08:39:39.573503] log_dir: ./exp/debug/cifar100-LT/debug
[08:39:41.064606] Epoch: [262]  [ 0/42]  eta: 0:01:02  lr: 0.001929  loss: 0.5289 (0.5289)  time: 1.4898  data: 0.9846  max mem: 9341
[08:39:51.224181] Epoch: [262]  [20/42]  eta: 0:00:12  lr: 0.001928  loss: 0.5577 (0.5557)  time: 0.5079  data: 0.0001  max mem: 9341
[08:40:01.374586] Epoch: [262]  [40/42]  eta: 0:00:01  lr: 0.001926  loss: 0.5647 (0.5604)  time: 0.5075  data: 0.0001  max mem: 9341
[08:40:01.880531] Epoch: [262]  [41/42]  eta: 0:00:00  lr: 0.001926  loss: 0.5647 (0.5608)  time: 0.5075  data: 0.0001  max mem: 9341
[08:40:02.076275] Epoch: [262] Total time: 0:00:22 (0.5358 s / it)
[08:40:02.081313] Averaged stats: lr: 0.001926  loss: 0.5647 (0.5721)
[08:40:06.653379] {"train_lr": 0.0019279136597808455, "train_loss": 0.572076046750659, "epoch": 262}
[08:40:06.653655] [08:40:06.653737] Training epoch 262 for 0:00:27
[08:40:06.653845] [08:40:06.658594] log_dir: ./exp/debug/cifar100-LT/debug
[08:40:08.166804] Epoch: [263]  [ 0/42]  eta: 0:01:03  lr: 0.001925  loss: 0.6138 (0.6138)  time: 1.5074  data: 0.9926  max mem: 9341
[08:40:18.336924] Epoch: [263]  [20/42]  eta: 0:00:12  lr: 0.001924  loss: 0.5731 (0.5749)  time: 0.5085  data: 0.0001  max mem: 9341
[08:40:28.483205] Epoch: [263]  [40/42]  eta: 0:00:01  lr: 0.001922  loss: 0.5653 (0.5717)  time: 0.5073  data: 0.0001  max mem: 9341
[08:40:28.990919] Epoch: [263]  [41/42]  eta: 0:00:00  lr: 0.001922  loss: 0.5653 (0.5711)  time: 0.5074  data: 0.0001  max mem: 9341
[08:40:29.158084] Epoch: [263] Total time: 0:00:22 (0.5357 s / it)
[08:40:29.179418] Averaged stats: lr: 0.001922  loss: 0.5653 (0.5720)
[08:40:33.674530] {"train_lr": 0.001923963875422646, "train_loss": 0.5720326703573976, "epoch": 263}
[08:40:33.674789] [08:40:33.674868] Training epoch 263 for 0:00:27
[08:40:33.674919] [08:40:33.679295] log_dir: ./exp/debug/cifar100-LT/debug
[08:40:35.286269] Epoch: [264]  [ 0/42]  eta: 0:01:07  lr: 0.001921  loss: 0.5697 (0.5697)  time: 1.6060  data: 1.0853  max mem: 9341
[08:40:45.464781] Epoch: [264]  [20/42]  eta: 0:00:12  lr: 0.001920  loss: 0.5829 (0.5781)  time: 0.5089  data: 0.0001  max mem: 9341
[08:40:55.632851] Epoch: [264]  [40/42]  eta: 0:00:01  lr: 0.001918  loss: 0.5621 (0.5710)  time: 0.5084  data: 0.0001  max mem: 9341
[08:40:56.138526] Epoch: [264]  [41/42]  eta: 0:00:00  lr: 0.001918  loss: 0.5571 (0.5699)  time: 0.5084  data: 0.0001  max mem: 9341
[08:40:56.302095] Epoch: [264] Total time: 0:00:22 (0.5386 s / it)
[08:40:56.302793] Averaged stats: lr: 0.001918  loss: 0.5571 (0.5708)
[08:41:00.960660] {"train_lr": 0.001920001720519304, "train_loss": 0.5707939689358076, "epoch": 264}
[08:41:00.961069] [08:41:00.961170] Training epoch 264 for 0:00:27
[08:41:00.961223] [08:41:00.966031] log_dir: ./exp/debug/cifar100-LT/debug
[08:41:02.420707] Epoch: [265]  [ 0/42]  eta: 0:01:01  lr: 0.001917  loss: 0.5430 (0.5430)  time: 1.4537  data: 0.9373  max mem: 9341
[08:41:12.590563] Epoch: [265]  [20/42]  eta: 0:00:12  lr: 0.001916  loss: 0.5709 (0.5723)  time: 0.5084  data: 0.0001  max mem: 9341
[08:41:22.743182] Epoch: [265]  [40/42]  eta: 0:00:01  lr: 0.001914  loss: 0.5779 (0.5749)  time: 0.5076  data: 0.0001  max mem: 9341
[08:41:23.247671] Epoch: [265]  [41/42]  eta: 0:00:00  lr: 0.001914  loss: 0.5779 (0.5763)  time: 0.5075  data: 0.0001  max mem: 9341
[08:41:23.408966] Epoch: [265] Total time: 0:00:22 (0.5344 s / it)
[08:41:23.412352] Averaged stats: lr: 0.001914  loss: 0.5779 (0.5720)
[08:41:28.012995] {"train_lr": 0.0019160272627731076, "train_loss": 0.5719919971057347, "epoch": 265}
[08:41:28.013491] [08:41:28.013620] Training epoch 265 for 0:00:27
[08:41:28.013697] [08:41:28.018567] log_dir: ./exp/debug/cifar100-LT/debug
[08:41:29.523354] Epoch: [266]  [ 0/42]  eta: 0:01:03  lr: 0.001913  loss: 0.5499 (0.5499)  time: 1.5039  data: 0.9959  max mem: 9341
[08:41:39.758206] Epoch: [266]  [20/42]  eta: 0:00:12  lr: 0.001912  loss: 0.5699 (0.5742)  time: 0.5117  data: 0.0001  max mem: 9341
[08:41:49.910414] Epoch: [266]  [40/42]  eta: 0:00:01  lr: 0.001910  loss: 0.5697 (0.5724)  time: 0.5076  data: 0.0001  max mem: 9341
[08:41:50.418446] Epoch: [266]  [41/42]  eta: 0:00:00  lr: 0.001910  loss: 0.5697 (0.5721)  time: 0.5077  data: 0.0001  max mem: 9341
[08:41:50.580316] Epoch: [266] Total time: 0:00:22 (0.5372 s / it)
[08:41:50.591399] Averaged stats: lr: 0.001910  loss: 0.5697 (0.5738)
[08:41:55.099169] {"train_lr": 0.0019120405700965766, "train_loss": 0.5737986220490365, "epoch": 266}
[08:41:55.099556] [08:41:55.099667] Training epoch 266 for 0:00:27
[08:41:55.099722] [08:41:55.104724] log_dir: ./exp/debug/cifar100-LT/debug
[08:41:56.603412] Epoch: [267]  [ 0/42]  eta: 0:01:02  lr: 0.001909  loss: 0.5308 (0.5308)  time: 1.4978  data: 1.0026  max mem: 9341
[08:42:06.764526] Epoch: [267]  [20/42]  eta: 0:00:12  lr: 0.001908  loss: 0.5705 (0.5697)  time: 0.5080  data: 0.0001  max mem: 9341
[08:42:16.910108] Epoch: [267]  [40/42]  eta: 0:00:01  lr: 0.001906  loss: 0.5692 (0.5693)  time: 0.5072  data: 0.0001  max mem: 9341
[08:42:17.417462] Epoch: [267]  [41/42]  eta: 0:00:00  lr: 0.001906  loss: 0.5692 (0.5696)  time: 0.5074  data: 0.0001  max mem: 9341
[08:42:17.585448] Epoch: [267] Total time: 0:00:22 (0.5353 s / it)
[08:42:17.590827] Averaged stats: lr: 0.001906  loss: 0.5692 (0.5645)
[08:42:22.270265] {"train_lr": 0.0019080417106112902, "train_loss": 0.5645067837266695, "epoch": 267}
[08:42:22.270547] [08:42:22.270626] Training epoch 267 for 0:00:27
[08:42:22.270676] [08:42:22.275046] log_dir: ./exp/debug/cifar100-LT/debug
[08:42:23.700513] Epoch: [268]  [ 0/42]  eta: 0:00:59  lr: 0.001905  loss: 0.6015 (0.6015)  time: 1.4245  data: 0.9075  max mem: 9341
[08:42:33.877977] Epoch: [268]  [20/42]  eta: 0:00:12  lr: 0.001904  loss: 0.5728 (0.5742)  time: 0.5088  data: 0.0001  max mem: 9341
[08:42:44.025662] Epoch: [268]  [40/42]  eta: 0:00:01  lr: 0.001902  loss: 0.5691 (0.5718)  time: 0.5073  data: 0.0001  max mem: 9341
[08:42:44.531185] Epoch: [268]  [41/42]  eta: 0:00:00  lr: 0.001902  loss: 0.5660 (0.5714)  time: 0.5074  data: 0.0001  max mem: 9341
[08:42:44.687653] Epoch: [268] Total time: 0:00:22 (0.5336 s / it)
[08:42:44.701512] Averaged stats: lr: 0.001902  loss: 0.5660 (0.5669)
[08:42:49.225064] {"train_lr": 0.001904030752646727, "train_loss": 0.5668641965658892, "epoch": 268}
[08:42:49.225430] [08:42:49.225534] Training epoch 268 for 0:00:26
[08:42:49.225585] [08:42:49.229887] log_dir: ./exp/debug/cifar100-LT/debug
[08:42:50.877015] Epoch: [269]  [ 0/42]  eta: 0:01:09  lr: 0.001901  loss: 0.5395 (0.5395)  time: 1.6461  data: 1.1557  max mem: 9341
[08:43:01.039157] Epoch: [269]  [20/42]  eta: 0:00:12  lr: 0.001900  loss: 0.5640 (0.5737)  time: 0.5081  data: 0.0001  max mem: 9341
[08:43:11.189793] Epoch: [269]  [40/42]  eta: 0:00:01  lr: 0.001898  loss: 0.5610 (0.5698)  time: 0.5075  data: 0.0001  max mem: 9341
[08:43:11.694108] Epoch: [269]  [41/42]  eta: 0:00:00  lr: 0.001898  loss: 0.5647 (0.5698)  time: 0.5075  data: 0.0001  max mem: 9341
[08:43:11.859074] Epoch: [269] Total time: 0:00:22 (0.5388 s / it)
[08:43:11.862192] Averaged stats: lr: 0.001898  loss: 0.5647 (0.5678)
[08:43:16.381833] {"train_lr": 0.0019000077647390791, "train_loss": 0.567771913749831, "epoch": 269}
[08:43:16.382105] [08:43:16.382188] Training epoch 269 for 0:00:27
[08:43:16.382241] [08:43:16.386537] log_dir: ./exp/debug/cifar100-LT/debug
[08:43:17.865739] Epoch: [270]  [ 0/42]  eta: 0:01:02  lr: 0.001897  loss: 0.5517 (0.5517)  time: 1.4778  data: 0.9600  max mem: 9341
[08:43:28.049596] Epoch: [270]  [20/42]  eta: 0:00:12  lr: 0.001896  loss: 0.5647 (0.5651)  time: 0.5091  data: 0.0001  max mem: 9341
[08:43:38.216316] Epoch: [270]  [40/42]  eta: 0:00:01  lr: 0.001894  loss: 0.5590 (0.5616)  time: 0.5083  data: 0.0001  max mem: 9341
[08:43:38.724321] Epoch: [270]  [41/42]  eta: 0:00:00  lr: 0.001894  loss: 0.5590 (0.5620)  time: 0.5084  data: 0.0001  max mem: 9341
[08:43:38.883879] Epoch: [270] Total time: 0:00:22 (0.5356 s / it)
[08:43:38.897194] Averaged stats: lr: 0.001894  loss: 0.5590 (0.5640)
[08:43:43.534244] {"train_lr": 0.0018959728156301256, "train_loss": 0.5639775022864342, "epoch": 270}
[08:43:43.534560] [08:43:43.534650] Training epoch 270 for 0:00:27
[08:43:43.534702] [08:43:43.539619] log_dir: ./exp/debug/cifar100-LT/debug
[08:43:45.175530] Epoch: [271]  [ 0/42]  eta: 0:01:08  lr: 0.001893  loss: 0.6044 (0.6044)  time: 1.6352  data: 1.1274  max mem: 9341
[08:43:55.337268] Epoch: [271]  [20/42]  eta: 0:00:12  lr: 0.001892  loss: 0.5563 (0.5691)  time: 0.5080  data: 0.0001  max mem: 9341
[08:44:05.491592] Epoch: [271]  [40/42]  eta: 0:00:01  lr: 0.001890  loss: 0.5635 (0.5644)  time: 0.5077  data: 0.0001  max mem: 9341
[08:44:05.996825] Epoch: [271]  [41/42]  eta: 0:00:00  lr: 0.001890  loss: 0.5542 (0.5637)  time: 0.5076  data: 0.0001  max mem: 9341
[08:44:06.152099] Epoch: [271] Total time: 0:00:22 (0.5384 s / it)
[08:44:06.169888] Averaged stats: lr: 0.001890  loss: 0.5542 (0.5644)
[08:44:10.786135] {"train_lr": 0.0018919259742660051, "train_loss": 0.5643812391374793, "epoch": 271}
[08:44:10.786440] [08:44:10.786518] Training epoch 271 for 0:00:27
[08:44:10.786568] [08:44:10.790987] log_dir: ./exp/debug/cifar100-LT/debug
[08:44:12.301954] Epoch: [272]  [ 0/42]  eta: 0:01:03  lr: 0.001889  loss: 0.5437 (0.5437)  time: 1.5096  data: 1.0134  max mem: 9341
[08:44:22.460931] Epoch: [272]  [20/42]  eta: 0:00:12  lr: 0.001888  loss: 0.5540 (0.5522)  time: 0.5079  data: 0.0001  max mem: 9341
[08:44:32.609965] Epoch: [272]  [40/42]  eta: 0:00:01  lr: 0.001886  loss: 0.5442 (0.5502)  time: 0.5074  data: 0.0001  max mem: 9341
[08:44:33.117169] Epoch: [272]  [41/42]  eta: 0:00:00  lr: 0.001886  loss: 0.5435 (0.5500)  time: 0.5075  data: 0.0001  max mem: 9341
[08:44:33.281504] Epoch: [272] Total time: 0:00:22 (0.5355 s / it)
[08:44:33.283020] Averaged stats: lr: 0.001886  loss: 0.5435 (0.5617)
[08:44:37.815535] {"train_lr": 0.0018878673097960745, "train_loss": 0.5617021325798262, "epoch": 272}
[08:44:37.815890] [08:44:37.815970] Training epoch 272 for 0:00:27
[08:44:37.816021] [08:44:37.820500] log_dir: ./exp/debug/cifar100-LT/debug
[08:44:39.373170] Epoch: [273]  [ 0/42]  eta: 0:01:05  lr: 0.001885  loss: 0.5737 (0.5737)  time: 1.5512  data: 1.0421  max mem: 9341
[08:44:49.532712] Epoch: [273]  [20/42]  eta: 0:00:12  lr: 0.001884  loss: 0.5557 (0.5590)  time: 0.5079  data: 0.0001  max mem: 9341
[08:44:59.684104] Epoch: [273]  [40/42]  eta: 0:00:01  lr: 0.001882  loss: 0.5744 (0.5654)  time: 0.5075  data: 0.0001  max mem: 9341
[08:45:00.190552] Epoch: [273]  [41/42]  eta: 0:00:00  lr: 0.001882  loss: 0.5744 (0.5664)  time: 0.5076  data: 0.0001  max mem: 9341
[08:45:00.353227] Epoch: [273] Total time: 0:00:22 (0.5365 s / it)
[08:45:00.360979] Averaged stats: lr: 0.001882  loss: 0.5744 (0.5609)
[08:45:04.866056] {"train_lr": 0.001883796891571711, "train_loss": 0.5609047008412225, "epoch": 273}
[08:45:04.866335] [08:45:04.866418] Training epoch 273 for 0:00:27
[08:45:04.866469] [08:45:04.871322] log_dir: ./exp/debug/cifar100-LT/debug
[08:45:06.446142] Epoch: [274]  [ 0/42]  eta: 0:01:06  lr: 0.001881  loss: 0.5750 (0.5750)  time: 1.5739  data: 1.0596  max mem: 9341
[08:45:16.636903] Epoch: [274]  [20/42]  eta: 0:00:12  lr: 0.001879  loss: 0.5553 (0.5548)  time: 0.5095  data: 0.0001  max mem: 9341
[08:45:26.809143] Epoch: [274]  [40/42]  eta: 0:00:01  lr: 0.001878  loss: 0.5574 (0.5566)  time: 0.5086  data: 0.0001  max mem: 9341
[08:45:27.315602] Epoch: [274]  [41/42]  eta: 0:00:00  lr: 0.001878  loss: 0.5574 (0.5576)  time: 0.5086  data: 0.0001  max mem: 9341
[08:45:27.480802] Epoch: [274] Total time: 0:00:22 (0.5383 s / it)
[08:45:27.488651] Averaged stats: lr: 0.001878  loss: 0.5574 (0.5622)
[08:45:32.059043] {"train_lr": 0.0018797147891451277, "train_loss": 0.5621701805364518, "epoch": 274}
[08:45:32.059377] [08:45:32.059474] Training epoch 274 for 0:00:27
[08:45:32.059525] [08:45:32.063971] log_dir: ./exp/debug/cifar100-LT/debug
[08:45:33.657001] Epoch: [275]  [ 0/42]  eta: 0:01:06  lr: 0.001877  loss: 0.5817 (0.5817)  time: 1.5920  data: 1.0840  max mem: 9341
[08:45:43.818556] Epoch: [275]  [20/42]  eta: 0:00:12  lr: 0.001875  loss: 0.5596 (0.5641)  time: 0.5080  data: 0.0001  max mem: 9341
[08:45:53.964747] Epoch: [275]  [40/42]  eta: 0:00:01  lr: 0.001874  loss: 0.5596 (0.5609)  time: 0.5073  data: 0.0001  max mem: 9341
[08:45:54.470506] Epoch: [275]  [41/42]  eta: 0:00:00  lr: 0.001874  loss: 0.5596 (0.5608)  time: 0.5073  data: 0.0001  max mem: 9341
[08:45:54.631168] Epoch: [275] Total time: 0:00:22 (0.5373 s / it)
[08:45:54.642559] Averaged stats: lr: 0.001874  loss: 0.5596 (0.5582)
[08:45:59.216530] {"train_lr": 0.001875621072268197, "train_loss": 0.5582288594118187, "epoch": 275}
[08:45:59.216790] [08:45:59.216870] Training epoch 275 for 0:00:27
[08:45:59.216921] [08:45:59.221158] log_dir: ./exp/debug/cifar100-LT/debug
[08:46:00.819870] Epoch: [276]  [ 0/42]  eta: 0:01:07  lr: 0.001873  loss: 0.5928 (0.5928)  time: 1.5978  data: 1.0931  max mem: 9341
[08:46:10.979196] Epoch: [276]  [20/42]  eta: 0:00:12  lr: 0.001871  loss: 0.5634 (0.5594)  time: 0.5079  data: 0.0001  max mem: 9341
[08:46:21.122274] Epoch: [276]  [40/42]  eta: 0:00:01  lr: 0.001870  loss: 0.5631 (0.5637)  time: 0.5071  data: 0.0001  max mem: 9341
[08:46:21.627555] Epoch: [276]  [41/42]  eta: 0:00:00  lr: 0.001870  loss: 0.5589 (0.5625)  time: 0.5071  data: 0.0001  max mem: 9341
[08:46:21.795395] Epoch: [276] Total time: 0:00:22 (0.5375 s / it)
[08:46:21.813746] Averaged stats: lr: 0.001870  loss: 0.5589 (0.5609)
[08:46:26.357248] {"train_lr": 0.0018715158108912403, "train_loss": 0.5608849339187145, "epoch": 276}
[08:46:26.357566] [08:46:26.357648] Training epoch 276 for 0:00:27
[08:46:26.357697] [08:46:26.362113] log_dir: ./exp/debug/cifar100-LT/debug
[08:46:27.838130] Epoch: [277]  [ 0/42]  eta: 0:01:01  lr: 0.001869  loss: 0.5342 (0.5342)  time: 1.4751  data: 0.9688  max mem: 9341
[08:46:38.002337] Epoch: [277]  [20/42]  eta: 0:00:12  lr: 0.001867  loss: 0.5627 (0.5569)  time: 0.5082  data: 0.0001  max mem: 9341
[08:46:48.150857] Epoch: [277]  [40/42]  eta: 0:00:01  lr: 0.001866  loss: 0.5292 (0.5467)  time: 0.5074  data: 0.0001  max mem: 9341
[08:46:48.657440] Epoch: [277]  [41/42]  eta: 0:00:00  lr: 0.001866  loss: 0.5292 (0.5467)  time: 0.5075  data: 0.0001  max mem: 9341
[08:46:48.818757] Epoch: [277] Total time: 0:00:22 (0.5347 s / it)
[08:46:48.820283] Averaged stats: lr: 0.001866  loss: 0.5292 (0.5549)
[08:46:53.457790] {"train_lr": 0.001867399075161854, "train_loss": 0.5548575013166382, "epoch": 277}
[08:46:53.458136] [08:46:53.458222] Training epoch 277 for 0:00:27
[08:46:53.458275] [08:46:53.462602] log_dir: ./exp/debug/cifar100-LT/debug
[08:46:55.106968] Epoch: [278]  [ 0/42]  eta: 0:01:09  lr: 0.001865  loss: 0.4894 (0.4894)  time: 1.6436  data: 1.1352  max mem: 9341
[08:47:05.289371] Epoch: [278]  [20/42]  eta: 0:00:12  lr: 0.001863  loss: 0.5584 (0.5616)  time: 0.5091  data: 0.0001  max mem: 9341
[08:47:15.463133] Epoch: [278]  [40/42]  eta: 0:00:01  lr: 0.001861  loss: 0.5491 (0.5573)  time: 0.5086  data: 0.0001  max mem: 9341
[08:47:15.968790] Epoch: [278]  [41/42]  eta: 0:00:00  lr: 0.001861  loss: 0.5434 (0.5565)  time: 0.5087  data: 0.0001  max mem: 9341
[08:47:16.130122] Epoch: [278] Total time: 0:00:22 (0.5397 s / it)
[08:47:16.132090] Averaged stats: lr: 0.001861  loss: 0.5434 (0.5559)
[08:47:20.728615] {"train_lr": 0.0018632709354236875, "train_loss": 0.5558656505530789, "epoch": 278}
[08:47:20.728871] [08:47:20.728956] Training epoch 278 for 0:00:27
[08:47:20.729026] [08:47:20.733218] log_dir: ./exp/debug/cifar100-LT/debug
[08:47:22.226738] Epoch: [279]  [ 0/42]  eta: 0:01:02  lr: 0.001860  loss: 0.5866 (0.5866)  time: 1.4924  data: 0.9896  max mem: 9341
[08:47:32.387658] Epoch: [279]  [20/42]  eta: 0:00:12  lr: 0.001859  loss: 0.5646 (0.5644)  time: 0.5080  data: 0.0001  max mem: 9341
[08:47:42.528368] Epoch: [279]  [40/42]  eta: 0:00:01  lr: 0.001857  loss: 0.5506 (0.5602)  time: 0.5070  data: 0.0001  max mem: 9341
[08:47:43.034274] Epoch: [279]  [41/42]  eta: 0:00:00  lr: 0.001857  loss: 0.5506 (0.5599)  time: 0.5070  data: 0.0001  max mem: 9341
[08:47:43.203342] Epoch: [279] Total time: 0:00:22 (0.5350 s / it)
[08:47:43.214191] Averaged stats: lr: 0.001857  loss: 0.5506 (0.5571)
[08:47:47.760998] {"train_lr": 0.0018591314622152617, "train_loss": 0.5570702733738082, "epoch": 279}
[08:47:47.761347] [08:47:47.761428] Training epoch 279 for 0:00:27
[08:47:47.761479] [08:47:47.765830] log_dir: ./exp/debug/cifar100-LT/debug
[08:47:49.402850] Epoch: [280]  [ 0/42]  eta: 0:01:08  lr: 0.001856  loss: 0.5368 (0.5368)  time: 1.6362  data: 1.1267  max mem: 9341
[08:47:59.564487] Epoch: [280]  [20/42]  eta: 0:00:12  lr: 0.001855  loss: 0.5466 (0.5475)  time: 0.5080  data: 0.0001  max mem: 9341
[08:48:09.714687] Epoch: [280]  [40/42]  eta: 0:00:01  lr: 0.001853  loss: 0.5555 (0.5521)  time: 0.5075  data: 0.0001  max mem: 9341
[08:48:10.219721] Epoch: [280]  [41/42]  eta: 0:00:00  lr: 0.001853  loss: 0.5555 (0.5533)  time: 0.5075  data: 0.0001  max mem: 9341
[08:48:10.389363] Epoch: [280] Total time: 0:00:22 (0.5387 s / it)
[08:48:10.398568] Averaged stats: lr: 0.001853  loss: 0.5555 (0.5562)
[08:48:14.893073] {"train_lr": 0.0018549807262687546, "train_loss": 0.5561963332196077, "epoch": 280}
[08:48:14.893394] [08:48:14.893477] Training epoch 280 for 0:00:27
[08:48:14.893528] [08:48:14.897903] log_dir: ./exp/debug/cifar100-LT/debug
[08:48:16.521715] Epoch: [281]  [ 0/42]  eta: 0:01:08  lr: 0.001852  loss: 0.5963 (0.5963)  time: 1.6231  data: 1.1286  max mem: 9341
[08:48:26.698007] Epoch: [281]  [20/42]  eta: 0:00:12  lr: 0.001851  loss: 0.5492 (0.5542)  time: 0.5088  data: 0.0001  max mem: 9341
[08:48:36.837758] Epoch: [281]  [40/42]  eta: 0:00:01  lr: 0.001849  loss: 0.5539 (0.5556)  time: 0.5069  data: 0.0001  max mem: 9341
[08:48:37.344237] Epoch: [281]  [41/42]  eta: 0:00:00  lr: 0.001849  loss: 0.5516 (0.5550)  time: 0.5071  data: 0.0001  max mem: 9341
[08:48:37.524805] Epoch: [281] Total time: 0:00:22 (0.5387 s / it)
[08:48:37.525675] Averaged stats: lr: 0.001849  loss: 0.5516 (0.5545)
[08:48:42.103633] {"train_lr": 0.0018508187985087877, "train_loss": 0.5544618242198512, "epoch": 281}
[08:48:42.103953] [08:48:42.104034] Training epoch 281 for 0:00:27
[08:48:42.104130] [08:48:42.108558] log_dir: ./exp/debug/cifar100-LT/debug
[08:48:43.545452] Epoch: [282]  [ 0/42]  eta: 0:01:00  lr: 0.001848  loss: 0.5743 (0.5743)  time: 1.4359  data: 0.9303  max mem: 9341
[08:48:53.704189] Epoch: [282]  [20/42]  eta: 0:00:12  lr: 0.001846  loss: 0.5628 (0.5591)  time: 0.5079  data: 0.0001  max mem: 9341
[08:49:03.852150] Epoch: [282]  [40/42]  eta: 0:00:01  lr: 0.001845  loss: 0.5552 (0.5583)  time: 0.5074  data: 0.0001  max mem: 9341
[08:49:04.358753] Epoch: [282]  [41/42]  eta: 0:00:00  lr: 0.001845  loss: 0.5588 (0.5588)  time: 0.5074  data: 0.0001  max mem: 9341
[08:49:04.528644] Epoch: [282] Total time: 0:00:22 (0.5338 s / it)
[08:49:04.531354] Averaged stats: lr: 0.001845  loss: 0.5588 (0.5554)
[08:49:09.064381] {"train_lr": 0.0018466457500512217, "train_loss": 0.555431238242558, "epoch": 282}
[08:49:09.064725] [08:49:09.064804] Training epoch 282 for 0:00:26
[08:49:09.064855] [08:49:09.069584] log_dir: ./exp/debug/cifar100-LT/debug
[08:49:10.496353] Epoch: [283]  [ 0/42]  eta: 0:00:59  lr: 0.001844  loss: 0.5969 (0.5969)  time: 1.4215  data: 0.9174  max mem: 9341
[08:49:20.653513] Epoch: [283]  [20/42]  eta: 0:00:12  lr: 0.001842  loss: 0.5440 (0.5423)  time: 0.5078  data: 0.0002  max mem: 9341
[08:49:30.803610] Epoch: [283]  [40/42]  eta: 0:00:01  lr: 0.001841  loss: 0.5572 (0.5494)  time: 0.5075  data: 0.0001  max mem: 9341
[08:49:31.308776] Epoch: [283]  [41/42]  eta: 0:00:00  lr: 0.001841  loss: 0.5572 (0.5478)  time: 0.5074  data: 0.0001  max mem: 9341
[08:49:31.470579] Epoch: [283] Total time: 0:00:22 (0.5334 s / it)
[08:49:31.479710] Averaged stats: lr: 0.001841  loss: 0.5572 (0.5524)
[08:49:35.941277] {"train_lr": 0.0018424616522019432, "train_loss": 0.5523779122602372, "epoch": 283}
[08:49:35.941539] [08:49:35.941617] Training epoch 283 for 0:00:26
[08:49:35.941669] [08:49:35.945969] log_dir: ./exp/debug/cifar100-LT/debug
[08:49:37.501281] Epoch: [284]  [ 0/42]  eta: 0:01:05  lr: 0.001840  loss: 0.5297 (0.5297)  time: 1.5546  data: 1.0496  max mem: 9341
[08:49:47.657924] Epoch: [284]  [20/42]  eta: 0:00:12  lr: 0.001838  loss: 0.5539 (0.5516)  time: 0.5078  data: 0.0001  max mem: 9341
[08:49:57.799035] Epoch: [284]  [40/42]  eta: 0:00:01  lr: 0.001836  loss: 0.5511 (0.5512)  time: 0.5070  data: 0.0001  max mem: 9341
[08:49:58.305712] Epoch: [284]  [41/42]  eta: 0:00:00  lr: 0.001836  loss: 0.5511 (0.5519)  time: 0.5071  data: 0.0001  max mem: 9341
[08:49:58.468473] Epoch: [284] Total time: 0:00:22 (0.5362 s / it)
[08:49:58.480997] Averaged stats: lr: 0.001836  loss: 0.5511 (0.5542)
[08:50:03.144849] {"train_lr": 0.001838266576455644, "train_loss": 0.5541621907835915, "epoch": 284}
[08:50:03.145097] [08:50:03.145175] Training epoch 284 for 0:00:27
[08:50:03.145227] [08:50:03.149586] log_dir: ./exp/debug/cifar100-LT/debug
[08:50:04.608942] Epoch: [285]  [ 0/42]  eta: 0:01:01  lr: 0.001835  loss: 0.5231 (0.5231)  time: 1.4583  data: 0.9472  max mem: 9341
[08:50:14.773806] Epoch: [285]  [20/42]  eta: 0:00:12  lr: 0.001834  loss: 0.5459 (0.5536)  time: 0.5082  data: 0.0001  max mem: 9341
[08:50:24.931332] Epoch: [285]  [40/42]  eta: 0:00:01  lr: 0.001832  loss: 0.5456 (0.5510)  time: 0.5078  data: 0.0001  max mem: 9341
[08:50:25.436653] Epoch: [285]  [41/42]  eta: 0:00:00  lr: 0.001832  loss: 0.5456 (0.5513)  time: 0.5076  data: 0.0001  max mem: 9341
[08:50:25.600045] Epoch: [285] Total time: 0:00:22 (0.5345 s / it)
[08:50:25.605863] Averaged stats: lr: 0.001832  loss: 0.5456 (0.5520)
[08:50:30.224848] {"train_lr": 0.0018340605944945898, "train_loss": 0.5519896788256509, "epoch": 285}
[08:50:30.225179] [08:50:30.225260] Training epoch 285 for 0:00:27
[08:50:30.225312] [08:50:30.229830] log_dir: ./exp/debug/cifar100-LT/debug
[08:50:31.907615] Epoch: [286]  [ 0/42]  eta: 0:01:10  lr: 0.001831  loss: 0.5447 (0.5447)  time: 1.6770  data: 1.1667  max mem: 9341
[08:50:42.074504] Epoch: [286]  [20/42]  eta: 0:00:12  lr: 0.001830  loss: 0.5663 (0.5584)  time: 0.5083  data: 0.0001  max mem: 9341
[08:50:52.226304] Epoch: [286]  [40/42]  eta: 0:00:01  lr: 0.001828  loss: 0.5475 (0.5572)  time: 0.5075  data: 0.0001  max mem: 9341
[08:50:52.731508] Epoch: [286]  [41/42]  eta: 0:00:00  lr: 0.001828  loss: 0.5487 (0.5580)  time: 0.5075  data: 0.0001  max mem: 9341
[08:50:52.897749] Epoch: [286] Total time: 0:00:22 (0.5397 s / it)
[08:50:52.918830] Averaged stats: lr: 0.001828  loss: 0.5487 (0.5539)
[08:50:57.408682] {"train_lr": 0.0018298437781874093, "train_loss": 0.5538977794349194, "epoch": 286}
[08:50:57.409015] [08:50:57.409101] Training epoch 286 for 0:00:27
[08:50:57.409152] [08:50:57.413613] log_dir: ./exp/debug/cifar100-LT/debug
[08:50:58.975167] Epoch: [287]  [ 0/42]  eta: 0:01:05  lr: 0.001827  loss: 0.5310 (0.5310)  time: 1.5603  data: 1.0544  max mem: 9341
[08:51:09.134786] Epoch: [287]  [20/42]  eta: 0:00:12  lr: 0.001825  loss: 0.5599 (0.5592)  time: 0.5079  data: 0.0001  max mem: 9341
[08:51:19.289289] Epoch: [287]  [40/42]  eta: 0:00:01  lr: 0.001824  loss: 0.5460 (0.5515)  time: 0.5077  data: 0.0001  max mem: 9341
[08:51:19.793791] Epoch: [287]  [41/42]  eta: 0:00:00  lr: 0.001824  loss: 0.5460 (0.5517)  time: 0.5075  data: 0.0001  max mem: 9341
[08:51:19.956361] Epoch: [287] Total time: 0:00:22 (0.5367 s / it)
[08:51:19.957222] Averaged stats: lr: 0.001824  loss: 0.5460 (0.5517)
[08:51:24.743853] {"train_lr": 0.0018256161995878566, "train_loss": 0.5516583089317594, "epoch": 287}
[08:51:24.744199] [08:51:24.744289] Training epoch 287 for 0:00:27
[08:51:24.744343] [08:51:24.749485] log_dir: ./exp/debug/cifar100-LT/debug
[08:51:26.327400] Epoch: [288]  [ 0/42]  eta: 0:01:06  lr: 0.001823  loss: 0.6029 (0.6029)  time: 1.5769  data: 1.0655  max mem: 9341
[08:51:36.489423] Epoch: [288]  [20/42]  eta: 0:00:12  lr: 0.001821  loss: 0.5548 (0.5556)  time: 0.5081  data: 0.0001  max mem: 9341
[08:51:46.643241] Epoch: [288]  [40/42]  eta: 0:00:01  lr: 0.001820  loss: 0.5555 (0.5563)  time: 0.5076  data: 0.0001  max mem: 9341
[08:51:47.149396] Epoch: [288]  [41/42]  eta: 0:00:00  lr: 0.001820  loss: 0.5555 (0.5559)  time: 0.5077  data: 0.0001  max mem: 9341
[08:51:47.319374] Epoch: [288] Total time: 0:00:22 (0.5374 s / it)
[08:51:47.328055] Averaged stats: lr: 0.001820  loss: 0.5555 (0.5531)
[08:51:51.909636] {"train_lr": 0.0018213779309335845, "train_loss": 0.5531171923946767, "epoch": 288}
[08:51:51.909924] [08:51:51.910009] Training epoch 288 for 0:00:27
[08:51:51.910059] [08:51:51.914567] log_dir: ./exp/debug/cifar100-LT/debug
[08:51:53.454468] Epoch: [289]  [ 0/42]  eta: 0:01:04  lr: 0.001819  loss: 0.5154 (0.5154)  time: 1.5390  data: 1.0345  max mem: 9341
[08:52:03.623238] Epoch: [289]  [20/42]  eta: 0:00:12  lr: 0.001817  loss: 0.5507 (0.5511)  time: 0.5084  data: 0.0001  max mem: 9341
[08:52:13.771467] Epoch: [289]  [40/42]  eta: 0:00:01  lr: 0.001815  loss: 0.5503 (0.5507)  time: 0.5074  data: 0.0001  max mem: 9341
[08:52:14.277993] Epoch: [289]  [41/42]  eta: 0:00:00  lr: 0.001815  loss: 0.5503 (0.5508)  time: 0.5074  data: 0.0001  max mem: 9341
[08:52:14.464503] Epoch: [289] Total time: 0:00:22 (0.5369 s / it)
[08:52:14.470278] Averaged stats: lr: 0.001815  loss: 0.5503 (0.5521)
[08:52:19.058253] {"train_lr": 0.0018171290446449154, "train_loss": 0.552106643893889, "epoch": 289}
[08:52:19.058576] [08:52:19.058658] Training epoch 289 for 0:00:27
[08:52:19.058710] [08:52:19.063068] log_dir: ./exp/debug/cifar100-LT/debug
[08:52:20.479266] Epoch: [290]  [ 0/42]  eta: 0:00:59  lr: 0.001814  loss: 0.5564 (0.5564)  time: 1.4149  data: 0.9064  max mem: 9341
[08:52:30.641306] Epoch: [290]  [20/42]  eta: 0:00:12  lr: 0.001813  loss: 0.5595 (0.5593)  time: 0.5081  data: 0.0001  max mem: 9341
[08:52:40.788197] Epoch: [290]  [40/42]  eta: 0:00:01  lr: 0.001811  loss: 0.5585 (0.5579)  time: 0.5073  data: 0.0001  max mem: 9341
[08:52:41.293747] Epoch: [290]  [41/42]  eta: 0:00:00  lr: 0.001811  loss: 0.5585 (0.5593)  time: 0.5072  data: 0.0001  max mem: 9341
[08:52:41.461866] Epoch: [290] Total time: 0:00:22 (0.5333 s / it)
[08:52:41.462560] Averaged stats: lr: 0.001811  loss: 0.5585 (0.5512)
[08:52:46.009207] {"train_lr": 0.0018128696133235924, "train_loss": 0.5511518546513149, "epoch": 290}
[08:52:46.009594] [08:52:46.009680] Training epoch 290 for 0:00:26
[08:52:46.009731] [08:52:46.014166] log_dir: ./exp/debug/cifar100-LT/debug
[08:52:47.584185] Epoch: [291]  [ 0/42]  eta: 0:01:05  lr: 0.001810  loss: 0.5935 (0.5935)  time: 1.5688  data: 1.0614  max mem: 9341
[08:52:57.742674] Epoch: [291]  [20/42]  eta: 0:00:12  lr: 0.001808  loss: 0.5615 (0.5618)  time: 0.5079  data: 0.0001  max mem: 9341
[08:53:07.895840] Epoch: [291]  [40/42]  eta: 0:00:01  lr: 0.001807  loss: 0.5448 (0.5572)  time: 0.5076  data: 0.0001  max mem: 9341
[08:53:08.400480] Epoch: [291]  [41/42]  eta: 0:00:00  lr: 0.001807  loss: 0.5448 (0.5564)  time: 0.5076  data: 0.0001  max mem: 9341
[08:53:08.569462] Epoch: [291] Total time: 0:00:22 (0.5370 s / it)
[08:53:08.570168] Averaged stats: lr: 0.001807  loss: 0.5448 (0.5580)
[08:53:13.175556] {"train_lr": 0.0018085997097515383, "train_loss": 0.5580092585157781, "epoch": 291}
[08:53:13.175805] [08:53:13.175885] Training epoch 291 for 0:00:27
[08:53:13.175935] [08:53:13.180375] log_dir: ./exp/debug/cifar100-LT/debug
[08:53:14.760233] Epoch: [292]  [ 0/42]  eta: 0:01:06  lr: 0.001806  loss: 0.5628 (0.5628)  time: 1.5790  data: 1.0766  max mem: 9341
[08:53:24.925961] Epoch: [292]  [20/42]  eta: 0:00:12  lr: 0.001804  loss: 0.5542 (0.5529)  time: 0.5082  data: 0.0001  max mem: 9341
[08:53:35.075154] Epoch: [292]  [40/42]  eta: 0:00:01  lr: 0.001802  loss: 0.5534 (0.5546)  time: 0.5074  data: 0.0001  max mem: 9341
[08:53:35.579590] Epoch: [292]  [41/42]  eta: 0:00:00  lr: 0.001802  loss: 0.5534 (0.5546)  time: 0.5073  data: 0.0001  max mem: 9341
[08:53:35.758277] Epoch: [292] Total time: 0:00:22 (0.5376 s / it)
[08:53:35.763434] Averaged stats: lr: 0.001802  loss: 0.5534 (0.5570)
[08:53:40.382404] {"train_lr": 0.00180431940688963, "train_loss": 0.5570018307438919, "epoch": 292}
[08:53:40.382694] [08:53:40.382780] Training epoch 292 for 0:00:27
[08:53:40.382832] [08:53:40.387394] log_dir: ./exp/debug/cifar100-LT/debug
[08:53:41.859171] Epoch: [293]  [ 0/42]  eta: 0:01:01  lr: 0.001801  loss: 0.5645 (0.5645)  time: 1.4710  data: 0.9706  max mem: 9341
[08:53:52.039363] Epoch: [293]  [20/42]  eta: 0:00:12  lr: 0.001800  loss: 0.5498 (0.5495)  time: 0.5090  data: 0.0001  max mem: 9341
[08:54:02.213954] Epoch: [293]  [40/42]  eta: 0:00:01  lr: 0.001798  loss: 0.5477 (0.5490)  time: 0.5087  data: 0.0001  max mem: 9341
[08:54:02.720343] Epoch: [293]  [41/42]  eta: 0:00:00  lr: 0.001798  loss: 0.5477 (0.5489)  time: 0.5087  data: 0.0001  max mem: 9341
[08:54:02.890806] Epoch: [293] Total time: 0:00:22 (0.5358 s / it)
[08:54:02.901071] Averaged stats: lr: 0.001798  loss: 0.5477 (0.5525)
[08:54:07.470164] {"train_lr": 0.0018000287778764297, "train_loss": 0.552504317746276, "epoch": 293}
[08:54:07.470531] [08:54:07.470631] Training epoch 293 for 0:00:27
[08:54:07.470684] [08:54:07.475041] log_dir: ./exp/debug/cifar100-LT/debug
[08:54:08.952406] Epoch: [294]  [ 0/42]  eta: 0:01:02  lr: 0.001797  loss: 0.5563 (0.5563)  time: 1.4764  data: 0.9676  max mem: 9341
[08:54:19.121175] Epoch: [294]  [20/42]  eta: 0:00:12  lr: 0.001795  loss: 0.5540 (0.5561)  time: 0.5084  data: 0.0001  max mem: 9341
[08:54:29.259248] Epoch: [294]  [40/42]  eta: 0:00:01  lr: 0.001794  loss: 0.5650 (0.5607)  time: 0.5069  data: 0.0001  max mem: 9341
[08:54:29.764055] Epoch: [294]  [41/42]  eta: 0:00:00  lr: 0.001794  loss: 0.5650 (0.5599)  time: 0.5068  data: 0.0001  max mem: 9341
[08:54:29.930110] Epoch: [294] Total time: 0:00:22 (0.5346 s / it)
[08:54:29.936131] Averaged stats: lr: 0.001794  loss: 0.5650 (0.5538)
[08:54:34.414977] {"train_lr": 0.0017957278960269547, "train_loss": 0.5537781318028768, "epoch": 294}
[08:54:34.415399] [08:54:34.415494] Training epoch 294 for 0:00:26
[08:54:34.415548] [08:54:34.420479] log_dir: ./exp/debug/cifar100-LT/debug
[08:54:35.962911] Epoch: [295]  [ 0/42]  eta: 0:01:04  lr: 0.001793  loss: 0.5783 (0.5783)  time: 1.5412  data: 1.0446  max mem: 9341
[08:54:46.126494] Epoch: [295]  [20/42]  eta: 0:00:12  lr: 0.001791  loss: 0.5490 (0.5520)  time: 0.5081  data: 0.0001  max mem: 9341
[08:54:56.278648] Epoch: [295]  [40/42]  eta: 0:00:01  lr: 0.001790  loss: 0.5476 (0.5524)  time: 0.5076  data: 0.0001  max mem: 9341
[08:54:56.783680] Epoch: [295]  [41/42]  eta: 0:00:00  lr: 0.001790  loss: 0.5512 (0.5526)  time: 0.5075  data: 0.0001  max mem: 9341
[08:54:56.946892] Epoch: [295] Total time: 0:00:22 (0.5363 s / it)
[08:54:56.948341] Averaged stats: lr: 0.001790  loss: 0.5512 (0.5518)
[08:55:01.507043] {"train_lr": 0.0017914168348314044, "train_loss": 0.5517556548473381, "epoch": 295}
[08:55:01.507359] [08:55:01.507437] Training epoch 295 for 0:00:27
[08:55:01.507488] [08:55:01.511749] log_dir: ./exp/debug/cifar100-LT/debug
[08:55:02.952498] Epoch: [296]  [ 0/42]  eta: 0:01:00  lr: 0.001789  loss: 0.5552 (0.5552)  time: 1.4395  data: 0.9274  max mem: 9341
[08:55:13.110679] Epoch: [296]  [20/42]  eta: 0:00:12  lr: 0.001787  loss: 0.5553 (0.5539)  time: 0.5079  data: 0.0001  max mem: 9341
[08:55:23.254789] Epoch: [296]  [40/42]  eta: 0:00:01  lr: 0.001785  loss: 0.5389 (0.5481)  time: 0.5072  data: 0.0001  max mem: 9341
[08:55:23.760577] Epoch: [296]  [41/42]  eta: 0:00:00  lr: 0.001785  loss: 0.5389 (0.5481)  time: 0.5072  data: 0.0001  max mem: 9341
[08:55:23.921038] Epoch: [296] Total time: 0:00:22 (0.5336 s / it)
[08:55:23.921725] Averaged stats: lr: 0.001785  loss: 0.5389 (0.5494)
[08:55:28.432751] {"train_lr": 0.0017870956679539214, "train_loss": 0.549445015866132, "epoch": 296}
[08:55:28.433104] [08:55:28.433186] Training epoch 296 for 0:00:26
[08:55:28.433238] [08:55:28.437674] log_dir: ./exp/debug/cifar100-LT/debug
[08:55:30.031545] Epoch: [297]  [ 0/42]  eta: 0:01:06  lr: 0.001784  loss: 0.5366 (0.5366)  time: 1.5929  data: 1.0900  max mem: 9341
[08:55:40.192313] Epoch: [297]  [20/42]  eta: 0:00:12  lr: 0.001783  loss: 0.5532 (0.5522)  time: 0.5080  data: 0.0001  max mem: 9341
[08:55:50.345204] Epoch: [297]  [40/42]  eta: 0:00:01  lr: 0.001781  loss: 0.5447 (0.5491)  time: 0.5076  data: 0.0001  max mem: 9341
[08:55:50.851104] Epoch: [297]  [41/42]  eta: 0:00:00  lr: 0.001781  loss: 0.5493 (0.5491)  time: 0.5076  data: 0.0001  max mem: 9341
[08:55:51.024268] Epoch: [297] Total time: 0:00:22 (0.5378 s / it)
[08:55:51.028268] Averaged stats: lr: 0.001781  loss: 0.5493 (0.5506)
[08:55:55.630323] {"train_lr": 0.0017827644692313249, "train_loss": 0.5505978944046157, "epoch": 297}
[08:55:55.630635] [08:55:55.630714] Training epoch 297 for 0:00:27
[08:55:55.630763] [08:55:55.635145] log_dir: ./exp/debug/cifar100-LT/debug
[08:55:57.114587] Epoch: [298]  [ 0/42]  eta: 0:01:02  lr: 0.001780  loss: 0.5556 (0.5556)  time: 1.4785  data: 0.9667  max mem: 9341
[08:56:07.275578] Epoch: [298]  [20/42]  eta: 0:00:12  lr: 0.001778  loss: 0.5216 (0.5314)  time: 0.5080  data: 0.0001  max mem: 9341
[08:56:17.418728] Epoch: [298]  [40/42]  eta: 0:00:01  lr: 0.001777  loss: 0.5670 (0.5432)  time: 0.5071  data: 0.0001  max mem: 9341
[08:56:17.924736] Epoch: [298]  [41/42]  eta: 0:00:00  lr: 0.001777  loss: 0.5670 (0.5432)  time: 0.5071  data: 0.0001  max mem: 9341
[08:56:18.092760] Epoch: [298] Total time: 0:00:22 (0.5347 s / it)
[08:56:18.096371] Averaged stats: lr: 0.001777  loss: 0.5670 (0.5479)
[08:56:22.624360] {"train_lr": 0.0017784233126718516, "train_loss": 0.5478802059023153, "epoch": 298}
[08:56:22.624760] [08:56:22.624850] Training epoch 298 for 0:00:26
[08:56:22.624903] [08:56:22.629405] log_dir: ./exp/debug/cifar100-LT/debug
[08:56:24.290993] Epoch: [299]  [ 0/42]  eta: 0:01:09  lr: 0.001775  loss: 0.5632 (0.5632)  time: 1.6608  data: 1.1548  max mem: 9341
[08:56:34.462505] Epoch: [299]  [20/42]  eta: 0:00:12  lr: 0.001774  loss: 0.5341 (0.5407)  time: 0.5085  data: 0.0001  max mem: 9341
[08:56:44.616183] Epoch: [299]  [40/42]  eta: 0:00:01  lr: 0.001772  loss: 0.5398 (0.5397)  time: 0.5076  data: 0.0001  max mem: 9341
[08:56:45.121677] Epoch: [299]  [41/42]  eta: 0:00:00  lr: 0.001772  loss: 0.5398 (0.5384)  time: 0.5076  data: 0.0001  max mem: 9341
[08:56:45.293013] Epoch: [299] Total time: 0:00:22 (0.5396 s / it)
[08:56:45.296793] Averaged stats: lr: 0.001772  loss: 0.5398 (0.5452)
[08:56:49.769626] {"train_lr": 0.0017740722724538904, "train_loss": 0.5452399936815103, "epoch": 299}
[08:56:49.769888] [08:56:49.769968] Training epoch 299 for 0:00:27
[08:56:49.770019] [08:56:49.774419] log_dir: ./exp/debug/cifar100-LT/debug
[08:56:51.423822] Epoch: [300]  [ 0/42]  eta: 0:01:09  lr: 0.001771  loss: 0.5991 (0.5991)  time: 1.6485  data: 1.1395  max mem: 9341
[08:57:01.599737] Epoch: [300]  [20/42]  eta: 0:00:12  lr: 0.001769  loss: 0.5445 (0.5510)  time: 0.5088  data: 0.0001  max mem: 9341
[08:57:11.747702] Epoch: [300]  [40/42]  eta: 0:00:01  lr: 0.001768  loss: 0.5556 (0.5539)  time: 0.5074  data: 0.0001  max mem: 9341
[08:57:12.253586] Epoch: [300]  [41/42]  eta: 0:00:00  lr: 0.001768  loss: 0.5620 (0.5545)  time: 0.5073  data: 0.0001  max mem: 9341
[08:57:12.411747] Epoch: [300] Total time: 0:00:22 (0.5390 s / it)
[08:57:12.434458] Averaged stats: lr: 0.001768  loss: 0.5620 (0.5487)
[08:57:17.158147] {"train_lr": 0.001769711422924709, "train_loss": 0.5487330830877736, "epoch": 300}
[08:57:17.158485] [08:57:17.158583] Training epoch 300 for 0:00:27
[08:57:17.158635] [08:57:17.163219] log_dir: ./exp/debug/cifar100-LT/debug
[08:57:18.670928] Epoch: [301]  [ 0/42]  eta: 0:01:03  lr: 0.001767  loss: 0.5540 (0.5540)  time: 1.5065  data: 1.0001  max mem: 9341
[08:57:28.839139] Epoch: [301]  [20/42]  eta: 0:00:12  lr: 0.001765  loss: 0.5551 (0.5560)  time: 0.5084  data: 0.0001  max mem: 9341
[08:57:38.988853] Epoch: [301]  [40/42]  eta: 0:00:01  lr: 0.001763  loss: 0.5386 (0.5511)  time: 0.5074  data: 0.0001  max mem: 9341
[08:57:39.495502] Epoch: [301]  [41/42]  eta: 0:00:00  lr: 0.001763  loss: 0.5433 (0.5513)  time: 0.5074  data: 0.0001  max mem: 9341
[08:57:39.665226] Epoch: [301] Total time: 0:00:22 (0.5358 s / it)
[08:57:39.666014] Averaged stats: lr: 0.001763  loss: 0.5433 (0.5522)
[08:57:44.279392] {"train_lr": 0.0017653408385991986, "train_loss": 0.5522358669411569, "epoch": 301}
[08:57:44.279743] [08:57:44.279841] Training epoch 301 for 0:00:27
[08:57:44.279892] [08:57:44.284382] log_dir: ./exp/debug/cifar100-LT/debug
[08:57:45.782328] Epoch: [302]  [ 0/42]  eta: 0:01:02  lr: 0.001762  loss: 0.5393 (0.5393)  time: 1.4968  data: 0.9924  max mem: 9341
[08:57:55.956866] Epoch: [302]  [20/42]  eta: 0:00:12  lr: 0.001761  loss: 0.5499 (0.5501)  time: 0.5087  data: 0.0001  max mem: 9341
[08:58:06.119415] Epoch: [302]  [40/42]  eta: 0:00:01  lr: 0.001759  loss: 0.5497 (0.5493)  time: 0.5081  data: 0.0001  max mem: 9341
[08:58:06.625038] Epoch: [302]  [41/42]  eta: 0:00:00  lr: 0.001759  loss: 0.5497 (0.5487)  time: 0.5081  data: 0.0001  max mem: 9341
[08:58:06.788423] Epoch: [302] Total time: 0:00:22 (0.5358 s / it)
[08:58:06.792930] Averaged stats: lr: 0.001759  loss: 0.5497 (0.5456)
[08:58:11.225160] {"train_lr": 0.0017609605941585834, "train_loss": 0.5456117395134199, "epoch": 302}
[08:58:11.225500] [08:58:11.225582] Training epoch 302 for 0:00:26
[08:58:11.225631] [08:58:11.229998] log_dir: ./exp/debug/cifar100-LT/debug
[08:58:12.803796] Epoch: [303]  [ 0/42]  eta: 0:01:06  lr: 0.001758  loss: 0.5820 (0.5820)  time: 1.5726  data: 1.0584  max mem: 9341
[08:58:22.969777] Epoch: [303]  [20/42]  eta: 0:00:12  lr: 0.001756  loss: 0.5267 (0.5348)  time: 0.5083  data: 0.0001  max mem: 9341
[08:58:33.115430] Epoch: [303]  [40/42]  eta: 0:00:01  lr: 0.001755  loss: 0.5341 (0.5418)  time: 0.5072  data: 0.0001  max mem: 9341
[08:58:33.621176] Epoch: [303]  [41/42]  eta: 0:00:00  lr: 0.001755  loss: 0.5341 (0.5416)  time: 0.5073  data: 0.0001  max mem: 9341
[08:58:33.767726] Epoch: [303] Total time: 0:00:22 (0.5366 s / it)
[08:58:33.790975] Averaged stats: lr: 0.001755  loss: 0.5341 (0.5440)
[08:58:38.284198] {"train_lr": 0.0017565707644491596, "train_loss": 0.544015992965017, "epoch": 303}
[08:58:38.284540] [08:58:38.284617] Training epoch 303 for 0:00:27
[08:58:38.284667] [08:58:38.288974] log_dir: ./exp/debug/cifar100-LT/debug
[08:58:39.774091] Epoch: [304]  [ 0/42]  eta: 0:01:02  lr: 0.001754  loss: 0.5465 (0.5465)  time: 1.4842  data: 0.9904  max mem: 9341
[08:58:49.940242] Epoch: [304]  [20/42]  eta: 0:00:12  lr: 0.001752  loss: 0.5456 (0.5465)  time: 0.5083  data: 0.0001  max mem: 9341
[08:59:00.089297] Epoch: [304]  [40/42]  eta: 0:00:01  lr: 0.001750  loss: 0.5520 (0.5499)  time: 0.5074  data: 0.0001  max mem: 9341
[08:59:00.594752] Epoch: [304]  [41/42]  eta: 0:00:00  lr: 0.001750  loss: 0.5514 (0.5494)  time: 0.5074  data: 0.0001  max mem: 9341
[08:59:00.750175] Epoch: [304] Total time: 0:00:22 (0.5348 s / it)
[08:59:00.755698] Averaged stats: lr: 0.001750  loss: 0.5514 (0.5491)
[08:59:05.346729] {"train_lr": 0.0017521714244810027, "train_loss": 0.5490963026171639, "epoch": 304}
[08:59:05.346979] [08:59:05.347061] Training epoch 304 for 0:00:27
[08:59:05.347112] [08:59:05.351453] log_dir: ./exp/debug/cifar100-LT/debug
[08:59:06.995503] Epoch: [305]  [ 0/42]  eta: 0:01:09  lr: 0.001749  loss: 0.5083 (0.5083)  time: 1.6430  data: 1.1373  max mem: 9341
[08:59:17.179542] Epoch: [305]  [20/42]  eta: 0:00:12  lr: 0.001748  loss: 0.5475 (0.5434)  time: 0.5091  data: 0.0002  max mem: 9341
[08:59:27.347091] Epoch: [305]  [40/42]  eta: 0:00:01  lr: 0.001746  loss: 0.5420 (0.5448)  time: 0.5083  data: 0.0001  max mem: 9341
[08:59:27.852084] Epoch: [305]  [41/42]  eta: 0:00:00  lr: 0.001746  loss: 0.5420 (0.5442)  time: 0.5082  data: 0.0001  max mem: 9341
[08:59:28.020219] Epoch: [305] Total time: 0:00:22 (0.5397 s / it)
[08:59:28.038134] Averaged stats: lr: 0.001746  loss: 0.5420 (0.5416)
[08:59:32.578909] {"train_lr": 0.001747762649426691, "train_loss": 0.5416433351735274, "epoch": 305}
[08:59:32.579242] [08:59:32.579323] Training epoch 305 for 0:00:27
[08:59:32.579374] [08:59:32.585945] log_dir: ./exp/debug/cifar100-LT/debug
[08:59:34.193698] Epoch: [306]  [ 0/42]  eta: 0:01:07  lr: 0.001745  loss: 0.5285 (0.5285)  time: 1.6070  data: 1.1077  max mem: 9341
[08:59:44.357068] Epoch: [306]  [20/42]  eta: 0:00:12  lr: 0.001743  loss: 0.5304 (0.5354)  time: 0.5081  data: 0.0001  max mem: 9341
[08:59:54.505536] Epoch: [306]  [40/42]  eta: 0:00:01  lr: 0.001741  loss: 0.5464 (0.5407)  time: 0.5074  data: 0.0001  max mem: 9341
[08:59:55.013231] Epoch: [306]  [41/42]  eta: 0:00:00  lr: 0.001741  loss: 0.5491 (0.5412)  time: 0.5075  data: 0.0001  max mem: 9341
[08:59:55.174189] Epoch: [306] Total time: 0:00:22 (0.5378 s / it)
[08:59:55.190651] Averaged stats: lr: 0.001741  loss: 0.5491 (0.5418)
[08:59:59.818389] {"train_lr": 0.0017433445146200317, "train_loss": 0.5417970521819024, "epoch": 306}
[08:59:59.818646] [08:59:59.818728] Training epoch 306 for 0:00:27
[08:59:59.818777] [08:59:59.823982] log_dir: ./exp/debug/cifar100-LT/debug
[09:00:01.452012] Epoch: [307]  [ 0/42]  eta: 0:01:08  lr: 0.001740  loss: 0.5089 (0.5089)  time: 1.6270  data: 1.1119  max mem: 9341
[09:00:11.612652] Epoch: [307]  [20/42]  eta: 0:00:12  lr: 0.001739  loss: 0.5498 (0.5499)  time: 0.5080  data: 0.0001  max mem: 9341
[09:00:21.764898] Epoch: [307]  [40/42]  eta: 0:00:01  lr: 0.001737  loss: 0.5421 (0.5496)  time: 0.5076  data: 0.0001  max mem: 9341
[09:00:22.270268] Epoch: [307]  [41/42]  eta: 0:00:00  lr: 0.001737  loss: 0.5421 (0.5505)  time: 0.5076  data: 0.0001  max mem: 9341
[09:00:22.447251] Epoch: [307] Total time: 0:00:22 (0.5386 s / it)
[09:00:22.451426] Averaged stats: lr: 0.001737  loss: 0.5421 (0.5432)
[09:00:27.027513] {"train_lr": 0.0017389170955547557, "train_loss": 0.5431574022486096, "epoch": 307}
[09:00:27.027917] [09:00:27.028025] Training epoch 307 for 0:00:27
[09:00:27.028127] [09:00:27.032363] log_dir: ./exp/debug/cifar100-LT/debug
[09:00:28.694601] Epoch: [308]  [ 0/42]  eta: 0:01:09  lr: 0.001736  loss: 0.5620 (0.5620)  time: 1.6615  data: 1.1611  max mem: 9341
[09:00:38.859961] Epoch: [308]  [20/42]  eta: 0:00:12  lr: 0.001734  loss: 0.5535 (0.5543)  time: 0.5082  data: 0.0001  max mem: 9341
[09:00:49.010911] Epoch: [308]  [40/42]  eta: 0:00:01  lr: 0.001733  loss: 0.5461 (0.5514)  time: 0.5075  data: 0.0001  max mem: 9341
[09:00:49.516061] Epoch: [308]  [41/42]  eta: 0:00:00  lr: 0.001733  loss: 0.5491 (0.5514)  time: 0.5075  data: 0.0001  max mem: 9341
[09:00:49.681085] Epoch: [308] Total time: 0:00:22 (0.5393 s / it)
[09:00:49.685760] Averaged stats: lr: 0.001733  loss: 0.5491 (0.5497)
[09:00:54.346499] {"train_lr": 0.0017344804678832387, "train_loss": 0.549668842837924, "epoch": 308}
[09:00:54.346859] [09:00:54.346944] Training epoch 308 for 0:00:27
[09:00:54.346997] [09:00:54.351240] log_dir: ./exp/debug/cifar100-LT/debug
[09:00:55.789834] Epoch: [309]  [ 0/42]  eta: 0:01:00  lr: 0.001731  loss: 0.5197 (0.5197)  time: 1.4372  data: 0.9177  max mem: 9341
[09:01:05.947107] Epoch: [309]  [20/42]  eta: 0:00:12  lr: 0.001730  loss: 0.5463 (0.5436)  time: 0.5078  data: 0.0001  max mem: 9341
[09:01:16.085013] Epoch: [309]  [40/42]  eta: 0:00:01  lr: 0.001728  loss: 0.5392 (0.5416)  time: 0.5069  data: 0.0001  max mem: 9341
[09:01:16.590149] Epoch: [309]  [41/42]  eta: 0:00:00  lr: 0.001728  loss: 0.5392 (0.5426)  time: 0.5069  data: 0.0001  max mem: 9341
[09:01:16.771005] Epoch: [309] Total time: 0:00:22 (0.5338 s / it)
[09:01:16.777929] Averaged stats: lr: 0.001728  loss: 0.5392 (0.5470)
[09:01:21.312355] {"train_lr": 0.0017300347074152077, "train_loss": 0.5470062831328029, "epoch": 309}
[09:01:21.312704] [09:01:21.312784] Training epoch 309 for 0:00:26
[09:01:21.312837] [09:01:21.317701] log_dir: ./exp/debug/cifar100-LT/debug
[09:01:23.113003] Epoch: [310]  [ 0/42]  eta: 0:01:15  lr: 0.001727  loss: 0.5222 (0.5222)  time: 1.7943  data: 1.2968  max mem: 9341
[09:01:33.284063] Epoch: [310]  [20/42]  eta: 0:00:12  lr: 0.001725  loss: 0.5597 (0.5525)  time: 0.5085  data: 0.0001  max mem: 9341
[09:01:43.429880] Epoch: [310]  [40/42]  eta: 0:00:01  lr: 0.001724  loss: 0.5371 (0.5477)  time: 0.5073  data: 0.0001  max mem: 9341
[09:01:43.935628] Epoch: [310]  [41/42]  eta: 0:00:00  lr: 0.001724  loss: 0.5371 (0.5469)  time: 0.5073  data: 0.0001  max mem: 9341
[09:01:44.107225] Epoch: [310] Total time: 0:00:22 (0.5426 s / it)
[09:01:44.124156] Averaged stats: lr: 0.001724  loss: 0.5371 (0.5496)
[09:01:48.717505] {"train_lr": 0.001725579890116442, "train_loss": 0.5495510406437374, "epoch": 310}
[09:01:48.718018] [09:01:48.718112] Training epoch 310 for 0:00:27
[09:01:48.718164] [09:01:48.723308] log_dir: ./exp/debug/cifar100-LT/debug
[09:01:50.266134] Epoch: [311]  [ 0/42]  eta: 0:01:04  lr: 0.001723  loss: 0.5363 (0.5363)  time: 1.5419  data: 1.0295  max mem: 9341
[09:02:00.469626] Epoch: [311]  [20/42]  eta: 0:00:12  lr: 0.001721  loss: 0.5379 (0.5425)  time: 0.5101  data: 0.0001  max mem: 9341
[09:02:10.609484] Epoch: [311]  [40/42]  eta: 0:00:01  lr: 0.001719  loss: 0.5341 (0.5396)  time: 0.5070  data: 0.0001  max mem: 9341
[09:02:11.115097] Epoch: [311]  [41/42]  eta: 0:00:00  lr: 0.001719  loss: 0.5357 (0.5417)  time: 0.5070  data: 0.0001  max mem: 9341
[09:02:11.280703] Epoch: [311] Total time: 0:00:22 (0.5371 s / it)
[09:02:11.286255] Averaged stats: lr: 0.001719  loss: 0.5357 (0.5474)
[09:02:16.053837] {"train_lr": 0.0017211160921074794, "train_loss": 0.5474230492753642, "epoch": 311}
[09:02:16.054093] [09:02:16.054174] Training epoch 311 for 0:00:27
[09:02:16.054225] [09:02:16.058582] log_dir: ./exp/debug/cifar100-LT/debug
[09:02:17.517857] Epoch: [312]  [ 0/42]  eta: 0:01:01  lr: 0.001718  loss: 0.5115 (0.5115)  time: 1.4583  data: 0.9507  max mem: 9341
[09:02:27.689405] Epoch: [312]  [20/42]  eta: 0:00:12  lr: 0.001716  loss: 0.5551 (0.5517)  time: 0.5085  data: 0.0001  max mem: 9341
[09:02:37.843095] Epoch: [312]  [40/42]  eta: 0:00:01  lr: 0.001715  loss: 0.5357 (0.5501)  time: 0.5076  data: 0.0001  max mem: 9341
[09:02:38.349366] Epoch: [312]  [41/42]  eta: 0:00:00  lr: 0.001715  loss: 0.5338 (0.5494)  time: 0.5076  data: 0.0001  max mem: 9341
[09:02:38.511476] Epoch: [312] Total time: 0:00:22 (0.5346 s / it)
[09:02:38.528327] Averaged stats: lr: 0.001715  loss: 0.5338 (0.5452)
[09:02:43.112628] {"train_lr": 0.0017166433896623101, "train_loss": 0.5451820985901923, "epoch": 312}
[09:02:43.113020] [09:02:43.113113] Training epoch 312 for 0:00:27
[09:02:43.113167] [09:02:43.117923] log_dir: ./exp/debug/cifar100-LT/debug
[09:02:44.782010] Epoch: [313]  [ 0/42]  eta: 0:01:09  lr: 0.001714  loss: 0.5474 (0.5474)  time: 1.6632  data: 1.1583  max mem: 9341
[09:02:54.939638] Epoch: [313]  [20/42]  eta: 0:00:12  lr: 0.001712  loss: 0.5372 (0.5405)  time: 0.5078  data: 0.0001  max mem: 9341
[09:03:05.085245] Epoch: [313]  [40/42]  eta: 0:00:01  lr: 0.001710  loss: 0.5429 (0.5434)  time: 0.5072  data: 0.0001  max mem: 9341
[09:03:05.589735] Epoch: [313]  [41/42]  eta: 0:00:00  lr: 0.001710  loss: 0.5423 (0.5434)  time: 0.5071  data: 0.0001  max mem: 9341
[09:03:05.753564] Epoch: [313] Total time: 0:00:22 (0.5389 s / it)
[09:03:05.765978] Averaged stats: lr: 0.001710  loss: 0.5423 (0.5431)
[09:03:10.368678] {"train_lr": 0.0017121618592070799, "train_loss": 0.5431365709574449, "epoch": 313}
[09:03:10.369082] [09:03:10.369161] Training epoch 313 for 0:00:27
[09:03:10.369214] [09:03:10.373618] log_dir: ./exp/debug/cifar100-LT/debug
[09:03:11.818735] Epoch: [314]  [ 0/42]  eta: 0:01:00  lr: 0.001709  loss: 0.5124 (0.5124)  time: 1.4438  data: 0.9388  max mem: 9341
[09:03:21.989003] Epoch: [314]  [20/42]  eta: 0:00:12  lr: 0.001707  loss: 0.5439 (0.5492)  time: 0.5085  data: 0.0001  max mem: 9341
[09:03:32.137083] Epoch: [314]  [40/42]  eta: 0:00:01  lr: 0.001706  loss: 0.5289 (0.5411)  time: 0.5074  data: 0.0001  max mem: 9341
[09:03:32.643780] Epoch: [314]  [41/42]  eta: 0:00:00  lr: 0.001706  loss: 0.5313 (0.5414)  time: 0.5074  data: 0.0001  max mem: 9341
[09:03:32.814410] Epoch: [314] Total time: 0:00:22 (0.5343 s / it)
[09:03:32.818743] Averaged stats: lr: 0.001706  loss: 0.5313 (0.5440)
[09:03:37.372054] {"train_lr": 0.001707671577318776, "train_loss": 0.5440088189428761, "epoch": 314}
[09:03:37.372361] [09:03:37.372463] Training epoch 314 for 0:00:27
[09:03:37.372515] [09:03:37.376814] log_dir: ./exp/debug/cifar100-LT/debug
[09:03:39.007425] Epoch: [315]  [ 0/42]  eta: 0:01:08  lr: 0.001705  loss: 0.5378 (0.5378)  time: 1.6297  data: 1.1175  max mem: 9341
[09:03:49.169888] Epoch: [315]  [20/42]  eta: 0:00:12  lr: 0.001703  loss: 0.5343 (0.5379)  time: 0.5081  data: 0.0001  max mem: 9341
[09:03:59.317183] Epoch: [315]  [40/42]  eta: 0:00:01  lr: 0.001701  loss: 0.5438 (0.5437)  time: 0.5073  data: 0.0001  max mem: 9341
[09:03:59.823262] Epoch: [315]  [41/42]  eta: 0:00:00  lr: 0.001701  loss: 0.5403 (0.5429)  time: 0.5074  data: 0.0001  max mem: 9341
[09:03:59.995164] Epoch: [315] Total time: 0:00:22 (0.5385 s / it)
[09:03:59.997133] Averaged stats: lr: 0.001701  loss: 0.5403 (0.5417)
[09:04:04.594061] {"train_lr": 0.001703172620723932, "train_loss": 0.5417209072482019, "epoch": 315}
[09:04:04.594382] [09:04:04.594463] Training epoch 315 for 0:00:27
[09:04:04.594573] [09:04:04.599051] log_dir: ./exp/debug/cifar100-LT/debug
[09:04:06.146700] Epoch: [316]  [ 0/42]  eta: 0:01:04  lr: 0.001700  loss: 0.5210 (0.5210)  time: 1.5467  data: 1.0443  max mem: 9341
[09:04:16.310654] Epoch: [316]  [20/42]  eta: 0:00:12  lr: 0.001698  loss: 0.5464 (0.5490)  time: 0.5082  data: 0.0001  max mem: 9341
[09:04:26.463061] Epoch: [316]  [40/42]  eta: 0:00:01  lr: 0.001697  loss: 0.5330 (0.5451)  time: 0.5076  data: 0.0001  max mem: 9341
[09:04:26.969868] Epoch: [316]  [41/42]  eta: 0:00:00  lr: 0.001697  loss: 0.5385 (0.5464)  time: 0.5076  data: 0.0001  max mem: 9341
[09:04:27.136213] Epoch: [316] Total time: 0:00:22 (0.5366 s / it)
[09:04:27.137134] Averaged stats: lr: 0.001697  loss: 0.5385 (0.5476)
[09:04:31.661854] {"train_lr": 0.0016986650662972997, "train_loss": 0.5475786511032354, "epoch": 316}
[09:04:31.662210] [09:04:31.662291] Training epoch 316 for 0:00:27
[09:04:31.662341] [09:04:31.666743] log_dir: ./exp/debug/cifar100-LT/debug
[09:04:33.288133] Epoch: [317]  [ 0/42]  eta: 0:01:08  lr: 0.001696  loss: 0.5642 (0.5642)  time: 1.6203  data: 1.1156  max mem: 9341
[09:04:43.447438] Epoch: [317]  [20/42]  eta: 0:00:12  lr: 0.001694  loss: 0.5307 (0.5338)  time: 0.5079  data: 0.0001  max mem: 9341
[09:04:53.604499] Epoch: [317]  [40/42]  eta: 0:00:01  lr: 0.001692  loss: 0.5461 (0.5453)  time: 0.5078  data: 0.0001  max mem: 9341
[09:04:54.111682] Epoch: [317]  [41/42]  eta: 0:00:00  lr: 0.001692  loss: 0.5437 (0.5450)  time: 0.5078  data: 0.0001  max mem: 9341
[09:04:54.273219] Epoch: [317] Total time: 0:00:22 (0.5382 s / it)
[09:04:54.285292] Averaged stats: lr: 0.001692  loss: 0.5437 (0.5432)
[09:04:58.829603] {"train_lr": 0.0016941489910605487, "train_loss": 0.5432130446036657, "epoch": 317}
[09:04:58.829932] [09:04:58.830010] Training epoch 317 for 0:00:27
[09:04:58.830060] [09:04:58.834847] log_dir: ./exp/debug/cifar100-LT/debug
[09:05:00.353166] Epoch: [318]  [ 0/42]  eta: 0:01:03  lr: 0.001691  loss: 0.5592 (0.5592)  time: 1.5173  data: 0.9990  max mem: 9341
[09:05:10.556839] Epoch: [318]  [20/42]  eta: 0:00:12  lr: 0.001689  loss: 0.5504 (0.5469)  time: 0.5101  data: 0.0001  max mem: 9341
[09:05:20.700650] Epoch: [318]  [40/42]  eta: 0:00:01  lr: 0.001688  loss: 0.5437 (0.5434)  time: 0.5072  data: 0.0001  max mem: 9341
[09:05:21.206723] Epoch: [318]  [41/42]  eta: 0:00:00  lr: 0.001688  loss: 0.5437 (0.5435)  time: 0.5071  data: 0.0001  max mem: 9341
[09:05:21.363567] Epoch: [318] Total time: 0:00:22 (0.5364 s / it)
[09:05:21.367232] Averaged stats: lr: 0.001688  loss: 0.5437 (0.5436)
[09:05:25.936299] {"train_lr": 0.0016896244721809432, "train_loss": 0.5436495748304185, "epoch": 318}
[09:05:25.936658] [09:05:25.936748] Training epoch 318 for 0:00:27
[09:05:25.936799] [09:05:25.941022] log_dir: ./exp/debug/cifar100-LT/debug
[09:05:27.384975] Epoch: [319]  [ 0/42]  eta: 0:01:00  lr: 0.001687  loss: 0.5229 (0.5229)  time: 1.4428  data: 0.9487  max mem: 9341
[09:05:37.549991] Epoch: [319]  [20/42]  eta: 0:00:12  lr: 0.001685  loss: 0.5453 (0.5442)  time: 0.5082  data: 0.0001  max mem: 9341
[09:05:47.700257] Epoch: [319]  [40/42]  eta: 0:00:01  lr: 0.001683  loss: 0.5284 (0.5389)  time: 0.5075  data: 0.0001  max mem: 9341
[09:05:48.206835] Epoch: [319]  [41/42]  eta: 0:00:00  lr: 0.001683  loss: 0.5266 (0.5372)  time: 0.5075  data: 0.0001  max mem: 9341
[09:05:48.362882] Epoch: [319] Total time: 0:00:22 (0.5339 s / it)
[09:05:48.375521] Averaged stats: lr: 0.001683  loss: 0.5266 (0.5416)
[09:05:53.001423] {"train_lr": 0.0016850915869700285, "train_loss": 0.5415960584013235, "epoch": 319}
[09:05:53.001811] [09:05:53.001895] Training epoch 319 for 0:00:27
[09:05:53.001947] [09:05:53.006341] log_dir: ./exp/debug/cifar100-LT/debug
[09:05:54.761862] Epoch: [320]  [ 0/42]  eta: 0:01:13  lr: 0.001682  loss: 0.5711 (0.5711)  time: 1.7545  data: 1.2541  max mem: 9341
[09:06:04.934533] Epoch: [320]  [20/42]  eta: 0:00:12  lr: 0.001680  loss: 0.5409 (0.5450)  time: 0.5086  data: 0.0001  max mem: 9341
[09:06:15.087205] Epoch: [320]  [40/42]  eta: 0:00:01  lr: 0.001679  loss: 0.5378 (0.5405)  time: 0.5076  data: 0.0001  max mem: 9341
[09:06:15.593810] Epoch: [320]  [41/42]  eta: 0:00:00  lr: 0.001679  loss: 0.5378 (0.5405)  time: 0.5077  data: 0.0001  max mem: 9341
[09:06:15.762050] Epoch: [320] Total time: 0:00:22 (0.5418 s / it)
[09:06:15.762981] Averaged stats: lr: 0.001679  loss: 0.5378 (0.5414)
[09:06:20.396952] {"train_lr": 0.0016805504128823044, "train_loss": 0.5413771415395396, "epoch": 320}
[09:06:20.397383] [09:06:20.397475] Training epoch 320 for 0:00:27
[09:06:20.397527] [09:06:20.402713] log_dir: ./exp/debug/cifar100-LT/debug
[09:06:21.993369] Epoch: [321]  [ 0/42]  eta: 0:01:06  lr: 0.001677  loss: 0.5554 (0.5554)  time: 1.5896  data: 1.0768  max mem: 9341
[09:06:32.151782] Epoch: [321]  [20/42]  eta: 0:00:12  lr: 0.001676  loss: 0.5268 (0.5305)  time: 0.5079  data: 0.0001  max mem: 9341
[09:06:42.307028] Epoch: [321]  [40/42]  eta: 0:00:01  lr: 0.001674  loss: 0.5511 (0.5406)  time: 0.5077  data: 0.0001  max mem: 9341
[09:06:42.813556] Epoch: [321]  [41/42]  eta: 0:00:00  lr: 0.001674  loss: 0.5600 (0.5412)  time: 0.5076  data: 0.0001  max mem: 9341
[09:06:42.981936] Epoch: [321] Total time: 0:00:22 (0.5376 s / it)
[09:06:42.988263] Averaged stats: lr: 0.001674  loss: 0.5600 (0.5456)
[09:06:47.576783] {"train_lr": 0.0016760010275139088, "train_loss": 0.5456391379591965, "epoch": 321}
[09:06:47.577152] [09:06:47.577235] Training epoch 321 for 0:00:27
[09:06:47.577287] [09:06:47.581653] log_dir: ./exp/debug/cifar100-LT/debug
[09:06:49.330365] Epoch: [322]  [ 0/42]  eta: 0:01:13  lr: 0.001673  loss: 0.5345 (0.5345)  time: 1.7480  data: 1.2499  max mem: 9341
[09:06:59.500851] Epoch: [322]  [20/42]  eta: 0:00:12  lr: 0.001671  loss: 0.5477 (0.5481)  time: 0.5085  data: 0.0001  max mem: 9341
[09:07:09.654138] Epoch: [322]  [40/42]  eta: 0:00:01  lr: 0.001669  loss: 0.5310 (0.5434)  time: 0.5076  data: 0.0001  max mem: 9341
[09:07:10.160217] Epoch: [322]  [41/42]  eta: 0:00:00  lr: 0.001669  loss: 0.5418 (0.5435)  time: 0.5076  data: 0.0001  max mem: 9341
[09:07:10.317988] Epoch: [322] Total time: 0:00:22 (0.5413 s / it)
[09:07:10.330133] Averaged stats: lr: 0.001669  loss: 0.5418 (0.5417)
[09:07:15.077952] {"train_lr": 0.0016714435086012853, "train_loss": 0.541687072742553, "epoch": 322}
[09:07:15.078182] [09:07:15.078264] Training epoch 322 for 0:00:27
[09:07:15.078314] [09:07:15.082745] log_dir: ./exp/debug/cifar100-LT/debug
[09:07:16.700616] Epoch: [323]  [ 0/42]  eta: 0:01:07  lr: 0.001668  loss: 0.5444 (0.5444)  time: 1.6167  data: 1.1200  max mem: 9341
[09:07:26.864613] Epoch: [323]  [20/42]  eta: 0:00:12  lr: 0.001667  loss: 0.5624 (0.5577)  time: 0.5081  data: 0.0001  max mem: 9341
[09:07:37.015358] Epoch: [323]  [40/42]  eta: 0:00:01  lr: 0.001665  loss: 0.5407 (0.5539)  time: 0.5075  data: 0.0001  max mem: 9341
[09:07:37.521240] Epoch: [323]  [41/42]  eta: 0:00:00  lr: 0.001665  loss: 0.5407 (0.5528)  time: 0.5075  data: 0.0001  max mem: 9341
[09:07:37.694887] Epoch: [323] Total time: 0:00:22 (0.5384 s / it)
[09:07:37.705945] Averaged stats: lr: 0.001665  loss: 0.5407 (0.5449)
[09:07:42.253655] {"train_lr": 0.001666877934019854, "train_loss": 0.5449180611897082, "epoch": 323}
[09:07:42.254017] [09:07:42.254096] Training epoch 323 for 0:00:27
[09:07:42.254147] [09:07:42.258529] log_dir: ./exp/debug/cifar100-LT/debug
[09:07:43.747448] Epoch: [324]  [ 0/42]  eta: 0:01:02  lr: 0.001664  loss: 0.5151 (0.5151)  time: 1.4877  data: 0.9714  max mem: 9341
[09:07:53.916550] Epoch: [324]  [20/42]  eta: 0:00:12  lr: 0.001662  loss: 0.5414 (0.5420)  time: 0.5084  data: 0.0001  max mem: 9341
[09:08:04.067262] Epoch: [324]  [40/42]  eta: 0:00:01  lr: 0.001660  loss: 0.5517 (0.5443)  time: 0.5075  data: 0.0001  max mem: 9341
[09:08:04.572802] Epoch: [324]  [41/42]  eta: 0:00:00  lr: 0.001660  loss: 0.5517 (0.5446)  time: 0.5074  data: 0.0001  max mem: 9341
[09:08:04.749806] Epoch: [324] Total time: 0:00:22 (0.5355 s / it)
[09:08:04.753906] Averaged stats: lr: 0.001660  loss: 0.5517 (0.5405)
[09:08:09.354913] {"train_lr": 0.0016623043817826934, "train_loss": 0.5404829993134453, "epoch": 324}
[09:08:09.355242] [09:08:09.355320] Training epoch 324 for 0:00:27
[09:08:09.355370] [09:08:09.359729] log_dir: ./exp/debug/cifar100-LT/debug
[09:08:11.023273] Epoch: [325]  [ 0/42]  eta: 0:01:09  lr: 0.001659  loss: 0.5357 (0.5357)  time: 1.6621  data: 1.1658  max mem: 9341
[09:08:21.215979] Epoch: [325]  [20/42]  eta: 0:00:12  lr: 0.001657  loss: 0.5321 (0.5403)  time: 0.5096  data: 0.0001  max mem: 9341
[09:08:31.383051] Epoch: [325]  [40/42]  eta: 0:00:01  lr: 0.001656  loss: 0.5336 (0.5396)  time: 0.5083  data: 0.0001  max mem: 9341
[09:08:31.890207] Epoch: [325]  [41/42]  eta: 0:00:00  lr: 0.001656  loss: 0.5336 (0.5404)  time: 0.5084  data: 0.0001  max mem: 9341
[09:08:32.059606] Epoch: [325] Total time: 0:00:22 (0.5405 s / it)
[09:08:32.070811] Averaged stats: lr: 0.001656  loss: 0.5336 (0.5430)
[09:08:36.715352] {"train_lr": 0.0016577229300391875, "train_loss": 0.5429820315468878, "epoch": 325}
[09:08:36.715703] [09:08:36.715784] Training epoch 325 for 0:00:27
[09:08:36.715835] [09:08:36.720319] log_dir: ./exp/debug/cifar100-LT/debug
[09:08:38.184805] Epoch: [326]  [ 0/42]  eta: 0:01:01  lr: 0.001655  loss: 0.5693 (0.5693)  time: 1.4631  data: 0.9522  max mem: 9341
[09:08:48.371622] Epoch: [326]  [20/42]  eta: 0:00:12  lr: 0.001653  loss: 0.5325 (0.5425)  time: 0.5093  data: 0.0001  max mem: 9341
[09:08:58.551786] Epoch: [326]  [40/42]  eta: 0:00:01  lr: 0.001651  loss: 0.5536 (0.5480)  time: 0.5090  data: 0.0001  max mem: 9341
[09:08:59.058462] Epoch: [326]  [41/42]  eta: 0:00:00  lr: 0.001651  loss: 0.5536 (0.5475)  time: 0.5090  data: 0.0001  max mem: 9341
[09:08:59.222162] Epoch: [326] Total time: 0:00:22 (0.5358 s / it)
[09:08:59.229885] Averaged stats: lr: 0.001651  loss: 0.5536 (0.5441)
[09:09:03.825982] {"train_lr": 0.0016531336570737084, "train_loss": 0.5441431598294348, "epoch": 326}
[09:09:03.826337] [09:09:03.826420] Training epoch 326 for 0:00:27
[09:09:03.826472] [09:09:03.830872] log_dir: ./exp/debug/cifar100-LT/debug
[09:09:05.542221] Epoch: [327]  [ 0/42]  eta: 0:01:11  lr: 0.001650  loss: 0.5730 (0.5730)  time: 1.7103  data: 1.1912  max mem: 9341
[09:09:15.728055] Epoch: [327]  [20/42]  eta: 0:00:12  lr: 0.001648  loss: 0.5304 (0.5414)  time: 0.5092  data: 0.0001  max mem: 9341
[09:09:25.900717] Epoch: [327]  [40/42]  eta: 0:00:01  lr: 0.001647  loss: 0.5294 (0.5358)  time: 0.5086  data: 0.0001  max mem: 9341
[09:09:26.407512] Epoch: [327]  [41/42]  eta: 0:00:00  lr: 0.001647  loss: 0.5294 (0.5347)  time: 0.5086  data: 0.0001  max mem: 9341
[09:09:26.570561] Epoch: [327] Total time: 0:00:22 (0.5414 s / it)
[09:09:26.573734] Averaged stats: lr: 0.001647  loss: 0.5294 (0.5418)
[09:09:31.124196] {"train_lr": 0.001648536641304269, "train_loss": 0.5417892519562018, "epoch": 327}
[09:09:31.124628] [09:09:31.124721] Training epoch 327 for 0:00:27
[09:09:31.124832] [09:09:31.131616] log_dir: ./exp/debug/cifar100-LT/debug
[09:09:32.926160] Epoch: [328]  [ 0/42]  eta: 0:01:15  lr: 0.001645  loss: 0.5572 (0.5572)  time: 1.7934  data: 1.2884  max mem: 9341
[09:09:43.113201] Epoch: [328]  [20/42]  eta: 0:00:12  lr: 0.001644  loss: 0.5427 (0.5355)  time: 0.5093  data: 0.0001  max mem: 9341
[09:09:53.281684] Epoch: [328]  [40/42]  eta: 0:00:01  lr: 0.001642  loss: 0.5426 (0.5423)  time: 0.5084  data: 0.0001  max mem: 9341
[09:09:53.787321] Epoch: [328]  [41/42]  eta: 0:00:00  lr: 0.001642  loss: 0.5426 (0.5422)  time: 0.5084  data: 0.0001  max mem: 9341
[09:09:53.948498] Epoch: [328] Total time: 0:00:22 (0.5433 s / it)
[09:09:53.955687] Averaged stats: lr: 0.001642  loss: 0.5426 (0.5425)
[09:09:58.502268] {"train_lr": 0.0016439319612811826, "train_loss": 0.5424718095787934, "epoch": 328}
[09:09:58.502627] [09:09:58.502711] Training epoch 328 for 0:00:27
[09:09:58.502762] [09:09:58.507146] log_dir: ./exp/debug/cifar100-LT/debug
[09:10:00.037240] Epoch: [329]  [ 0/42]  eta: 0:01:04  lr: 0.001641  loss: 0.5453 (0.5453)  time: 1.5287  data: 1.0093  max mem: 9341
[09:10:10.210597] Epoch: [329]  [20/42]  eta: 0:00:12  lr: 0.001639  loss: 0.5359 (0.5366)  time: 0.5086  data: 0.0001  max mem: 9341
[09:10:20.363109] Epoch: [329]  [40/42]  eta: 0:00:01  lr: 0.001637  loss: 0.5412 (0.5379)  time: 0.5076  data: 0.0001  max mem: 9341
[09:10:20.870538] Epoch: [329]  [41/42]  eta: 0:00:00  lr: 0.001637  loss: 0.5370 (0.5374)  time: 0.5076  data: 0.0001  max mem: 9341
[09:10:21.037313] Epoch: [329] Total time: 0:00:22 (0.5364 s / it)
[09:10:21.039599] Averaged stats: lr: 0.001637  loss: 0.5370 (0.5424)
[09:10:25.614779] {"train_lr": 0.0016393196956857297, "train_loss": 0.5424389102984042, "epoch": 329}
[09:10:25.615144] [09:10:25.615227] Training epoch 329 for 0:00:27
[09:10:25.615281] [09:10:25.619662] log_dir: ./exp/debug/cifar100-LT/debug
[09:10:27.046212] Epoch: [330]  [ 0/42]  eta: 0:00:59  lr: 0.001636  loss: 0.5681 (0.5681)  time: 1.4252  data: 0.9233  max mem: 9341
[09:10:37.211397] Epoch: [330]  [20/42]  eta: 0:00:12  lr: 0.001634  loss: 0.5462 (0.5444)  time: 0.5082  data: 0.0001  max mem: 9341
[09:10:47.360178] Epoch: [330]  [40/42]  eta: 0:00:01  lr: 0.001633  loss: 0.5571 (0.5455)  time: 0.5074  data: 0.0001  max mem: 9341
[09:10:47.866406] Epoch: [330]  [41/42]  eta: 0:00:00  lr: 0.001633  loss: 0.5544 (0.5455)  time: 0.5074  data: 0.0001  max mem: 9341
[09:10:48.030107] Epoch: [330] Total time: 0:00:22 (0.5336 s / it)
[09:10:48.060479] Averaged stats: lr: 0.001633  loss: 0.5544 (0.5431)
[09:10:52.744904] {"train_lr": 0.0016346999233288, "train_loss": 0.5431466013902709, "epoch": 330}
[09:10:52.745375] [09:10:52.745470] Training epoch 330 for 0:00:27
[09:10:52.745523] [09:10:52.750575] log_dir: ./exp/debug/cifar100-LT/debug
[09:10:54.359965] Epoch: [331]  [ 0/42]  eta: 0:01:07  lr: 0.001632  loss: 0.5176 (0.5176)  time: 1.6082  data: 1.0917  max mem: 9341
[09:11:04.524568] Epoch: [331]  [20/42]  eta: 0:00:12  lr: 0.001630  loss: 0.5437 (0.5432)  time: 0.5082  data: 0.0001  max mem: 9341
[09:11:14.668521] Epoch: [331]  [40/42]  eta: 0:00:01  lr: 0.001628  loss: 0.5346 (0.5408)  time: 0.5071  data: 0.0001  max mem: 9341
[09:11:15.175117] Epoch: [331]  [41/42]  eta: 0:00:00  lr: 0.001628  loss: 0.5339 (0.5403)  time: 0.5072  data: 0.0001  max mem: 9341
[09:11:15.339429] Epoch: [331] Total time: 0:00:22 (0.5378 s / it)
[09:11:15.343849] Averaged stats: lr: 0.001628  loss: 0.5339 (0.5444)
[09:11:19.922174] {"train_lr": 0.001630072723149559, "train_loss": 0.5444416838387648, "epoch": 331}
[09:11:19.922530] [09:11:19.922610] Training epoch 331 for 0:00:27
[09:11:19.922661] [09:11:19.927049] log_dir: ./exp/debug/cifar100-LT/debug
[09:11:21.515443] Epoch: [332]  [ 0/42]  eta: 0:01:06  lr: 0.001627  loss: 0.5901 (0.5901)  time: 1.5872  data: 1.0756  max mem: 9341
[09:11:31.680897] Epoch: [332]  [20/42]  eta: 0:00:12  lr: 0.001625  loss: 0.5292 (0.5359)  time: 0.5082  data: 0.0001  max mem: 9341
[09:11:41.826019] Epoch: [332]  [40/42]  eta: 0:00:01  lr: 0.001623  loss: 0.5545 (0.5445)  time: 0.5072  data: 0.0001  max mem: 9341
[09:11:42.332660] Epoch: [332]  [41/42]  eta: 0:00:00  lr: 0.001623  loss: 0.5468 (0.5441)  time: 0.5073  data: 0.0001  max mem: 9341
[09:11:42.509224] Epoch: [332] Total time: 0:00:22 (0.5377 s / it)
[09:11:42.516594] Averaged stats: lr: 0.001623  loss: 0.5468 (0.5381)
[09:11:47.124917] {"train_lr": 0.001625438174214092, "train_loss": 0.5381166638717765, "epoch": 332}
[09:11:47.125273] [09:11:47.125354] Training epoch 332 for 0:00:27
[09:11:47.125406] [09:11:47.129729] log_dir: ./exp/debug/cifar100-LT/debug
[09:11:48.715893] Epoch: [333]  [ 0/42]  eta: 0:01:06  lr: 0.001622  loss: 0.5268 (0.5268)  time: 1.5850  data: 1.0825  max mem: 9341
[09:11:58.883207] Epoch: [333]  [20/42]  eta: 0:00:12  lr: 0.001621  loss: 0.5523 (0.5464)  time: 0.5083  data: 0.0001  max mem: 9341
[09:12:09.038412] Epoch: [333]  [40/42]  eta: 0:00:01  lr: 0.001619  loss: 0.5469 (0.5488)  time: 0.5077  data: 0.0001  max mem: 9341
[09:12:09.546226] Epoch: [333]  [41/42]  eta: 0:00:00  lr: 0.001619  loss: 0.5510 (0.5498)  time: 0.5079  data: 0.0001  max mem: 9341
[09:12:09.713090] Epoch: [333] Total time: 0:00:22 (0.5377 s / it)
[09:12:09.714334] Averaged stats: lr: 0.001619  loss: 0.5510 (0.5420)
[09:12:14.379704] {"train_lr": 0.001620796355714051, "train_loss": 0.5420273597396555, "epoch": 333}
[09:12:14.380016] [09:12:14.380123] Training epoch 333 for 0:00:27
[09:12:14.380178] [09:12:14.384593] log_dir: ./exp/debug/cifar100-LT/debug
[09:12:15.824738] Epoch: [334]  [ 0/42]  eta: 0:01:00  lr: 0.001618  loss: 0.5434 (0.5434)  time: 1.4390  data: 0.9293  max mem: 9341
[09:12:25.991176] Epoch: [334]  [20/42]  eta: 0:00:12  lr: 0.001616  loss: 0.5270 (0.5349)  time: 0.5083  data: 0.0001  max mem: 9341
[09:12:36.145551] Epoch: [334]  [40/42]  eta: 0:00:01  lr: 0.001614  loss: 0.5325 (0.5364)  time: 0.5077  data: 0.0001  max mem: 9341
[09:12:36.650585] Epoch: [334]  [41/42]  eta: 0:00:00  lr: 0.001614  loss: 0.5325 (0.5365)  time: 0.5076  data: 0.0001  max mem: 9341
[09:12:36.819464] Epoch: [334] Total time: 0:00:22 (0.5342 s / it)
[09:12:36.837651] Averaged stats: lr: 0.001614  loss: 0.5325 (0.5378)
[09:12:41.350016] {"train_lr": 0.001616147346965305, "train_loss": 0.5377542527303809, "epoch": 334}
[09:12:41.350332] [09:12:41.350412] Training epoch 334 for 0:00:26
[09:12:41.350462] [09:12:41.354797] log_dir: ./exp/debug/cifar100-LT/debug
[09:12:42.991264] Epoch: [335]  [ 0/42]  eta: 0:01:08  lr: 0.001613  loss: 0.5677 (0.5677)  time: 1.6354  data: 1.1355  max mem: 9341
[09:12:53.162169] Epoch: [335]  [20/42]  eta: 0:00:12  lr: 0.001611  loss: 0.5313 (0.5328)  time: 0.5085  data: 0.0001  max mem: 9341
[09:13:03.306779] Epoch: [335]  [40/42]  eta: 0:00:01  lr: 0.001609  loss: 0.5444 (0.5391)  time: 0.5072  data: 0.0001  max mem: 9341
[09:13:03.812876] Epoch: [335]  [41/42]  eta: 0:00:00  lr: 0.001609  loss: 0.5444 (0.5376)  time: 0.5073  data: 0.0001  max mem: 9341
[09:13:03.973558] Epoch: [335] Total time: 0:00:22 (0.5385 s / it)
[09:13:03.980975] Averaged stats: lr: 0.001609  loss: 0.5444 (0.5357)
[09:13:08.595237] {"train_lr": 0.0016114912274065901, "train_loss": 0.5356809411730085, "epoch": 335}
[09:13:08.595581] [09:13:08.595661] Training epoch 335 for 0:00:27
[09:13:08.595712] [09:13:08.599979] log_dir: ./exp/debug/cifar100-LT/debug
[09:13:10.225978] Epoch: [336]  [ 0/42]  eta: 0:01:08  lr: 0.001608  loss: 0.5688 (0.5688)  time: 1.6249  data: 1.1130  max mem: 9341
[09:13:20.392639] Epoch: [336]  [20/42]  eta: 0:00:12  lr: 0.001607  loss: 0.5478 (0.5496)  time: 0.5083  data: 0.0001  max mem: 9341
[09:13:30.544260] Epoch: [336]  [40/42]  eta: 0:00:01  lr: 0.001605  loss: 0.5434 (0.5449)  time: 0.5075  data: 0.0001  max mem: 9341
[09:13:31.051782] Epoch: [336]  [41/42]  eta: 0:00:00  lr: 0.001605  loss: 0.5433 (0.5448)  time: 0.5076  data: 0.0001  max mem: 9341
[09:13:31.219276] Epoch: [336] Total time: 0:00:22 (0.5386 s / it)
[09:13:31.237122] Averaged stats: lr: 0.001605  loss: 0.5433 (0.5407)
[09:13:35.746591] {"train_lr": 0.001606828076598144, "train_loss": 0.5406977887309733, "epoch": 336}
[09:13:35.746955] [09:13:35.747038] Training epoch 336 for 0:00:27
[09:13:35.747089] [09:13:35.751458] log_dir: ./exp/debug/cifar100-LT/debug
[09:13:37.444669] Epoch: [337]  [ 0/42]  eta: 0:01:11  lr: 0.001604  loss: 0.5167 (0.5167)  time: 1.6922  data: 1.1902  max mem: 9341
[09:13:47.608778] Epoch: [337]  [20/42]  eta: 0:00:12  lr: 0.001602  loss: 0.5434 (0.5486)  time: 0.5081  data: 0.0001  max mem: 9341
[09:13:57.767987] Epoch: [337]  [40/42]  eta: 0:00:01  lr: 0.001600  loss: 0.5430 (0.5448)  time: 0.5079  data: 0.0001  max mem: 9341
[09:13:58.273802] Epoch: [337]  [41/42]  eta: 0:00:00  lr: 0.001600  loss: 0.5381 (0.5444)  time: 0.5079  data: 0.0001  max mem: 9341
[09:13:58.437751] Epoch: [337] Total time: 0:00:22 (0.5401 s / it)
[09:13:58.440057] Averaged stats: lr: 0.001600  loss: 0.5381 (0.5408)
[09:14:03.078588] {"train_lr": 0.0016021579742203443, "train_loss": 0.5408132649248555, "epoch": 337}
[09:14:03.078939] [09:14:03.079020] Training epoch 337 for 0:00:27
[09:14:03.079070] [09:14:03.083461] log_dir: ./exp/debug/cifar100-LT/debug
[09:14:04.662592] Epoch: [338]  [ 0/42]  eta: 0:01:06  lr: 0.001599  loss: 0.5571 (0.5571)  time: 1.5779  data: 1.0616  max mem: 9341
[09:14:14.826372] Epoch: [338]  [20/42]  eta: 0:00:12  lr: 0.001597  loss: 0.5285 (0.5303)  time: 0.5081  data: 0.0001  max mem: 9341
[09:14:24.966433] Epoch: [338]  [40/42]  eta: 0:00:01  lr: 0.001595  loss: 0.5327 (0.5316)  time: 0.5069  data: 0.0001  max mem: 9341
[09:14:25.472419] Epoch: [338]  [41/42]  eta: 0:00:00  lr: 0.001595  loss: 0.5297 (0.5316)  time: 0.5070  data: 0.0001  max mem: 9341
[09:14:25.642343] Epoch: [338] Total time: 0:00:22 (0.5371 s / it)
[09:14:25.644207] Averaged stats: lr: 0.001595  loss: 0.5297 (0.5354)
[09:14:30.276808] {"train_lr": 0.0015974810000723555, "train_loss": 0.5354374464423883, "epoch": 338}
[09:14:30.277171] [09:14:30.277255] Training epoch 338 for 0:00:27
[09:14:30.277307] [09:14:30.281712] log_dir: ./exp/debug/cifar100-LT/debug
[09:14:31.880482] Epoch: [339]  [ 0/42]  eta: 0:01:07  lr: 0.001594  loss: 0.5192 (0.5192)  time: 1.5976  data: 1.0882  max mem: 9341
[09:14:42.041055] Epoch: [339]  [20/42]  eta: 0:00:12  lr: 0.001593  loss: 0.5326 (0.5365)  time: 0.5080  data: 0.0001  max mem: 9341
[09:14:52.182504] Epoch: [339]  [40/42]  eta: 0:00:01  lr: 0.001591  loss: 0.5423 (0.5377)  time: 0.5070  data: 0.0001  max mem: 9341
[09:14:52.687176] Epoch: [339]  [41/42]  eta: 0:00:00  lr: 0.001591  loss: 0.5423 (0.5368)  time: 0.5070  data: 0.0001  max mem: 9341
[09:14:52.850523] Epoch: [339] Total time: 0:00:22 (0.5374 s / it)
[09:14:52.855768] Averaged stats: lr: 0.001591  loss: 0.5423 (0.5379)
[09:14:57.467337] {"train_lr": 0.0015927972340707651, "train_loss": 0.5379447738329569, "epoch": 339}
[09:14:57.467699] [09:14:57.467783] Training epoch 339 for 0:00:27
[09:14:57.467833] [09:14:57.472148] log_dir: ./exp/debug/cifar100-LT/debug
[09:14:59.036925] Epoch: [340]  [ 0/42]  eta: 0:01:05  lr: 0.001590  loss: 0.4944 (0.4944)  time: 1.5637  data: 1.0558  max mem: 9341
[09:15:09.192113] Epoch: [340]  [20/42]  eta: 0:00:12  lr: 0.001588  loss: 0.5372 (0.5377)  time: 0.5077  data: 0.0001  max mem: 9341
[09:15:19.333729] Epoch: [340]  [40/42]  eta: 0:00:01  lr: 0.001586  loss: 0.5188 (0.5316)  time: 0.5070  data: 0.0001  max mem: 9341
[09:15:19.840919] Epoch: [340]  [41/42]  eta: 0:00:00  lr: 0.001586  loss: 0.5220 (0.5320)  time: 0.5071  data: 0.0001  max mem: 9341
[09:15:20.009548] Epoch: [340] Total time: 0:00:22 (0.5366 s / it)
[09:15:20.010258] Averaged stats: lr: 0.001586  loss: 0.5220 (0.5315)
[09:15:24.631961] {"train_lr": 0.0015881067562482077, "train_loss": 0.5314825789204666, "epoch": 340}
[09:15:24.632342] [09:15:24.632443] Training epoch 340 for 0:00:27
[09:15:24.632494] [09:15:24.636800] log_dir: ./exp/debug/cifar100-LT/debug
[09:15:26.307366] Epoch: [341]  [ 0/42]  eta: 0:01:10  lr: 0.001585  loss: 0.5975 (0.5975)  time: 1.6695  data: 1.1669  max mem: 9341
[09:15:36.488445] Epoch: [341]  [20/42]  eta: 0:00:12  lr: 0.001583  loss: 0.5365 (0.5374)  time: 0.5090  data: 0.0001  max mem: 9341
[09:15:46.657452] Epoch: [341]  [40/42]  eta: 0:00:01  lr: 0.001581  loss: 0.5221 (0.5339)  time: 0.5084  data: 0.0001  max mem: 9341
[09:15:47.164247] Epoch: [341]  [41/42]  eta: 0:00:00  lr: 0.001581  loss: 0.5221 (0.5343)  time: 0.5084  data: 0.0001  max mem: 9341
[09:15:47.327026] Epoch: [341] Total time: 0:00:22 (0.5402 s / it)
[09:15:47.332507] Averaged stats: lr: 0.001581  loss: 0.5221 (0.5378)
[09:15:51.835953] {"train_lr": 0.0015834096467520083, "train_loss": 0.5378287058501017, "epoch": 341}
[09:15:51.836335] [09:15:51.836451] Training epoch 341 for 0:00:27
[09:15:51.836501] [09:15:51.840871] log_dir: ./exp/debug/cifar100-LT/debug
[09:15:53.328629] Epoch: [342]  [ 0/42]  eta: 0:01:02  lr: 0.001580  loss: 0.5010 (0.5010)  time: 1.4864  data: 0.9799  max mem: 9341
[09:16:03.487202] Epoch: [342]  [20/42]  eta: 0:00:12  lr: 0.001578  loss: 0.5260 (0.5311)  time: 0.5079  data: 0.0001  max mem: 9341
[09:16:13.619969] Epoch: [342]  [40/42]  eta: 0:00:01  lr: 0.001577  loss: 0.5396 (0.5336)  time: 0.5066  data: 0.0001  max mem: 9341
[09:16:14.125043] Epoch: [342]  [41/42]  eta: 0:00:00  lr: 0.001577  loss: 0.5409 (0.5354)  time: 0.5066  data: 0.0001  max mem: 9341
[09:16:14.289358] Epoch: [342] Total time: 0:00:22 (0.5345 s / it)
[09:16:14.291122] Averaged stats: lr: 0.001577  loss: 0.5409 (0.5363)
[09:16:18.803505] {"train_lr": 0.0015787059858428103, "train_loss": 0.5362614210517633, "epoch": 342}
[09:16:18.803851] [09:16:18.803933] Training epoch 342 for 0:00:26
[09:16:18.803984] [09:16:18.808556] log_dir: ./exp/debug/cifar100-LT/debug
[09:16:20.392463] Epoch: [343]  [ 0/42]  eta: 0:01:06  lr: 0.001576  loss: 0.5311 (0.5311)  time: 1.5828  data: 1.0698  max mem: 9341
[09:16:30.564837] Epoch: [343]  [20/42]  eta: 0:00:12  lr: 0.001574  loss: 0.5267 (0.5257)  time: 0.5086  data: 0.0001  max mem: 9341
[09:16:40.709666] Epoch: [343]  [40/42]  eta: 0:00:01  lr: 0.001572  loss: 0.5270 (0.5285)  time: 0.5072  data: 0.0001  max mem: 9341
[09:16:41.215217] Epoch: [343]  [41/42]  eta: 0:00:00  lr: 0.001572  loss: 0.5235 (0.5281)  time: 0.5072  data: 0.0001  max mem: 9341
[09:16:41.374548] Epoch: [343] Total time: 0:00:22 (0.5373 s / it)
[09:16:41.405441] Averaged stats: lr: 0.001572  loss: 0.5235 (0.5322)
[09:16:46.094481] {"train_lr": 0.0015739958538931978, "train_loss": 0.5321596200977053, "epoch": 343}
[09:16:46.094825] [09:16:46.094923] Training epoch 343 for 0:00:27
[09:16:46.094976] [09:16:46.099292] log_dir: ./exp/debug/cifar100-LT/debug
[09:16:47.776955] Epoch: [344]  [ 0/42]  eta: 0:01:10  lr: 0.001571  loss: 0.5724 (0.5724)  time: 1.6766  data: 1.1821  max mem: 9341
[09:16:57.938467] Epoch: [344]  [20/42]  eta: 0:00:12  lr: 0.001569  loss: 0.5293 (0.5332)  time: 0.5080  data: 0.0001  max mem: 9341
[09:17:08.091622] Epoch: [344]  [40/42]  eta: 0:00:01  lr: 0.001567  loss: 0.5440 (0.5393)  time: 0.5076  data: 0.0001  max mem: 9341
[09:17:08.599209] Epoch: [344]  [41/42]  eta: 0:00:00  lr: 0.001567  loss: 0.5440 (0.5399)  time: 0.5076  data: 0.0001  max mem: 9341
[09:17:08.765825] Epoch: [344] Total time: 0:00:22 (0.5397 s / it)
[09:17:08.766552] Averaged stats: lr: 0.001567  loss: 0.5440 (0.5368)
[09:17:13.439016] {"train_lr": 0.0015692793313863336, "train_loss": 0.5368014928840456, "epoch": 344}
[09:17:13.439357] [09:17:13.439445] Training epoch 344 for 0:00:27
[09:17:13.439497] [09:17:13.444416] log_dir: ./exp/debug/cifar100-LT/debug
[09:17:14.899148] Epoch: [345]  [ 0/42]  eta: 0:01:01  lr: 0.001566  loss: 0.4841 (0.4841)  time: 1.4535  data: 0.9507  max mem: 9341
[09:17:25.064000] Epoch: [345]  [20/42]  eta: 0:00:12  lr: 0.001564  loss: 0.5319 (0.5290)  time: 0.5082  data: 0.0001  max mem: 9341
[09:17:35.211583] Epoch: [345]  [40/42]  eta: 0:00:01  lr: 0.001562  loss: 0.5408 (0.5351)  time: 0.5073  data: 0.0001  max mem: 9341
[09:17:35.717200] Epoch: [345]  [41/42]  eta: 0:00:00  lr: 0.001562  loss: 0.5292 (0.5341)  time: 0.5073  data: 0.0001  max mem: 9341
[09:17:35.876476] Epoch: [345] Total time: 0:00:22 (0.5341 s / it)
[09:17:35.901704] Averaged stats: lr: 0.001562  loss: 0.5292 (0.5324)
[09:17:40.503068] {"train_lr": 0.0015645564989145728, "train_loss": 0.5323516543777216, "epoch": 345}
[09:17:40.503446] [09:17:40.503526] Training epoch 345 for 0:00:27
[09:17:40.503577] [09:17:40.507958] log_dir: ./exp/debug/cifar100-LT/debug
[09:17:42.136721] Epoch: [346]  [ 0/42]  eta: 0:01:08  lr: 0.001561  loss: 0.5305 (0.5305)  time: 1.6277  data: 1.1131  max mem: 9341
[09:17:52.296726] Epoch: [346]  [20/42]  eta: 0:00:12  lr: 0.001560  loss: 0.5352 (0.5286)  time: 0.5079  data: 0.0001  max mem: 9341
[09:18:02.444702] Epoch: [346]  [40/42]  eta: 0:00:01  lr: 0.001558  loss: 0.5446 (0.5348)  time: 0.5073  data: 0.0001  max mem: 9341
[09:18:02.949764] Epoch: [346]  [41/42]  eta: 0:00:00  lr: 0.001558  loss: 0.5446 (0.5345)  time: 0.5074  data: 0.0001  max mem: 9341
[09:18:03.117567] Epoch: [346] Total time: 0:00:22 (0.5383 s / it)
[09:18:03.124144] Averaged stats: lr: 0.001558  loss: 0.5446 (0.5311)
[09:18:07.749823] {"train_lr": 0.0015598274371780903, "train_loss": 0.5311264110108217, "epoch": 346}
[09:18:07.750163] [09:18:07.750244] Training epoch 346 for 0:00:27
[09:18:07.750294] [09:18:07.754728] log_dir: ./exp/debug/cifar100-LT/debug
[09:18:09.333442] Epoch: [347]  [ 0/42]  eta: 0:01:06  lr: 0.001557  loss: 0.5433 (0.5433)  time: 1.5770  data: 1.0670  max mem: 9341
[09:18:19.502306] Epoch: [347]  [20/42]  eta: 0:00:12  lr: 0.001555  loss: 0.5376 (0.5437)  time: 0.5084  data: 0.0001  max mem: 9341
[09:18:29.645445] Epoch: [347]  [40/42]  eta: 0:00:01  lr: 0.001553  loss: 0.5245 (0.5383)  time: 0.5071  data: 0.0001  max mem: 9341
[09:18:30.150620] Epoch: [347]  [41/42]  eta: 0:00:00  lr: 0.001553  loss: 0.5252 (0.5380)  time: 0.5071  data: 0.0001  max mem: 9341
[09:18:30.321142] Epoch: [347] Total time: 0:00:22 (0.5373 s / it)
[09:18:30.349824] Averaged stats: lr: 0.001553  loss: 0.5252 (0.5329)
[09:18:34.960627] {"train_lr": 0.0015550922269835035, "train_loss": 0.5328529145391214, "epoch": 347}
[09:18:34.960983] [09:18:34.961063] Training epoch 347 for 0:00:27
[09:18:34.961115] [09:18:34.965438] log_dir: ./exp/debug/cifar100-LT/debug
[09:18:36.434460] Epoch: [348]  [ 0/42]  eta: 0:01:01  lr: 0.001552  loss: 0.5522 (0.5522)  time: 1.4678  data: 0.9534  max mem: 9341
[09:18:46.635008] Epoch: [348]  [20/42]  eta: 0:00:12  lr: 0.001550  loss: 0.5337 (0.5363)  time: 0.5100  data: 0.0001  max mem: 9341
[09:18:56.774621] Epoch: [348]  [40/42]  eta: 0:00:01  lr: 0.001548  loss: 0.5296 (0.5357)  time: 0.5069  data: 0.0001  max mem: 9341
[09:18:57.279923] Epoch: [348]  [41/42]  eta: 0:00:00  lr: 0.001548  loss: 0.5313 (0.5360)  time: 0.5069  data: 0.0001  max mem: 9341
[09:18:57.447545] Epoch: [348] Total time: 0:00:22 (0.5353 s / it)
[09:18:57.454381] Averaged stats: lr: 0.001548  loss: 0.5313 (0.5301)
[09:19:01.950433] {"train_lr": 0.0015503509492424909, "train_loss": 0.5300724123205457, "epoch": 348}
[09:19:01.950814] [09:19:01.950901] Training epoch 348 for 0:00:26
[09:19:01.950955] [09:19:01.955359] log_dir: ./exp/debug/cifar100-LT/debug
[09:19:03.421375] Epoch: [349]  [ 0/42]  eta: 0:01:01  lr: 0.001547  loss: 0.5136 (0.5136)  time: 1.4646  data: 0.9647  max mem: 9341
[09:19:13.588302] Epoch: [349]  [20/42]  eta: 0:00:12  lr: 0.001545  loss: 0.5301 (0.5381)  time: 0.5083  data: 0.0001  max mem: 9341
[09:19:23.732039] Epoch: [349]  [40/42]  eta: 0:00:01  lr: 0.001544  loss: 0.5358 (0.5357)  time: 0.5071  data: 0.0001  max mem: 9341
[09:19:24.236893] Epoch: [349]  [41/42]  eta: 0:00:00  lr: 0.001544  loss: 0.5358 (0.5338)  time: 0.5071  data: 0.0001  max mem: 9341
[09:19:24.406925] Epoch: [349] Total time: 0:00:22 (0.5346 s / it)
[09:19:24.417078] Averaged stats: lr: 0.001544  loss: 0.5358 (0.5348)
[09:19:28.989730] {"train_lr": 0.001545603684970406, "train_loss": 0.5348473867135388, "epoch": 349}
[09:19:28.990083] [09:19:28.990163] Training epoch 349 for 0:00:27
[09:19:28.990214] [09:19:28.994593] log_dir: ./exp/debug/cifar100-LT/debug
[09:19:30.674116] Epoch: [350]  [ 0/42]  eta: 0:01:10  lr: 0.001542  loss: 0.5410 (0.5410)  time: 1.6783  data: 1.1585  max mem: 9341
[09:19:40.843362] Epoch: [350]  [20/42]  eta: 0:00:12  lr: 0.001541  loss: 0.5311 (0.5360)  time: 0.5084  data: 0.0001  max mem: 9341
[09:19:50.991039] Epoch: [350]  [40/42]  eta: 0:00:01  lr: 0.001539  loss: 0.5144 (0.5290)  time: 0.5073  data: 0.0001  max mem: 9341
[09:19:51.497106] Epoch: [350]  [41/42]  eta: 0:00:00  lr: 0.001539  loss: 0.5144 (0.5304)  time: 0.5074  data: 0.0001  max mem: 9341
[09:19:51.667718] Epoch: [350] Total time: 0:00:22 (0.5398 s / it)
[09:19:51.671840] Averaged stats: lr: 0.001539  loss: 0.5144 (0.5327)
[09:19:56.150123] {"train_lr": 0.0015408505152849001, "train_loss": 0.5326594387491544, "epoch": 350}
[09:19:56.150551] [09:19:56.150647] Training epoch 350 for 0:00:27
[09:19:56.150699] [09:19:56.155599] log_dir: ./exp/debug/cifar100-LT/debug
[09:19:57.663729] Epoch: [351]  [ 0/42]  eta: 0:01:03  lr: 0.001538  loss: 0.5927 (0.5927)  time: 1.5068  data: 1.0042  max mem: 9341
[09:20:07.830550] Epoch: [351]  [20/42]  eta: 0:00:12  lr: 0.001536  loss: 0.5223 (0.5336)  time: 0.5083  data: 0.0001  max mem: 9341
[09:20:17.981100] Epoch: [351]  [40/42]  eta: 0:00:01  lr: 0.001534  loss: 0.5321 (0.5328)  time: 0.5075  data: 0.0001  max mem: 9341
[09:20:18.487850] Epoch: [351]  [41/42]  eta: 0:00:00  lr: 0.001534  loss: 0.5302 (0.5324)  time: 0.5075  data: 0.0001  max mem: 9341
[09:20:18.663404] Epoch: [351] Total time: 0:00:22 (0.5359 s / it)
[09:20:18.677708] Averaged stats: lr: 0.001534  loss: 0.5302 (0.5290)
[09:20:23.197204] {"train_lr": 0.0015360915214045253, "train_loss": 0.5290232689252922, "epoch": 351}
[09:20:23.197574] [09:20:23.197653] Training epoch 351 for 0:00:27
[09:20:23.197704] [09:20:23.202026] log_dir: ./exp/debug/cifar100-LT/debug
[09:20:24.650853] Epoch: [352]  [ 0/42]  eta: 0:01:00  lr: 0.001533  loss: 0.4781 (0.4781)  time: 1.4477  data: 0.9405  max mem: 9341
[09:20:34.816680] Epoch: [352]  [20/42]  eta: 0:00:12  lr: 0.001531  loss: 0.5202 (0.5291)  time: 0.5082  data: 0.0001  max mem: 9341
[09:20:44.969826] Epoch: [352]  [40/42]  eta: 0:00:01  lr: 0.001529  loss: 0.5306 (0.5319)  time: 0.5076  data: 0.0001  max mem: 9341
[09:20:45.475144] Epoch: [352]  [41/42]  eta: 0:00:00  lr: 0.001529  loss: 0.5306 (0.5326)  time: 0.5076  data: 0.0001  max mem: 9341
[09:20:45.632637] Epoch: [352] Total time: 0:00:22 (0.5341 s / it)
[09:20:45.645941] Averaged stats: lr: 0.001529  loss: 0.5306 (0.5329)
[09:20:50.267823] {"train_lr": 0.0015313267846473566, "train_loss": 0.5328867847011203, "epoch": 352}
[09:20:50.268240] [09:20:50.268324] Training epoch 352 for 0:00:27
[09:20:50.268379] [09:20:50.272810] log_dir: ./exp/debug/cifar100-LT/debug
[09:20:51.729871] Epoch: [353]  [ 0/42]  eta: 0:01:01  lr: 0.001528  loss: 0.4912 (0.4912)  time: 1.4557  data: 0.9488  max mem: 9341
[09:21:01.890677] Epoch: [353]  [20/42]  eta: 0:00:12  lr: 0.001526  loss: 0.5179 (0.5253)  time: 0.5080  data: 0.0001  max mem: 9341
[09:21:12.028826] Epoch: [353]  [40/42]  eta: 0:00:01  lr: 0.001524  loss: 0.5424 (0.5314)  time: 0.5069  data: 0.0001  max mem: 9341
[09:21:12.534985] Epoch: [353]  [41/42]  eta: 0:00:00  lr: 0.001524  loss: 0.5411 (0.5315)  time: 0.5070  data: 0.0001  max mem: 9341
[09:21:12.709318] Epoch: [353] Total time: 0:00:22 (0.5342 s / it)
[09:21:12.710241] Averaged stats: lr: 0.001524  loss: 0.5411 (0.5307)
[09:21:17.365522] {"train_lr": 0.0015265563864296009, "train_loss": 0.5307373280326525, "epoch": 353}
[09:21:17.365832] [09:21:17.365913] Training epoch 353 for 0:00:27
[09:21:17.366023] [09:21:17.370378] log_dir: ./exp/debug/cifar100-LT/debug
[09:21:18.938362] Epoch: [354]  [ 0/42]  eta: 0:01:05  lr: 0.001523  loss: 0.5553 (0.5553)  time: 1.5665  data: 1.0656  max mem: 9341
[09:21:29.107703] Epoch: [354]  [20/42]  eta: 0:00:12  lr: 0.001522  loss: 0.5479 (0.5449)  time: 0.5084  data: 0.0001  max mem: 9341
[09:21:39.265886] Epoch: [354]  [40/42]  eta: 0:00:01  lr: 0.001520  loss: 0.5410 (0.5429)  time: 0.5079  data: 0.0001  max mem: 9341
[09:21:39.772533] Epoch: [354]  [41/42]  eta: 0:00:00  lr: 0.001520  loss: 0.5340 (0.5427)  time: 0.5079  data: 0.0001  max mem: 9341
[09:21:39.950920] Epoch: [354] Total time: 0:00:22 (0.5376 s / it)
[09:21:39.951899] Averaged stats: lr: 0.001520  loss: 0.5340 (0.5367)
[09:21:44.450178] {"train_lr": 0.0015217804082642019, "train_loss": 0.5367008178007036, "epoch": 354}
[09:21:44.450618] [09:21:44.450713] Training epoch 354 for 0:00:27
[09:21:44.450766] [09:21:44.455380] log_dir: ./exp/debug/cifar100-LT/debug
[09:21:46.085000] Epoch: [355]  [ 0/42]  eta: 0:01:08  lr: 0.001519  loss: 0.5662 (0.5662)  time: 1.6283  data: 1.1248  max mem: 9341
[09:21:56.252701] Epoch: [355]  [20/42]  eta: 0:00:12  lr: 0.001517  loss: 0.5340 (0.5339)  time: 0.5083  data: 0.0001  max mem: 9341
[09:22:06.393697] Epoch: [355]  [40/42]  eta: 0:00:01  lr: 0.001515  loss: 0.5350 (0.5357)  time: 0.5070  data: 0.0001  max mem: 9341
[09:22:06.899627] Epoch: [355]  [41/42]  eta: 0:00:00  lr: 0.001515  loss: 0.5346 (0.5356)  time: 0.5070  data: 0.0001  max mem: 9341
[09:22:07.058808] Epoch: [355] Total time: 0:00:22 (0.5382 s / it)
[09:22:07.071570] Averaged stats: lr: 0.001515  loss: 0.5346 (0.5354)
[09:22:11.721607] {"train_lr": 0.001516998931759449, "train_loss": 0.535398851902712, "epoch": 355}
[09:22:11.721943] [09:22:11.722041] Training epoch 355 for 0:00:27
[09:22:11.722092] [09:22:11.726387] log_dir: ./exp/debug/cifar100-LT/debug
[09:22:13.246163] Epoch: [356]  [ 0/42]  eta: 0:01:03  lr: 0.001514  loss: 0.5500 (0.5500)  time: 1.5186  data: 1.0094  max mem: 9341
[09:22:23.405768] Epoch: [356]  [20/42]  eta: 0:00:12  lr: 0.001512  loss: 0.5190 (0.5255)  time: 0.5079  data: 0.0001  max mem: 9341
[09:22:33.550436] Epoch: [356]  [40/42]  eta: 0:00:01  lr: 0.001510  loss: 0.5299 (0.5301)  time: 0.5072  data: 0.0001  max mem: 9341
[09:22:34.055681] Epoch: [356]  [41/42]  eta: 0:00:00  lr: 0.001510  loss: 0.5356 (0.5318)  time: 0.5072  data: 0.0001  max mem: 9341
[09:22:34.225625] Epoch: [356] Total time: 0:00:22 (0.5357 s / it)
[09:22:34.227540] Averaged stats: lr: 0.001510  loss: 0.5356 (0.5326)
[09:22:38.824856] {"train_lr": 0.0015122120386175783, "train_loss": 0.5325566877921423, "epoch": 356}
[09:22:38.825216] [09:22:38.825300] Training epoch 356 for 0:00:27
[09:22:38.825351] [09:22:38.829849] log_dir: ./exp/debug/cifar100-LT/debug
[09:22:40.463759] Epoch: [357]  [ 0/42]  eta: 0:01:08  lr: 0.001509  loss: 0.5631 (0.5631)  time: 1.6328  data: 1.1321  max mem: 9341
[09:22:50.647484] Epoch: [357]  [20/42]  eta: 0:00:12  lr: 0.001507  loss: 0.5208 (0.5340)  time: 0.5091  data: 0.0001  max mem: 9341
[09:23:00.816790] Epoch: [357]  [40/42]  eta: 0:00:01  lr: 0.001505  loss: 0.5203 (0.5299)  time: 0.5084  data: 0.0001  max mem: 9341
[09:23:01.322069] Epoch: [357]  [41/42]  eta: 0:00:00  lr: 0.001505  loss: 0.5203 (0.5295)  time: 0.5083  data: 0.0001  max mem: 9341
[09:23:01.484966] Epoch: [357] Total time: 0:00:22 (0.5394 s / it)
[09:23:01.489726] Averaged stats: lr: 0.001505  loss: 0.5203 (0.5343)
[09:23:05.979507] {"train_lr": 0.0015074198106333927, "train_loss": 0.5343086698225566, "epoch": 357}
[09:23:05.979860] [09:23:05.979946] Training epoch 357 for 0:00:27
[09:23:05.979997] [09:23:05.984510] log_dir: ./exp/debug/cifar100-LT/debug
[09:23:07.604626] Epoch: [358]  [ 0/42]  eta: 0:01:07  lr: 0.001504  loss: 0.4836 (0.4836)  time: 1.6190  data: 1.1057  max mem: 9341
[09:23:17.761986] Epoch: [358]  [20/42]  eta: 0:00:12  lr: 0.001502  loss: 0.5349 (0.5303)  time: 0.5078  data: 0.0001  max mem: 9341
[09:23:27.908000] Epoch: [358]  [40/42]  eta: 0:00:01  lr: 0.001501  loss: 0.5531 (0.5393)  time: 0.5073  data: 0.0001  max mem: 9341
[09:23:28.414826] Epoch: [358]  [41/42]  eta: 0:00:00  lr: 0.001501  loss: 0.5531 (0.5398)  time: 0.5073  data: 0.0001  max mem: 9341
[09:23:28.570797] Epoch: [358] Total time: 0:00:22 (0.5378 s / it)
[09:23:28.581606] Averaged stats: lr: 0.001501  loss: 0.5531 (0.5348)
[09:23:33.221019] {"train_lr": 0.0015026223296928432, "train_loss": 0.5347591397308168, "epoch": 358}
[09:23:33.221394] [09:23:33.221479] Training epoch 358 for 0:00:27
[09:23:33.221534] [09:23:33.226315] log_dir: ./exp/debug/cifar100-LT/debug
[09:23:34.955694] Epoch: [359]  [ 0/42]  eta: 0:01:12  lr: 0.001499  loss: 0.5291 (0.5291)  time: 1.7277  data: 1.2304  max mem: 9341
[09:23:45.125101] Epoch: [359]  [20/42]  eta: 0:00:12  lr: 0.001498  loss: 0.5138 (0.5214)  time: 0.5084  data: 0.0001  max mem: 9341
[09:23:55.272597] Epoch: [359]  [40/42]  eta: 0:00:01  lr: 0.001496  loss: 0.5192 (0.5210)  time: 0.5073  data: 0.0001  max mem: 9341
[09:23:55.779272] Epoch: [359]  [41/42]  eta: 0:00:00  lr: 0.001496  loss: 0.5104 (0.5206)  time: 0.5074  data: 0.0001  max mem: 9341
[09:23:55.946751] Epoch: [359] Total time: 0:00:22 (0.5410 s / it)
[09:23:55.950854] Averaged stats: lr: 0.001496  loss: 0.5104 (0.5266)
[09:24:00.545397] {"train_lr": 0.0014978196777716414, "train_loss": 0.5266009876061053, "epoch": 359}
[09:24:00.545773] [09:24:00.545856] Training epoch 359 for 0:00:27
[09:24:00.545909] [09:24:00.550139] log_dir: ./exp/debug/cifar100-LT/debug
[09:24:02.036684] Epoch: [360]  [ 0/42]  eta: 0:01:02  lr: 0.001495  loss: 0.5075 (0.5075)  time: 1.4852  data: 0.9771  max mem: 9341
[09:24:12.219429] Epoch: [360]  [20/42]  eta: 0:00:12  lr: 0.001493  loss: 0.5322 (0.5320)  time: 0.5091  data: 0.0001  max mem: 9341
[09:24:22.385819] Epoch: [360]  [40/42]  eta: 0:00:01  lr: 0.001491  loss: 0.5258 (0.5285)  time: 0.5083  data: 0.0001  max mem: 9341
[09:24:22.891218] Epoch: [360]  [41/42]  eta: 0:00:00  lr: 0.001491  loss: 0.5243 (0.5280)  time: 0.5083  data: 0.0001  max mem: 9341
[09:24:23.060993] Epoch: [360] Total time: 0:00:22 (0.5360 s / it)
[09:24:23.061799] Averaged stats: lr: 0.001491  loss: 0.5243 (0.5267)
[09:24:27.635160] {"train_lr": 0.001493011936933861, "train_loss": 0.5266873226279304, "epoch": 360}
[09:24:27.635517] [09:24:27.635599] Training epoch 360 for 0:00:27
[09:24:27.635650] [09:24:27.639885] log_dir: ./exp/debug/cifar100-LT/debug
[09:24:29.095472] Epoch: [361]  [ 0/42]  eta: 0:01:01  lr: 0.001490  loss: 0.4733 (0.4733)  time: 1.4543  data: 0.9490  max mem: 9341
[09:24:39.257109] Epoch: [361]  [20/42]  eta: 0:00:12  lr: 0.001488  loss: 0.5347 (0.5297)  time: 0.5080  data: 0.0001  max mem: 9341
[09:24:49.402686] Epoch: [361]  [40/42]  eta: 0:00:01  lr: 0.001486  loss: 0.5250 (0.5295)  time: 0.5072  data: 0.0001  max mem: 9341
[09:24:49.907672] Epoch: [361]  [41/42]  eta: 0:00:00  lr: 0.001486  loss: 0.5250 (0.5286)  time: 0.5071  data: 0.0001  max mem: 9341
[09:24:50.079405] Epoch: [361] Total time: 0:00:22 (0.5343 s / it)
[09:24:50.089694] Averaged stats: lr: 0.001486  loss: 0.5250 (0.5293)
[09:24:54.717750] {"train_lr": 0.0014881991893305236, "train_loss": 0.5292869623573053, "epoch": 361}
[09:24:54.718093] [09:24:54.718171] Training epoch 361 for 0:00:27
[09:24:54.718222] [09:24:54.723009] log_dir: ./exp/debug/cifar100-LT/debug
[09:24:56.317276] Epoch: [362]  [ 0/42]  eta: 0:01:06  lr: 0.001485  loss: 0.5458 (0.5458)  time: 1.5929  data: 1.0801  max mem: 9341
[09:25:06.503400] Epoch: [362]  [20/42]  eta: 0:00:12  lr: 0.001483  loss: 0.5347 (0.5303)  time: 0.5093  data: 0.0001  max mem: 9341
[09:25:16.662503] Epoch: [362]  [40/42]  eta: 0:00:01  lr: 0.001481  loss: 0.5339 (0.5298)  time: 0.5079  data: 0.0001  max mem: 9341
[09:25:17.168328] Epoch: [362]  [41/42]  eta: 0:00:00  lr: 0.001481  loss: 0.5322 (0.5294)  time: 0.5080  data: 0.0001  max mem: 9341
[09:25:17.344283] Epoch: [362] Total time: 0:00:22 (0.5386 s / it)
[09:25:17.345217] Averaged stats: lr: 0.001481  loss: 0.5322 (0.5314)
[09:25:21.863863] {"train_lr": 0.0014833815171982118, "train_loss": 0.5313956304675057, "epoch": 362}
[09:25:21.864215] [09:25:21.864304] Training epoch 362 for 0:00:27
[09:25:21.864451] [09:25:21.868797] log_dir: ./exp/debug/cifar100-LT/debug
[09:25:23.568485] Epoch: [363]  [ 0/42]  eta: 0:01:11  lr: 0.001480  loss: 0.4677 (0.4677)  time: 1.6986  data: 1.2068  max mem: 9341
[09:25:33.736863] Epoch: [363]  [20/42]  eta: 0:00:12  lr: 0.001478  loss: 0.5316 (0.5257)  time: 0.5084  data: 0.0001  max mem: 9341
[09:25:43.898130] Epoch: [363]  [40/42]  eta: 0:00:01  lr: 0.001476  loss: 0.5257 (0.5254)  time: 0.5080  data: 0.0001  max mem: 9341
[09:25:44.404347] Epoch: [363]  [41/42]  eta: 0:00:00  lr: 0.001476  loss: 0.5291 (0.5267)  time: 0.5080  data: 0.0001  max mem: 9341
[09:25:44.564745] Epoch: [363] Total time: 0:00:22 (0.5404 s / it)
[09:25:44.569089] Averaged stats: lr: 0.001476  loss: 0.5291 (0.5307)
[09:25:49.129869] {"train_lr": 0.0014785590028576438, "train_loss": 0.5306658533712229, "epoch": 363}
[09:25:49.130220] [09:25:49.130302] Training epoch 363 for 0:00:27
[09:25:49.130353] [09:25:49.134736] log_dir: ./exp/debug/cifar100-LT/debug
[09:25:50.837806] Epoch: [364]  [ 0/42]  eta: 0:01:11  lr: 0.001475  loss: 0.5775 (0.5775)  time: 1.7021  data: 1.1912  max mem: 9341
[09:26:00.999474] Epoch: [364]  [20/42]  eta: 0:00:12  lr: 0.001473  loss: 0.5245 (0.5247)  time: 0.5080  data: 0.0001  max mem: 9341
[09:26:11.153254] Epoch: [364]  [40/42]  eta: 0:00:01  lr: 0.001472  loss: 0.5330 (0.5279)  time: 0.5076  data: 0.0001  max mem: 9341
[09:26:11.658181] Epoch: [364]  [41/42]  eta: 0:00:00  lr: 0.001472  loss: 0.5340 (0.5285)  time: 0.5076  data: 0.0001  max mem: 9341
[09:26:11.834444] Epoch: [364] Total time: 0:00:22 (0.5405 s / it)
[09:26:11.838882] Averaged stats: lr: 0.001472  loss: 0.5340 (0.5281)
[09:26:16.299615] {"train_lr": 0.0014737317287122891, "train_loss": 0.52812634781003, "epoch": 364}
[09:26:16.300003] [09:26:16.300130] Training epoch 364 for 0:00:27
[09:26:16.300186] [09:26:16.304977] log_dir: ./exp/debug/cifar100-LT/debug
[09:26:17.982540] Epoch: [365]  [ 0/42]  eta: 0:01:10  lr: 0.001470  loss: 0.5866 (0.5866)  time: 1.6762  data: 1.1637  max mem: 9341
[09:26:28.146276] Epoch: [365]  [20/42]  eta: 0:00:12  lr: 0.001469  loss: 0.5300 (0.5293)  time: 0.5081  data: 0.0001  max mem: 9341
[09:26:38.293971] Epoch: [365]  [40/42]  eta: 0:00:01  lr: 0.001467  loss: 0.5262 (0.5284)  time: 0.5073  data: 0.0001  max mem: 9341
[09:26:38.799646] Epoch: [365]  [41/42]  eta: 0:00:00  lr: 0.001467  loss: 0.5271 (0.5285)  time: 0.5073  data: 0.0001  max mem: 9341
[09:26:38.962786] Epoch: [365] Total time: 0:00:22 (0.5395 s / it)
[09:26:38.964762] Averaged stats: lr: 0.001467  loss: 0.5271 (0.5253)
[09:26:43.552932] {"train_lr": 0.0014688997772469442, "train_loss": 0.5252651260012672, "epoch": 365}
[09:26:43.553284] [09:26:43.553363] Training epoch 365 for 0:00:27
[09:26:43.553414] [09:26:43.557753] log_dir: ./exp/debug/cifar100-LT/debug
[09:26:45.116809] Epoch: [366]  [ 0/42]  eta: 0:01:05  lr: 0.001466  loss: 0.5499 (0.5499)  time: 1.5579  data: 1.0515  max mem: 9341
[09:26:55.279014] Epoch: [366]  [20/42]  eta: 0:00:12  lr: 0.001464  loss: 0.5224 (0.5216)  time: 0.5081  data: 0.0001  max mem: 9341
[09:27:05.428137] Epoch: [366]  [40/42]  eta: 0:00:01  lr: 0.001462  loss: 0.5256 (0.5232)  time: 0.5074  data: 0.0001  max mem: 9341
[09:27:05.933368] Epoch: [366]  [41/42]  eta: 0:00:00  lr: 0.001462  loss: 0.5256 (0.5224)  time: 0.5074  data: 0.0001  max mem: 9341
[09:27:06.101784] Epoch: [366] Total time: 0:00:22 (0.5368 s / it)
[09:27:06.106732] Averaged stats: lr: 0.001462  loss: 0.5256 (0.5267)
[09:27:10.714239] {"train_lr": 0.0014640632310263227, "train_loss": 0.5266818460964021, "epoch": 366}
[09:27:10.714560] [09:27:10.714643] Training epoch 366 for 0:00:27
[09:27:10.714694] [09:27:10.719130] log_dir: ./exp/debug/cifar100-LT/debug
[09:27:12.321900] Epoch: [367]  [ 0/42]  eta: 0:01:07  lr: 0.001461  loss: 0.5388 (0.5388)  time: 1.6016  data: 1.0930  max mem: 9341
[09:27:22.497845] Epoch: [367]  [20/42]  eta: 0:00:12  lr: 0.001459  loss: 0.5316 (0.5267)  time: 0.5087  data: 0.0001  max mem: 9341
[09:27:32.654765] Epoch: [367]  [40/42]  eta: 0:00:01  lr: 0.001457  loss: 0.5218 (0.5228)  time: 0.5078  data: 0.0001  max mem: 9341
[09:27:33.159261] Epoch: [367]  [41/42]  eta: 0:00:00  lr: 0.001457  loss: 0.5218 (0.5221)  time: 0.5078  data: 0.0001  max mem: 9341
[09:27:33.314604] Epoch: [367] Total time: 0:00:22 (0.5380 s / it)
[09:27:33.333063] Averaged stats: lr: 0.001457  loss: 0.5218 (0.5239)
[09:27:37.902976] {"train_lr": 0.00145922217269366, "train_loss": 0.52388838997909, "epoch": 367}
[09:27:37.903400] [09:27:37.903491] Training epoch 367 for 0:00:27
[09:27:37.903542] [09:27:37.908433] log_dir: ./exp/debug/cifar100-LT/debug
[09:27:39.408426] Epoch: [368]  [ 0/42]  eta: 0:01:02  lr: 0.001456  loss: 0.5199 (0.5199)  time: 1.4987  data: 0.9987  max mem: 9341
[09:27:49.572404] Epoch: [368]  [20/42]  eta: 0:00:12  lr: 0.001454  loss: 0.5351 (0.5269)  time: 0.5081  data: 0.0001  max mem: 9341
[09:27:59.725507] Epoch: [368]  [40/42]  eta: 0:00:01  lr: 0.001452  loss: 0.5349 (0.5311)  time: 0.5076  data: 0.0001  max mem: 9341
[09:28:00.230485] Epoch: [368]  [41/42]  eta: 0:00:00  lr: 0.001452  loss: 0.5317 (0.5311)  time: 0.5076  data: 0.0001  max mem: 9341
[09:28:00.408142] Epoch: [368] Total time: 0:00:22 (0.5357 s / it)
[09:28:00.408848] Averaged stats: lr: 0.001452  loss: 0.5317 (0.5296)
[09:28:04.999351] {"train_lr": 0.001454376684969283, "train_loss": 0.5295694324941862, "epoch": 368}
[09:28:04.999689] [09:28:04.999768] Training epoch 368 for 0:00:27
[09:28:04.999835] [09:28:05.004255] log_dir: ./exp/debug/cifar100-LT/debug
[09:28:06.624745] Epoch: [369]  [ 0/42]  eta: 0:01:08  lr: 0.001451  loss: 0.5591 (0.5591)  time: 1.6193  data: 1.1050  max mem: 9341
[09:28:16.823193] Epoch: [369]  [20/42]  eta: 0:00:12  lr: 0.001449  loss: 0.5163 (0.5226)  time: 0.5099  data: 0.0001  max mem: 9341
[09:28:26.964694] Epoch: [369]  [40/42]  eta: 0:00:01  lr: 0.001447  loss: 0.5227 (0.5223)  time: 0.5070  data: 0.0001  max mem: 9341
[09:28:27.469205] Epoch: [369]  [41/42]  eta: 0:00:00  lr: 0.001447  loss: 0.5227 (0.5225)  time: 0.5070  data: 0.0001  max mem: 9341
[09:28:27.625668] Epoch: [369] Total time: 0:00:22 (0.5386 s / it)
[09:28:27.632496] Averaged stats: lr: 0.001447  loss: 0.5227 (0.5259)
[09:28:32.150903] {"train_lr": 0.0014495268506492104, "train_loss": 0.5258685455081009, "epoch": 369}
[09:28:32.151245] [09:28:32.151326] Training epoch 369 for 0:00:27
[09:28:32.151376] [09:28:32.155777] log_dir: ./exp/debug/cifar100-LT/debug
[09:28:33.631070] Epoch: [370]  [ 0/42]  eta: 0:01:01  lr: 0.001446  loss: 0.5798 (0.5798)  time: 1.4741  data: 0.9713  max mem: 9341
[09:28:43.789632] Epoch: [370]  [20/42]  eta: 0:00:12  lr: 0.001444  loss: 0.5313 (0.5313)  time: 0.5079  data: 0.0001  max mem: 9341
[09:28:53.940266] Epoch: [370]  [40/42]  eta: 0:00:01  lr: 0.001443  loss: 0.5102 (0.5276)  time: 0.5075  data: 0.0001  max mem: 9341
[09:28:54.447554] Epoch: [370]  [41/42]  eta: 0:00:00  lr: 0.001443  loss: 0.5141 (0.5278)  time: 0.5076  data: 0.0001  max mem: 9341
[09:28:54.618043] Epoch: [370] Total time: 0:00:22 (0.5348 s / it)
[09:28:54.620594] Averaged stats: lr: 0.001443  loss: 0.5141 (0.5255)
[09:28:59.082064] {"train_lr": 0.0014446727526037269, "train_loss": 0.5254905450911749, "epoch": 370}
[09:28:59.082444] [09:28:59.082526] Training epoch 370 for 0:00:26
[09:28:59.082577] [09:28:59.086957] log_dir: ./exp/debug/cifar100-LT/debug
[09:29:00.722777] Epoch: [371]  [ 0/42]  eta: 0:01:08  lr: 0.001441  loss: 0.5188 (0.5188)  time: 1.6345  data: 1.1197  max mem: 9341
[09:29:10.882197] Epoch: [371]  [20/42]  eta: 0:00:12  lr: 0.001440  loss: 0.5228 (0.5234)  time: 0.5079  data: 0.0001  max mem: 9341
[09:29:21.018813] Epoch: [371]  [40/42]  eta: 0:00:01  lr: 0.001438  loss: 0.5369 (0.5303)  time: 0.5068  data: 0.0001  max mem: 9341
[09:29:21.525139] Epoch: [371]  [41/42]  eta: 0:00:00  lr: 0.001438  loss: 0.5373 (0.5312)  time: 0.5068  data: 0.0001  max mem: 9341
[09:29:21.695194] Epoch: [371] Total time: 0:00:22 (0.5383 s / it)
[09:29:21.695909] Averaged stats: lr: 0.001438  loss: 0.5373 (0.5263)
[09:29:26.377243] {"train_lr": 0.0014398144737759774, "train_loss": 0.5262819446978115, "epoch": 371}
[09:29:26.377596] [09:29:26.377689] Training epoch 371 for 0:00:27
[09:29:26.377741] [09:29:26.382216] log_dir: ./exp/debug/cifar100-LT/debug
[09:29:27.832682] Epoch: [372]  [ 0/42]  eta: 0:01:00  lr: 0.001437  loss: 0.5388 (0.5388)  time: 1.4493  data: 0.9384  max mem: 9341
[09:29:38.004815] Epoch: [372]  [20/42]  eta: 0:00:12  lr: 0.001435  loss: 0.5269 (0.5272)  time: 0.5086  data: 0.0001  max mem: 9341
[09:29:48.160887] Epoch: [372]  [40/42]  eta: 0:00:01  lr: 0.001433  loss: 0.5198 (0.5244)  time: 0.5078  data: 0.0001  max mem: 9341
[09:29:48.669089] Epoch: [372]  [41/42]  eta: 0:00:00  lr: 0.001433  loss: 0.5255 (0.5245)  time: 0.5078  data: 0.0001  max mem: 9341
[09:29:48.825422] Epoch: [372] Total time: 0:00:22 (0.5344 s / it)
[09:29:48.840431] Averaged stats: lr: 0.001433  loss: 0.5255 (0.5250)
[09:29:53.472451] {"train_lr": 0.0014349520971805416, "train_loss": 0.525021968852906, "epoch": 372}
[09:29:53.472808] [09:29:53.472890] Training epoch 372 for 0:00:27
[09:29:53.472943] [09:29:53.477425] log_dir: ./exp/debug/cifar100-LT/debug
[09:29:54.928610] Epoch: [373]  [ 0/42]  eta: 0:01:00  lr: 0.001432  loss: 0.5379 (0.5379)  time: 1.4494  data: 0.9522  max mem: 9341
[09:30:05.086787] Epoch: [373]  [20/42]  eta: 0:00:12  lr: 0.001430  loss: 0.5305 (0.5320)  time: 0.5078  data: 0.0001  max mem: 9341
[09:30:15.230392] Epoch: [373]  [40/42]  eta: 0:00:01  lr: 0.001428  loss: 0.5290 (0.5328)  time: 0.5071  data: 0.0001  max mem: 9341
[09:30:15.734621] Epoch: [373]  [41/42]  eta: 0:00:00  lr: 0.001428  loss: 0.5290 (0.5312)  time: 0.5070  data: 0.0001  max mem: 9341
[09:30:15.897677] Epoch: [373] Total time: 0:00:22 (0.5338 s / it)
[09:30:15.905102] Averaged stats: lr: 0.001428  loss: 0.5290 (0.5244)
[09:30:20.544213] {"train_lr": 0.0014300857059020213, "train_loss": 0.5244432749847571, "epoch": 373}
[09:30:20.544600] [09:30:20.544682] Training epoch 373 for 0:00:27
[09:30:20.544733] [09:30:20.549130] log_dir: ./exp/debug/cifar100-LT/debug
[09:30:22.011312] Epoch: [374]  [ 0/42]  eta: 0:01:01  lr: 0.001427  loss: 0.5037 (0.5037)  time: 1.4609  data: 0.9580  max mem: 9341
[09:30:32.173251] Epoch: [374]  [20/42]  eta: 0:00:12  lr: 0.001425  loss: 0.5039 (0.5133)  time: 0.5080  data: 0.0001  max mem: 9341
[09:30:42.326646] Epoch: [374]  [40/42]  eta: 0:00:01  lr: 0.001423  loss: 0.5189 (0.5204)  time: 0.5076  data: 0.0001  max mem: 9341
[09:30:42.833298] Epoch: [374]  [41/42]  eta: 0:00:00  lr: 0.001423  loss: 0.5159 (0.5194)  time: 0.5076  data: 0.0001  max mem: 9341
[09:30:43.013747] Epoch: [374] Total time: 0:00:22 (0.5349 s / it)
[09:30:43.014440] Averaged stats: lr: 0.001423  loss: 0.5159 (0.5255)
[09:30:47.620865] {"train_lr": 0.001425215383093614, "train_loss": 0.5254844499840623, "epoch": 374}
[09:30:47.621195] [09:30:47.621273] Training epoch 374 for 0:00:27
[09:30:47.621323] [09:30:47.625629] log_dir: ./exp/debug/cifar100-LT/debug
[09:30:49.349426] Epoch: [375]  [ 0/42]  eta: 0:01:12  lr: 0.001422  loss: 0.4970 (0.4970)  time: 1.7228  data: 1.2253  max mem: 9341
[09:30:59.554407] Epoch: [375]  [20/42]  eta: 0:00:12  lr: 0.001420  loss: 0.5178 (0.5235)  time: 0.5102  data: 0.0001  max mem: 9341
[09:31:09.719216] Epoch: [375]  [40/42]  eta: 0:00:01  lr: 0.001418  loss: 0.5271 (0.5270)  time: 0.5082  data: 0.0001  max mem: 9341
[09:31:10.223558] Epoch: [375]  [41/42]  eta: 0:00:00  lr: 0.001418  loss: 0.5268 (0.5270)  time: 0.5081  data: 0.0001  max mem: 9341
[09:31:10.386542] Epoch: [375] Total time: 0:00:22 (0.5419 s / it)
[09:31:10.403254] Averaged stats: lr: 0.001418  loss: 0.5268 (0.5245)
[09:31:15.017332] {"train_lr": 0.0014203412119756984, "train_loss": 0.5244782558154493, "epoch": 375}
[09:31:15.017700] [09:31:15.017780] Training epoch 375 for 0:00:27
[09:31:15.017830] [09:31:15.022186] log_dir: ./exp/debug/cifar100-LT/debug
[09:31:16.458471] Epoch: [376]  [ 0/42]  eta: 0:01:00  lr: 0.001417  loss: 0.5873 (0.5873)  time: 1.4351  data: 0.9306  max mem: 9341
[09:31:26.655305] Epoch: [376]  [20/42]  eta: 0:00:12  lr: 0.001415  loss: 0.5220 (0.5256)  time: 0.5098  data: 0.0001  max mem: 9341
[09:31:36.820287] Epoch: [376]  [40/42]  eta: 0:00:01  lr: 0.001413  loss: 0.5244 (0.5261)  time: 0.5082  data: 0.0001  max mem: 9341
[09:31:37.327740] Epoch: [376]  [41/42]  eta: 0:00:00  lr: 0.001413  loss: 0.5244 (0.5265)  time: 0.5083  data: 0.0001  max mem: 9341
[09:31:37.489058] Epoch: [376] Total time: 0:00:22 (0.5349 s / it)
[09:31:37.497528] Averaged stats: lr: 0.001413  loss: 0.5244 (0.5256)
[09:31:42.012860] {"train_lr": 0.001415463275834414, "train_loss": 0.5256354047783783, "epoch": 376}
[09:31:42.013244] [09:31:42.013325] Training epoch 376 for 0:00:26
[09:31:42.013375] [09:31:42.017832] log_dir: ./exp/debug/cifar100-LT/debug
[09:31:43.454879] Epoch: [377]  [ 0/42]  eta: 0:01:00  lr: 0.001412  loss: 0.5448 (0.5448)  time: 1.4357  data: 0.9414  max mem: 9341
[09:31:53.618912] Epoch: [377]  [20/42]  eta: 0:00:12  lr: 0.001410  loss: 0.5239 (0.5262)  time: 0.5081  data: 0.0001  max mem: 9341
[09:32:03.762122] Epoch: [377]  [40/42]  eta: 0:00:01  lr: 0.001408  loss: 0.5122 (0.5223)  time: 0.5071  data: 0.0001  max mem: 9341
[09:32:04.269000] Epoch: [377]  [41/42]  eta: 0:00:00  lr: 0.001408  loss: 0.5239 (0.5233)  time: 0.5072  data: 0.0001  max mem: 9341
[09:32:04.420022] Epoch: [377] Total time: 0:00:22 (0.5334 s / it)
[09:32:04.442816] Averaged stats: lr: 0.001408  loss: 0.5239 (0.5217)
[09:32:09.136214] {"train_lr": 0.0014105816580202283, "train_loss": 0.5217213618258635, "epoch": 377}
[09:32:09.136587] [09:32:09.136691] Training epoch 377 for 0:00:27
[09:32:09.136744] [09:32:09.140991] log_dir: ./exp/debug/cifar100-LT/debug
[09:32:10.757019] Epoch: [378]  [ 0/42]  eta: 0:01:07  lr: 0.001407  loss: 0.5166 (0.5166)  time: 1.6149  data: 1.1091  max mem: 9341
[09:32:20.925959] Epoch: [378]  [20/42]  eta: 0:00:12  lr: 0.001405  loss: 0.5186 (0.5229)  time: 0.5084  data: 0.0001  max mem: 9341
[09:32:31.067860] Epoch: [378]  [40/42]  eta: 0:00:01  lr: 0.001404  loss: 0.5231 (0.5210)  time: 0.5070  data: 0.0001  max mem: 9341
[09:32:31.573946] Epoch: [378]  [41/42]  eta: 0:00:00  lr: 0.001404  loss: 0.5235 (0.5214)  time: 0.5071  data: 0.0001  max mem: 9341
[09:32:31.744339] Epoch: [378] Total time: 0:00:22 (0.5382 s / it)
[09:32:31.749101] Averaged stats: lr: 0.001404  loss: 0.5235 (0.5244)
[09:32:36.255805] {"train_lr": 0.0014056964419465214, "train_loss": 0.5244090645795777, "epoch": 378}
[09:32:36.256197] [09:32:36.256287] Training epoch 378 for 0:00:27
[09:32:36.256342] [09:32:36.260633] log_dir: ./exp/debug/cifar100-LT/debug
[09:32:37.912887] Epoch: [379]  [ 0/42]  eta: 0:01:09  lr: 0.001402  loss: 0.5317 (0.5317)  time: 1.6510  data: 1.1460  max mem: 9341
[09:32:48.078677] Epoch: [379]  [20/42]  eta: 0:00:12  lr: 0.001401  loss: 0.5209 (0.5243)  time: 0.5082  data: 0.0001  max mem: 9341
[09:32:58.229690] Epoch: [379]  [40/42]  eta: 0:00:01  lr: 0.001399  loss: 0.5183 (0.5241)  time: 0.5075  data: 0.0001  max mem: 9341
[09:32:58.736347] Epoch: [379]  [41/42]  eta: 0:00:00  lr: 0.001399  loss: 0.5183 (0.5240)  time: 0.5076  data: 0.0001  max mem: 9341
[09:32:58.911117] Epoch: [379] Total time: 0:00:22 (0.5393 s / it)
[09:32:58.912017] Averaged stats: lr: 0.001399  loss: 0.5183 (0.5239)
[09:33:03.480332] {"train_lr": 0.0014008077110881586, "train_loss": 0.5238558834507352, "epoch": 379}
[09:33:03.480728] [09:33:03.480831] Training epoch 379 for 0:00:27
[09:33:03.480883] [09:33:03.485158] log_dir: ./exp/debug/cifar100-LT/debug
[09:33:05.208137] Epoch: [380]  [ 0/42]  eta: 0:01:12  lr: 0.001398  loss: 0.5304 (0.5304)  time: 1.7217  data: 1.2225  max mem: 9341
[09:33:15.373295] Epoch: [380]  [20/42]  eta: 0:00:12  lr: 0.001396  loss: 0.5331 (0.5268)  time: 0.5082  data: 0.0001  max mem: 9341
[09:33:25.522234] Epoch: [380]  [40/42]  eta: 0:00:01  lr: 0.001394  loss: 0.5242 (0.5262)  time: 0.5074  data: 0.0001  max mem: 9341
[09:33:26.028876] Epoch: [380]  [41/42]  eta: 0:00:00  lr: 0.001394  loss: 0.5304 (0.5265)  time: 0.5074  data: 0.0001  max mem: 9341
[09:33:26.192781] Epoch: [380] Total time: 0:00:22 (0.5407 s / it)
[09:33:26.205242] Averaged stats: lr: 0.001394  loss: 0.5304 (0.5228)
[09:33:30.810615] {"train_lr": 0.001395915548980059, "train_loss": 0.522755298408724, "epoch": 380}
[09:33:30.810969] [09:33:30.811051] Training epoch 380 for 0:00:27
[09:33:30.811101] [09:33:30.815434] log_dir: ./exp/debug/cifar100-LT/debug
[09:33:32.350269] Epoch: [381]  [ 0/42]  eta: 0:01:04  lr: 0.001393  loss: 0.5463 (0.5463)  time: 1.5335  data: 1.0214  max mem: 9341
[09:33:42.580914] Epoch: [381]  [20/42]  eta: 0:00:12  lr: 0.001391  loss: 0.5295 (0.5286)  time: 0.5115  data: 0.0001  max mem: 9341
[09:33:52.735136] Epoch: [381]  [40/42]  eta: 0:00:01  lr: 0.001389  loss: 0.5298 (0.5249)  time: 0.5077  data: 0.0001  max mem: 9341
[09:33:53.239563] Epoch: [381]  [41/42]  eta: 0:00:00  lr: 0.001389  loss: 0.5298 (0.5250)  time: 0.5075  data: 0.0001  max mem: 9341
[09:33:53.407365] Epoch: [381] Total time: 0:00:22 (0.5379 s / it)
[09:33:53.416231] Averaged stats: lr: 0.001389  loss: 0.5298 (0.5267)
[09:33:58.005767] {"train_lr": 0.0013910200392157773, "train_loss": 0.5267046643864541, "epoch": 381}
[09:33:58.006129] [09:33:58.006212] Training epoch 381 for 0:00:27
[09:33:58.006264] [09:33:58.012571] log_dir: ./exp/debug/cifar100-LT/debug
[09:33:59.738557] Epoch: [382]  [ 0/42]  eta: 0:01:12  lr: 0.001388  loss: 0.5627 (0.5627)  time: 1.7250  data: 1.2287  max mem: 9341
[09:34:09.910806] Epoch: [382]  [20/42]  eta: 0:00:12  lr: 0.001386  loss: 0.5283 (0.5321)  time: 0.5086  data: 0.0001  max mem: 9341
[09:34:20.064395] Epoch: [382]  [40/42]  eta: 0:00:01  lr: 0.001384  loss: 0.5256 (0.5280)  time: 0.5076  data: 0.0001  max mem: 9341
[09:34:20.569299] Epoch: [382]  [41/42]  eta: 0:00:00  lr: 0.001384  loss: 0.5255 (0.5273)  time: 0.5076  data: 0.0001  max mem: 9341
[09:34:20.731966] Epoch: [382] Total time: 0:00:22 (0.5409 s / it)
[09:34:20.734729] Averaged stats: lr: 0.001384  loss: 0.5255 (0.5238)
[09:34:25.371931] {"train_lr": 0.0013861212654460687, "train_loss": 0.5238181003147647, "epoch": 382}
[09:34:25.372334] [09:34:25.372458] Training epoch 382 for 0:00:27
[09:34:25.372508] [09:34:25.376846] log_dir: ./exp/debug/cifar100-LT/debug
[09:34:26.914767] Epoch: [383]  [ 0/42]  eta: 0:01:04  lr: 0.001383  loss: 0.5693 (0.5693)  time: 1.5366  data: 1.0135  max mem: 9341
[09:34:37.084213] Epoch: [383]  [20/42]  eta: 0:00:12  lr: 0.001381  loss: 0.5261 (0.5298)  time: 0.5084  data: 0.0001  max mem: 9341
[09:34:47.229321] Epoch: [383]  [40/42]  eta: 0:00:01  lr: 0.001379  loss: 0.5279 (0.5276)  time: 0.5072  data: 0.0001  max mem: 9341
[09:34:47.734030] Epoch: [383]  [41/42]  eta: 0:00:00  lr: 0.001379  loss: 0.5273 (0.5272)  time: 0.5072  data: 0.0001  max mem: 9341
[09:34:47.901082] Epoch: [383] Total time: 0:00:22 (0.5363 s / it)
[09:34:47.904913] Averaged stats: lr: 0.001379  loss: 0.5273 (0.5273)
[09:34:52.432778] {"train_lr": 0.00138121931137746, "train_loss": 0.5273233100417114, "epoch": 383}
[09:34:52.433122] [09:34:52.433205] Training epoch 383 for 0:00:27
[09:34:52.433256] [09:34:52.437439] log_dir: ./exp/debug/cifar100-LT/debug
[09:34:53.949266] Epoch: [384]  [ 0/42]  eta: 0:01:03  lr: 0.001378  loss: 0.5417 (0.5417)  time: 1.5107  data: 1.0050  max mem: 9341
[09:35:04.117084] Epoch: [384]  [20/42]  eta: 0:00:12  lr: 0.001376  loss: 0.5216 (0.5257)  time: 0.5083  data: 0.0001  max mem: 9341
[09:35:14.264990] Epoch: [384]  [40/42]  eta: 0:00:01  lr: 0.001374  loss: 0.5145 (0.5228)  time: 0.5073  data: 0.0001  max mem: 9341
[09:35:14.769922] Epoch: [384]  [41/42]  eta: 0:00:00  lr: 0.001374  loss: 0.5145 (0.5228)  time: 0.5072  data: 0.0001  max mem: 9341
[09:35:14.918754] Epoch: [384] Total time: 0:00:22 (0.5353 s / it)
[09:35:14.934209] Averaged stats: lr: 0.001374  loss: 0.5145 (0.5245)
[09:35:19.469352] {"train_lr": 0.0013763142607708194, "train_loss": 0.5245228730851695, "epoch": 384}
[09:35:19.469784] [09:35:19.469877] Training epoch 384 for 0:00:27
[09:35:19.469930] [09:35:19.474932] log_dir: ./exp/debug/cifar100-LT/debug
[09:35:21.095388] Epoch: [385]  [ 0/42]  eta: 0:01:08  lr: 0.001373  loss: 0.5670 (0.5670)  time: 1.6192  data: 1.1118  max mem: 9341
[09:35:31.269599] Epoch: [385]  [20/42]  eta: 0:00:12  lr: 0.001371  loss: 0.5135 (0.5199)  time: 0.5087  data: 0.0001  max mem: 9341
[09:35:41.412835] Epoch: [385]  [40/42]  eta: 0:00:01  lr: 0.001369  loss: 0.5221 (0.5234)  time: 0.5071  data: 0.0001  max mem: 9341
[09:35:41.918592] Epoch: [385]  [41/42]  eta: 0:00:00  lr: 0.001369  loss: 0.5258 (0.5244)  time: 0.5071  data: 0.0001  max mem: 9341
[09:35:42.088071] Epoch: [385] Total time: 0:00:22 (0.5384 s / it)
[09:35:42.088864] Averaged stats: lr: 0.001369  loss: 0.5258 (0.5259)
[09:35:46.635974] {"train_lr": 0.0013714061974399307, "train_loss": 0.5258798260419142, "epoch": 385}
[09:35:46.636373] [09:35:46.636473] Training epoch 385 for 0:00:27
[09:35:46.636525] [09:35:46.640716] log_dir: ./exp/debug/cifar100-LT/debug
[09:35:48.222368] Epoch: [386]  [ 0/42]  eta: 0:01:06  lr: 0.001368  loss: 0.5028 (0.5028)  time: 1.5805  data: 1.0689  max mem: 9341
[09:35:58.380034] Epoch: [386]  [20/42]  eta: 0:00:12  lr: 0.001366  loss: 0.5390 (0.5355)  time: 0.5078  data: 0.0001  max mem: 9341
[09:36:08.512793] Epoch: [386]  [40/42]  eta: 0:00:01  lr: 0.001364  loss: 0.5162 (0.5274)  time: 0.5066  data: 0.0001  max mem: 9341
[09:36:09.018466] Epoch: [386]  [41/42]  eta: 0:00:00  lr: 0.001364  loss: 0.5162 (0.5267)  time: 0.5066  data: 0.0001  max mem: 9341
[09:36:09.173823] Epoch: [386] Total time: 0:00:22 (0.5365 s / it)
[09:36:09.190630] Averaged stats: lr: 0.001364  loss: 0.5162 (0.5250)
[09:36:13.674681] {"train_lr": 0.0013664952052500525, "train_loss": 0.5250068003577846, "epoch": 386}
[09:36:13.675049] [09:36:13.675132] Training epoch 386 for 0:00:27
[09:36:13.675182] [09:36:13.679379] log_dir: ./exp/debug/cifar100-LT/debug
[09:36:15.108889] Epoch: [387]  [ 0/42]  eta: 0:00:59  lr: 0.001363  loss: 0.4963 (0.4963)  time: 1.4281  data: 0.9220  max mem: 9341
[09:36:25.274636] Epoch: [387]  [20/42]  eta: 0:00:12  lr: 0.001361  loss: 0.5204 (0.5155)  time: 0.5082  data: 0.0001  max mem: 9341
[09:36:35.425335] Epoch: [387]  [40/42]  eta: 0:00:01  lr: 0.001359  loss: 0.5130 (0.5175)  time: 0.5075  data: 0.0001  max mem: 9341
[09:36:35.930160] Epoch: [387]  [41/42]  eta: 0:00:00  lr: 0.001359  loss: 0.5140 (0.5186)  time: 0.5073  data: 0.0001  max mem: 9341
[09:36:36.100222] Epoch: [387] Total time: 0:00:22 (0.5338 s / it)
[09:36:36.100929] Averaged stats: lr: 0.001359  loss: 0.5140 (0.5210)
[09:36:40.657860] {"train_lr": 0.0013615813681164923, "train_loss": 0.5209619406433332, "epoch": 387}
[09:36:40.658234] [09:36:40.658312] Training epoch 387 for 0:00:26
[09:36:40.658363] [09:36:40.662704] log_dir: ./exp/debug/cifar100-LT/debug
[09:36:42.239235] Epoch: [388]  [ 0/42]  eta: 0:01:06  lr: 0.001358  loss: 0.5246 (0.5246)  time: 1.5752  data: 1.0598  max mem: 9341
[09:36:52.405611] Epoch: [388]  [20/42]  eta: 0:00:12  lr: 0.001356  loss: 0.5270 (0.5293)  time: 0.5083  data: 0.0001  max mem: 9341
[09:37:02.557510] Epoch: [388]  [40/42]  eta: 0:00:01  lr: 0.001355  loss: 0.5210 (0.5257)  time: 0.5075  data: 0.0001  max mem: 9341
[09:37:03.064071] Epoch: [388]  [41/42]  eta: 0:00:00  lr: 0.001355  loss: 0.5136 (0.5254)  time: 0.5077  data: 0.0001  max mem: 9341
[09:37:03.227787] Epoch: [388] Total time: 0:00:22 (0.5373 s / it)
[09:37:03.236201] Averaged stats: lr: 0.001355  loss: 0.5136 (0.5239)
[09:37:07.910778] {"train_lr": 0.001356664770003171, "train_loss": 0.5239016427880242, "epoch": 388}
[09:37:07.911085] [09:37:07.911167] Training epoch 388 for 0:00:27
[09:37:07.911218] [09:37:07.915648] log_dir: ./exp/debug/cifar100-LT/debug
[09:37:09.487265] Epoch: [389]  [ 0/42]  eta: 0:01:05  lr: 0.001353  loss: 0.5384 (0.5384)  time: 1.5704  data: 1.0750  max mem: 9341
[09:37:19.658244] Epoch: [389]  [20/42]  eta: 0:00:12  lr: 0.001351  loss: 0.5191 (0.5220)  time: 0.5085  data: 0.0001  max mem: 9341
[09:37:29.818410] Epoch: [389]  [40/42]  eta: 0:00:01  lr: 0.001350  loss: 0.5203 (0.5218)  time: 0.5080  data: 0.0001  max mem: 9341
[09:37:30.323908] Epoch: [389]  [41/42]  eta: 0:00:00  lr: 0.001350  loss: 0.5179 (0.5215)  time: 0.5080  data: 0.0001  max mem: 9341
[09:37:30.497802] Epoch: [389] Total time: 0:00:22 (0.5377 s / it)
[09:37:30.498497] Averaged stats: lr: 0.001350  loss: 0.5179 (0.5197)
[09:37:34.998508] {"train_lr": 0.001351745494921181, "train_loss": 0.5197470771769682, "epoch": 389}
[09:37:34.998808] [09:37:34.998889] Training epoch 389 for 0:00:27
[09:37:34.999002] [09:37:35.003487] log_dir: ./exp/debug/cifar100-LT/debug
[09:37:36.419094] Epoch: [390]  [ 0/42]  eta: 0:00:59  lr: 0.001348  loss: 0.4665 (0.4665)  time: 1.4143  data: 0.9171  max mem: 9341
[09:37:46.588535] Epoch: [390]  [20/42]  eta: 0:00:12  lr: 0.001347  loss: 0.5291 (0.5198)  time: 0.5084  data: 0.0002  max mem: 9341
[09:37:56.729853] Epoch: [390]  [40/42]  eta: 0:00:01  lr: 0.001345  loss: 0.5164 (0.5191)  time: 0.5070  data: 0.0001  max mem: 9341
[09:37:57.237288] Epoch: [390]  [41/42]  eta: 0:00:00  lr: 0.001345  loss: 0.5202 (0.5199)  time: 0.5071  data: 0.0001  max mem: 9341
[09:37:57.417137] Epoch: [390] Total time: 0:00:22 (0.5337 s / it)
[09:37:57.418027] Averaged stats: lr: 0.001345  loss: 0.5202 (0.5245)
[09:38:01.981602] {"train_lr": 0.001346823626927365, "train_loss": 0.5245102000023637, "epoch": 390}
[09:38:01.981904] [09:38:01.981988] Training epoch 390 for 0:00:26
[09:38:01.982101] [09:38:01.986466] log_dir: ./exp/debug/cifar100-LT/debug
[09:38:03.700652] Epoch: [391]  [ 0/42]  eta: 0:01:11  lr: 0.001344  loss: 0.5074 (0.5074)  time: 1.7131  data: 1.2171  max mem: 9341
[09:38:13.887704] Epoch: [391]  [20/42]  eta: 0:00:12  lr: 0.001342  loss: 0.5249 (0.5196)  time: 0.5093  data: 0.0001  max mem: 9341
[09:38:24.056330] Epoch: [391]  [40/42]  eta: 0:00:01  lr: 0.001340  loss: 0.5202 (0.5225)  time: 0.5084  data: 0.0001  max mem: 9341
[09:38:24.562429] Epoch: [391]  [41/42]  eta: 0:00:00  lr: 0.001340  loss: 0.5281 (0.5232)  time: 0.5084  data: 0.0001  max mem: 9341
[09:38:24.720533] Epoch: [391] Total time: 0:00:22 (0.5413 s / it)
[09:38:24.738783] Averaged stats: lr: 0.001340  loss: 0.5281 (0.5212)
[09:38:29.393783] {"train_lr": 0.0013418992501228627, "train_loss": 0.5211878938689118, "epoch": 391}
[09:38:29.394131] [09:38:29.394215] Training epoch 391 for 0:00:27
[09:38:29.394266] [09:38:29.399204] log_dir: ./exp/debug/cifar100-LT/debug
[09:38:30.965968] Epoch: [392]  [ 0/42]  eta: 0:01:05  lr: 0.001339  loss: 0.5068 (0.5068)  time: 1.5655  data: 1.0502  max mem: 9341
[09:38:41.131148] Epoch: [392]  [20/42]  eta: 0:00:12  lr: 0.001337  loss: 0.5083 (0.5178)  time: 0.5082  data: 0.0001  max mem: 9341
[09:38:51.275815] Epoch: [392]  [40/42]  eta: 0:00:01  lr: 0.001335  loss: 0.5168 (0.5199)  time: 0.5072  data: 0.0001  max mem: 9341
[09:38:51.782504] Epoch: [392]  [41/42]  eta: 0:00:00  lr: 0.001335  loss: 0.5187 (0.5213)  time: 0.5072  data: 0.0001  max mem: 9341
[09:38:51.939496] Epoch: [392] Total time: 0:00:22 (0.5367 s / it)
[09:38:51.957090] Averaged stats: lr: 0.001335  loss: 0.5187 (0.5236)
[09:38:56.505246] {"train_lr": 0.0013369724486516916, "train_loss": 0.5235925439213004, "epoch": 392}
[09:38:56.505601] [09:38:56.505684] Training epoch 392 for 0:00:27
[09:38:56.505736] [09:38:56.510244] log_dir: ./exp/debug/cifar100-LT/debug
[09:38:58.022115] Epoch: [393]  [ 0/42]  eta: 0:01:03  lr: 0.001334  loss: 0.4845 (0.4845)  time: 1.5106  data: 1.0075  max mem: 9341
[09:39:08.182049] Epoch: [393]  [20/42]  eta: 0:00:12  lr: 0.001332  loss: 0.5244 (0.5224)  time: 0.5079  data: 0.0001  max mem: 9341
[09:39:18.327239] Epoch: [393]  [40/42]  eta: 0:00:01  lr: 0.001330  loss: 0.5204 (0.5205)  time: 0.5072  data: 0.0001  max mem: 9341
[09:39:18.832903] Epoch: [393]  [41/42]  eta: 0:00:00  lr: 0.001330  loss: 0.5211 (0.5209)  time: 0.5072  data: 0.0001  max mem: 9341
[09:39:18.994404] Epoch: [393] Total time: 0:00:22 (0.5353 s / it)
[09:39:19.000912] Averaged stats: lr: 0.001330  loss: 0.5211 (0.5221)
[09:39:23.549460] {"train_lr": 0.0013320433066992923, "train_loss": 0.5220656655728817, "epoch": 393}
[09:39:23.549811] [09:39:23.549892] Training epoch 393 for 0:00:27
[09:39:23.549943] [09:39:23.554393] log_dir: ./exp/debug/cifar100-LT/debug
[09:39:25.010156] Epoch: [394]  [ 0/42]  eta: 0:01:01  lr: 0.001329  loss: 0.5013 (0.5013)  time: 1.4543  data: 0.9464  max mem: 9341
[09:39:35.171913] Epoch: [394]  [20/42]  eta: 0:00:12  lr: 0.001327  loss: 0.5216 (0.5165)  time: 0.5080  data: 0.0001  max mem: 9341
[09:39:45.328331] Epoch: [394]  [40/42]  eta: 0:00:01  lr: 0.001325  loss: 0.5096 (0.5150)  time: 0.5078  data: 0.0001  max mem: 9341
[09:39:45.834638] Epoch: [394]  [41/42]  eta: 0:00:00  lr: 0.001325  loss: 0.5096 (0.5153)  time: 0.5077  data: 0.0001  max mem: 9341
[09:39:46.009645] Epoch: [394] Total time: 0:00:22 (0.5346 s / it)
[09:39:46.011976] Averaged stats: lr: 0.001325  loss: 0.5096 (0.5197)
[09:39:50.661398] {"train_lr": 0.0013271119084910996, "train_loss": 0.5197298675775528, "epoch": 394}
[09:39:50.661732] [09:39:50.661813] Training epoch 394 for 0:00:27
[09:39:50.661864] [09:39:50.666495] log_dir: ./exp/debug/cifar100-LT/debug
[09:39:52.248072] Epoch: [395]  [ 0/42]  eta: 0:01:06  lr: 0.001324  loss: 0.5027 (0.5027)  time: 1.5801  data: 1.0731  max mem: 9341
[09:40:02.436070] Epoch: [395]  [20/42]  eta: 0:00:12  lr: 0.001322  loss: 0.5243 (0.5288)  time: 0.5093  data: 0.0001  max mem: 9341
[09:40:12.586993] Epoch: [395]  [40/42]  eta: 0:00:01  lr: 0.001320  loss: 0.5098 (0.5220)  time: 0.5075  data: 0.0001  max mem: 9341
[09:40:13.091627] Epoch: [395]  [41/42]  eta: 0:00:00  lr: 0.001320  loss: 0.5098 (0.5222)  time: 0.5074  data: 0.0001  max mem: 9341
[09:40:13.265884] Epoch: [395] Total time: 0:00:22 (0.5381 s / it)
[09:40:13.269358] Averaged stats: lr: 0.001320  loss: 0.5098 (0.5197)
[09:40:17.931046] {"train_lr": 0.0013221783382911059, "train_loss": 0.5197217001446656, "epoch": 395}
[09:40:17.931506] [09:40:17.931613] Training epoch 395 for 0:00:27
[09:40:17.931665] [09:40:17.936595] log_dir: ./exp/debug/cifar100-LT/debug
[09:40:19.368664] Epoch: [396]  [ 0/42]  eta: 0:01:00  lr: 0.001319  loss: 0.5263 (0.5263)  time: 1.4304  data: 0.9275  max mem: 9341
[09:40:29.535937] Epoch: [396]  [20/42]  eta: 0:00:12  lr: 0.001317  loss: 0.5046 (0.5162)  time: 0.5083  data: 0.0001  max mem: 9341
[09:40:39.679882] Epoch: [396]  [40/42]  eta: 0:00:01  lr: 0.001315  loss: 0.5134 (0.5166)  time: 0.5071  data: 0.0001  max mem: 9341
[09:40:40.185378] Epoch: [396]  [41/42]  eta: 0:00:00  lr: 0.001315  loss: 0.5134 (0.5166)  time: 0.5072  data: 0.0001  max mem: 9341
[09:40:40.358772] Epoch: [396] Total time: 0:00:22 (0.5339 s / it)
[09:40:40.361703] Averaged stats: lr: 0.001315  loss: 0.5134 (0.5189)
[09:40:44.996050] {"train_lr": 0.0013172426804004115, "train_loss": 0.518853907961221, "epoch": 396}
[09:40:44.996584] [09:40:44.996677] Training epoch 396 for 0:00:27
[09:40:44.996730] [09:40:45.001865] log_dir: ./exp/debug/cifar100-LT/debug
[09:40:46.456160] Epoch: [397]  [ 0/42]  eta: 0:01:01  lr: 0.001314  loss: 0.4917 (0.4917)  time: 1.4531  data: 0.9492  max mem: 9341
[09:40:56.627719] Epoch: [397]  [20/42]  eta: 0:00:12  lr: 0.001312  loss: 0.5136 (0.5159)  time: 0.5085  data: 0.0001  max mem: 9341
[09:41:06.768488] Epoch: [397]  [40/42]  eta: 0:00:01  lr: 0.001310  loss: 0.5102 (0.5149)  time: 0.5070  data: 0.0001  max mem: 9341
[09:41:07.274939] Epoch: [397]  [41/42]  eta: 0:00:00  lr: 0.001310  loss: 0.5142 (0.5164)  time: 0.5070  data: 0.0001  max mem: 9341
[09:41:07.446880] Epoch: [397] Total time: 0:00:22 (0.5344 s / it)
[09:41:07.451136] Averaged stats: lr: 0.001310  loss: 0.5142 (0.5203)
[09:41:12.116292] {"train_lr": 0.0013123050191557939, "train_loss": 0.5202797676126162, "epoch": 397}
[09:41:12.116656] [09:41:12.116760] Training epoch 397 for 0:00:27
[09:41:12.116829] [09:41:12.121843] log_dir: ./exp/debug/cifar100-LT/debug
[09:41:13.843002] Epoch: [398]  [ 0/42]  eta: 0:01:12  lr: 0.001309  loss: 0.4825 (0.4825)  time: 1.7204  data: 1.2115  max mem: 9341
[09:41:24.023649] Epoch: [398]  [20/42]  eta: 0:00:12  lr: 0.001307  loss: 0.5272 (0.5198)  time: 0.5090  data: 0.0001  max mem: 9341
[09:41:34.189991] Epoch: [398]  [40/42]  eta: 0:00:01  lr: 0.001305  loss: 0.5197 (0.5207)  time: 0.5083  data: 0.0001  max mem: 9341
[09:41:34.696169] Epoch: [398]  [41/42]  eta: 0:00:00  lr: 0.001305  loss: 0.5241 (0.5212)  time: 0.5083  data: 0.0001  max mem: 9341
[09:41:34.882341] Epoch: [398] Total time: 0:00:22 (0.5419 s / it)
[09:41:34.897663] Averaged stats: lr: 0.001305  loss: 0.5241 (0.5193)
[09:41:39.497604] {"train_lr": 0.001307365438928258, "train_loss": 0.5192997835221744, "epoch": 398}
[09:41:39.497946] [09:41:39.498030] Training epoch 398 for 0:00:27
[09:41:39.498082] [09:41:39.502337] log_dir: ./exp/debug/cifar100-LT/debug
[09:41:41.134201] Epoch: [399]  [ 0/42]  eta: 0:01:08  lr: 0.001304  loss: 0.4895 (0.4895)  time: 1.6306  data: 1.1275  max mem: 9341
[09:41:51.296682] Epoch: [399]  [20/42]  eta: 0:00:12  lr: 0.001302  loss: 0.5127 (0.5151)  time: 0.5081  data: 0.0001  max mem: 9341
[09:42:01.448305] Epoch: [399]  [40/42]  eta: 0:00:01  lr: 0.001300  loss: 0.5194 (0.5184)  time: 0.5075  data: 0.0001  max mem: 9341
[09:42:01.954120] Epoch: [399]  [41/42]  eta: 0:00:00  lr: 0.001300  loss: 0.5196 (0.5195)  time: 0.5076  data: 0.0001  max mem: 9341
[09:42:02.124960] Epoch: [399] Total time: 0:00:22 (0.5386 s / it)
[09:42:02.125710] Averaged stats: lr: 0.001300  loss: 0.5196 (0.5195)
[09:42:06.863448] {"train_lr": 0.0013024240241216033, "train_loss": 0.5194856987467834, "epoch": 399}
[09:42:06.863773] [09:42:06.863853] Training epoch 399 for 0:00:27
[09:42:06.863904] [09:42:06.868359] log_dir: ./exp/debug/cifar100-LT/debug
[09:42:08.509727] Epoch: [400]  [ 0/42]  eta: 0:01:08  lr: 0.001299  loss: 0.4813 (0.4813)  time: 1.6403  data: 1.1261  max mem: 9341
[09:42:18.681113] Epoch: [400]  [20/42]  eta: 0:00:12  lr: 0.001297  loss: 0.5178 (0.5241)  time: 0.5085  data: 0.0001  max mem: 9341
[09:42:28.823265] Epoch: [400]  [40/42]  eta: 0:00:01  lr: 0.001295  loss: 0.5263 (0.5233)  time: 0.5071  data: 0.0001  max mem: 9341
[09:42:29.329495] Epoch: [400]  [41/42]  eta: 0:00:00  lr: 0.001295  loss: 0.5263 (0.5235)  time: 0.5071  data: 0.0001  max mem: 9341
[09:42:29.491883] Epoch: [400] Total time: 0:00:22 (0.5387 s / it)
[09:42:29.492906] Averaged stats: lr: 0.001295  loss: 0.5263 (0.5214)
[09:42:34.070478] {"train_lr": 0.0012974808591709724, "train_loss": 0.5214212339903627, "epoch": 400}
[09:42:34.070823] [09:42:34.070909] Training epoch 400 for 0:00:27
[09:42:34.070959] [09:42:34.075590] log_dir: ./exp/debug/cifar100-LT/debug
[09:42:35.752285] Epoch: [401]  [ 0/42]  eta: 0:01:10  lr: 0.001294  loss: 0.5042 (0.5042)  time: 1.6756  data: 1.1538  max mem: 9341
[09:42:45.916487] Epoch: [401]  [20/42]  eta: 0:00:12  lr: 0.001292  loss: 0.5199 (0.5212)  time: 0.5081  data: 0.0001  max mem: 9341
[09:42:56.063660] Epoch: [401]  [40/42]  eta: 0:00:01  lr: 0.001290  loss: 0.5205 (0.5218)  time: 0.5073  data: 0.0001  max mem: 9341
[09:42:56.568854] Epoch: [401]  [41/42]  eta: 0:00:00  lr: 0.001290  loss: 0.5232 (0.5226)  time: 0.5073  data: 0.0001  max mem: 9341
[09:42:56.741164] Epoch: [401] Total time: 0:00:22 (0.5397 s / it)
[09:42:56.753783] Averaged stats: lr: 0.001290  loss: 0.5232 (0.5210)
[09:43:01.300576] {"train_lr": 0.001292536028541418, "train_loss": 0.5210498113717351, "epoch": 401}
[09:43:01.300964] [09:43:01.301062] Training epoch 401 for 0:00:27
[09:43:01.301115] [09:43:01.305496] log_dir: ./exp/debug/cifar100-LT/debug
[09:43:02.737741] Epoch: [402]  [ 0/42]  eta: 0:01:00  lr: 0.001289  loss: 0.5183 (0.5183)  time: 1.4309  data: 0.9162  max mem: 9341
[09:43:12.901959] Epoch: [402]  [20/42]  eta: 0:00:12  lr: 0.001287  loss: 0.5296 (0.5266)  time: 0.5082  data: 0.0001  max mem: 9341
[09:43:23.058043] Epoch: [402]  [40/42]  eta: 0:00:01  lr: 0.001285  loss: 0.5148 (0.5210)  time: 0.5078  data: 0.0001  max mem: 9341
[09:43:23.564512] Epoch: [402]  [41/42]  eta: 0:00:00  lr: 0.001285  loss: 0.5148 (0.5196)  time: 0.5077  data: 0.0001  max mem: 9341
[09:43:23.742899] Epoch: [402] Total time: 0:00:22 (0.5342 s / it)
[09:43:23.743666] Averaged stats: lr: 0.001285  loss: 0.5148 (0.5217)
[09:43:28.264299] {"train_lr": 0.0012875896167264503, "train_loss": 0.5216788216715768, "epoch": 402}
[09:43:28.264682] [09:43:28.264763] Training epoch 402 for 0:00:26
[09:43:28.264814] [09:43:28.269263] log_dir: ./exp/debug/cifar100-LT/debug
[09:43:29.837595] Epoch: [403]  [ 0/42]  eta: 0:01:05  lr: 0.001284  loss: 0.5544 (0.5544)  time: 1.5674  data: 1.0529  max mem: 9341
[09:43:40.004971] Epoch: [403]  [20/42]  eta: 0:00:12  lr: 0.001282  loss: 0.5196 (0.5187)  time: 0.5083  data: 0.0001  max mem: 9341
[09:43:50.154095] Epoch: [403]  [40/42]  eta: 0:00:01  lr: 0.001280  loss: 0.5052 (0.5117)  time: 0.5074  data: 0.0001  max mem: 9341
[09:43:50.660413] Epoch: [403]  [41/42]  eta: 0:00:00  lr: 0.001280  loss: 0.5060 (0.5118)  time: 0.5074  data: 0.0001  max mem: 9341
[09:43:50.815329] Epoch: [403] Total time: 0:00:22 (0.5368 s / it)
[09:43:50.836904] Averaged stats: lr: 0.001280  loss: 0.5060 (0.5210)
[09:43:55.413388] {"train_lr": 0.001282641708246601, "train_loss": 0.5210042113349551, "epoch": 403}
[09:43:55.413745] [09:43:55.413828] Training epoch 403 for 0:00:27
[09:43:55.413880] [09:43:55.418133] log_dir: ./exp/debug/cifar100-LT/debug
[09:43:57.023950] Epoch: [404]  [ 0/42]  eta: 0:01:07  lr: 0.001279  loss: 0.5052 (0.5052)  time: 1.6046  data: 1.0963  max mem: 9341
[09:44:07.185886] Epoch: [404]  [20/42]  eta: 0:00:12  lr: 0.001277  loss: 0.5061 (0.5120)  time: 0.5080  data: 0.0001  max mem: 9341
[09:44:17.331370] Epoch: [404]  [40/42]  eta: 0:00:01  lr: 0.001276  loss: 0.5297 (0.5200)  time: 0.5072  data: 0.0001  max mem: 9341
[09:44:17.836670] Epoch: [404]  [41/42]  eta: 0:00:00  lr: 0.001276  loss: 0.5297 (0.5208)  time: 0.5071  data: 0.0001  max mem: 9341
[09:44:18.005355] Epoch: [404] Total time: 0:00:22 (0.5378 s / it)
[09:44:18.014045] Averaged stats: lr: 0.001276  loss: 0.5297 (0.5197)
[09:44:22.525383] {"train_lr": 0.0012776923876479755, "train_loss": 0.5197066561806769, "epoch": 404}
[09:44:22.525716] [09:44:22.525799] Training epoch 404 for 0:00:27
[09:44:22.525850] [09:44:22.530231] log_dir: ./exp/debug/cifar100-LT/debug
[09:44:24.322140] Epoch: [405]  [ 0/42]  eta: 0:01:15  lr: 0.001274  loss: 0.5250 (0.5250)  time: 1.7909  data: 1.2871  max mem: 9341
[09:44:34.510786] Epoch: [405]  [20/42]  eta: 0:00:12  lr: 0.001272  loss: 0.5172 (0.5163)  time: 0.5094  data: 0.0001  max mem: 9341
[09:44:44.686669] Epoch: [405]  [40/42]  eta: 0:00:01  lr: 0.001271  loss: 0.5209 (0.5182)  time: 0.5087  data: 0.0001  max mem: 9341
[09:44:45.192822] Epoch: [405]  [41/42]  eta: 0:00:00  lr: 0.001271  loss: 0.5209 (0.5179)  time: 0.5088  data: 0.0001  max mem: 9341
[09:44:45.359334] Epoch: [405] Total time: 0:00:22 (0.5435 s / it)
[09:44:45.362730] Averaged stats: lr: 0.001271  loss: 0.5209 (0.5172)
[09:44:49.961482] {"train_lr": 0.0012727417395008048, "train_loss": 0.5172294089127154, "epoch": 405}
[09:44:49.961825] [09:44:49.961908] Training epoch 405 for 0:00:27
[09:44:49.961959] [09:44:49.966377] log_dir: ./exp/debug/cifar100-LT/debug
[09:44:51.405897] Epoch: [406]  [ 0/42]  eta: 0:01:00  lr: 0.001269  loss: 0.5310 (0.5310)  time: 1.4382  data: 0.9178  max mem: 9341
[09:45:01.567947] Epoch: [406]  [20/42]  eta: 0:00:12  lr: 0.001268  loss: 0.5063 (0.5136)  time: 0.5080  data: 0.0001  max mem: 9341
[09:45:11.719712] Epoch: [406]  [40/42]  eta: 0:00:01  lr: 0.001266  loss: 0.5111 (0.5159)  time: 0.5075  data: 0.0001  max mem: 9341
[09:45:12.226551] Epoch: [406]  [41/42]  eta: 0:00:00  lr: 0.001266  loss: 0.5166 (0.5161)  time: 0.5076  data: 0.0001  max mem: 9341
[09:45:12.403312] Epoch: [406] Total time: 0:00:22 (0.5342 s / it)
[09:45:12.412998] Averaged stats: lr: 0.001266  loss: 0.5166 (0.5176)
[09:45:17.054454] {"train_lr": 0.0012677898483980064, "train_loss": 0.5176394381338641, "epoch": 406}
[09:45:17.054857] [09:45:17.054946] Training epoch 406 for 0:00:27
[09:45:17.054998] [09:45:17.059991] log_dir: ./exp/debug/cifar100-LT/debug
[09:45:18.580016] Epoch: [407]  [ 0/42]  eta: 0:01:03  lr: 0.001264  loss: 0.5052 (0.5052)  time: 1.5189  data: 1.0042  max mem: 9341
[09:45:28.745642] Epoch: [407]  [20/42]  eta: 0:00:12  lr: 0.001263  loss: 0.5208 (0.5224)  time: 0.5082  data: 0.0001  max mem: 9341
[09:45:38.894002] Epoch: [407]  [40/42]  eta: 0:00:01  lr: 0.001261  loss: 0.5214 (0.5225)  time: 0.5074  data: 0.0001  max mem: 9341
[09:45:39.400758] Epoch: [407]  [41/42]  eta: 0:00:00  lr: 0.001261  loss: 0.5170 (0.5218)  time: 0.5075  data: 0.0001  max mem: 9341
[09:45:39.555838] Epoch: [407] Total time: 0:00:22 (0.5356 s / it)
[09:45:39.570106] Averaged stats: lr: 0.001261  loss: 0.5170 (0.5198)
[09:45:44.068650] {"train_lr": 0.001262836798953738, "train_loss": 0.51980667685469, "epoch": 407}
[09:45:44.068991] [09:45:44.069084] Training epoch 407 for 0:00:27
[09:45:44.069136] [09:45:44.073479] log_dir: ./exp/debug/cifar100-LT/debug
[09:45:45.724684] Epoch: [408]  [ 0/42]  eta: 0:01:09  lr: 0.001260  loss: 0.5678 (0.5678)  time: 1.6494  data: 1.1366  max mem: 9341
[09:45:55.893701] Epoch: [408]  [20/42]  eta: 0:00:12  lr: 0.001258  loss: 0.5178 (0.5218)  time: 0.5084  data: 0.0001  max mem: 9341
[09:46:06.042612] Epoch: [408]  [40/42]  eta: 0:00:01  lr: 0.001256  loss: 0.5100 (0.5213)  time: 0.5074  data: 0.0001  max mem: 9341
[09:46:06.546802] Epoch: [408]  [41/42]  eta: 0:00:00  lr: 0.001256  loss: 0.5087 (0.5209)  time: 0.5073  data: 0.0001  max mem: 9341
[09:46:06.712096] Epoch: [408] Total time: 0:00:22 (0.5390 s / it)
[09:46:06.714882] Averaged stats: lr: 0.001256  loss: 0.5087 (0.5179)
[09:46:11.400644] {"train_lr": 0.0012578826758019492, "train_loss": 0.5179391865219388, "epoch": 408}
[09:46:11.401057] [09:46:11.401147] Training epoch 408 for 0:00:27
[09:46:11.401199] [09:46:11.406136] log_dir: ./exp/debug/cifar100-LT/debug
[09:46:13.020720] Epoch: [409]  [ 0/42]  eta: 0:01:07  lr: 0.001255  loss: 0.5293 (0.5293)  time: 1.6133  data: 1.1016  max mem: 9341
[09:46:23.189181] Epoch: [409]  [20/42]  eta: 0:00:12  lr: 0.001253  loss: 0.5302 (0.5298)  time: 0.5084  data: 0.0001  max mem: 9341
[09:46:33.327341] Epoch: [409]  [40/42]  eta: 0:00:01  lr: 0.001251  loss: 0.5160 (0.5245)  time: 0.5069  data: 0.0001  max mem: 9341
[09:46:33.832961] Epoch: [409]  [41/42]  eta: 0:00:00  lr: 0.001251  loss: 0.5160 (0.5246)  time: 0.5069  data: 0.0001  max mem: 9341
[09:46:34.003310] Epoch: [409] Total time: 0:00:22 (0.5380 s / it)
[09:46:34.005438] Averaged stats: lr: 0.001251  loss: 0.5160 (0.5204)
[09:46:38.531750] {"train_lr": 0.001252927563594934, "train_loss": 0.5203771873244217, "epoch": 409}
[09:46:38.532267] [09:46:38.532367] Training epoch 409 for 0:00:27
[09:46:38.532456] [09:46:38.537527] log_dir: ./exp/debug/cifar100-LT/debug
[09:46:40.095891] Epoch: [410]  [ 0/42]  eta: 0:01:05  lr: 0.001250  loss: 0.5164 (0.5164)  time: 1.5572  data: 1.0485  max mem: 9341
[09:46:50.264540] Epoch: [410]  [20/42]  eta: 0:00:12  lr: 0.001248  loss: 0.5110 (0.5166)  time: 0.5084  data: 0.0001  max mem: 9341
[09:47:00.411563] Epoch: [410]  [40/42]  eta: 0:00:01  lr: 0.001246  loss: 0.5152 (0.5153)  time: 0.5073  data: 0.0001  max mem: 9341
[09:47:00.916162] Epoch: [410]  [41/42]  eta: 0:00:00  lr: 0.001246  loss: 0.5152 (0.5159)  time: 0.5072  data: 0.0001  max mem: 9341
[09:47:01.085458] Epoch: [410] Total time: 0:00:22 (0.5369 s / it)
[09:47:01.089056] Averaged stats: lr: 0.001246  loss: 0.5152 (0.5147)
[09:47:05.885904] {"train_lr": 0.0012479715470018879, "train_loss": 0.514733957215434, "epoch": 410}
[09:47:05.886227] [09:47:05.886308] Training epoch 410 for 0:00:27
[09:47:05.886359] [09:47:05.890802] log_dir: ./exp/debug/cifar100-LT/debug
[09:47:07.362367] Epoch: [411]  [ 0/42]  eta: 0:01:01  lr: 0.001245  loss: 0.5010 (0.5010)  time: 1.4703  data: 0.9579  max mem: 9341
[09:47:17.534196] Epoch: [411]  [20/42]  eta: 0:00:12  lr: 0.001243  loss: 0.5094 (0.5099)  time: 0.5085  data: 0.0001  max mem: 9341
[09:47:27.687472] Epoch: [411]  [40/42]  eta: 0:00:01  lr: 0.001241  loss: 0.4975 (0.5042)  time: 0.5076  data: 0.0001  max mem: 9341
[09:47:28.195573] Epoch: [411]  [41/42]  eta: 0:00:00  lr: 0.001241  loss: 0.4923 (0.5039)  time: 0.5078  data: 0.0001  max mem: 9341
[09:47:28.358755] Epoch: [411] Total time: 0:00:22 (0.5349 s / it)
[09:47:28.369416] Averaged stats: lr: 0.001241  loss: 0.4923 (0.5145)
[09:47:32.858669] {"train_lr": 0.0012430147107074627, "train_loss": 0.5144769073951811, "epoch": 411}
[09:47:32.859024] [09:47:32.859105] Training epoch 411 for 0:00:26
[09:47:32.859155] [09:47:32.863430] log_dir: ./exp/debug/cifar100-LT/debug
[09:47:34.369807] Epoch: [412]  [ 0/42]  eta: 0:01:03  lr: 0.001240  loss: 0.5263 (0.5263)  time: 1.5049  data: 1.0119  max mem: 9341
[09:47:44.548500] Epoch: [412]  [20/42]  eta: 0:00:12  lr: 0.001238  loss: 0.5210 (0.5133)  time: 0.5089  data: 0.0001  max mem: 9341
[09:47:54.711993] Epoch: [412]  [40/42]  eta: 0:00:01  lr: 0.001236  loss: 0.5154 (0.5159)  time: 0.5081  data: 0.0001  max mem: 9341
[09:47:55.218573] Epoch: [412]  [41/42]  eta: 0:00:00  lr: 0.001236  loss: 0.5189 (0.5166)  time: 0.5081  data: 0.0001  max mem: 9341
[09:47:55.395445] Epoch: [412] Total time: 0:00:22 (0.5365 s / it)
[09:47:55.396155] Averaged stats: lr: 0.001236  loss: 0.5189 (0.5153)
[09:48:00.023661] {"train_lr": 0.0012380571394103137, "train_loss": 0.5152865259775093, "epoch": 412}
[09:48:00.024041] [09:48:00.024166] Training epoch 412 for 0:00:27
[09:48:00.024224] [09:48:00.028493] log_dir: ./exp/debug/cifar100-LT/debug
[09:48:01.520442] Epoch: [413]  [ 0/42]  eta: 0:01:02  lr: 0.001235  loss: 0.5133 (0.5133)  time: 1.4907  data: 0.9859  max mem: 9341
[09:48:11.680669] Epoch: [413]  [20/42]  eta: 0:00:12  lr: 0.001233  loss: 0.5002 (0.5090)  time: 0.5080  data: 0.0001  max mem: 9341
[09:48:21.821883] Epoch: [413]  [40/42]  eta: 0:00:01  lr: 0.001231  loss: 0.5091 (0.5082)  time: 0.5070  data: 0.0001  max mem: 9341
[09:48:22.327759] Epoch: [413]  [41/42]  eta: 0:00:00  lr: 0.001231  loss: 0.5091 (0.5086)  time: 0.5070  data: 0.0001  max mem: 9341
[09:48:22.496594] Epoch: [413] Total time: 0:00:22 (0.5350 s / it)
[09:48:22.500121] Averaged stats: lr: 0.001231  loss: 0.5091 (0.5138)
[09:48:26.948198] {"train_lr": 0.001233098917821655, "train_loss": 0.513823489880278, "epoch": 413}
[09:48:26.948526] [09:48:26.948606] Training epoch 413 for 0:00:26
[09:48:26.948655] [09:48:26.952783] log_dir: ./exp/debug/cifar100-LT/debug
[09:48:28.545944] Epoch: [414]  [ 0/42]  eta: 0:01:06  lr: 0.001230  loss: 0.5002 (0.5002)  time: 1.5921  data: 1.0770  max mem: 9341
[09:48:38.702445] Epoch: [414]  [20/42]  eta: 0:00:12  lr: 0.001228  loss: 0.5054 (0.5056)  time: 0.5078  data: 0.0001  max mem: 9341
[09:48:48.842094] Epoch: [414]  [40/42]  eta: 0:00:01  lr: 0.001226  loss: 0.5116 (0.5100)  time: 0.5069  data: 0.0001  max mem: 9341
[09:48:49.348345] Epoch: [414]  [41/42]  eta: 0:00:00  lr: 0.001226  loss: 0.5117 (0.5111)  time: 0.5070  data: 0.0001  max mem: 9341
[09:48:49.520660] Epoch: [414] Total time: 0:00:22 (0.5373 s / it)
[09:48:49.521558] Averaged stats: lr: 0.001226  loss: 0.5117 (0.5117)
[09:48:54.141967] {"train_lr": 0.0012281401306638126, "train_loss": 0.511687333917334, "epoch": 414}
[09:48:54.142331] [09:48:54.142417] Training epoch 414 for 0:00:27
[09:48:54.142470] [09:48:54.146850] log_dir: ./exp/debug/cifar100-LT/debug
[09:48:55.679358] Epoch: [415]  [ 0/42]  eta: 0:01:04  lr: 0.001225  loss: 0.5136 (0.5136)  time: 1.5313  data: 1.0296  max mem: 9341
[09:49:05.867138] Epoch: [415]  [20/42]  eta: 0:00:12  lr: 0.001223  loss: 0.5209 (0.5214)  time: 0.5093  data: 0.0001  max mem: 9341
[09:49:16.032367] Epoch: [415]  [40/42]  eta: 0:00:01  lr: 0.001221  loss: 0.5003 (0.5179)  time: 0.5082  data: 0.0001  max mem: 9341
[09:49:16.537684] Epoch: [415]  [41/42]  eta: 0:00:00  lr: 0.001221  loss: 0.5003 (0.5171)  time: 0.5082  data: 0.0001  max mem: 9341
[09:49:16.708258] Epoch: [415] Total time: 0:00:22 (0.5372 s / it)
[09:49:16.719944] Averaged stats: lr: 0.001221  loss: 0.5003 (0.5136)
[09:49:21.364921] {"train_lr": 0.0012231808626687803, "train_loss": 0.5136373344631422, "epoch": 415}
[09:49:21.365351] [09:49:21.365441] Training epoch 415 for 0:00:27
[09:49:21.365492] [09:49:21.370488] log_dir: ./exp/debug/cifar100-LT/debug
[09:49:22.845289] Epoch: [416]  [ 0/42]  eta: 0:01:01  lr: 0.001220  loss: 0.5423 (0.5423)  time: 1.4734  data: 0.9676  max mem: 9341
[09:49:32.996902] Epoch: [416]  [20/42]  eta: 0:00:12  lr: 0.001218  loss: 0.5199 (0.5209)  time: 0.5075  data: 0.0001  max mem: 9341
[09:49:43.131166] Epoch: [416]  [40/42]  eta: 0:00:01  lr: 0.001216  loss: 0.5047 (0.5150)  time: 0.5067  data: 0.0001  max mem: 9341
[09:49:43.635038] Epoch: [416]  [41/42]  eta: 0:00:00  lr: 0.001216  loss: 0.4988 (0.5143)  time: 0.5067  data: 0.0001  max mem: 9341
[09:49:43.808622] Epoch: [416] Total time: 0:00:22 (0.5342 s / it)
[09:49:43.817293] Averaged stats: lr: 0.001216  loss: 0.4988 (0.5113)
[09:49:48.452213] {"train_lr": 0.0012182211985767612, "train_loss": 0.5112846908824784, "epoch": 416}
[09:49:48.452723] [09:49:48.452816] Training epoch 416 for 0:00:27
[09:49:48.452868] [09:49:48.457852] log_dir: ./exp/debug/cifar100-LT/debug
[09:49:49.902951] Epoch: [417]  [ 0/42]  eta: 0:01:00  lr: 0.001215  loss: 0.5261 (0.5261)  time: 1.4434  data: 0.9208  max mem: 9341
[09:50:00.085348] Epoch: [417]  [20/42]  eta: 0:00:12  lr: 0.001213  loss: 0.5109 (0.5174)  time: 0.5091  data: 0.0001  max mem: 9341
[09:50:10.250175] Epoch: [417]  [40/42]  eta: 0:00:01  lr: 0.001211  loss: 0.4973 (0.5104)  time: 0.5082  data: 0.0001  max mem: 9341
[09:50:10.756231] Epoch: [417]  [41/42]  eta: 0:00:00  lr: 0.001211  loss: 0.5034 (0.5115)  time: 0.5082  data: 0.0001  max mem: 9341
[09:50:10.920552] Epoch: [417] Total time: 0:00:22 (0.5348 s / it)
[09:50:10.926865] Averaged stats: lr: 0.001211  loss: 0.5034 (0.5159)
[09:50:15.529498] {"train_lr": 0.0012132612231347335, "train_loss": 0.5159129122538226, "epoch": 417}
[09:50:15.529887] [09:50:15.529970] Training epoch 417 for 0:00:27
[09:50:15.530021] [09:50:15.534583] log_dir: ./exp/debug/cifar100-LT/debug
[09:50:17.093939] Epoch: [418]  [ 0/42]  eta: 0:01:05  lr: 0.001210  loss: 0.5387 (0.5387)  time: 1.5581  data: 1.0431  max mem: 9341
[09:50:27.275421] Epoch: [418]  [20/42]  eta: 0:00:12  lr: 0.001208  loss: 0.5028 (0.5088)  time: 0.5090  data: 0.0001  max mem: 9341
[09:50:37.445777] Epoch: [418]  [40/42]  eta: 0:00:01  lr: 0.001206  loss: 0.5061 (0.5091)  time: 0.5085  data: 0.0001  max mem: 9341
[09:50:37.952300] Epoch: [418]  [41/42]  eta: 0:00:00  lr: 0.001206  loss: 0.5061 (0.5090)  time: 0.5085  data: 0.0001  max mem: 9341
[09:50:38.096889] Epoch: [418] Total time: 0:00:22 (0.5372 s / it)
[09:50:38.124181] Averaged stats: lr: 0.001206  loss: 0.5061 (0.5116)
[09:50:42.640684] {"train_lr": 0.0012083010210949907, "train_loss": 0.5115702287072227, "epoch": 418}
[09:50:42.641050] [09:50:42.641134] Training epoch 418 for 0:00:27
[09:50:42.641186] [09:50:42.645509] log_dir: ./exp/debug/cifar100-LT/debug
[09:50:44.283212] Epoch: [419]  [ 0/42]  eta: 0:01:08  lr: 0.001205  loss: 0.4912 (0.4912)  time: 1.6365  data: 1.1315  max mem: 9341
[09:50:54.451506] Epoch: [419]  [20/42]  eta: 0:00:12  lr: 0.001203  loss: 0.5126 (0.5094)  time: 0.5084  data: 0.0001  max mem: 9341
[09:51:04.597673] Epoch: [419]  [40/42]  eta: 0:00:01  lr: 0.001201  loss: 0.5139 (0.5134)  time: 0.5073  data: 0.0001  max mem: 9341
[09:51:05.103077] Epoch: [419]  [41/42]  eta: 0:00:00  lr: 0.001201  loss: 0.5139 (0.5122)  time: 0.5072  data: 0.0001  max mem: 9341
[09:51:05.279921] Epoch: [419] Total time: 0:00:22 (0.5389 s / it)
[09:51:05.287838] Averaged stats: lr: 0.001201  loss: 0.5139 (0.5107)
[09:51:09.863143] {"train_lr": 0.0012033406772137024, "train_loss": 0.5107055851036594, "epoch": 419}
[09:51:09.863495] [09:51:09.863572] Training epoch 419 for 0:00:27
[09:51:09.863622] [09:51:09.868361] log_dir: ./exp/debug/cifar100-LT/debug
[09:51:11.350232] Epoch: [420]  [ 0/42]  eta: 0:01:02  lr: 0.001200  loss: 0.5145 (0.5145)  time: 1.4804  data: 0.9771  max mem: 9341
[09:51:21.511257] Epoch: [420]  [20/42]  eta: 0:00:12  lr: 0.001198  loss: 0.5237 (0.5188)  time: 0.5080  data: 0.0001  max mem: 9341
[09:51:31.661702] Epoch: [420]  [40/42]  eta: 0:00:01  lr: 0.001196  loss: 0.5110 (0.5174)  time: 0.5075  data: 0.0001  max mem: 9341
[09:51:32.168900] Epoch: [420]  [41/42]  eta: 0:00:00  lr: 0.001196  loss: 0.5108 (0.5166)  time: 0.5075  data: 0.0001  max mem: 9341
[09:51:32.337704] Epoch: [420] Total time: 0:00:22 (0.5350 s / it)
[09:51:32.339475] Averaged stats: lr: 0.001196  loss: 0.5108 (0.5140)
[09:51:36.873782] {"train_lr": 0.0011983802762494556, "train_loss": 0.5140472052707559, "epoch": 420}
[09:51:36.874124] [09:51:36.874203] Training epoch 420 for 0:00:27
[09:51:36.874254] [09:51:36.878593] log_dir: ./exp/debug/cifar100-LT/debug
[09:51:38.307709] Epoch: [421]  [ 0/42]  eta: 0:00:59  lr: 0.001195  loss: 0.5257 (0.5257)  time: 1.4278  data: 0.9194  max mem: 9341
[09:51:48.467258] Epoch: [421]  [20/42]  eta: 0:00:12  lr: 0.001193  loss: 0.5147 (0.5187)  time: 0.5079  data: 0.0001  max mem: 9341
[09:51:58.617403] Epoch: [421]  [40/42]  eta: 0:00:01  lr: 0.001191  loss: 0.5148 (0.5196)  time: 0.5075  data: 0.0001  max mem: 9341
[09:51:59.121960] Epoch: [421]  [41/42]  eta: 0:00:00  lr: 0.001191  loss: 0.5148 (0.5195)  time: 0.5073  data: 0.0001  max mem: 9341
[09:51:59.302141] Epoch: [421] Total time: 0:00:22 (0.5339 s / it)
[09:51:59.302863] Averaged stats: lr: 0.001191  loss: 0.5148 (0.5156)
[09:52:04.066570] {"train_lr": 0.0011934199029618195, "train_loss": 0.51564112447557, "epoch": 421}
[09:52:04.066925] [09:52:04.067009] Training epoch 421 for 0:00:27
[09:52:04.067060] [09:52:04.071451] log_dir: ./exp/debug/cifar100-LT/debug
[09:52:05.515867] Epoch: [422]  [ 0/42]  eta: 0:01:00  lr: 0.001190  loss: 0.4943 (0.4943)  time: 1.4432  data: 0.9456  max mem: 9341
[09:52:15.688559] Epoch: [422]  [20/42]  eta: 0:00:12  lr: 0.001188  loss: 0.5111 (0.5086)  time: 0.5086  data: 0.0001  max mem: 9341
[09:52:25.835455] Epoch: [422]  [40/42]  eta: 0:00:01  lr: 0.001186  loss: 0.5019 (0.5092)  time: 0.5073  data: 0.0001  max mem: 9341
[09:52:26.341229] Epoch: [422]  [41/42]  eta: 0:00:00  lr: 0.001186  loss: 0.5019 (0.5104)  time: 0.5074  data: 0.0001  max mem: 9341
[09:52:26.520618] Epoch: [422] Total time: 0:00:22 (0.5345 s / it)
[09:52:26.525437] Averaged stats: lr: 0.001186  loss: 0.5019 (0.5122)
[09:52:31.087568] {"train_lr": 0.0011884596421098883, "train_loss": 0.5121752928410258, "epoch": 422}
[09:52:31.087970] [09:52:31.088051] Training epoch 422 for 0:00:27
[09:52:31.088148] [09:52:31.092508] log_dir: ./exp/debug/cifar100-LT/debug
[09:52:32.769182] Epoch: [423]  [ 0/42]  eta: 0:01:10  lr: 0.001185  loss: 0.5323 (0.5323)  time: 1.6756  data: 1.1741  max mem: 9341
[09:52:42.939140] Epoch: [423]  [20/42]  eta: 0:00:12  lr: 0.001183  loss: 0.4944 (0.5028)  time: 0.5084  data: 0.0001  max mem: 9341
[09:52:53.083105] Epoch: [423]  [40/42]  eta: 0:00:01  lr: 0.001181  loss: 0.5107 (0.5097)  time: 0.5071  data: 0.0001  max mem: 9341
[09:52:53.591440] Epoch: [423]  [41/42]  eta: 0:00:00  lr: 0.001181  loss: 0.5071 (0.5092)  time: 0.5073  data: 0.0001  max mem: 9341
[09:52:53.757038] Epoch: [423] Total time: 0:00:22 (0.5396 s / it)
[09:52:53.769467] Averaged stats: lr: 0.001181  loss: 0.5071 (0.5119)
[09:52:58.444671] {"train_lr": 0.0011834995784508295, "train_loss": 0.5119231047020072, "epoch": 423}
[09:52:58.445003] [09:52:58.445083] Training epoch 423 for 0:00:27
[09:52:58.445192] [09:52:58.449574] log_dir: ./exp/debug/cifar100-LT/debug
[09:52:59.953556] Epoch: [424]  [ 0/42]  eta: 0:01:03  lr: 0.001180  loss: 0.4950 (0.4950)  time: 1.5027  data: 1.0053  max mem: 9341
[09:53:10.118171] Epoch: [424]  [20/42]  eta: 0:00:12  lr: 0.001178  loss: 0.5178 (0.5207)  time: 0.5082  data: 0.0001  max mem: 9341
[09:53:20.262532] Epoch: [424]  [40/42]  eta: 0:00:01  lr: 0.001176  loss: 0.5020 (0.5127)  time: 0.5072  data: 0.0001  max mem: 9341
[09:53:20.768836] Epoch: [424]  [41/42]  eta: 0:00:00  lr: 0.001176  loss: 0.5078 (0.5135)  time: 0.5072  data: 0.0001  max mem: 9341
[09:53:20.939015] Epoch: [424] Total time: 0:00:22 (0.5355 s / it)
[09:53:20.939827] Averaged stats: lr: 0.001176  loss: 0.5078 (0.5125)
[09:53:25.454407] {"train_lr": 0.0011785397967384508, "train_loss": 0.5124759734386489, "epoch": 424}
[09:53:25.454741] [09:53:25.454820] Training epoch 424 for 0:00:27
[09:53:25.454870] [09:53:25.459216] log_dir: ./exp/debug/cifar100-LT/debug
[09:53:27.168295] Epoch: [425]  [ 0/42]  eta: 0:01:11  lr: 0.001175  loss: 0.5564 (0.5564)  time: 1.7080  data: 1.2042  max mem: 9341
[09:53:37.338439] Epoch: [425]  [20/42]  eta: 0:00:12  lr: 0.001173  loss: 0.5042 (0.5134)  time: 0.5085  data: 0.0001  max mem: 9341
[09:53:47.488811] Epoch: [425]  [40/42]  eta: 0:00:01  lr: 0.001171  loss: 0.5111 (0.5127)  time: 0.5075  data: 0.0001  max mem: 9341
[09:53:47.994877] Epoch: [425]  [41/42]  eta: 0:00:00  lr: 0.001171  loss: 0.5088 (0.5119)  time: 0.5075  data: 0.0001  max mem: 9341
[09:53:48.160433] Epoch: [425] Total time: 0:00:22 (0.5405 s / it)
[09:53:48.174509] Averaged stats: lr: 0.001171  loss: 0.5088 (0.5115)
[09:53:52.750408] {"train_lr": 0.001173580381721735, "train_loss": 0.5115006048054922, "epoch": 425}
[09:53:52.750769] [09:53:52.750848] Training epoch 425 for 0:00:27
[09:53:52.750900] [09:53:52.755281] log_dir: ./exp/debug/cifar100-LT/debug
[09:53:54.418320] Epoch: [426]  [ 0/42]  eta: 0:01:09  lr: 0.001170  loss: 0.5314 (0.5314)  time: 1.6620  data: 1.1518  max mem: 9341
[09:54:04.577860] Epoch: [426]  [20/42]  eta: 0:00:12  lr: 0.001168  loss: 0.5124 (0.5087)  time: 0.5079  data: 0.0001  max mem: 9341
[09:54:14.723628] Epoch: [426]  [40/42]  eta: 0:00:01  lr: 0.001166  loss: 0.5118 (0.5137)  time: 0.5072  data: 0.0001  max mem: 9341
[09:54:15.229110] Epoch: [426]  [41/42]  eta: 0:00:00  lr: 0.001166  loss: 0.5118 (0.5141)  time: 0.5073  data: 0.0001  max mem: 9341
[09:54:15.406223] Epoch: [426] Total time: 0:00:22 (0.5393 s / it)
[09:54:15.413611] Averaged stats: lr: 0.001166  loss: 0.5118 (0.5111)
[09:54:20.007016] {"train_lr": 0.001168621418143401, "train_loss": 0.5110981111370382, "epoch": 426}
[09:54:20.007359] [09:54:20.007441] Training epoch 426 for 0:00:27
[09:54:20.007492] [09:54:20.011768] log_dir: ./exp/debug/cifar100-LT/debug
[09:54:21.608070] Epoch: [427]  [ 0/42]  eta: 0:01:06  lr: 0.001165  loss: 0.4458 (0.4458)  time: 1.5950  data: 1.1000  max mem: 9341
[09:54:31.769486] Epoch: [427]  [20/42]  eta: 0:00:12  lr: 0.001163  loss: 0.4979 (0.5053)  time: 0.5080  data: 0.0001  max mem: 9341
[09:54:41.919212] Epoch: [427]  [40/42]  eta: 0:00:01  lr: 0.001162  loss: 0.4946 (0.5030)  time: 0.5074  data: 0.0001  max mem: 9341
[09:54:42.425137] Epoch: [427]  [41/42]  eta: 0:00:00  lr: 0.001162  loss: 0.4946 (0.5033)  time: 0.5074  data: 0.0001  max mem: 9341
[09:54:42.595020] Epoch: [427] Total time: 0:00:22 (0.5377 s / it)
[09:54:42.606828] Averaged stats: lr: 0.001162  loss: 0.4946 (0.5083)
[09:54:47.277825] {"train_lr": 0.0011636629907384542, "train_loss": 0.5082950004864306, "epoch": 427}
[09:54:47.278243] [09:54:47.278325] Training epoch 427 for 0:00:27
[09:54:47.278376] [09:54:47.282729] log_dir: ./exp/debug/cifar100-LT/debug
[09:54:48.927362] Epoch: [428]  [ 0/42]  eta: 0:01:09  lr: 0.001160  loss: 0.5101 (0.5101)  time: 1.6436  data: 1.1308  max mem: 9341
[09:54:59.100142] Epoch: [428]  [20/42]  eta: 0:00:12  lr: 0.001158  loss: 0.5161 (0.5097)  time: 0.5086  data: 0.0001  max mem: 9341
[09:55:09.248777] Epoch: [428]  [40/42]  eta: 0:00:01  lr: 0.001157  loss: 0.5174 (0.5180)  time: 0.5074  data: 0.0001  max mem: 9341
[09:55:09.753860] Epoch: [428]  [41/42]  eta: 0:00:00  lr: 0.001157  loss: 0.5171 (0.5177)  time: 0.5074  data: 0.0001  max mem: 9341
[09:55:09.919572] Epoch: [428] Total time: 0:00:22 (0.5390 s / it)
[09:55:09.920345] Averaged stats: lr: 0.001157  loss: 0.5171 (0.5102)
[09:55:14.569010] {"train_lr": 0.0011587051842327385, "train_loss": 0.5102254019251892, "epoch": 428}
[09:55:14.569417] [09:55:14.569501] Training epoch 428 for 0:00:27
[09:55:14.569553] [09:55:14.573969] log_dir: ./exp/debug/cifar100-LT/debug
[09:55:16.063620] Epoch: [429]  [ 0/42]  eta: 0:01:02  lr: 0.001155  loss: 0.5072 (0.5072)  time: 1.4872  data: 0.9710  max mem: 9341
[09:55:26.213986] Epoch: [429]  [20/42]  eta: 0:00:12  lr: 0.001153  loss: 0.5169 (0.5187)  time: 0.5075  data: 0.0002  max mem: 9341
[09:55:36.351521] Epoch: [429]  [40/42]  eta: 0:00:01  lr: 0.001152  loss: 0.5072 (0.5132)  time: 0.5068  data: 0.0001  max mem: 9341
[09:55:36.856422] Epoch: [429]  [41/42]  eta: 0:00:00  lr: 0.001152  loss: 0.5072 (0.5138)  time: 0.5069  data: 0.0001  max mem: 9341
[09:55:37.018142] Epoch: [429] Total time: 0:00:22 (0.5344 s / it)
[09:55:37.021751] Averaged stats: lr: 0.001152  loss: 0.5072 (0.5112)
[09:55:41.595660] {"train_lr": 0.001153748083341489, "train_loss": 0.5111620630181971, "epoch": 429}
[09:55:41.595989] [09:55:41.596069] Training epoch 429 for 0:00:27
[09:55:41.596170] [09:55:41.600484] log_dir: ./exp/debug/cifar100-LT/debug
[09:55:43.109714] Epoch: [430]  [ 0/42]  eta: 0:01:03  lr: 0.001150  loss: 0.5741 (0.5741)  time: 1.5080  data: 1.0059  max mem: 9341
[09:55:53.292686] Epoch: [430]  [20/42]  eta: 0:00:12  lr: 0.001149  loss: 0.5113 (0.5225)  time: 0.5091  data: 0.0001  max mem: 9341
[09:56:03.459235] Epoch: [430]  [40/42]  eta: 0:00:01  lr: 0.001147  loss: 0.5195 (0.5212)  time: 0.5083  data: 0.0001  max mem: 9341
[09:56:03.964429] Epoch: [430]  [41/42]  eta: 0:00:00  lr: 0.001147  loss: 0.5195 (0.5206)  time: 0.5082  data: 0.0001  max mem: 9341
[09:56:04.134852] Epoch: [430] Total time: 0:00:22 (0.5365 s / it)
[09:56:04.136289] Averaged stats: lr: 0.001147  loss: 0.5195 (0.5112)
[09:56:08.726043] {"train_lr": 0.0011487917727678809, "train_loss": 0.5112483228246371, "epoch": 430}
[09:56:08.726499] [09:56:08.726600] Training epoch 430 for 0:00:27
[09:56:08.726655] [09:56:08.731777] log_dir: ./exp/debug/cifar100-LT/debug
[09:56:10.316571] Epoch: [431]  [ 0/42]  eta: 0:01:06  lr: 0.001145  loss: 0.4964 (0.4964)  time: 1.5835  data: 1.0776  max mem: 9341
[09:56:20.521149] Epoch: [431]  [20/42]  eta: 0:00:12  lr: 0.001144  loss: 0.5276 (0.5214)  time: 0.5102  data: 0.0001  max mem: 9341
[09:56:30.690173] Epoch: [431]  [40/42]  eta: 0:00:01  lr: 0.001142  loss: 0.5084 (0.5178)  time: 0.5084  data: 0.0001  max mem: 9341
[09:56:31.196532] Epoch: [431]  [41/42]  eta: 0:00:00  lr: 0.001142  loss: 0.5078 (0.5172)  time: 0.5084  data: 0.0001  max mem: 9341
[09:56:31.358772] Epoch: [431] Total time: 0:00:22 (0.5387 s / it)
[09:56:31.371298] Averaged stats: lr: 0.001142  loss: 0.5078 (0.5114)
[09:56:35.974353] {"train_lr": 0.0011438363372015886, "train_loss": 0.5113837580595698, "epoch": 431}
[09:56:35.974779] [09:56:35.974862] Training epoch 431 for 0:00:27
[09:56:35.974912] [09:56:35.979297] log_dir: ./exp/debug/cifar100-LT/debug
[09:56:37.554764] Epoch: [432]  [ 0/42]  eta: 0:01:06  lr: 0.001140  loss: 0.5515 (0.5515)  time: 1.5746  data: 1.0695  max mem: 9341
[09:56:47.721504] Epoch: [432]  [20/42]  eta: 0:00:12  lr: 0.001139  loss: 0.5108 (0.5096)  time: 0.5083  data: 0.0001  max mem: 9341
[09:56:57.874568] Epoch: [432]  [40/42]  eta: 0:00:01  lr: 0.001137  loss: 0.4982 (0.5092)  time: 0.5076  data: 0.0001  max mem: 9341
[09:56:58.380157] Epoch: [432]  [41/42]  eta: 0:00:00  lr: 0.001137  loss: 0.4973 (0.5087)  time: 0.5076  data: 0.0001  max mem: 9341
[09:56:58.553650] Epoch: [432] Total time: 0:00:22 (0.5375 s / it)
[09:56:58.554470] Averaged stats: lr: 0.001137  loss: 0.4973 (0.5115)
[09:57:03.162138] {"train_lr": 0.001138881861317333, "train_loss": 0.5115077245448317, "epoch": 432}
[09:57:03.162457] [09:57:03.162537] Training epoch 432 for 0:00:27
[09:57:03.162587] [09:57:03.166965] log_dir: ./exp/debug/cifar100-LT/debug
[09:57:04.860904] Epoch: [433]  [ 0/42]  eta: 0:01:11  lr: 0.001136  loss: 0.5619 (0.5619)  time: 1.6929  data: 1.1818  max mem: 9341
[09:57:15.104450] Epoch: [433]  [20/42]  eta: 0:00:12  lr: 0.001134  loss: 0.5128 (0.5119)  time: 0.5121  data: 0.0001  max mem: 9341
[09:57:25.244647] Epoch: [433]  [40/42]  eta: 0:00:01  lr: 0.001132  loss: 0.5058 (0.5090)  time: 0.5070  data: 0.0001  max mem: 9341
[09:57:25.749793] Epoch: [433]  [41/42]  eta: 0:00:00  lr: 0.001132  loss: 0.4993 (0.5085)  time: 0.5070  data: 0.0001  max mem: 9341
[09:57:25.910911] Epoch: [433] Total time: 0:00:22 (0.5415 s / it)
[09:57:25.921405] Averaged stats: lr: 0.001132  loss: 0.4993 (0.5057)
[09:57:30.432388] {"train_lr": 0.001133928429773436, "train_loss": 0.5056826356975805, "epoch": 433}
[09:57:30.432773] [09:57:30.432857] Training epoch 433 for 0:00:27
[09:57:30.432908] [09:57:30.437215] log_dir: ./exp/debug/cifar100-LT/debug
[09:57:31.977312] Epoch: [434]  [ 0/42]  eta: 0:01:04  lr: 0.001131  loss: 0.5197 (0.5197)  time: 1.5386  data: 1.0245  max mem: 9341
[09:57:42.136965] Epoch: [434]  [20/42]  eta: 0:00:12  lr: 0.001129  loss: 0.4949 (0.5036)  time: 0.5079  data: 0.0001  max mem: 9341
[09:57:52.290176] Epoch: [434]  [40/42]  eta: 0:00:01  lr: 0.001127  loss: 0.5137 (0.5025)  time: 0.5076  data: 0.0001  max mem: 9341
[09:57:52.795837] Epoch: [434]  [41/42]  eta: 0:00:00  lr: 0.001127  loss: 0.5137 (0.5022)  time: 0.5076  data: 0.0001  max mem: 9341
[09:57:52.970953] Epoch: [434] Total time: 0:00:22 (0.5365 s / it)
[09:57:52.971653] Averaged stats: lr: 0.001127  loss: 0.5137 (0.5057)
[09:57:57.598906] {"train_lr": 0.0011289761272103762, "train_loss": 0.5056898558423633, "epoch": 434}
[09:57:57.599251] [09:57:57.599338] Training epoch 434 for 0:00:27
[09:57:57.599390] [09:57:57.603853] log_dir: ./exp/debug/cifar100-LT/debug
[09:57:59.204709] Epoch: [435]  [ 0/42]  eta: 0:01:07  lr: 0.001126  loss: 0.5086 (0.5086)  time: 1.5990  data: 1.0902  max mem: 9341
[09:58:09.368751] Epoch: [435]  [20/42]  eta: 0:00:12  lr: 0.001124  loss: 0.5080 (0.5161)  time: 0.5081  data: 0.0001  max mem: 9341
[09:58:19.518083] Epoch: [435]  [40/42]  eta: 0:00:01  lr: 0.001122  loss: 0.5006 (0.5075)  time: 0.5074  data: 0.0001  max mem: 9341
[09:58:20.023824] Epoch: [435]  [41/42]  eta: 0:00:00  lr: 0.001122  loss: 0.5006 (0.5071)  time: 0.5073  data: 0.0001  max mem: 9341
[09:58:20.189224] Epoch: [435] Total time: 0:00:22 (0.5377 s / it)
[09:58:20.200478] Averaged stats: lr: 0.001122  loss: 0.5006 (0.5064)
[09:58:24.884339] {"train_lr": 0.0011240250382493404, "train_loss": 0.5064360742412862, "epoch": 435}
[09:58:24.884735] [09:58:24.884834] Training epoch 435 for 0:00:27
[09:58:24.884886] [09:58:24.889158] log_dir: ./exp/debug/cifar100-LT/debug
[09:58:26.419335] Epoch: [436]  [ 0/42]  eta: 0:01:04  lr: 0.001121  loss: 0.5006 (0.5006)  time: 1.5290  data: 1.0332  max mem: 9341
[09:58:36.615717] Epoch: [436]  [20/42]  eta: 0:00:12  lr: 0.001119  loss: 0.5000 (0.5053)  time: 0.5098  data: 0.0001  max mem: 9341
[09:58:46.779647] Epoch: [436]  [40/42]  eta: 0:00:01  lr: 0.001117  loss: 0.4976 (0.5073)  time: 0.5081  data: 0.0001  max mem: 9341
[09:58:47.287401] Epoch: [436]  [41/42]  eta: 0:00:00  lr: 0.001117  loss: 0.4976 (0.5087)  time: 0.5083  data: 0.0001  max mem: 9341
[09:58:47.458097] Epoch: [436] Total time: 0:00:22 (0.5374 s / it)
[09:58:47.464133] Averaged stats: lr: 0.001117  loss: 0.4976 (0.5090)
[09:58:52.044930] {"train_lr": 0.0011190752474907791, "train_loss": 0.5089822890503066, "epoch": 436}
[09:58:52.045296] [09:58:52.045375] Training epoch 436 for 0:00:27
[09:58:52.045426] [09:58:52.049739] log_dir: ./exp/debug/cifar100-LT/debug
[09:58:53.520880] Epoch: [437]  [ 0/42]  eta: 0:01:01  lr: 0.001116  loss: 0.5447 (0.5447)  time: 1.4698  data: 0.9467  max mem: 9341
[09:59:03.687818] Epoch: [437]  [20/42]  eta: 0:00:12  lr: 0.001114  loss: 0.5027 (0.5104)  time: 0.5083  data: 0.0001  max mem: 9341
[09:59:13.839763] Epoch: [437]  [40/42]  eta: 0:00:01  lr: 0.001112  loss: 0.5043 (0.5078)  time: 0.5076  data: 0.0001  max mem: 9341
[09:59:14.345651] Epoch: [437]  [41/42]  eta: 0:00:00  lr: 0.001112  loss: 0.5045 (0.5077)  time: 0.5075  data: 0.0001  max mem: 9341
[09:59:14.505548] Epoch: [437] Total time: 0:00:22 (0.5347 s / it)
[09:59:14.506238] Averaged stats: lr: 0.001112  loss: 0.5045 (0.5060)
[09:59:19.040997] {"train_lr": 0.00111412683951296, "train_loss": 0.5059970244765282, "epoch": 437}
[09:59:19.041368] [09:59:19.041445] Training epoch 437 for 0:00:26
[09:59:19.041496] [09:59:19.045802] log_dir: ./exp/debug/cifar100-LT/debug
[09:59:20.595236] Epoch: [438]  [ 0/42]  eta: 0:01:05  lr: 0.001111  loss: 0.5461 (0.5461)  time: 1.5481  data: 1.0327  max mem: 9341
[09:59:30.778999] Epoch: [438]  [20/42]  eta: 0:00:12  lr: 0.001109  loss: 0.5065 (0.5091)  time: 0.5091  data: 0.0001  max mem: 9341
[09:59:40.949362] Epoch: [438]  [40/42]  eta: 0:00:01  lr: 0.001107  loss: 0.5167 (0.5128)  time: 0.5085  data: 0.0001  max mem: 9341
[09:59:41.453820] Epoch: [438]  [41/42]  eta: 0:00:00  lr: 0.001107  loss: 0.5167 (0.5114)  time: 0.5084  data: 0.0001  max mem: 9341
[09:59:41.620119] Epoch: [438] Total time: 0:00:22 (0.5375 s / it)
[09:59:41.625383] Averaged stats: lr: 0.001107  loss: 0.5167 (0.5068)
[09:59:46.274888] {"train_lr": 0.00110917989887052, "train_loss": 0.506758625840857, "epoch": 438}
[09:59:46.275250] [09:59:46.275333] Training epoch 438 for 0:00:27
[09:59:46.275384] [09:59:46.279767] log_dir: ./exp/debug/cifar100-LT/debug
[09:59:47.980986] Epoch: [439]  [ 0/42]  eta: 0:01:11  lr: 0.001106  loss: 0.4910 (0.4910)  time: 1.7002  data: 1.1928  max mem: 9341
[09:59:58.145348] Epoch: [439]  [20/42]  eta: 0:00:12  lr: 0.001104  loss: 0.5165 (0.5108)  time: 0.5082  data: 0.0001  max mem: 9341
[10:00:08.293438] Epoch: [439]  [40/42]  eta: 0:00:01  lr: 0.001102  loss: 0.5147 (0.5116)  time: 0.5074  data: 0.0001  max mem: 9341
[10:00:08.797812] Epoch: [439]  [41/42]  eta: 0:00:00  lr: 0.001102  loss: 0.5171 (0.5122)  time: 0.5074  data: 0.0001  max mem: 9341
[10:00:08.974868] Epoch: [439] Total time: 0:00:22 (0.5404 s / it)
[10:00:08.975586] Averaged stats: lr: 0.001102  loss: 0.5171 (0.5107)
[10:00:13.490899] {"train_lr": 0.00110423451009303, "train_loss": 0.5106731227466038, "epoch": 439}
[10:00:13.491270] [10:00:13.491349] Training epoch 439 for 0:00:27
[10:00:13.491399] [10:00:13.495745] log_dir: ./exp/debug/cifar100-LT/debug
[10:00:15.013043] Epoch: [440]  [ 0/42]  eta: 0:01:03  lr: 0.001101  loss: 0.4823 (0.4823)  time: 1.5159  data: 0.9898  max mem: 9341
[10:00:25.182510] Epoch: [440]  [20/42]  eta: 0:00:12  lr: 0.001099  loss: 0.5112 (0.5078)  time: 0.5084  data: 0.0002  max mem: 9341
[10:00:35.336881] Epoch: [440]  [40/42]  eta: 0:00:01  lr: 0.001097  loss: 0.5026 (0.5060)  time: 0.5077  data: 0.0001  max mem: 9341
[10:00:35.841151] Epoch: [440]  [41/42]  eta: 0:00:00  lr: 0.001097  loss: 0.5026 (0.5055)  time: 0.5076  data: 0.0001  max mem: 9341
[10:00:36.014758] Epoch: [440] Total time: 0:00:22 (0.5362 s / it)
[10:00:36.015439] Averaged stats: lr: 0.001097  loss: 0.5026 (0.5059)
[10:00:40.606206] {"train_lr": 0.0010992907576835341, "train_loss": 0.5058505215815136, "epoch": 440}
[10:00:40.606566] [10:00:40.606648] Training epoch 440 for 0:00:27
[10:00:40.606698] [10:00:40.611121] log_dir: ./exp/debug/cifar100-LT/debug
[10:00:42.056102] Epoch: [441]  [ 0/42]  eta: 0:01:00  lr: 0.001096  loss: 0.4827 (0.4827)  time: 1.4437  data: 0.9243  max mem: 9341
[10:00:52.247048] Epoch: [441]  [20/42]  eta: 0:00:12  lr: 0.001094  loss: 0.5156 (0.5116)  time: 0.5095  data: 0.0001  max mem: 9341
[10:01:02.425030] Epoch: [441]  [40/42]  eta: 0:00:01  lr: 0.001092  loss: 0.5132 (0.5099)  time: 0.5088  data: 0.0001  max mem: 9341
[10:01:02.930857] Epoch: [441]  [41/42]  eta: 0:00:00  lr: 0.001092  loss: 0.5145 (0.5100)  time: 0.5087  data: 0.0001  max mem: 9341
[10:01:03.093475] Epoch: [441] Total time: 0:00:22 (0.5353 s / it)
[10:01:03.103629] Averaged stats: lr: 0.001092  loss: 0.5145 (0.5065)
[10:01:07.715769] {"train_lr": 0.001094348726117125, "train_loss": 0.5065149116728987, "epoch": 441}
[10:01:07.716163] [10:01:07.716250] Training epoch 441 for 0:00:27
[10:01:07.716304] [10:01:07.720667] log_dir: ./exp/debug/cifar100-LT/debug
[10:01:09.277285] Epoch: [442]  [ 0/42]  eta: 0:01:05  lr: 0.001091  loss: 0.5187 (0.5187)  time: 1.5555  data: 1.0448  max mem: 9341
[10:01:19.431622] Epoch: [442]  [20/42]  eta: 0:00:12  lr: 0.001089  loss: 0.5170 (0.5171)  time: 0.5077  data: 0.0001  max mem: 9341
[10:01:29.574382] Epoch: [442]  [40/42]  eta: 0:00:01  lr: 0.001087  loss: 0.5100 (0.5156)  time: 0.5071  data: 0.0001  max mem: 9341
[10:01:30.079965] Epoch: [442]  [41/42]  eta: 0:00:00  lr: 0.001087  loss: 0.5100 (0.5155)  time: 0.5070  data: 0.0001  max mem: 9341
[10:01:30.243391] Epoch: [442] Total time: 0:00:22 (0.5363 s / it)
[10:01:30.245582] Averaged stats: lr: 0.001087  loss: 0.5100 (0.5098)
[10:01:34.834867] {"train_lr": 0.0010894084998394821, "train_loss": 0.5097858478270826, "epoch": 442}
[10:01:34.835257] [10:01:34.835338] Training epoch 442 for 0:00:27
[10:01:34.835389] [10:01:34.839794] log_dir: ./exp/debug/cifar100-LT/debug
[10:01:36.379338] Epoch: [443]  [ 0/42]  eta: 0:01:04  lr: 0.001086  loss: 0.4582 (0.4582)  time: 1.5382  data: 1.0320  max mem: 9341
[10:01:46.543522] Epoch: [443]  [20/42]  eta: 0:00:12  lr: 0.001084  loss: 0.5067 (0.5057)  time: 0.5082  data: 0.0001  max mem: 9341
[10:01:56.686661] Epoch: [443]  [40/42]  eta: 0:00:01  lr: 0.001082  loss: 0.5225 (0.5076)  time: 0.5071  data: 0.0001  max mem: 9341
[10:01:57.191464] Epoch: [443]  [41/42]  eta: 0:00:00  lr: 0.001082  loss: 0.5225 (0.5078)  time: 0.5070  data: 0.0001  max mem: 9341
[10:01:57.356563] Epoch: [443] Total time: 0:00:22 (0.5361 s / it)
[10:01:57.361104] Averaged stats: lr: 0.001082  loss: 0.5225 (0.5076)
[10:02:02.098228] {"train_lr": 0.001084470163265447, "train_loss": 0.5076443733913558, "epoch": 443}
[10:02:02.098534] [10:02:02.098624] Training epoch 443 for 0:00:27
[10:02:02.098675] [10:02:02.103006] log_dir: ./exp/debug/cifar100-LT/debug
[10:02:03.578231] Epoch: [444]  [ 0/42]  eta: 0:01:01  lr: 0.001081  loss: 0.5311 (0.5311)  time: 1.4740  data: 0.9536  max mem: 9341
[10:02:13.759607] Epoch: [444]  [20/42]  eta: 0:00:12  lr: 0.001079  loss: 0.5099 (0.5084)  time: 0.5090  data: 0.0001  max mem: 9341
[10:02:23.924012] Epoch: [444]  [40/42]  eta: 0:00:01  lr: 0.001077  loss: 0.4976 (0.5014)  time: 0.5082  data: 0.0001  max mem: 9341
[10:02:24.430179] Epoch: [444]  [41/42]  eta: 0:00:00  lr: 0.001077  loss: 0.4976 (0.5007)  time: 0.5082  data: 0.0001  max mem: 9341
[10:02:24.595476] Epoch: [444] Total time: 0:00:22 (0.5355 s / it)
[10:02:24.612262] Averaged stats: lr: 0.001077  loss: 0.4976 (0.5040)
[10:02:29.121349] {"train_lr": 0.0010795338007775625, "train_loss": 0.5040099874493622, "epoch": 444}
[10:02:29.121727] [10:02:29.121813] Training epoch 444 for 0:00:27
[10:02:29.121877] [10:02:29.126210] log_dir: ./exp/debug/cifar100-LT/debug
[10:02:30.570948] Epoch: [445]  [ 0/42]  eta: 0:01:00  lr: 0.001076  loss: 0.5494 (0.5494)  time: 1.4434  data: 0.9277  max mem: 9341
[10:02:40.725846] Epoch: [445]  [20/42]  eta: 0:00:12  lr: 0.001074  loss: 0.5083 (0.5095)  time: 0.5077  data: 0.0001  max mem: 9341
[10:02:50.871994] Epoch: [445]  [40/42]  eta: 0:00:01  lr: 0.001072  loss: 0.4975 (0.5043)  time: 0.5073  data: 0.0001  max mem: 9341
[10:02:51.378250] Epoch: [445]  [41/42]  eta: 0:00:00  lr: 0.001072  loss: 0.4975 (0.5050)  time: 0.5074  data: 0.0001  max mem: 9341
[10:02:51.552846] Epoch: [445] Total time: 0:00:22 (0.5340 s / it)
[10:02:51.553534] Averaged stats: lr: 0.001072  loss: 0.4975 (0.5043)
[10:02:56.035498] {"train_lr": 0.001074599496724646, "train_loss": 0.5042830741121656, "epoch": 445}
[10:02:56.035846] [10:02:56.035929] Training epoch 445 for 0:00:26
[10:02:56.035979] [10:02:56.040372] log_dir: ./exp/debug/cifar100-LT/debug
[10:02:57.593717] Epoch: [446]  [ 0/42]  eta: 0:01:05  lr: 0.001071  loss: 0.5525 (0.5525)  time: 1.5521  data: 1.0439  max mem: 9341
[10:03:07.758605] Epoch: [446]  [20/42]  eta: 0:00:12  lr: 0.001069  loss: 0.5054 (0.5046)  time: 0.5082  data: 0.0001  max mem: 9341
[10:03:17.910972] Epoch: [446]  [40/42]  eta: 0:00:01  lr: 0.001068  loss: 0.4925 (0.5011)  time: 0.5076  data: 0.0001  max mem: 9341
[10:03:18.419436] Epoch: [446]  [41/42]  eta: 0:00:00  lr: 0.001068  loss: 0.4925 (0.5020)  time: 0.5077  data: 0.0001  max mem: 9341
[10:03:18.575350] Epoch: [446] Total time: 0:00:22 (0.5365 s / it)
[10:03:18.586178] Averaged stats: lr: 0.001068  loss: 0.4925 (0.5025)
[10:03:23.105188] {"train_lr": 0.0010696673354203391, "train_loss": 0.5025371788513093, "epoch": 446}
[10:03:23.105524] [10:03:23.105604] Training epoch 446 for 0:00:27
[10:03:23.105655] [10:03:23.110349] log_dir: ./exp/debug/cifar100-LT/debug
[10:03:24.821691] Epoch: [447]  [ 0/42]  eta: 0:01:11  lr: 0.001066  loss: 0.4754 (0.4754)  time: 1.7102  data: 1.2003  max mem: 9341
[10:03:34.981992] Epoch: [447]  [20/42]  eta: 0:00:12  lr: 0.001064  loss: 0.5006 (0.5055)  time: 0.5080  data: 0.0001  max mem: 9341
[10:03:45.125202] Epoch: [447]  [40/42]  eta: 0:00:01  lr: 0.001063  loss: 0.4933 (0.5008)  time: 0.5071  data: 0.0001  max mem: 9341
[10:03:45.630806] Epoch: [447]  [41/42]  eta: 0:00:00  lr: 0.001063  loss: 0.4933 (0.5019)  time: 0.5072  data: 0.0001  max mem: 9341
[10:03:45.784532] Epoch: [447] Total time: 0:00:22 (0.5399 s / it)
[10:03:45.796802] Averaged stats: lr: 0.001063  loss: 0.4933 (0.5047)
[10:03:50.354023] {"train_lr": 0.0010647374011416696, "train_loss": 0.5046795936567443, "epoch": 447}
[10:03:50.354378] [10:03:50.354458] Training epoch 447 for 0:00:27
[10:03:50.354510] [10:03:50.358889] log_dir: ./exp/debug/cifar100-LT/debug
[10:03:51.985569] Epoch: [448]  [ 0/42]  eta: 0:01:08  lr: 0.001061  loss: 0.5576 (0.5576)  time: 1.6256  data: 1.1127  max mem: 9341
[10:04:02.150223] Epoch: [448]  [20/42]  eta: 0:00:12  lr: 0.001060  loss: 0.4935 (0.4978)  time: 0.5082  data: 0.0001  max mem: 9341
[10:04:12.300315] Epoch: [448]  [40/42]  eta: 0:00:01  lr: 0.001058  loss: 0.4924 (0.4996)  time: 0.5075  data: 0.0001  max mem: 9341
[10:04:12.806335] Epoch: [448]  [41/42]  eta: 0:00:00  lr: 0.001058  loss: 0.4924 (0.4995)  time: 0.5075  data: 0.0001  max mem: 9341
[10:04:12.970980] Epoch: [448] Total time: 0:00:22 (0.5384 s / it)
[10:04:12.989415] Averaged stats: lr: 0.001058  loss: 0.4924 (0.5046)
[10:04:17.522192] {"train_lr": 0.0010598097781276115, "train_loss": 0.5046418245349612, "epoch": 448}
[10:04:17.522526] [10:04:17.522610] Training epoch 448 for 0:00:27
[10:04:17.522661] [10:04:17.527022] log_dir: ./exp/debug/cifar100-LT/debug
[10:04:19.164160] Epoch: [449]  [ 0/42]  eta: 0:01:08  lr: 0.001056  loss: 0.4553 (0.4553)  time: 1.6359  data: 1.1300  max mem: 9341
[10:04:29.349644] Epoch: [449]  [20/42]  eta: 0:00:12  lr: 0.001055  loss: 0.4980 (0.5042)  time: 0.5092  data: 0.0001  max mem: 9341
[10:04:39.515852] Epoch: [449]  [40/42]  eta: 0:00:01  lr: 0.001053  loss: 0.5012 (0.5026)  time: 0.5083  data: 0.0001  max mem: 9341
[10:04:40.021396] Epoch: [449]  [41/42]  eta: 0:00:00  lr: 0.001053  loss: 0.4997 (0.5022)  time: 0.5082  data: 0.0001  max mem: 9341
[10:04:40.190788] Epoch: [449] Total time: 0:00:22 (0.5396 s / it)
[10:04:40.191659] Averaged stats: lr: 0.001053  loss: 0.4997 (0.5032)
[10:04:44.794584] {"train_lr": 0.0010548845505776484, "train_loss": 0.5032000559426489, "epoch": 449}
[10:04:44.794938] [10:04:44.795019] Training epoch 449 for 0:00:27
[10:04:44.795069] [10:04:44.799408] log_dir: ./exp/debug/cifar100-LT/debug
[10:04:46.487927] Epoch: [450]  [ 0/42]  eta: 0:01:10  lr: 0.001052  loss: 0.4967 (0.4967)  time: 1.6875  data: 1.1789  max mem: 9341
[10:04:56.661213] Epoch: [450]  [20/42]  eta: 0:00:12  lr: 0.001050  loss: 0.5119 (0.5106)  time: 0.5086  data: 0.0001  max mem: 9341
[10:05:06.811872] Epoch: [450]  [40/42]  eta: 0:00:01  lr: 0.001048  loss: 0.4848 (0.5021)  time: 0.5075  data: 0.0001  max mem: 9341
[10:05:07.318033] Epoch: [450]  [41/42]  eta: 0:00:00  lr: 0.001048  loss: 0.4848 (0.5013)  time: 0.5075  data: 0.0001  max mem: 9341
[10:05:07.476717] Epoch: [450] Total time: 0:00:22 (0.5399 s / it)
[10:05:07.483523] Averaged stats: lr: 0.001048  loss: 0.4848 (0.5024)
[10:05:11.983088] {"train_lr": 0.0010499618026503289, "train_loss": 0.5023987839619318, "epoch": 450}
[10:05:11.983411] [10:05:11.983496] Training epoch 450 for 0:00:27
[10:05:11.983626] [10:05:11.988022] log_dir: ./exp/debug/cifar100-LT/debug
[10:05:13.505329] Epoch: [451]  [ 0/42]  eta: 0:01:03  lr: 0.001047  loss: 0.4918 (0.4918)  time: 1.5160  data: 0.9994  max mem: 9341
[10:05:23.669302] Epoch: [451]  [20/42]  eta: 0:00:12  lr: 0.001045  loss: 0.5074 (0.5108)  time: 0.5081  data: 0.0001  max mem: 9341
[10:05:33.819277] Epoch: [451]  [40/42]  eta: 0:00:01  lr: 0.001043  loss: 0.4907 (0.5065)  time: 0.5074  data: 0.0001  max mem: 9341
[10:05:34.325496] Epoch: [451]  [41/42]  eta: 0:00:00  lr: 0.001043  loss: 0.4904 (0.5054)  time: 0.5075  data: 0.0001  max mem: 9341
[10:05:34.494939] Epoch: [451] Total time: 0:00:22 (0.5359 s / it)
[10:05:34.495641] Averaged stats: lr: 0.001043  loss: 0.4904 (0.5037)
[10:05:39.035334] {"train_lr": 0.0010450416184618302, "train_loss": 0.5037447198161057, "epoch": 451}
[10:05:39.035703] [10:05:39.035786] Training epoch 451 for 0:00:27
[10:05:39.035840] [10:05:39.040177] log_dir: ./exp/debug/cifar100-LT/debug
[10:05:40.485148] Epoch: [452]  [ 0/42]  eta: 0:01:00  lr: 0.001042  loss: 0.4827 (0.4827)  time: 1.4436  data: 0.9329  max mem: 9341
[10:05:50.651009] Epoch: [452]  [20/42]  eta: 0:00:12  lr: 0.001040  loss: 0.4978 (0.5026)  time: 0.5082  data: 0.0001  max mem: 9341
[10:06:00.797387] Epoch: [452]  [40/42]  eta: 0:00:01  lr: 0.001038  loss: 0.5155 (0.5125)  time: 0.5073  data: 0.0001  max mem: 9341
[10:06:01.303565] Epoch: [452]  [41/42]  eta: 0:00:00  lr: 0.001038  loss: 0.5134 (0.5116)  time: 0.5073  data: 0.0001  max mem: 9341
[10:06:01.466648] Epoch: [452] Total time: 0:00:22 (0.5340 s / it)
[10:06:01.480907] Averaged stats: lr: 0.001038  loss: 0.5134 (0.5076)
[10:06:06.036326] {"train_lr": 0.0010401240820845296, "train_loss": 0.5075759508070492, "epoch": 452}
[10:06:06.036788] [10:06:06.036881] Training epoch 452 for 0:00:27
[10:06:06.036933] [10:06:06.041904] log_dir: ./exp/debug/cifar100-LT/debug
[10:06:07.706300] Epoch: [453]  [ 0/42]  eta: 0:01:09  lr: 0.001037  loss: 0.4732 (0.4732)  time: 1.6633  data: 1.1586  max mem: 9341
[10:06:17.874532] Epoch: [453]  [20/42]  eta: 0:00:12  lr: 0.001035  loss: 0.5123 (0.5037)  time: 0.5084  data: 0.0001  max mem: 9341
[10:06:28.026545] Epoch: [453]  [40/42]  eta: 0:00:01  lr: 0.001033  loss: 0.5018 (0.5069)  time: 0.5075  data: 0.0001  max mem: 9341
[10:06:28.531972] Epoch: [453]  [41/42]  eta: 0:00:00  lr: 0.001033  loss: 0.5028 (0.5068)  time: 0.5076  data: 0.0001  max mem: 9341
[10:06:28.701541] Epoch: [453] Total time: 0:00:22 (0.5395 s / it)
[10:06:28.706785] Averaged stats: lr: 0.001033  loss: 0.5028 (0.5045)
[10:06:33.252806] {"train_lr": 0.0010352092775455528, "train_loss": 0.504457322259744, "epoch": 453}
[10:06:33.253105] [10:06:33.253183] Training epoch 453 for 0:00:27
[10:06:33.253233] [10:06:33.257538] log_dir: ./exp/debug/cifar100-LT/debug
[10:06:34.950911] Epoch: [454]  [ 0/42]  eta: 0:01:11  lr: 0.001032  loss: 0.5114 (0.5114)  time: 1.6923  data: 1.1877  max mem: 9341
[10:06:45.143950] Epoch: [454]  [20/42]  eta: 0:00:12  lr: 0.001030  loss: 0.5038 (0.5049)  time: 0.5096  data: 0.0001  max mem: 9341
[10:06:55.321098] Epoch: [454]  [40/42]  eta: 0:00:01  lr: 0.001028  loss: 0.4980 (0.5072)  time: 0.5088  data: 0.0001  max mem: 9341
[10:06:55.826994] Epoch: [454]  [41/42]  eta: 0:00:00  lr: 0.001028  loss: 0.4948 (0.5067)  time: 0.5088  data: 0.0001  max mem: 9341
[10:06:56.000608] Epoch: [454] Total time: 0:00:22 (0.5415 s / it)
[10:06:56.008959] Averaged stats: lr: 0.001028  loss: 0.4948 (0.5073)
[10:07:00.712484] {"train_lr": 0.0010302972888253492, "train_loss": 0.5072634220123291, "epoch": 454}
[10:07:00.712831] [10:07:00.712933] Training epoch 454 for 0:00:27
[10:07:00.712985] [10:07:00.717333] log_dir: ./exp/debug/cifar100-LT/debug
[10:07:02.403450] Epoch: [455]  [ 0/42]  eta: 0:01:10  lr: 0.001027  loss: 0.4905 (0.4905)  time: 1.6851  data: 1.1944  max mem: 9341
[10:07:12.576063] Epoch: [455]  [20/42]  eta: 0:00:12  lr: 0.001025  loss: 0.5252 (0.5183)  time: 0.5086  data: 0.0001  max mem: 9341
[10:07:22.726553] Epoch: [455]  [40/42]  eta: 0:00:01  lr: 0.001023  loss: 0.4934 (0.5089)  time: 0.5075  data: 0.0001  max mem: 9341
[10:07:23.232234] Epoch: [455]  [41/42]  eta: 0:00:00  lr: 0.001023  loss: 0.4934 (0.5083)  time: 0.5075  data: 0.0001  max mem: 9341
[10:07:23.395278] Epoch: [455] Total time: 0:00:22 (0.5399 s / it)
[10:07:23.400152] Averaged stats: lr: 0.001023  loss: 0.4934 (0.5032)
[10:07:28.022776] {"train_lr": 0.0010253881998562565, "train_loss": 0.5031829307831469, "epoch": 455}
[10:07:28.023179] [10:07:28.023266] Training epoch 455 for 0:00:27
[10:07:28.023318] [10:07:28.027823] log_dir: ./exp/debug/cifar100-LT/debug
[10:07:29.704042] Epoch: [456]  [ 0/42]  eta: 0:01:10  lr: 0.001022  loss: 0.4556 (0.4556)  time: 1.6749  data: 1.1795  max mem: 9341
[10:07:39.865647] Epoch: [456]  [20/42]  eta: 0:00:12  lr: 0.001020  loss: 0.4865 (0.4976)  time: 0.5080  data: 0.0001  max mem: 9341
[10:07:50.020531] Epoch: [456]  [40/42]  eta: 0:00:01  lr: 0.001018  loss: 0.4957 (0.4988)  time: 0.5077  data: 0.0001  max mem: 9341
[10:07:50.526687] Epoch: [456]  [41/42]  eta: 0:00:00  lr: 0.001018  loss: 0.4957 (0.4995)  time: 0.5078  data: 0.0001  max mem: 9341
[10:07:50.692467] Epoch: [456] Total time: 0:00:22 (0.5396 s / it)
[10:07:50.699611] Averaged stats: lr: 0.001018  loss: 0.4957 (0.5031)
[10:07:55.207842] {"train_lr": 0.001020482094521054, "train_loss": 0.5031246780639603, "epoch": 456}
[10:07:55.208232] [10:07:55.208316] Training epoch 456 for 0:00:27
[10:07:55.208370] [10:07:55.212847] log_dir: ./exp/debug/cifar100-LT/debug
[10:07:56.929576] Epoch: [457]  [ 0/42]  eta: 0:01:12  lr: 0.001017  loss: 0.5146 (0.5146)  time: 1.7157  data: 1.2056  max mem: 9341
[10:08:07.102476] Epoch: [457]  [20/42]  eta: 0:00:12  lr: 0.001015  loss: 0.4957 (0.5009)  time: 0.5086  data: 0.0001  max mem: 9341
[10:08:17.260143] Epoch: [457]  [40/42]  eta: 0:00:01  lr: 0.001013  loss: 0.5061 (0.5032)  time: 0.5078  data: 0.0001  max mem: 9341
[10:08:17.767048] Epoch: [457]  [41/42]  eta: 0:00:00  lr: 0.001013  loss: 0.5061 (0.5051)  time: 0.5079  data: 0.0001  max mem: 9341
[10:08:17.944990] Epoch: [457] Total time: 0:00:22 (0.5412 s / it)
[10:08:17.950504] Averaged stats: lr: 0.001013  loss: 0.5061 (0.5054)
[10:08:22.487119] {"train_lr": 0.0010155790566515523, "train_loss": 0.5054307434530485, "epoch": 457}
[10:08:22.487485] [10:08:22.487566] Training epoch 457 for 0:00:27
[10:08:22.487617] [10:08:22.492036] log_dir: ./exp/debug/cifar100-LT/debug
[10:08:24.169949] Epoch: [458]  [ 0/42]  eta: 0:01:10  lr: 0.001012  loss: 0.5117 (0.5117)  time: 1.6769  data: 1.1782  max mem: 9341
[10:08:34.356388] Epoch: [458]  [20/42]  eta: 0:00:12  lr: 0.001010  loss: 0.4977 (0.5041)  time: 0.5093  data: 0.0001  max mem: 9341
[10:08:44.541348] Epoch: [458]  [40/42]  eta: 0:00:01  lr: 0.001009  loss: 0.5055 (0.5041)  time: 0.5092  data: 0.0001  max mem: 9341
[10:08:45.049837] Epoch: [458]  [41/42]  eta: 0:00:00  lr: 0.001009  loss: 0.5065 (0.5046)  time: 0.5093  data: 0.0001  max mem: 9341
[10:08:45.219065] Epoch: [458] Total time: 0:00:22 (0.5411 s / it)
[10:08:45.222385] Averaged stats: lr: 0.001009  loss: 0.5065 (0.5018)
[10:08:49.871969] {"train_lr": 0.0010106791700271356, "train_loss": 0.5017933267213049, "epoch": 458}
[10:08:49.872519] [10:08:49.872615] Training epoch 458 for 0:00:27
[10:08:49.872667] [10:08:49.877983] log_dir: ./exp/debug/cifar100-LT/debug
[10:08:51.368010] Epoch: [459]  [ 0/42]  eta: 0:01:02  lr: 0.001007  loss: 0.4979 (0.4979)  time: 1.4887  data: 0.9753  max mem: 9341
[10:09:01.535342] Epoch: [459]  [20/42]  eta: 0:00:12  lr: 0.001006  loss: 0.4935 (0.5018)  time: 0.5083  data: 0.0001  max mem: 9341
[10:09:11.682704] Epoch: [459]  [40/42]  eta: 0:00:01  lr: 0.001004  loss: 0.4932 (0.5002)  time: 0.5073  data: 0.0001  max mem: 9341
[10:09:12.188703] Epoch: [459]  [41/42]  eta: 0:00:00  lr: 0.001004  loss: 0.4932 (0.5010)  time: 0.5073  data: 0.0001  max mem: 9341
[10:09:12.348686] Epoch: [459] Total time: 0:00:22 (0.5350 s / it)
[10:09:12.352110] Averaged stats: lr: 0.001004  loss: 0.4932 (0.5019)
[10:09:16.931185] {"train_lr": 0.001005782518373351, "train_loss": 0.5018654316663742, "epoch": 459}
[10:09:16.931527] [10:09:16.931604] Training epoch 459 for 0:00:27
[10:09:16.931713] [10:09:16.936132] log_dir: ./exp/debug/cifar100-LT/debug
[10:09:18.395542] Epoch: [460]  [ 0/42]  eta: 0:01:01  lr: 0.001002  loss: 0.4866 (0.4866)  time: 1.4581  data: 0.9538  max mem: 9341
[10:09:28.567572] Epoch: [460]  [20/42]  eta: 0:00:12  lr: 0.001001  loss: 0.5004 (0.4985)  time: 0.5085  data: 0.0001  max mem: 9341
[10:09:38.730784] Epoch: [460]  [40/42]  eta: 0:00:01  lr: 0.000999  loss: 0.5093 (0.5030)  time: 0.5081  data: 0.0001  max mem: 9341
[10:09:39.237034] Epoch: [460]  [41/42]  eta: 0:00:00  lr: 0.000999  loss: 0.5088 (0.5024)  time: 0.5081  data: 0.0001  max mem: 9341
[10:09:39.396827] Epoch: [460] Total time: 0:00:22 (0.5348 s / it)
[10:09:39.402743] Averaged stats: lr: 0.000999  loss: 0.5088 (0.5026)
[10:09:43.930681] {"train_lr": 0.001000889185360462, "train_loss": 0.5025941420878682, "epoch": 460}
[10:09:43.931043] [10:09:43.931125] Training epoch 460 for 0:00:26
[10:09:43.931195] [10:09:43.935579] log_dir: ./exp/debug/cifar100-LT/debug
[10:09:45.539657] Epoch: [461]  [ 0/42]  eta: 0:01:07  lr: 0.000998  loss: 0.4547 (0.4547)  time: 1.6028  data: 1.0923  max mem: 9341
[10:09:55.705279] Epoch: [461]  [20/42]  eta: 0:00:12  lr: 0.000996  loss: 0.4968 (0.5012)  time: 0.5082  data: 0.0001  max mem: 9341
[10:10:05.857606] Epoch: [461]  [40/42]  eta: 0:00:01  lr: 0.000994  loss: 0.5076 (0.5044)  time: 0.5076  data: 0.0001  max mem: 9341
[10:10:06.363906] Epoch: [461]  [41/42]  eta: 0:00:00  lr: 0.000994  loss: 0.5144 (0.5048)  time: 0.5075  data: 0.0001  max mem: 9341
[10:10:06.515354] Epoch: [461] Total time: 0:00:22 (0.5376 s / it)
[10:10:06.530029] Averaged stats: lr: 0.000994  loss: 0.5144 (0.5045)
[10:10:11.113258] {"train_lr": 0.0009959992546020309, "train_loss": 0.5044577611344201, "epoch": 461}
[10:10:11.113725] [10:10:11.113816] Training epoch 461 for 0:00:27
[10:10:11.113869] [10:10:11.118448] log_dir: ./exp/debug/cifar100-LT/debug
[10:10:12.636546] Epoch: [462]  [ 0/42]  eta: 0:01:03  lr: 0.000993  loss: 0.5194 (0.5194)  time: 1.5166  data: 0.9980  max mem: 9341
[10:10:22.809133] Epoch: [462]  [20/42]  eta: 0:00:12  lr: 0.000991  loss: 0.4938 (0.4963)  time: 0.5086  data: 0.0001  max mem: 9341
[10:10:32.968177] Epoch: [462]  [40/42]  eta: 0:00:01  lr: 0.000989  loss: 0.4834 (0.4964)  time: 0.5079  data: 0.0001  max mem: 9341
[10:10:33.474547] Epoch: [462]  [41/42]  eta: 0:00:00  lr: 0.000989  loss: 0.4792 (0.4960)  time: 0.5079  data: 0.0001  max mem: 9341
[10:10:33.632956] Epoch: [462] Total time: 0:00:22 (0.5361 s / it)
[10:10:33.646554] Averaged stats: lr: 0.000989  loss: 0.4792 (0.4983)
[10:10:38.089025] {"train_lr": 0.0009911128096534805, "train_loss": 0.4982933804747604, "epoch": 462}
[10:10:38.089387] [10:10:38.089468] Training epoch 462 for 0:00:26
[10:10:38.089516] [10:10:38.093908] log_dir: ./exp/debug/cifar100-LT/debug
[10:10:39.572578] Epoch: [463]  [ 0/42]  eta: 0:01:02  lr: 0.000988  loss: 0.5199 (0.5199)  time: 1.4774  data: 0.9556  max mem: 9341
[10:10:49.742436] Epoch: [463]  [20/42]  eta: 0:00:12  lr: 0.000986  loss: 0.4954 (0.4991)  time: 0.5084  data: 0.0001  max mem: 9341
[10:10:59.893042] Epoch: [463]  [40/42]  eta: 0:00:01  lr: 0.000984  loss: 0.5096 (0.5031)  time: 0.5075  data: 0.0001  max mem: 9341
[10:11:00.399067] Epoch: [463]  [41/42]  eta: 0:00:00  lr: 0.000984  loss: 0.5097 (0.5039)  time: 0.5074  data: 0.0001  max mem: 9341
[10:11:00.563030] Epoch: [463] Total time: 0:00:22 (0.5350 s / it)
[10:11:00.564423] Averaged stats: lr: 0.000984  loss: 0.5097 (0.4996)
[10:11:05.187028] {"train_lr": 0.0009862299340106736, "train_loss": 0.499640720585982, "epoch": 463}
[10:11:05.187411] [10:11:05.187496] Training epoch 463 for 0:00:27
[10:11:05.187546] [10:11:05.192000] log_dir: ./exp/debug/cifar100-LT/debug
[10:11:06.703370] Epoch: [464]  [ 0/42]  eta: 0:01:03  lr: 0.000983  loss: 0.4819 (0.4819)  time: 1.5100  data: 0.9903  max mem: 9341
[10:11:16.871810] Epoch: [464]  [20/42]  eta: 0:00:12  lr: 0.000981  loss: 0.5106 (0.5097)  time: 0.5084  data: 0.0001  max mem: 9341
[10:11:27.025229] Epoch: [464]  [40/42]  eta: 0:00:01  lr: 0.000979  loss: 0.4873 (0.5003)  time: 0.5076  data: 0.0001  max mem: 9341
[10:11:27.530962] Epoch: [464]  [41/42]  eta: 0:00:00  lr: 0.000979  loss: 0.4883 (0.5009)  time: 0.5076  data: 0.0001  max mem: 9341
[10:11:27.690403] Epoch: [464] Total time: 0:00:22 (0.5357 s / it)
[10:11:27.693403] Averaged stats: lr: 0.000979  loss: 0.4883 (0.5019)
[10:11:32.200815] {"train_lr": 0.000981350711108483, "train_loss": 0.5019283816218376, "epoch": 464}
[10:11:32.201206] [10:11:32.201291] Training epoch 464 for 0:00:27
[10:11:32.201341] [10:11:32.205851] log_dir: ./exp/debug/cifar100-LT/debug
[10:11:33.703242] Epoch: [465]  [ 0/42]  eta: 0:01:02  lr: 0.000978  loss: 0.5678 (0.5678)  time: 1.4961  data: 0.9878  max mem: 9341
[10:11:43.872759] Epoch: [465]  [20/42]  eta: 0:00:12  lr: 0.000976  loss: 0.4947 (0.5046)  time: 0.5084  data: 0.0001  max mem: 9341
[10:11:54.029684] Epoch: [465]  [40/42]  eta: 0:00:01  lr: 0.000974  loss: 0.4894 (0.5001)  time: 0.5078  data: 0.0001  max mem: 9341
[10:11:54.535402] Epoch: [465]  [41/42]  eta: 0:00:00  lr: 0.000974  loss: 0.4847 (0.4992)  time: 0.5077  data: 0.0001  max mem: 9341
[10:11:54.700780] Epoch: [465] Total time: 0:00:22 (0.5356 s / it)
[10:11:54.711347] Averaged stats: lr: 0.000974  loss: 0.4847 (0.5010)
[10:11:59.431781] {"train_lr": 0.0009764752243193656, "train_loss": 0.5009855769929432, "epoch": 465}
[10:11:59.432041] [10:11:59.432158] Training epoch 465 for 0:00:27
[10:11:59.432274] [10:11:59.436587] log_dir: ./exp/debug/cifar100-LT/debug
[10:12:00.888869] Epoch: [466]  [ 0/42]  eta: 0:01:00  lr: 0.000973  loss: 0.4949 (0.4949)  time: 1.4513  data: 0.9424  max mem: 9341
[10:12:11.055954] Epoch: [466]  [20/42]  eta: 0:00:12  lr: 0.000971  loss: 0.4877 (0.4948)  time: 0.5083  data: 0.0001  max mem: 9341
[10:12:21.209723] Epoch: [466]  [40/42]  eta: 0:00:01  lr: 0.000969  loss: 0.5064 (0.5012)  time: 0.5076  data: 0.0001  max mem: 9341
[10:12:21.715451] Epoch: [466]  [41/42]  eta: 0:00:00  lr: 0.000969  loss: 0.5064 (0.5017)  time: 0.5076  data: 0.0001  max mem: 9341
[10:12:21.882300] Epoch: [466] Total time: 0:00:22 (0.5344 s / it)
[10:12:21.883182] Averaged stats: lr: 0.000969  loss: 0.5064 (0.4992)
[10:12:26.509127] {"train_lr": 0.0009716035569519402, "train_loss": 0.49918108628619284, "epoch": 466}
[10:12:26.509486] [10:12:26.509572] Training epoch 466 for 0:00:27
[10:12:26.509624] [10:12:26.513925] log_dir: ./exp/debug/cifar100-LT/debug
[10:12:28.194153] Epoch: [467]  [ 0/42]  eta: 0:01:10  lr: 0.000968  loss: 0.4852 (0.4852)  time: 1.6792  data: 1.1763  max mem: 9341
[10:12:38.362930] Epoch: [467]  [20/42]  eta: 0:00:12  lr: 0.000966  loss: 0.5007 (0.4990)  time: 0.5084  data: 0.0001  max mem: 9341
[10:12:48.520361] Epoch: [467]  [40/42]  eta: 0:00:01  lr: 0.000965  loss: 0.4968 (0.5003)  time: 0.5078  data: 0.0001  max mem: 9341
[10:12:49.027083] Epoch: [467]  [41/42]  eta: 0:00:00  lr: 0.000965  loss: 0.5050 (0.5010)  time: 0.5078  data: 0.0001  max mem: 9341
[10:12:49.179831] Epoch: [467] Total time: 0:00:22 (0.5397 s / it)
[10:12:49.194569] Averaged stats: lr: 0.000965  loss: 0.5050 (0.5013)
[10:12:53.709984] {"train_lr": 0.0009667357922495577, "train_loss": 0.5013071151361579, "epoch": 467}
[10:12:53.710295] [10:12:53.710374] Training epoch 467 for 0:00:27
[10:12:53.710424] [10:12:53.715233] log_dir: ./exp/debug/cifar100-LT/debug
[10:12:55.328632] Epoch: [468]  [ 0/42]  eta: 0:01:07  lr: 0.000963  loss: 0.4947 (0.4947)  time: 1.6125  data: 1.1032  max mem: 9341
[10:13:05.497504] Epoch: [468]  [20/42]  eta: 0:00:12  lr: 0.000962  loss: 0.5036 (0.4982)  time: 0.5084  data: 0.0001  max mem: 9341
[10:13:15.651715] Epoch: [468]  [40/42]  eta: 0:00:01  lr: 0.000960  loss: 0.5013 (0.4999)  time: 0.5077  data: 0.0001  max mem: 9341
[10:13:16.158310] Epoch: [468]  [41/42]  eta: 0:00:00  lr: 0.000960  loss: 0.5046 (0.5007)  time: 0.5077  data: 0.0001  max mem: 9341
[10:13:16.336342] Epoch: [468] Total time: 0:00:22 (0.5386 s / it)
[10:13:16.342742] Averaged stats: lr: 0.000960  loss: 0.5046 (0.4985)
[10:13:20.887456] {"train_lr": 0.0009618720133888876, "train_loss": 0.4985078496947175, "epoch": 468}
[10:13:20.887798] [10:13:20.887879] Training epoch 468 for 0:00:27
[10:13:20.887929] [10:13:20.892202] log_dir: ./exp/debug/cifar100-LT/debug
[10:13:22.527599] Epoch: [469]  [ 0/42]  eta: 0:01:08  lr: 0.000959  loss: 0.5047 (0.5047)  time: 1.6341  data: 1.1244  max mem: 9341
[10:13:32.691887] Epoch: [469]  [20/42]  eta: 0:00:12  lr: 0.000957  loss: 0.4997 (0.5009)  time: 0.5082  data: 0.0001  max mem: 9341
[10:13:42.842413] Epoch: [469]  [40/42]  eta: 0:00:01  lr: 0.000955  loss: 0.4976 (0.4995)  time: 0.5075  data: 0.0001  max mem: 9341
[10:13:43.347543] Epoch: [469]  [41/42]  eta: 0:00:00  lr: 0.000955  loss: 0.4885 (0.4992)  time: 0.5075  data: 0.0001  max mem: 9341
[10:13:43.517029] Epoch: [469] Total time: 0:00:22 (0.5387 s / it)
[10:13:43.520269] Averaged stats: lr: 0.000955  loss: 0.4885 (0.5008)
[10:13:48.044984] {"train_lr": 0.0009570123034784922, "train_loss": 0.5008103792511281, "epoch": 469}
[10:13:48.045331] [10:13:48.045412] Training epoch 469 for 0:00:27
[10:13:48.045462] [10:13:48.049890] log_dir: ./exp/debug/cifar100-LT/debug
[10:13:49.659221] Epoch: [470]  [ 0/42]  eta: 0:01:07  lr: 0.000954  loss: 0.5425 (0.5425)  time: 1.6077  data: 1.1148  max mem: 9341
[10:13:59.822736] Epoch: [470]  [20/42]  eta: 0:00:12  lr: 0.000952  loss: 0.4949 (0.4968)  time: 0.5081  data: 0.0001  max mem: 9341
[10:14:09.971517] Epoch: [470]  [40/42]  eta: 0:00:01  lr: 0.000950  loss: 0.5000 (0.4993)  time: 0.5074  data: 0.0001  max mem: 9341
[10:14:10.476682] Epoch: [470]  [41/42]  eta: 0:00:00  lr: 0.000950  loss: 0.5000 (0.4997)  time: 0.5073  data: 0.0001  max mem: 9341
[10:14:10.640270] Epoch: [470] Total time: 0:00:22 (0.5379 s / it)
[10:14:10.641409] Averaged stats: lr: 0.000950  loss: 0.5000 (0.4980)
[10:14:15.210698] {"train_lr": 0.0009521567455574037, "train_loss": 0.49797641166618895, "epoch": 470}
[10:14:15.211059] [10:14:15.211141] Training epoch 470 for 0:00:27
[10:14:15.211193] [10:14:15.215526] log_dir: ./exp/debug/cifar100-LT/debug
[10:14:16.889839] Epoch: [471]  [ 0/42]  eta: 0:01:10  lr: 0.000949  loss: 0.4886 (0.4886)  time: 1.6732  data: 1.1676  max mem: 9341
[10:14:27.058641] Epoch: [471]  [20/42]  eta: 0:00:12  lr: 0.000947  loss: 0.5051 (0.5038)  time: 0.5084  data: 0.0001  max mem: 9341
[10:14:37.207639] Epoch: [471]  [40/42]  eta: 0:00:01  lr: 0.000945  loss: 0.4926 (0.5026)  time: 0.5074  data: 0.0001  max mem: 9341
[10:14:37.713035] Epoch: [471]  [41/42]  eta: 0:00:00  lr: 0.000945  loss: 0.4955 (0.5027)  time: 0.5074  data: 0.0001  max mem: 9341
[10:14:37.879997] Epoch: [471] Total time: 0:00:22 (0.5396 s / it)
[10:14:37.882549] Averaged stats: lr: 0.000945  loss: 0.4955 (0.5013)
[10:14:42.443498] {"train_lr": 0.0009473054225937125, "train_loss": 0.5013307586667084, "epoch": 471}
[10:14:42.443863] [10:14:42.443942] Training epoch 471 for 0:00:27
[10:14:42.443992] [10:14:42.448302] log_dir: ./exp/debug/cifar100-LT/debug
[10:14:44.082839] Epoch: [472]  [ 0/42]  eta: 0:01:08  lr: 0.000944  loss: 0.5640 (0.5640)  time: 1.6334  data: 1.1239  max mem: 9341
[10:14:54.241767] Epoch: [472]  [20/42]  eta: 0:00:12  lr: 0.000942  loss: 0.4886 (0.4946)  time: 0.5079  data: 0.0001  max mem: 9341
[10:15:04.387627] Epoch: [472]  [40/42]  eta: 0:00:01  lr: 0.000940  loss: 0.5029 (0.4993)  time: 0.5072  data: 0.0001  max mem: 9341
[10:15:04.894115] Epoch: [472]  [41/42]  eta: 0:00:00  lr: 0.000940  loss: 0.5029 (0.4995)  time: 0.5072  data: 0.0001  max mem: 9341
[10:15:05.066255] Epoch: [472] Total time: 0:00:22 (0.5385 s / it)
[10:15:05.077721] Averaged stats: lr: 0.000940  loss: 0.5029 (0.4984)
[10:15:09.622950] {"train_lr": 0.0009424584174831397, "train_loss": 0.498380669880481, "epoch": 472}
[10:15:09.623321] [10:15:09.623406] Training epoch 472 for 0:00:27
[10:15:09.623456] [10:15:09.628366] log_dir: ./exp/debug/cifar100-LT/debug
[10:15:11.116957] Epoch: [473]  [ 0/42]  eta: 0:01:02  lr: 0.000939  loss: 0.5088 (0.5088)  time: 1.4873  data: 0.9895  max mem: 9341
[10:15:21.271610] Epoch: [473]  [20/42]  eta: 0:00:12  lr: 0.000937  loss: 0.4900 (0.4970)  time: 0.5077  data: 0.0001  max mem: 9341
[10:15:31.412760] Epoch: [473]  [40/42]  eta: 0:00:01  lr: 0.000936  loss: 0.5001 (0.4970)  time: 0.5070  data: 0.0001  max mem: 9341
[10:15:31.918107] Epoch: [473]  [41/42]  eta: 0:00:00  lr: 0.000936  loss: 0.5010 (0.4982)  time: 0.5070  data: 0.0001  max mem: 9341
[10:15:32.085026] Epoch: [473] Total time: 0:00:22 (0.5347 s / it)
[10:15:32.093992] Averaged stats: lr: 0.000936  loss: 0.5010 (0.4980)
[10:15:36.638029] {"train_lr": 0.0009376158130476295, "train_loss": 0.49803673209888594, "epoch": 473}
[10:15:36.638380] [10:15:36.638459] Training epoch 473 for 0:00:27
[10:15:36.638509] [10:15:36.642803] log_dir: ./exp/debug/cifar100-LT/debug
[10:15:38.099624] Epoch: [474]  [ 0/42]  eta: 0:01:01  lr: 0.000934  loss: 0.5214 (0.5214)  time: 1.4555  data: 0.9465  max mem: 9341
[10:15:48.275912] Epoch: [474]  [20/42]  eta: 0:00:12  lr: 0.000933  loss: 0.4793 (0.4905)  time: 0.5088  data: 0.0001  max mem: 9341
[10:15:58.433561] Epoch: [474]  [40/42]  eta: 0:00:01  lr: 0.000931  loss: 0.5034 (0.4976)  time: 0.5078  data: 0.0001  max mem: 9341
[10:15:58.938224] Epoch: [474]  [41/42]  eta: 0:00:00  lr: 0.000931  loss: 0.5034 (0.4968)  time: 0.5078  data: 0.0001  max mem: 9341
[10:15:59.095796] Epoch: [474] Total time: 0:00:22 (0.5346 s / it)
[10:15:59.117271] Averaged stats: lr: 0.000931  loss: 0.5034 (0.4970)
[10:16:03.668340] {"train_lr": 0.0009327776920339347, "train_loss": 0.49700252073151724, "epoch": 474}
[10:16:03.668724] [10:16:03.668805] Training epoch 474 for 0:00:27
[10:16:03.668855] [10:16:03.673232] log_dir: ./exp/debug/cifar100-LT/debug
[10:16:05.142838] Epoch: [475]  [ 0/42]  eta: 0:01:01  lr: 0.000930  loss: 0.5049 (0.5049)  time: 1.4683  data: 0.9707  max mem: 9341
[10:16:15.308819] Epoch: [475]  [20/42]  eta: 0:00:12  lr: 0.000928  loss: 0.4909 (0.4936)  time: 0.5082  data: 0.0001  max mem: 9341
[10:16:25.457552] Epoch: [475]  [40/42]  eta: 0:00:01  lr: 0.000926  loss: 0.4900 (0.4944)  time: 0.5074  data: 0.0001  max mem: 9341
[10:16:25.964632] Epoch: [475]  [41/42]  eta: 0:00:00  lr: 0.000926  loss: 0.4900 (0.4945)  time: 0.5074  data: 0.0001  max mem: 9341
[10:16:26.126313] Epoch: [475] Total time: 0:00:22 (0.5346 s / it)
[10:16:26.128422] Averaged stats: lr: 0.000926  loss: 0.4900 (0.4965)
[10:16:30.715440] {"train_lr": 0.0009279441371121887, "train_loss": 0.4965301202166648, "epoch": 475}
[10:16:30.715818] [10:16:30.715899] Training epoch 475 for 0:00:27
[10:16:30.715951] [10:16:30.720366] log_dir: ./exp/debug/cifar100-LT/debug
[10:16:32.316446] Epoch: [476]  [ 0/42]  eta: 0:01:06  lr: 0.000925  loss: 0.4895 (0.4895)  time: 1.5948  data: 1.0888  max mem: 9341
[10:16:42.488317] Epoch: [476]  [20/42]  eta: 0:00:12  lr: 0.000923  loss: 0.4935 (0.4991)  time: 0.5085  data: 0.0001  max mem: 9341
[10:16:52.640331] Epoch: [476]  [40/42]  eta: 0:00:01  lr: 0.000921  loss: 0.4835 (0.4943)  time: 0.5076  data: 0.0001  max mem: 9341
[10:16:53.145437] Epoch: [476]  [41/42]  eta: 0:00:00  lr: 0.000921  loss: 0.4841 (0.4943)  time: 0.5076  data: 0.0001  max mem: 9341
[10:16:53.316369] Epoch: [476] Total time: 0:00:22 (0.5380 s / it)
[10:16:53.317094] Averaged stats: lr: 0.000921  loss: 0.4841 (0.4958)
[10:16:58.031508] {"train_lr": 0.0009231152308745124, "train_loss": 0.49579150655439924, "epoch": 476}
[10:16:58.031826] [10:16:58.031909] Training epoch 476 for 0:00:27
[10:16:58.031960] [10:16:58.036388] log_dir: ./exp/debug/cifar100-LT/debug
[10:16:59.625923] Epoch: [477]  [ 0/42]  eta: 0:01:06  lr: 0.000920  loss: 0.5325 (0.5325)  time: 1.5883  data: 1.0846  max mem: 9341
[10:17:09.786994] Epoch: [477]  [20/42]  eta: 0:00:12  lr: 0.000918  loss: 0.4975 (0.4986)  time: 0.5080  data: 0.0001  max mem: 9341
[10:17:19.936875] Epoch: [477]  [40/42]  eta: 0:00:01  lr: 0.000916  loss: 0.5010 (0.4965)  time: 0.5074  data: 0.0001  max mem: 9341
[10:17:20.443199] Epoch: [477]  [41/42]  eta: 0:00:00  lr: 0.000916  loss: 0.4996 (0.4965)  time: 0.5075  data: 0.0001  max mem: 9341
[10:17:20.616681] Epoch: [477] Total time: 0:00:22 (0.5376 s / it)
[10:17:20.618586] Averaged stats: lr: 0.000916  loss: 0.4996 (0.4966)
[10:17:25.113484] {"train_lr": 0.0009182910558335899, "train_loss": 0.49660047569445204, "epoch": 477}
[10:17:25.113846] [10:17:25.113929] Training epoch 477 for 0:00:27
[10:17:25.113980] [10:17:25.118343] log_dir: ./exp/debug/cifar100-LT/debug
[10:17:26.794025] Epoch: [478]  [ 0/42]  eta: 0:01:10  lr: 0.000915  loss: 0.5387 (0.5387)  time: 1.6747  data: 1.1758  max mem: 9341
[10:17:36.946678] Epoch: [478]  [20/42]  eta: 0:00:12  lr: 0.000913  loss: 0.4935 (0.4982)  time: 0.5076  data: 0.0001  max mem: 9341
[10:17:47.090844] Epoch: [478]  [40/42]  eta: 0:00:01  lr: 0.000911  loss: 0.5012 (0.5003)  time: 0.5072  data: 0.0001  max mem: 9341
[10:17:47.597066] Epoch: [478]  [41/42]  eta: 0:00:00  lr: 0.000911  loss: 0.4953 (0.4996)  time: 0.5072  data: 0.0001  max mem: 9341
[10:17:47.768168] Epoch: [478] Total time: 0:00:22 (0.5393 s / it)
[10:17:47.769179] Averaged stats: lr: 0.000911  loss: 0.4953 (0.4978)
[10:17:52.292031] {"train_lr": 0.0009134716944212626, "train_loss": 0.4978492989071778, "epoch": 478}
[10:17:52.292509] [10:17:52.292630] Training epoch 478 for 0:00:27
[10:17:52.292685] [10:17:52.297147] log_dir: ./exp/debug/cifar100-LT/debug
[10:17:53.932309] Epoch: [479]  [ 0/42]  eta: 0:01:08  lr: 0.000910  loss: 0.4489 (0.4489)  time: 1.6340  data: 1.1226  max mem: 9341
[10:18:04.093954] Epoch: [479]  [20/42]  eta: 0:00:12  lr: 0.000908  loss: 0.4911 (0.4927)  time: 0.5080  data: 0.0001  max mem: 9341
[10:18:14.237097] Epoch: [479]  [40/42]  eta: 0:00:01  lr: 0.000907  loss: 0.4979 (0.4950)  time: 0.5071  data: 0.0001  max mem: 9341
[10:18:14.741355] Epoch: [479]  [41/42]  eta: 0:00:00  lr: 0.000907  loss: 0.4979 (0.4947)  time: 0.5071  data: 0.0001  max mem: 9341
[10:18:14.915093] Epoch: [479] Total time: 0:00:22 (0.5385 s / it)
[10:18:14.919230] Averaged stats: lr: 0.000907  loss: 0.4979 (0.4963)
[10:18:19.553108] {"train_lr": 0.0009086572289871195, "train_loss": 0.49628233200027827, "epoch": 479}
[10:18:19.553448] [10:18:19.553526] Training epoch 479 for 0:00:27
[10:18:19.553576] [10:18:19.557985] log_dir: ./exp/debug/cifar100-LT/debug
[10:18:21.207908] Epoch: [480]  [ 0/42]  eta: 0:01:09  lr: 0.000905  loss: 0.5192 (0.5192)  time: 1.6486  data: 1.1436  max mem: 9341
[10:18:31.372804] Epoch: [480]  [20/42]  eta: 0:00:12  lr: 0.000904  loss: 0.5062 (0.5007)  time: 0.5082  data: 0.0001  max mem: 9341
[10:18:41.521149] Epoch: [480]  [40/42]  eta: 0:00:01  lr: 0.000902  loss: 0.4834 (0.4961)  time: 0.5074  data: 0.0001  max mem: 9341
[10:18:42.025259] Epoch: [480]  [41/42]  eta: 0:00:00  lr: 0.000902  loss: 0.4832 (0.4954)  time: 0.5073  data: 0.0001  max mem: 9341
[10:18:42.184961] Epoch: [480] Total time: 0:00:22 (0.5387 s / it)
[10:18:42.196805] Averaged stats: lr: 0.000902  loss: 0.4832 (0.4954)
[10:18:46.795482] {"train_lr": 0.0009038477417970883, "train_loss": 0.49539547236192794, "epoch": 480}
[10:18:46.795827] [10:18:46.795909] Training epoch 480 for 0:00:27
[10:18:46.795960] [10:18:46.800446] log_dir: ./exp/debug/cifar100-LT/debug
[10:18:48.333047] Epoch: [481]  [ 0/42]  eta: 0:01:04  lr: 0.000901  loss: 0.5136 (0.5136)  time: 1.5312  data: 1.0251  max mem: 9341
[10:18:58.514426] Epoch: [481]  [20/42]  eta: 0:00:12  lr: 0.000899  loss: 0.4955 (0.5016)  time: 0.5090  data: 0.0001  max mem: 9341
[10:19:08.677344] Epoch: [481]  [40/42]  eta: 0:00:01  lr: 0.000897  loss: 0.5038 (0.5029)  time: 0.5081  data: 0.0001  max mem: 9341
[10:19:09.185117] Epoch: [481]  [41/42]  eta: 0:00:00  lr: 0.000897  loss: 0.5038 (0.5029)  time: 0.5082  data: 0.0001  max mem: 9341
[10:19:09.360699] Epoch: [481] Total time: 0:00:22 (0.5371 s / it)
[10:19:09.367739] Averaged stats: lr: 0.000897  loss: 0.5038 (0.5001)
[10:19:13.991863] {"train_lr": 0.0008990433150320398, "train_loss": 0.5001447410100982, "epoch": 481}
[10:19:13.992236] [10:19:13.992320] Training epoch 481 for 0:00:27
[10:19:13.992373] [10:19:13.997131] log_dir: ./exp/debug/cifar100-LT/debug
[10:19:15.589537] Epoch: [482]  [ 0/42]  eta: 0:01:06  lr: 0.000896  loss: 0.4784 (0.4784)  time: 1.5913  data: 1.0668  max mem: 9341
[10:19:25.757230] Epoch: [482]  [20/42]  eta: 0:00:12  lr: 0.000894  loss: 0.5009 (0.5019)  time: 0.5083  data: 0.0001  max mem: 9341
[10:19:35.917775] Epoch: [482]  [40/42]  eta: 0:00:01  lr: 0.000892  loss: 0.5019 (0.5017)  time: 0.5080  data: 0.0001  max mem: 9341
[10:19:36.423088] Epoch: [482]  [41/42]  eta: 0:00:00  lr: 0.000892  loss: 0.5051 (0.5021)  time: 0.5080  data: 0.0001  max mem: 9341
[10:19:36.599027] Epoch: [482] Total time: 0:00:22 (0.5381 s / it)
[10:19:36.599959] Averaged stats: lr: 0.000892  loss: 0.5051 (0.4959)
[10:19:41.222058] {"train_lr": 0.0008942440307863691, "train_loss": 0.4959113292750858, "epoch": 482}
[10:19:41.222407] [10:19:41.222513] Training epoch 482 for 0:00:27
[10:19:41.222566] [10:19:41.226856] log_dir: ./exp/debug/cifar100-LT/debug
[10:19:42.863064] Epoch: [483]  [ 0/42]  eta: 0:01:08  lr: 0.000891  loss: 0.5092 (0.5092)  time: 1.6349  data: 1.1192  max mem: 9341
[10:19:53.074072] Epoch: [483]  [20/42]  eta: 0:00:12  lr: 0.000889  loss: 0.4940 (0.4971)  time: 0.5105  data: 0.0001  max mem: 9341
[10:20:03.218290] Epoch: [483]  [40/42]  eta: 0:00:01  lr: 0.000887  loss: 0.4825 (0.4896)  time: 0.5072  data: 0.0001  max mem: 9341
[10:20:03.724263] Epoch: [483]  [41/42]  eta: 0:00:00  lr: 0.000887  loss: 0.4825 (0.4886)  time: 0.5072  data: 0.0001  max mem: 9341
[10:20:03.887100] Epoch: [483] Total time: 0:00:22 (0.5395 s / it)
[10:20:03.892937] Averaged stats: lr: 0.000887  loss: 0.4825 (0.4946)
[10:20:08.466006] {"train_lr": 0.0008894499710666041, "train_loss": 0.494604481118066, "epoch": 483}
[10:20:08.466378] [10:20:08.466459] Training epoch 483 for 0:00:27
[10:20:08.466511] [10:20:08.470907] log_dir: ./exp/debug/cifar100-LT/debug
[10:20:09.964071] Epoch: [484]  [ 0/42]  eta: 0:01:02  lr: 0.000886  loss: 0.5239 (0.5239)  time: 1.4920  data: 0.9974  max mem: 9341
[10:20:20.127340] Epoch: [484]  [20/42]  eta: 0:00:12  lr: 0.000884  loss: 0.4983 (0.4964)  time: 0.5081  data: 0.0001  max mem: 9341
[10:20:30.273230] Epoch: [484]  [40/42]  eta: 0:00:01  lr: 0.000883  loss: 0.4928 (0.4940)  time: 0.5072  data: 0.0001  max mem: 9341
[10:20:30.779595] Epoch: [484]  [41/42]  eta: 0:00:00  lr: 0.000883  loss: 0.4903 (0.4937)  time: 0.5073  data: 0.0001  max mem: 9341
[10:20:30.946804] Epoch: [484] Total time: 0:00:22 (0.5351 s / it)
[10:20:30.947709] Averaged stats: lr: 0.000883  loss: 0.4903 (0.4930)
[10:20:35.492727] {"train_lr": 0.000884661217789996, "train_loss": 0.4929891345756395, "epoch": 484}
[10:20:35.493091] [10:20:35.493172] Training epoch 484 for 0:00:27
[10:20:35.493222] [10:20:35.497586] log_dir: ./exp/debug/cifar100-LT/debug
[10:20:37.118365] Epoch: [485]  [ 0/42]  eta: 0:01:08  lr: 0.000881  loss: 0.4844 (0.4844)  time: 1.6194  data: 1.1119  max mem: 9341
[10:20:47.285632] Epoch: [485]  [20/42]  eta: 0:00:12  lr: 0.000880  loss: 0.4977 (0.4941)  time: 0.5083  data: 0.0001  max mem: 9341
[10:20:57.427484] Epoch: [485]  [40/42]  eta: 0:00:01  lr: 0.000878  loss: 0.4950 (0.4967)  time: 0.5070  data: 0.0001  max mem: 9341
[10:20:57.932768] Epoch: [485]  [41/42]  eta: 0:00:00  lr: 0.000878  loss: 0.4968 (0.4967)  time: 0.5070  data: 0.0001  max mem: 9341
[10:20:58.082685] Epoch: [485] Total time: 0:00:22 (0.5377 s / it)
[10:20:58.091758] Averaged stats: lr: 0.000878  loss: 0.4968 (0.4951)
[10:21:02.698953] {"train_lr": 0.0008798778527831258, "train_loss": 0.49507290515161695, "epoch": 485}
[10:21:02.699293] [10:21:02.699377] Training epoch 485 for 0:00:27
[10:21:02.699426] [10:21:02.703764] log_dir: ./exp/debug/cifar100-LT/debug
[10:21:04.151744] Epoch: [486]  [ 0/42]  eta: 0:01:00  lr: 0.000877  loss: 0.4759 (0.4759)  time: 1.4466  data: 0.9274  max mem: 9341
[10:21:14.319313] Epoch: [486]  [20/42]  eta: 0:00:12  lr: 0.000875  loss: 0.5054 (0.5053)  time: 0.5083  data: 0.0001  max mem: 9341
[10:21:24.462774] Epoch: [486]  [40/42]  eta: 0:00:01  lr: 0.000873  loss: 0.4858 (0.4975)  time: 0.5071  data: 0.0001  max mem: 9341
[10:21:24.967082] Epoch: [486]  [41/42]  eta: 0:00:00  lr: 0.000873  loss: 0.4791 (0.4963)  time: 0.5071  data: 0.0001  max mem: 9341
[10:21:25.132957] Epoch: [486] Total time: 0:00:22 (0.5340 s / it)
[10:21:25.137805] Averaged stats: lr: 0.000873  loss: 0.4791 (0.4954)
[10:21:29.716925] {"train_lr": 0.0008750999577805052, "train_loss": 0.4954116411861919, "epoch": 486}
[10:21:29.717276] [10:21:29.717358] Training epoch 486 for 0:00:27
[10:21:29.717409] [10:21:29.721761] log_dir: ./exp/debug/cifar100-LT/debug
[10:21:31.288567] Epoch: [487]  [ 0/42]  eta: 0:01:05  lr: 0.000872  loss: 0.5010 (0.5010)  time: 1.5655  data: 1.0589  max mem: 9341
[10:21:41.452148] Epoch: [487]  [20/42]  eta: 0:00:12  lr: 0.000870  loss: 0.5108 (0.5133)  time: 0.5081  data: 0.0002  max mem: 9341
[10:21:51.597070] Epoch: [487]  [40/42]  eta: 0:00:01  lr: 0.000868  loss: 0.4907 (0.5040)  time: 0.5072  data: 0.0001  max mem: 9341
[10:21:52.103380] Epoch: [487]  [41/42]  eta: 0:00:00  lr: 0.000868  loss: 0.4957 (0.5043)  time: 0.5073  data: 0.0001  max mem: 9341
[10:21:52.269976] Epoch: [487] Total time: 0:00:22 (0.5369 s / it)
[10:21:52.285909] Averaged stats: lr: 0.000868  loss: 0.4957 (0.4992)
[10:21:56.970346] {"train_lr": 0.0008703276144231767, "train_loss": 0.4991830559003921, "epoch": 487}
[10:21:56.970681] [10:21:56.970762] Training epoch 487 for 0:00:27
[10:21:56.970813] [10:21:56.975180] log_dir: ./exp/debug/cifar100-LT/debug
[10:21:58.559131] Epoch: [488]  [ 0/42]  eta: 0:01:06  lr: 0.000867  loss: 0.5364 (0.5364)  time: 1.5827  data: 1.0641  max mem: 9341
[10:22:08.746041] Epoch: [488]  [20/42]  eta: 0:00:12  lr: 0.000865  loss: 0.4890 (0.4943)  time: 0.5093  data: 0.0001  max mem: 9341
[10:22:18.913863] Epoch: [488]  [40/42]  eta: 0:00:01  lr: 0.000863  loss: 0.4827 (0.4928)  time: 0.5083  data: 0.0001  max mem: 9341
[10:22:19.417763] Epoch: [488]  [41/42]  eta: 0:00:00  lr: 0.000863  loss: 0.4846 (0.4928)  time: 0.5083  data: 0.0001  max mem: 9341
[10:22:19.595760] Epoch: [488] Total time: 0:00:22 (0.5386 s / it)
[10:22:19.605385] Averaged stats: lr: 0.000863  loss: 0.4846 (0.4948)
[10:22:24.214267] {"train_lr": 0.00086556090425732, "train_loss": 0.4948174020364171, "epoch": 488}
[10:22:24.214628] [10:22:24.214711] Training epoch 488 for 0:00:27
[10:22:24.214763] [10:22:24.219172] log_dir: ./exp/debug/cifar100-LT/debug
[10:22:25.916673] Epoch: [489]  [ 0/42]  eta: 0:01:11  lr: 0.000862  loss: 0.4995 (0.4995)  time: 1.6964  data: 1.1882  max mem: 9341
[10:22:36.072227] Epoch: [489]  [20/42]  eta: 0:00:12  lr: 0.000861  loss: 0.4892 (0.4928)  time: 0.5077  data: 0.0001  max mem: 9341
[10:22:46.218196] Epoch: [489]  [40/42]  eta: 0:00:01  lr: 0.000859  loss: 0.4900 (0.4931)  time: 0.5073  data: 0.0001  max mem: 9341
[10:22:46.724939] Epoch: [489]  [41/42]  eta: 0:00:00  lr: 0.000859  loss: 0.4926 (0.4939)  time: 0.5073  data: 0.0001  max mem: 9341
[10:22:46.897735] Epoch: [489] Total time: 0:00:22 (0.5400 s / it)
[10:22:46.898621] Averaged stats: lr: 0.000859  loss: 0.4926 (0.4935)
[10:22:51.491018] {"train_lr": 0.0008607999087328616, "train_loss": 0.493501601297231, "epoch": 489}
[10:22:51.491441] [10:22:51.491530] Training epoch 489 for 0:00:27
[10:22:51.491581] [10:22:51.496725] log_dir: ./exp/debug/cifar100-LT/debug
[10:22:53.093205] Epoch: [490]  [ 0/42]  eta: 0:01:07  lr: 0.000858  loss: 0.5080 (0.5080)  time: 1.5953  data: 1.0870  max mem: 9341
[10:23:03.274569] Epoch: [490]  [20/42]  eta: 0:00:12  lr: 0.000856  loss: 0.4903 (0.4914)  time: 0.5090  data: 0.0001  max mem: 9341
[10:23:13.431305] Epoch: [490]  [40/42]  eta: 0:00:01  lr: 0.000854  loss: 0.4780 (0.4862)  time: 0.5078  data: 0.0001  max mem: 9341
[10:23:13.937242] Epoch: [490]  [41/42]  eta: 0:00:00  lr: 0.000854  loss: 0.4781 (0.4866)  time: 0.5078  data: 0.0001  max mem: 9341
[10:23:14.112203] Epoch: [490] Total time: 0:00:22 (0.5385 s / it)
[10:23:14.112951] Averaged stats: lr: 0.000854  loss: 0.4781 (0.4901)
[10:23:18.628421] {"train_lr": 0.0008560447092020761, "train_loss": 0.49005984603649094, "epoch": 490}
[10:23:18.628801] [10:23:18.628887] Training epoch 490 for 0:00:27
[10:23:18.628939] [10:23:18.633438] log_dir: ./exp/debug/cifar100-LT/debug
[10:23:20.283023] Epoch: [491]  [ 0/42]  eta: 0:01:09  lr: 0.000853  loss: 0.5575 (0.5575)  time: 1.6483  data: 1.1492  max mem: 9341
[10:23:30.451242] Epoch: [491]  [20/42]  eta: 0:00:12  lr: 0.000851  loss: 0.5014 (0.5028)  time: 0.5084  data: 0.0001  max mem: 9341
[10:23:40.590976] Epoch: [491]  [40/42]  eta: 0:00:01  lr: 0.000849  loss: 0.4856 (0.4980)  time: 0.5069  data: 0.0001  max mem: 9341
[10:23:41.095396] Epoch: [491]  [41/42]  eta: 0:00:00  lr: 0.000849  loss: 0.4856 (0.4977)  time: 0.5069  data: 0.0001  max mem: 9341
[10:23:41.252757] Epoch: [491] Total time: 0:00:22 (0.5386 s / it)
[10:23:41.262020] Averaged stats: lr: 0.000849  loss: 0.4856 (0.4940)
[10:23:45.854804] {"train_lr": 0.0008512953869182074, "train_loss": 0.49403056289468494, "epoch": 491}
[10:23:45.855161] [10:23:45.855241] Training epoch 491 for 0:00:27
[10:23:45.855292] [10:23:45.859554] log_dir: ./exp/debug/cifar100-LT/debug
[10:23:47.571928] Epoch: [492]  [ 0/42]  eta: 0:01:11  lr: 0.000848  loss: 0.4966 (0.4966)  time: 1.7113  data: 1.2044  max mem: 9341
[10:23:57.746147] Epoch: [492]  [20/42]  eta: 0:00:12  lr: 0.000846  loss: 0.4767 (0.4832)  time: 0.5086  data: 0.0001  max mem: 9341
[10:24:07.896311] Epoch: [492]  [40/42]  eta: 0:00:01  lr: 0.000844  loss: 0.4864 (0.4862)  time: 0.5075  data: 0.0001  max mem: 9341
[10:24:08.402589] Epoch: [492]  [41/42]  eta: 0:00:00  lr: 0.000844  loss: 0.4864 (0.4867)  time: 0.5075  data: 0.0001  max mem: 9341
[10:24:08.569251] Epoch: [492] Total time: 0:00:22 (0.5407 s / it)
[10:24:08.597895] Averaged stats: lr: 0.000844  loss: 0.4864 (0.4918)
[10:24:13.163697] {"train_lr": 0.0008465520230340669, "train_loss": 0.4917859534422557, "epoch": 492}
[10:24:13.164054] [10:24:13.164186] Training epoch 492 for 0:00:27
[10:24:13.164243] [10:24:13.168698] log_dir: ./exp/debug/cifar100-LT/debug
[10:24:14.655479] Epoch: [493]  [ 0/42]  eta: 0:01:02  lr: 0.000843  loss: 0.4813 (0.4813)  time: 1.4855  data: 0.9730  max mem: 9341
[10:24:24.892659] Epoch: [493]  [20/42]  eta: 0:00:12  lr: 0.000842  loss: 0.4898 (0.4884)  time: 0.5118  data: 0.0001  max mem: 9341
[10:24:35.046628] Epoch: [493]  [40/42]  eta: 0:00:01  lr: 0.000840  loss: 0.4943 (0.4938)  time: 0.5076  data: 0.0001  max mem: 9341
[10:24:35.552858] Epoch: [493]  [41/42]  eta: 0:00:00  lr: 0.000840  loss: 0.4933 (0.4931)  time: 0.5076  data: 0.0001  max mem: 9341
[10:24:35.720237] Epoch: [493] Total time: 0:00:22 (0.5369 s / it)
[10:24:35.721073] Averaged stats: lr: 0.000840  loss: 0.4933 (0.4921)
[10:24:40.251390] {"train_lr": 0.0008418146986006565, "train_loss": 0.49206612205931116, "epoch": 493}
[10:24:40.251791] [10:24:40.251873] Training epoch 493 for 0:00:27
[10:24:40.251924] [10:24:40.256366] log_dir: ./exp/debug/cifar100-LT/debug
[10:24:41.957258] Epoch: [494]  [ 0/42]  eta: 0:01:11  lr: 0.000839  loss: 0.4727 (0.4727)  time: 1.6997  data: 1.1966  max mem: 9341
[10:24:52.120022] Epoch: [494]  [20/42]  eta: 0:00:12  lr: 0.000837  loss: 0.4945 (0.4993)  time: 0.5081  data: 0.0001  max mem: 9341
[10:25:02.262537] Epoch: [494]  [40/42]  eta: 0:00:01  lr: 0.000835  loss: 0.4950 (0.4990)  time: 0.5071  data: 0.0001  max mem: 9341
[10:25:02.768357] Epoch: [494]  [41/42]  eta: 0:00:00  lr: 0.000835  loss: 0.4950 (0.4986)  time: 0.5071  data: 0.0001  max mem: 9341
[10:25:02.935651] Epoch: [494] Total time: 0:00:22 (0.5400 s / it)
[10:25:02.942712] Averaged stats: lr: 0.000835  loss: 0.4950 (0.4931)
[10:25:07.496906] {"train_lr": 0.0008370834945657809, "train_loss": 0.4931155617038409, "epoch": 494}
[10:25:07.497261] [10:25:07.497343] Training epoch 494 for 0:00:27
[10:25:07.497394] [10:25:07.501758] log_dir: ./exp/debug/cifar100-LT/debug
[10:25:09.021254] Epoch: [495]  [ 0/42]  eta: 0:01:03  lr: 0.000834  loss: 0.5151 (0.5151)  time: 1.5177  data: 1.0124  max mem: 9341
[10:25:19.191384] Epoch: [495]  [20/42]  eta: 0:00:12  lr: 0.000832  loss: 0.4882 (0.4947)  time: 0.5084  data: 0.0002  max mem: 9341
[10:25:29.346270] Epoch: [495]  [40/42]  eta: 0:00:01  lr: 0.000830  loss: 0.4944 (0.4920)  time: 0.5077  data: 0.0001  max mem: 9341
[10:25:29.852464] Epoch: [495]  [41/42]  eta: 0:00:00  lr: 0.000830  loss: 0.4989 (0.4934)  time: 0.5078  data: 0.0001  max mem: 9341
[10:25:30.017810] Epoch: [495] Total time: 0:00:22 (0.5361 s / it)
[10:25:30.021850] Averaged stats: lr: 0.000830  loss: 0.4989 (0.4929)
[10:25:34.498402] {"train_lr": 0.0008323584917726616, "train_loss": 0.4929346027118819, "epoch": 495}
[10:25:34.498757] [10:25:34.498837] Training epoch 495 for 0:00:27
[10:25:34.498888] [10:25:34.503187] log_dir: ./exp/debug/cifar100-LT/debug
[10:25:36.162203] Epoch: [496]  [ 0/42]  eta: 0:01:09  lr: 0.000829  loss: 0.4838 (0.4838)  time: 1.6578  data: 1.1601  max mem: 9341
[10:25:46.328382] Epoch: [496]  [20/42]  eta: 0:00:12  lr: 0.000827  loss: 0.4914 (0.4892)  time: 0.5083  data: 0.0001  max mem: 9341
[10:25:56.473201] Epoch: [496]  [40/42]  eta: 0:00:01  lr: 0.000826  loss: 0.5030 (0.4949)  time: 0.5072  data: 0.0001  max mem: 9341
[10:25:56.978859] Epoch: [496]  [41/42]  eta: 0:00:00  lr: 0.000826  loss: 0.4997 (0.4949)  time: 0.5072  data: 0.0001  max mem: 9341
[10:25:57.142542] Epoch: [496] Total time: 0:00:22 (0.5390 s / it)
[10:25:57.145072] Averaged stats: lr: 0.000826  loss: 0.4997 (0.4920)
[10:26:01.692231] {"train_lr": 0.0008276397709585612, "train_loss": 0.4920004247909501, "epoch": 496}
[10:26:01.692612] [10:26:01.692693] Training epoch 496 for 0:00:27
[10:26:01.692744] [10:26:01.697125] log_dir: ./exp/debug/cifar100-LT/debug
[10:26:03.305490] Epoch: [497]  [ 0/42]  eta: 0:01:07  lr: 0.000824  loss: 0.4843 (0.4843)  time: 1.6071  data: 1.1108  max mem: 9341
[10:26:13.476692] Epoch: [497]  [20/42]  eta: 0:00:12  lr: 0.000823  loss: 0.4774 (0.4917)  time: 0.5085  data: 0.0001  max mem: 9341
[10:26:23.629113] Epoch: [497]  [40/42]  eta: 0:00:01  lr: 0.000821  loss: 0.4876 (0.4912)  time: 0.5076  data: 0.0001  max mem: 9341
[10:26:24.134869] Epoch: [497]  [41/42]  eta: 0:00:00  lr: 0.000821  loss: 0.4878 (0.4920)  time: 0.5075  data: 0.0001  max mem: 9341
[10:26:24.306587] Epoch: [497] Total time: 0:00:22 (0.5383 s / it)
[10:26:24.315879] Averaged stats: lr: 0.000821  loss: 0.4878 (0.4910)
[10:26:28.847476] {"train_lr": 0.0008229274127533963, "train_loss": 0.4910331505040328, "epoch": 497}
[10:26:28.847882] [10:26:28.847970] Training epoch 497 for 0:00:27
[10:26:28.848024] [10:26:28.852600] log_dir: ./exp/debug/cifar100-LT/debug
[10:26:30.432123] Epoch: [498]  [ 0/42]  eta: 0:01:06  lr: 0.000820  loss: 0.5015 (0.5015)  time: 1.5782  data: 1.0705  max mem: 9341
[10:26:40.605756] Epoch: [498]  [20/42]  eta: 0:00:12  lr: 0.000818  loss: 0.4849 (0.4906)  time: 0.5086  data: 0.0001  max mem: 9341
[10:26:50.758288] Epoch: [498]  [40/42]  eta: 0:00:01  lr: 0.000816  loss: 0.4791 (0.4872)  time: 0.5076  data: 0.0001  max mem: 9341
[10:26:51.263913] Epoch: [498]  [41/42]  eta: 0:00:00  lr: 0.000816  loss: 0.4791 (0.4870)  time: 0.5075  data: 0.0001  max mem: 9341
[10:26:51.433004] Epoch: [498] Total time: 0:00:22 (0.5376 s / it)
[10:26:51.433821] Averaged stats: lr: 0.000816  loss: 0.4791 (0.4874)
[10:26:56.094799] {"train_lr": 0.000818221497678371, "train_loss": 0.4874255818625291, "epoch": 498}
[10:26:56.095116] [10:26:56.095199] Training epoch 498 for 0:00:27
[10:26:56.095248] [10:26:56.099625] log_dir: ./exp/debug/cifar100-LT/debug
[10:26:57.572718] Epoch: [499]  [ 0/42]  eta: 0:01:01  lr: 0.000815  loss: 0.5276 (0.5276)  time: 1.4717  data: 0.9691  max mem: 9341
[10:27:07.736325] Epoch: [499]  [20/42]  eta: 0:00:12  lr: 0.000813  loss: 0.4993 (0.4970)  time: 0.5081  data: 0.0001  max mem: 9341
[10:27:17.893012] Epoch: [499]  [40/42]  eta: 0:00:01  lr: 0.000811  loss: 0.4834 (0.4929)  time: 0.5078  data: 0.0001  max mem: 9341
[10:27:18.398213] Epoch: [499]  [41/42]  eta: 0:00:00  lr: 0.000811  loss: 0.4834 (0.4930)  time: 0.5078  data: 0.0001  max mem: 9341
[10:27:18.562248] Epoch: [499] Total time: 0:00:22 (0.5348 s / it)
[10:27:18.568717] Averaged stats: lr: 0.000811  loss: 0.4834 (0.4922)
[10:27:23.074852] {"train_lr": 0.0008135221061445877, "train_loss": 0.4922379439785367, "epoch": 499}
[10:27:23.075240] [10:27:23.075323] Training epoch 499 for 0:00:26
[10:27:23.075374] [10:27:23.079733] log_dir: ./exp/debug/cifar100-LT/debug
[10:27:24.534696] Epoch: [500]  [ 0/42]  eta: 0:01:01  lr: 0.000810  loss: 0.4463 (0.4463)  time: 1.4537  data: 0.9569  max mem: 9341
[10:27:34.724146] Epoch: [500]  [20/42]  eta: 0:00:12  lr: 0.000809  loss: 0.4843 (0.4798)  time: 0.5094  data: 0.0001  max mem: 9341
[10:27:44.895784] Epoch: [500]  [40/42]  eta: 0:00:01  lr: 0.000807  loss: 0.5005 (0.4857)  time: 0.5085  data: 0.0001  max mem: 9341
[10:27:45.402683] Epoch: [500]  [41/42]  eta: 0:00:00  lr: 0.000807  loss: 0.5044 (0.4867)  time: 0.5085  data: 0.0001  max mem: 9341
[10:27:45.577065] Epoch: [500] Total time: 0:00:22 (0.5356 s / it)
[10:27:45.579202] Averaged stats: lr: 0.000807  loss: 0.5044 (0.4891)
[10:27:50.197320] {"train_lr": 0.0008088293184516802, "train_loss": 0.4891134039277122, "epoch": 500}
[10:27:50.197723] [10:27:50.197810] Training epoch 500 for 0:00:27
[10:27:50.197864] [10:27:50.202185] log_dir: ./exp/debug/cifar100-LT/debug
[10:27:51.696012] Epoch: [501]  [ 0/42]  eta: 0:01:02  lr: 0.000806  loss: 0.4783 (0.4783)  time: 1.4923  data: 0.9873  max mem: 9341
[10:28:01.868052] Epoch: [501]  [20/42]  eta: 0:00:12  lr: 0.000804  loss: 0.4755 (0.4765)  time: 0.5085  data: 0.0002  max mem: 9341
[10:28:12.016488] Epoch: [501]  [40/42]  eta: 0:00:01  lr: 0.000802  loss: 0.4786 (0.4808)  time: 0.5074  data: 0.0001  max mem: 9341
[10:28:12.523190] Epoch: [501]  [41/42]  eta: 0:00:00  lr: 0.000802  loss: 0.4777 (0.4805)  time: 0.5074  data: 0.0001  max mem: 9341
[10:28:12.697699] Epoch: [501] Total time: 0:00:22 (0.5356 s / it)
[10:28:12.701424] Averaged stats: lr: 0.000802  loss: 0.4777 (0.4882)
[10:28:17.262324] {"train_lr": 0.0008041432147864442, "train_loss": 0.48816512223510516, "epoch": 501}
[10:28:17.262732] [10:28:17.262825] Training epoch 501 for 0:00:27
[10:28:17.262878] [10:28:17.267726] log_dir: ./exp/debug/cifar100-LT/debug
[10:28:18.758899] Epoch: [502]  [ 0/42]  eta: 0:01:02  lr: 0.000801  loss: 0.5087 (0.5087)  time: 1.4899  data: 0.9870  max mem: 9341
[10:28:28.934312] Epoch: [502]  [20/42]  eta: 0:00:12  lr: 0.000799  loss: 0.4931 (0.4947)  time: 0.5087  data: 0.0001  max mem: 9341
[10:28:39.090158] Epoch: [502]  [40/42]  eta: 0:00:01  lr: 0.000797  loss: 0.4804 (0.4918)  time: 0.5077  data: 0.0001  max mem: 9341
[10:28:39.596462] Epoch: [502]  [41/42]  eta: 0:00:00  lr: 0.000797  loss: 0.4890 (0.4918)  time: 0.5077  data: 0.0001  max mem: 9341
[10:28:39.767408] Epoch: [502] Total time: 0:00:22 (0.5357 s / it)
[10:28:39.770527] Averaged stats: lr: 0.000797  loss: 0.4890 (0.4916)
[10:28:44.334600] {"train_lr": 0.0007994638752214615, "train_loss": 0.491595759278252, "epoch": 502}
[10:28:44.334996] [10:28:44.335084] Training epoch 502 for 0:00:27
[10:28:44.335135] [10:28:44.339735] log_dir: ./exp/debug/cifar100-LT/debug
[10:28:46.035322] Epoch: [503]  [ 0/42]  eta: 0:01:11  lr: 0.000796  loss: 0.5213 (0.5213)  time: 1.6944  data: 1.1755  max mem: 9341
[10:28:56.204676] Epoch: [503]  [20/42]  eta: 0:00:12  lr: 0.000795  loss: 0.4922 (0.4924)  time: 0.5084  data: 0.0001  max mem: 9341
[10:29:06.368957] Epoch: [503]  [40/42]  eta: 0:00:01  lr: 0.000793  loss: 0.4915 (0.4916)  time: 0.5082  data: 0.0001  max mem: 9341
[10:29:06.875476] Epoch: [503]  [41/42]  eta: 0:00:00  lr: 0.000793  loss: 0.4918 (0.4920)  time: 0.5082  data: 0.0001  max mem: 9341
[10:29:07.046950] Epoch: [503] Total time: 0:00:22 (0.5406 s / it)
[10:29:07.051428] Averaged stats: lr: 0.000793  loss: 0.4918 (0.4930)
[10:29:11.680176] {"train_lr": 0.0007947913797137344, "train_loss": 0.49300903347986086, "epoch": 503}
[10:29:11.680592] [10:29:11.680675] Training epoch 503 for 0:00:27
[10:29:11.680727] [10:29:11.685213] log_dir: ./exp/debug/cifar100-LT/debug
[10:29:13.150630] Epoch: [504]  [ 0/42]  eta: 0:01:01  lr: 0.000792  loss: 0.5384 (0.5384)  time: 1.4637  data: 0.9610  max mem: 9341
[10:29:23.325236] Epoch: [504]  [20/42]  eta: 0:00:12  lr: 0.000790  loss: 0.4896 (0.4958)  time: 0.5087  data: 0.0002  max mem: 9341
[10:29:33.471953] Epoch: [504]  [40/42]  eta: 0:00:01  lr: 0.000788  loss: 0.5004 (0.4972)  time: 0.5073  data: 0.0001  max mem: 9341
[10:29:33.977136] Epoch: [504]  [41/42]  eta: 0:00:00  lr: 0.000788  loss: 0.4991 (0.4968)  time: 0.5073  data: 0.0001  max mem: 9341
[10:29:34.155615] Epoch: [504] Total time: 0:00:22 (0.5350 s / it)
[10:29:34.167088] Averaged stats: lr: 0.000788  loss: 0.4991 (0.4919)
[10:29:38.774149] {"train_lr": 0.0007901258081033195, "train_loss": 0.49187774920747396, "epoch": 504}
[10:29:38.774536] [10:29:38.774623] Training epoch 504 for 0:00:27
[10:29:38.774674] [10:29:38.779178] log_dir: ./exp/debug/cifar100-LT/debug
[10:29:40.312350] Epoch: [505]  [ 0/42]  eta: 0:01:04  lr: 0.000787  loss: 0.4664 (0.4664)  time: 1.5316  data: 1.0035  max mem: 9341
[10:29:50.486087] Epoch: [505]  [20/42]  eta: 0:00:12  lr: 0.000785  loss: 0.4900 (0.4921)  time: 0.5086  data: 0.0001  max mem: 9341
[10:30:00.641921] Epoch: [505]  [40/42]  eta: 0:00:01  lr: 0.000783  loss: 0.4796 (0.4912)  time: 0.5077  data: 0.0001  max mem: 9341
[10:30:01.149064] Epoch: [505]  [41/42]  eta: 0:00:00  lr: 0.000783  loss: 0.4809 (0.4915)  time: 0.5078  data: 0.0001  max mem: 9341
[10:30:01.318028] Epoch: [505] Total time: 0:00:22 (0.5366 s / it)
[10:30:01.334283] Averaged stats: lr: 0.000783  loss: 0.4809 (0.4919)
[10:30:05.973936] {"train_lr": 0.0007854672401119634, "train_loss": 0.4918649501743771, "epoch": 505}
[10:30:05.974320] [10:30:05.974401] Training epoch 505 for 0:00:27
[10:30:05.974452] [10:30:05.978901] log_dir: ./exp/debug/cifar100-LT/debug
[10:30:07.468923] Epoch: [506]  [ 0/42]  eta: 0:01:02  lr: 0.000782  loss: 0.5082 (0.5082)  time: 1.4887  data: 0.9816  max mem: 9341
[10:30:17.641027] Epoch: [506]  [20/42]  eta: 0:00:12  lr: 0.000781  loss: 0.4789 (0.4876)  time: 0.5085  data: 0.0001  max mem: 9341
[10:30:27.799638] Epoch: [506]  [40/42]  eta: 0:00:01  lr: 0.000779  loss: 0.4975 (0.4920)  time: 0.5079  data: 0.0001  max mem: 9341
[10:30:28.305605] Epoch: [506]  [41/42]  eta: 0:00:00  lr: 0.000779  loss: 0.4983 (0.4922)  time: 0.5079  data: 0.0001  max mem: 9341
[10:30:28.468422] Epoch: [506] Total time: 0:00:22 (0.5355 s / it)
[10:30:28.475494] Averaged stats: lr: 0.000779  loss: 0.4983 (0.4918)
[10:30:33.088517] {"train_lr": 0.0007808157553417381, "train_loss": 0.49182150122665225, "epoch": 506}
[10:30:33.088936] [10:30:33.089021] Training epoch 506 for 0:00:27
[10:30:33.089074] [10:30:33.093586] log_dir: ./exp/debug/cifar100-LT/debug
[10:30:34.622036] Epoch: [507]  [ 0/42]  eta: 0:01:04  lr: 0.000778  loss: 0.4980 (0.4980)  time: 1.5267  data: 1.0069  max mem: 9341
[10:30:44.794049] Epoch: [507]  [20/42]  eta: 0:00:12  lr: 0.000776  loss: 0.4880 (0.4907)  time: 0.5085  data: 0.0002  max mem: 9341
[10:30:54.950976] Epoch: [507]  [40/42]  eta: 0:00:01  lr: 0.000774  loss: 0.4765 (0.4849)  time: 0.5078  data: 0.0001  max mem: 9341
[10:30:55.458592] Epoch: [507]  [41/42]  eta: 0:00:00  lr: 0.000774  loss: 0.4765 (0.4848)  time: 0.5078  data: 0.0001  max mem: 9341
[10:30:55.622605] Epoch: [507] Total time: 0:00:22 (0.5364 s / it)
[10:30:55.636148] Averaged stats: lr: 0.000774  loss: 0.4765 (0.4853)
[10:31:00.190315] {"train_lr": 0.000776171433273686, "train_loss": 0.48532781767703237, "epoch": 507}
[10:31:00.190721] [10:31:00.190801] Training epoch 507 for 0:00:27
[10:31:00.190853] [10:31:00.195252] log_dir: ./exp/debug/cifar100-LT/debug
[10:31:01.898268] Epoch: [508]  [ 0/42]  eta: 0:01:11  lr: 0.000773  loss: 0.4664 (0.4664)  time: 1.7019  data: 1.2072  max mem: 9341
[10:31:12.061763] Epoch: [508]  [20/42]  eta: 0:00:12  lr: 0.000771  loss: 0.4905 (0.4900)  time: 0.5081  data: 0.0001  max mem: 9341
[10:31:22.222764] Epoch: [508]  [40/42]  eta: 0:00:01  lr: 0.000770  loss: 0.4875 (0.4889)  time: 0.5080  data: 0.0001  max mem: 9341
[10:31:22.730167] Epoch: [508]  [41/42]  eta: 0:00:00  lr: 0.000770  loss: 0.4875 (0.4894)  time: 0.5081  data: 0.0001  max mem: 9341
[10:31:22.897089] Epoch: [508] Total time: 0:00:22 (0.5405 s / it)
[10:31:22.915832] Averaged stats: lr: 0.000770  loss: 0.4875 (0.4892)
[10:31:27.519973] {"train_lr": 0.0007715343532664561, "train_loss": 0.4891557858458587, "epoch": 508}
[10:31:27.520508] [10:31:27.520601] Training epoch 508 for 0:00:27
[10:31:27.520653] [10:31:27.525434] log_dir: ./exp/debug/cifar100-LT/debug
[10:31:29.038624] Epoch: [509]  [ 0/42]  eta: 0:01:03  lr: 0.000768  loss: 0.4579 (0.4579)  time: 1.5119  data: 1.0040  max mem: 9341
[10:31:39.206346] Epoch: [509]  [20/42]  eta: 0:00:12  lr: 0.000767  loss: 0.4751 (0.4769)  time: 0.5083  data: 0.0001  max mem: 9341
[10:31:49.361068] Epoch: [509]  [40/42]  eta: 0:00:01  lr: 0.000765  loss: 0.4934 (0.4822)  time: 0.5077  data: 0.0001  max mem: 9341
[10:31:49.867269] Epoch: [509]  [41/42]  eta: 0:00:00  lr: 0.000765  loss: 0.4933 (0.4820)  time: 0.5077  data: 0.0001  max mem: 9341
[10:31:50.037617] Epoch: [509] Total time: 0:00:22 (0.5360 s / it)
[10:31:50.040090] Averaged stats: lr: 0.000765  loss: 0.4933 (0.4880)
[10:31:54.677000] {"train_lr": 0.0007669045945549518, "train_loss": 0.488019600687992, "epoch": 509}
[10:31:54.677343] [10:31:54.677442] Training epoch 509 for 0:00:27
[10:31:54.677492] [10:31:54.681655] log_dir: ./exp/debug/cifar100-LT/debug
[10:31:56.458039] Epoch: [510]  [ 0/42]  eta: 0:01:14  lr: 0.000764  loss: 0.4900 (0.4900)  time: 1.7752  data: 1.2736  max mem: 9341
[10:32:06.620893] Epoch: [510]  [20/42]  eta: 0:00:12  lr: 0.000762  loss: 0.4923 (0.4891)  time: 0.5081  data: 0.0001  max mem: 9341
[10:32:16.781337] Epoch: [510]  [40/42]  eta: 0:00:01  lr: 0.000760  loss: 0.4857 (0.4881)  time: 0.5080  data: 0.0001  max mem: 9341
[10:32:17.287272] Epoch: [510]  [41/42]  eta: 0:00:00  lr: 0.000760  loss: 0.4857 (0.4884)  time: 0.5080  data: 0.0001  max mem: 9341
[10:32:17.453204] Epoch: [510] Total time: 0:00:22 (0.5422 s / it)
[10:32:17.462224] Averaged stats: lr: 0.000760  loss: 0.4857 (0.4884)
[10:32:21.948993] {"train_lr": 0.0007622822362489761, "train_loss": 0.48836871183344294, "epoch": 510}
[10:32:21.949338] [10:32:21.949437] Training epoch 510 for 0:00:27
[10:32:21.949491] [10:32:21.953839] log_dir: ./exp/debug/cifar100-LT/debug
[10:32:23.620774] Epoch: [511]  [ 0/42]  eta: 0:01:09  lr: 0.000759  loss: 0.4582 (0.4582)  time: 1.6659  data: 1.1552  max mem: 9341
[10:32:33.793398] Epoch: [511]  [20/42]  eta: 0:00:12  lr: 0.000757  loss: 0.4860 (0.4875)  time: 0.5086  data: 0.0001  max mem: 9341
[10:32:43.947824] Epoch: [511]  [40/42]  eta: 0:00:01  lr: 0.000756  loss: 0.4879 (0.4883)  time: 0.5077  data: 0.0001  max mem: 9341
[10:32:44.453452] Epoch: [511]  [41/42]  eta: 0:00:00  lr: 0.000756  loss: 0.4874 (0.4877)  time: 0.5076  data: 0.0001  max mem: 9341
[10:32:44.625071] Epoch: [511] Total time: 0:00:22 (0.5398 s / it)
[10:32:44.626868] Averaged stats: lr: 0.000756  loss: 0.4874 (0.4856)
[10:32:49.124858] {"train_lr": 0.0007576673573318778, "train_loss": 0.48563242881071, "epoch": 511}
[10:32:49.125289] [10:32:49.125383] Training epoch 511 for 0:00:27
[10:32:49.125445] [10:32:49.130351] log_dir: ./exp/debug/cifar100-LT/debug
[10:32:50.559445] Epoch: [512]  [ 0/42]  eta: 0:00:59  lr: 0.000755  loss: 0.4968 (0.4968)  time: 1.4278  data: 0.9233  max mem: 9341
[10:33:00.721739] Epoch: [512]  [20/42]  eta: 0:00:12  lr: 0.000753  loss: 0.4904 (0.4959)  time: 0.5081  data: 0.0001  max mem: 9341
[10:33:10.874659] Epoch: [512]  [40/42]  eta: 0:00:01  lr: 0.000751  loss: 0.4929 (0.4950)  time: 0.5076  data: 0.0001  max mem: 9341
[10:33:11.381195] Epoch: [512]  [41/42]  eta: 0:00:00  lr: 0.000751  loss: 0.4901 (0.4940)  time: 0.5076  data: 0.0001  max mem: 9341
[10:33:11.548127] Epoch: [512] Total time: 0:00:22 (0.5338 s / it)
[10:33:11.549888] Averaged stats: lr: 0.000751  loss: 0.4901 (0.4919)
[10:33:16.104676] {"train_lr": 0.0007530600366592065, "train_loss": 0.4919361706290926, "epoch": 512}
[10:33:16.105106] [10:33:16.105199] Training epoch 512 for 0:00:26
[10:33:16.105251] [10:33:16.109770] log_dir: ./exp/debug/cifar100-LT/debug
[10:33:17.660370] Epoch: [513]  [ 0/42]  eta: 0:01:05  lr: 0.000750  loss: 0.4766 (0.4766)  time: 1.5494  data: 1.0298  max mem: 9341
[10:33:27.843224] Epoch: [513]  [20/42]  eta: 0:00:12  lr: 0.000748  loss: 0.4807 (0.4833)  time: 0.5091  data: 0.0001  max mem: 9341
[10:33:38.008925] Epoch: [513]  [40/42]  eta: 0:00:01  lr: 0.000746  loss: 0.4928 (0.4889)  time: 0.5082  data: 0.0001  max mem: 9341
[10:33:38.516358] Epoch: [513]  [41/42]  eta: 0:00:00  lr: 0.000746  loss: 0.4928 (0.4883)  time: 0.5083  data: 0.0001  max mem: 9341
[10:33:38.684365] Epoch: [513] Total time: 0:00:22 (0.5375 s / it)
[10:33:38.693973] Averaged stats: lr: 0.000746  loss: 0.4928 (0.4897)
[10:33:43.203343] {"train_lr": 0.0007484603529573587, "train_loss": 0.489678532772121, "epoch": 513}
[10:33:43.203697] [10:33:43.203783] Training epoch 513 for 0:00:27
[10:33:43.203833] [10:33:43.208657] log_dir: ./exp/debug/cifar100-LT/debug
[10:33:44.748060] Epoch: [514]  [ 0/42]  eta: 0:01:04  lr: 0.000745  loss: 0.4879 (0.4879)  time: 1.5381  data: 1.0290  max mem: 9341
[10:33:54.919194] Epoch: [514]  [20/42]  eta: 0:00:12  lr: 0.000744  loss: 0.4860 (0.4845)  time: 0.5085  data: 0.0001  max mem: 9341
[10:34:05.066428] Epoch: [514]  [40/42]  eta: 0:00:01  lr: 0.000742  loss: 0.4808 (0.4856)  time: 0.5073  data: 0.0001  max mem: 9341
[10:34:05.572825] Epoch: [514]  [41/42]  eta: 0:00:00  lr: 0.000742  loss: 0.4808 (0.4850)  time: 0.5073  data: 0.0001  max mem: 9341
[10:34:05.736526] Epoch: [514] Total time: 0:00:22 (0.5364 s / it)
[10:34:05.738875] Averaged stats: lr: 0.000742  loss: 0.4808 (0.4904)
[10:34:10.310036] {"train_lr": 0.0007438683848222378, "train_loss": 0.49043508760985877, "epoch": 514}
[10:34:10.310369] [10:34:10.310460] Training epoch 514 for 0:00:27
[10:34:10.310510] [10:34:10.314807] log_dir: ./exp/debug/cifar100-LT/debug
[10:34:11.781190] Epoch: [515]  [ 0/42]  eta: 0:01:01  lr: 0.000741  loss: 0.4946 (0.4946)  time: 1.4651  data: 0.9516  max mem: 9341
[10:34:21.941554] Epoch: [515]  [20/42]  eta: 0:00:12  lr: 0.000739  loss: 0.4884 (0.4922)  time: 0.5080  data: 0.0001  max mem: 9341
[10:34:32.088683] Epoch: [515]  [40/42]  eta: 0:00:01  lr: 0.000737  loss: 0.4770 (0.4870)  time: 0.5073  data: 0.0001  max mem: 9341
[10:34:32.594683] Epoch: [515]  [41/42]  eta: 0:00:00  lr: 0.000737  loss: 0.4774 (0.4872)  time: 0.5073  data: 0.0001  max mem: 9341
[10:34:32.769072] Epoch: [515] Total time: 0:00:22 (0.5346 s / it)
[10:34:32.772178] Averaged stats: lr: 0.000737  loss: 0.4774 (0.4850)
[10:34:37.329059] {"train_lr": 0.0007392842107179122, "train_loss": 0.48503139348966734, "epoch": 515}
[10:34:37.329400] [10:34:37.329497] Training epoch 515 for 0:00:27
[10:34:37.329550] [10:34:37.333939] log_dir: ./exp/debug/cifar100-LT/debug
[10:34:38.804178] Epoch: [516]  [ 0/42]  eta: 0:01:01  lr: 0.000736  loss: 0.4346 (0.4346)  time: 1.4689  data: 0.9698  max mem: 9341
[10:34:48.968964] Epoch: [516]  [20/42]  eta: 0:00:12  lr: 0.000734  loss: 0.4979 (0.4908)  time: 0.5082  data: 0.0002  max mem: 9341
[10:34:59.124128] Epoch: [516]  [40/42]  eta: 0:00:01  lr: 0.000733  loss: 0.4980 (0.4948)  time: 0.5077  data: 0.0001  max mem: 9341
[10:34:59.629624] Epoch: [516]  [41/42]  eta: 0:00:00  lr: 0.000733  loss: 0.4950 (0.4938)  time: 0.5076  data: 0.0001  max mem: 9341
[10:34:59.796205] Epoch: [516] Total time: 0:00:22 (0.5348 s / it)
[10:34:59.802659] Averaged stats: lr: 0.000733  loss: 0.4950 (0.4889)
[10:35:04.330485] {"train_lr": 0.0007347079089752657, "train_loss": 0.48885930826266605, "epoch": 516}
[10:35:04.330842] [10:35:04.330925] Training epoch 516 for 0:00:27
[10:35:04.330976] [10:35:04.335413] log_dir: ./exp/debug/cifar100-LT/debug
[10:35:05.805683] Epoch: [517]  [ 0/42]  eta: 0:01:01  lr: 0.000732  loss: 0.5027 (0.5027)  time: 1.4683  data: 0.9511  max mem: 9341
[10:35:15.970100] Epoch: [517]  [20/42]  eta: 0:00:12  lr: 0.000730  loss: 0.4697 (0.4784)  time: 0.5082  data: 0.0002  max mem: 9341
[10:35:26.115595] Epoch: [517]  [40/42]  eta: 0:00:01  lr: 0.000728  loss: 0.4834 (0.4821)  time: 0.5072  data: 0.0001  max mem: 9341
[10:35:26.622086] Epoch: [517]  [41/42]  eta: 0:00:00  lr: 0.000728  loss: 0.4834 (0.4826)  time: 0.5073  data: 0.0001  max mem: 9341
[10:35:26.791278] Epoch: [517] Total time: 0:00:22 (0.5347 s / it)
[10:35:26.803633] Averaged stats: lr: 0.000728  loss: 0.4834 (0.4881)
[10:35:31.384087] {"train_lr": 0.0007301395577906711, "train_loss": 0.48810448515273275, "epoch": 517}
[10:35:31.384442] [10:35:31.384519] Training epoch 517 for 0:00:27
[10:35:31.384569] [10:35:31.388844] log_dir: ./exp/debug/cifar100-LT/debug
[10:35:32.892340] Epoch: [518]  [ 0/42]  eta: 0:01:03  lr: 0.000727  loss: 0.4873 (0.4873)  time: 1.5021  data: 0.9974  max mem: 9341
[10:35:43.054875] Epoch: [518]  [20/42]  eta: 0:00:12  lr: 0.000725  loss: 0.4971 (0.4947)  time: 0.5081  data: 0.0001  max mem: 9341
[10:35:53.196015] Epoch: [518]  [40/42]  eta: 0:00:01  lr: 0.000724  loss: 0.4965 (0.4934)  time: 0.5070  data: 0.0001  max mem: 9341
[10:35:53.703178] Epoch: [518]  [41/42]  eta: 0:00:00  lr: 0.000724  loss: 0.4971 (0.4936)  time: 0.5071  data: 0.0001  max mem: 9341
[10:35:53.873715] Epoch: [518] Total time: 0:00:22 (0.5354 s / it)
[10:35:53.874571] Averaged stats: lr: 0.000724  loss: 0.4971 (0.4855)
[10:35:58.520531] {"train_lr": 0.0007255792352246451, "train_loss": 0.48554478195451556, "epoch": 518}
[10:35:58.520895] [10:35:58.520980] Training epoch 518 for 0:00:27
[10:35:58.521031] [10:35:58.525409] log_dir: ./exp/debug/cifar100-LT/debug
[10:36:00.086881] Epoch: [519]  [ 0/42]  eta: 0:01:05  lr: 0.000723  loss: 0.5033 (0.5033)  time: 1.5603  data: 1.0520  max mem: 9341
[10:36:10.253556] Epoch: [519]  [20/42]  eta: 0:00:12  lr: 0.000721  loss: 0.4953 (0.4860)  time: 0.5083  data: 0.0001  max mem: 9341
[10:36:20.396683] Epoch: [519]  [40/42]  eta: 0:00:01  lr: 0.000719  loss: 0.5059 (0.4942)  time: 0.5071  data: 0.0001  max mem: 9341
[10:36:20.901431] Epoch: [519]  [41/42]  eta: 0:00:00  lr: 0.000719  loss: 0.4944 (0.4934)  time: 0.5071  data: 0.0001  max mem: 9341
[10:36:21.071896] Epoch: [519] Total time: 0:00:22 (0.5368 s / it)
[10:36:21.074257] Averaged stats: lr: 0.000719  loss: 0.4944 (0.4885)
[10:36:25.655714] {"train_lr": 0.000721027019200517, "train_loss": 0.48845489890802474, "epoch": 519}
[10:36:25.656142] [10:36:25.656232] Training epoch 519 for 0:00:27
[10:36:25.656287] [10:36:25.660758] log_dir: ./exp/debug/cifar100-LT/debug
[10:36:27.226729] Epoch: [520]  [ 0/42]  eta: 0:01:05  lr: 0.000718  loss: 0.5001 (0.5001)  time: 1.5647  data: 1.0588  max mem: 9341
[10:36:37.392045] Epoch: [520]  [20/42]  eta: 0:00:12  lr: 0.000716  loss: 0.4821 (0.4779)  time: 0.5082  data: 0.0001  max mem: 9341
[10:36:47.551727] Epoch: [520]  [40/42]  eta: 0:00:01  lr: 0.000715  loss: 0.4925 (0.4819)  time: 0.5079  data: 0.0001  max mem: 9341
[10:36:48.057310] Epoch: [520]  [41/42]  eta: 0:00:00  lr: 0.000715  loss: 0.4925 (0.4819)  time: 0.5079  data: 0.0001  max mem: 9341
[10:36:48.233724] Epoch: [520] Total time: 0:00:22 (0.5374 s / it)
[10:36:48.234669] Averaged stats: lr: 0.000715  loss: 0.4925 (0.4864)
[10:36:52.860131] {"train_lr": 0.0007164829875030999, "train_loss": 0.4863767758721397, "epoch": 520}
[10:36:52.860529] [10:36:52.860622] Training epoch 520 for 0:00:27
[10:36:52.860674] [10:36:52.865245] log_dir: ./exp/debug/cifar100-LT/debug
[10:36:54.437144] Epoch: [521]  [ 0/42]  eta: 0:01:05  lr: 0.000713  loss: 0.5167 (0.5167)  time: 1.5707  data: 1.0612  max mem: 9341
[10:37:04.621846] Epoch: [521]  [20/42]  eta: 0:00:12  lr: 0.000712  loss: 0.4857 (0.4876)  time: 0.5092  data: 0.0001  max mem: 9341
[10:37:14.794242] Epoch: [521]  [40/42]  eta: 0:00:01  lr: 0.000710  loss: 0.4852 (0.4865)  time: 0.5085  data: 0.0001  max mem: 9341
[10:37:15.298147] Epoch: [521]  [41/42]  eta: 0:00:00  lr: 0.000710  loss: 0.4852 (0.4863)  time: 0.5085  data: 0.0001  max mem: 9341
[10:37:15.481529] Epoch: [521] Total time: 0:00:22 (0.5385 s / it)
[10:37:15.482275] Averaged stats: lr: 0.000710  loss: 0.4852 (0.4849)
[10:37:20.055968] {"train_lr": 0.000711947217777357, "train_loss": 0.4848600647279194, "epoch": 521}
[10:37:20.056367] [10:37:20.056484] Training epoch 521 for 0:00:27
[10:37:20.056553] [10:37:20.061208] log_dir: ./exp/debug/cifar100-LT/debug
[10:37:21.737813] Epoch: [522]  [ 0/42]  eta: 0:01:10  lr: 0.000709  loss: 0.4786 (0.4786)  time: 1.6755  data: 1.1723  max mem: 9341
[10:37:31.907574] Epoch: [522]  [20/42]  eta: 0:00:12  lr: 0.000707  loss: 0.4818 (0.4923)  time: 0.5084  data: 0.0001  max mem: 9341
[10:37:42.061282] Epoch: [522]  [40/42]  eta: 0:00:01  lr: 0.000705  loss: 0.4953 (0.4947)  time: 0.5076  data: 0.0001  max mem: 9341
[10:37:42.568355] Epoch: [522]  [41/42]  eta: 0:00:00  lr: 0.000705  loss: 0.4953 (0.4948)  time: 0.5076  data: 0.0001  max mem: 9341
[10:37:42.751225] Epoch: [522] Total time: 0:00:22 (0.5402 s / it)
[10:37:42.751943] Averaged stats: lr: 0.000705  loss: 0.4953 (0.4857)
[10:37:47.303444] {"train_lr": 0.0007074197875270786, "train_loss": 0.4857361117998759, "epoch": 522}
[10:37:47.303862] [10:37:47.303953] Training epoch 522 for 0:00:27
[10:37:47.304004] [10:37:47.308576] log_dir: ./exp/debug/cifar100-LT/debug
[10:37:48.902185] Epoch: [523]  [ 0/42]  eta: 0:01:06  lr: 0.000704  loss: 0.4899 (0.4899)  time: 1.5921  data: 1.0792  max mem: 9341
[10:37:59.061298] Epoch: [523]  [20/42]  eta: 0:00:12  lr: 0.000703  loss: 0.4817 (0.4851)  time: 0.5079  data: 0.0001  max mem: 9341
[10:38:09.210216] Epoch: [523]  [40/42]  eta: 0:00:01  lr: 0.000701  loss: 0.4854 (0.4872)  time: 0.5074  data: 0.0001  max mem: 9341
[10:38:09.714547] Epoch: [523]  [41/42]  eta: 0:00:00  lr: 0.000701  loss: 0.4854 (0.4876)  time: 0.5073  data: 0.0001  max mem: 9341
[10:38:09.883608] Epoch: [523] Total time: 0:00:22 (0.5375 s / it)
[10:38:09.888092] Averaged stats: lr: 0.000701  loss: 0.4854 (0.4859)
[10:38:14.409773] {"train_lr": 0.0007029007741135565, "train_loss": 0.4858802665202391, "epoch": 523}
[10:38:14.410170] [10:38:14.410273] Training epoch 523 for 0:00:27
[10:38:14.410324] [10:38:14.414674] log_dir: ./exp/debug/cifar100-LT/debug
[10:38:16.155687] Epoch: [524]  [ 0/42]  eta: 0:01:13  lr: 0.000700  loss: 0.4775 (0.4775)  time: 1.7399  data: 1.2355  max mem: 9341
[10:38:26.352006] Epoch: [524]  [20/42]  eta: 0:00:12  lr: 0.000698  loss: 0.4751 (0.4749)  time: 0.5098  data: 0.0001  max mem: 9341
[10:38:36.518315] Epoch: [524]  [40/42]  eta: 0:00:01  lr: 0.000696  loss: 0.4869 (0.4785)  time: 0.5083  data: 0.0001  max mem: 9341
[10:38:37.024414] Epoch: [524]  [41/42]  eta: 0:00:00  lr: 0.000696  loss: 0.4869 (0.4788)  time: 0.5083  data: 0.0001  max mem: 9341
[10:38:37.176060] Epoch: [524] Total time: 0:00:22 (0.5419 s / it)
[10:38:37.199401] Averaged stats: lr: 0.000696  loss: 0.4869 (0.4842)
[10:38:41.757564] {"train_lr": 0.000698390254754262, "train_loss": 0.48415833551968845, "epoch": 524}
[10:38:41.757877] [10:38:41.757961] Training epoch 524 for 0:00:27
[10:38:41.758011] [10:38:41.762146] log_dir: ./exp/debug/cifar100-LT/debug
[10:38:43.181115] Epoch: [525]  [ 0/42]  eta: 0:00:59  lr: 0.000695  loss: 0.4794 (0.4794)  time: 1.4176  data: 0.9042  max mem: 9341
[10:38:53.373178] Epoch: [525]  [20/42]  eta: 0:00:12  lr: 0.000694  loss: 0.4998 (0.4942)  time: 0.5095  data: 0.0001  max mem: 9341
[10:39:03.542935] Epoch: [525]  [40/42]  eta: 0:00:01  lr: 0.000692  loss: 0.4811 (0.4885)  time: 0.5084  data: 0.0001  max mem: 9341
[10:39:04.048202] Epoch: [525]  [41/42]  eta: 0:00:00  lr: 0.000692  loss: 0.4811 (0.4871)  time: 0.5084  data: 0.0001  max mem: 9341
[10:39:04.201066] Epoch: [525] Total time: 0:00:22 (0.5343 s / it)
[10:39:04.225688] Averaged stats: lr: 0.000692  loss: 0.4811 (0.4865)
[10:39:08.690668] {"train_lr": 0.000693888306521525, "train_loss": 0.4865237293498857, "epoch": 525}
[10:39:08.691041] [10:39:08.691122] Training epoch 525 for 0:00:26
[10:39:08.691172] [10:39:08.695479] log_dir: ./exp/debug/cifar100-LT/debug
[10:39:10.381255] Epoch: [526]  [ 0/42]  eta: 0:01:10  lr: 0.000691  loss: 0.5354 (0.5354)  time: 1.6847  data: 1.1836  max mem: 9341
[10:39:20.547610] Epoch: [526]  [20/42]  eta: 0:00:12  lr: 0.000689  loss: 0.4748 (0.4876)  time: 0.5083  data: 0.0002  max mem: 9341
[10:39:30.695302] Epoch: [526]  [40/42]  eta: 0:00:01  lr: 0.000687  loss: 0.4857 (0.4856)  time: 0.5073  data: 0.0001  max mem: 9341
[10:39:31.201403] Epoch: [526]  [41/42]  eta: 0:00:00  lr: 0.000687  loss: 0.4857 (0.4861)  time: 0.5073  data: 0.0001  max mem: 9341
[10:39:31.356478] Epoch: [526] Total time: 0:00:22 (0.5395 s / it)
[10:39:31.366151] Averaged stats: lr: 0.000687  loss: 0.4857 (0.4872)
[10:39:35.920153] {"train_lr": 0.0006893950063412195, "train_loss": 0.4872276195812793, "epoch": 526}
[10:39:35.920600] [10:39:35.920684] Training epoch 526 for 0:00:27
[10:39:35.920736] [10:39:35.925143] log_dir: ./exp/debug/cifar100-LT/debug
[10:39:37.555197] Epoch: [527]  [ 0/42]  eta: 0:01:08  lr: 0.000686  loss: 0.5330 (0.5330)  time: 1.6289  data: 1.1228  max mem: 9341
[10:39:47.712158] Epoch: [527]  [20/42]  eta: 0:00:12  lr: 0.000685  loss: 0.4960 (0.4983)  time: 0.5078  data: 0.0001  max mem: 9341
[10:39:57.849894] Epoch: [527]  [40/42]  eta: 0:00:01  lr: 0.000683  loss: 0.4839 (0.4952)  time: 0.5068  data: 0.0001  max mem: 9341
[10:39:58.353964] Epoch: [527]  [41/42]  eta: 0:00:00  lr: 0.000683  loss: 0.4839 (0.4946)  time: 0.5068  data: 0.0001  max mem: 9341
[10:39:58.513317] Epoch: [527] Total time: 0:00:22 (0.5378 s / it)
[10:39:58.520399] Averaged stats: lr: 0.000683  loss: 0.4839 (0.4921)
[10:40:03.091259] {"train_lr": 0.0006849104309914493, "train_loss": 0.49206696237836567, "epoch": 527}
[10:40:03.091666] [10:40:03.091756] Training epoch 527 for 0:00:27
[10:40:03.091808] [10:40:03.096509] log_dir: ./exp/debug/cifar100-LT/debug
[10:40:04.595132] Epoch: [528]  [ 0/42]  eta: 0:01:02  lr: 0.000682  loss: 0.4671 (0.4671)  time: 1.4974  data: 0.9976  max mem: 9341
[10:40:14.773075] Epoch: [528]  [20/42]  eta: 0:00:12  lr: 0.000680  loss: 0.4862 (0.4903)  time: 0.5088  data: 0.0001  max mem: 9341
[10:40:24.921974] Epoch: [528]  [40/42]  eta: 0:00:01  lr: 0.000678  loss: 0.4845 (0.4885)  time: 0.5074  data: 0.0001  max mem: 9341
[10:40:25.428483] Epoch: [528]  [41/42]  eta: 0:00:00  lr: 0.000678  loss: 0.4845 (0.4881)  time: 0.5075  data: 0.0001  max mem: 9341
[10:40:25.603725] Epoch: [528] Total time: 0:00:22 (0.5359 s / it)
[10:40:25.605670] Averaged stats: lr: 0.000678  loss: 0.4845 (0.4869)
[10:40:30.125702] {"train_lr": 0.0006804346571012329, "train_loss": 0.48691765946291743, "epoch": 528}
[10:40:30.126055] [10:40:30.126137] Training epoch 528 for 0:00:27
[10:40:30.126187] [10:40:30.130543] log_dir: ./exp/debug/cifar100-LT/debug
[10:40:31.626209] Epoch: [529]  [ 0/42]  eta: 0:01:02  lr: 0.000677  loss: 0.4792 (0.4792)  time: 1.4944  data: 0.9747  max mem: 9341
[10:40:41.797866] Epoch: [529]  [20/42]  eta: 0:00:12  lr: 0.000676  loss: 0.4813 (0.4882)  time: 0.5085  data: 0.0001  max mem: 9341
[10:40:51.945538] Epoch: [529]  [40/42]  eta: 0:00:01  lr: 0.000674  loss: 0.4849 (0.4883)  time: 0.5073  data: 0.0001  max mem: 9341
[10:40:52.451369] Epoch: [529]  [41/42]  eta: 0:00:00  lr: 0.000674  loss: 0.4849 (0.4879)  time: 0.5073  data: 0.0001  max mem: 9341
[10:40:52.611994] Epoch: [529] Total time: 0:00:22 (0.5353 s / it)
[10:40:52.616158] Averaged stats: lr: 0.000674  loss: 0.4849 (0.4879)
[10:40:57.229674] {"train_lr": 0.0006759677611491965, "train_loss": 0.4879182064462276, "epoch": 529}
[10:40:57.230020] [10:40:57.230100] Training epoch 529 for 0:00:27
[10:40:57.230149] [10:40:57.234478] log_dir: ./exp/debug/cifar100-LT/debug
[10:40:58.688198] Epoch: [530]  [ 0/42]  eta: 0:01:01  lr: 0.000673  loss: 0.5000 (0.5000)  time: 1.4525  data: 0.9451  max mem: 9341
[10:41:08.920668] Epoch: [530]  [20/42]  eta: 0:00:12  lr: 0.000671  loss: 0.4763 (0.4817)  time: 0.5116  data: 0.0001  max mem: 9341
[10:41:19.072361] Epoch: [530]  [40/42]  eta: 0:00:01  lr: 0.000670  loss: 0.4933 (0.4852)  time: 0.5075  data: 0.0001  max mem: 9341
[10:41:19.578490] Epoch: [530]  [41/42]  eta: 0:00:00  lr: 0.000670  loss: 0.4933 (0.4850)  time: 0.5075  data: 0.0001  max mem: 9341
[10:41:19.748173] Epoch: [530] Total time: 0:00:22 (0.5360 s / it)
[10:41:19.749632] Averaged stats: lr: 0.000670  loss: 0.4933 (0.4839)
[10:41:24.306425] {"train_lr": 0.000671509819462268, "train_loss": 0.4839276211957137, "epoch": 530}
[10:41:24.306762] [10:41:24.306849] Training epoch 530 for 0:00:27
[10:41:24.306900] [10:41:24.311212] log_dir: ./exp/debug/cifar100-LT/debug
[10:41:25.780863] Epoch: [531]  [ 0/42]  eta: 0:01:01  lr: 0.000669  loss: 0.4891 (0.4891)  time: 1.4680  data: 0.9646  max mem: 9341
[10:41:35.943910] Epoch: [531]  [20/42]  eta: 0:00:12  lr: 0.000667  loss: 0.4756 (0.4772)  time: 0.5081  data: 0.0001  max mem: 9341
[10:41:46.092403] Epoch: [531]  [40/42]  eta: 0:00:01  lr: 0.000665  loss: 0.4847 (0.4791)  time: 0.5074  data: 0.0001  max mem: 9341
[10:41:46.599330] Epoch: [531]  [41/42]  eta: 0:00:00  lr: 0.000665  loss: 0.4808 (0.4790)  time: 0.5075  data: 0.0001  max mem: 9341
[10:41:46.773046] Epoch: [531] Total time: 0:00:22 (0.5348 s / it)
[10:41:46.774068] Averaged stats: lr: 0.000665  loss: 0.4808 (0.4835)
[10:41:51.405100] {"train_lr": 0.0006670609082143712, "train_loss": 0.48351932281539556, "epoch": 531}
[10:41:51.405415] [10:41:51.405498] Training epoch 531 for 0:00:27
[10:41:51.405550] [10:41:51.409855] log_dir: ./exp/debug/cifar100-LT/debug
[10:41:52.865387] Epoch: [532]  [ 0/42]  eta: 0:01:01  lr: 0.000664  loss: 0.4896 (0.4896)  time: 1.4542  data: 0.9460  max mem: 9341
[10:42:03.030329] Epoch: [532]  [20/42]  eta: 0:00:12  lr: 0.000662  loss: 0.4746 (0.4842)  time: 0.5082  data: 0.0001  max mem: 9341
[10:42:13.177944] Epoch: [532]  [40/42]  eta: 0:00:01  lr: 0.000661  loss: 0.4788 (0.4852)  time: 0.5073  data: 0.0001  max mem: 9341
[10:42:13.683697] Epoch: [532]  [41/42]  eta: 0:00:00  lr: 0.000661  loss: 0.4783 (0.4848)  time: 0.5073  data: 0.0001  max mem: 9341
[10:42:13.842297] Epoch: [532] Total time: 0:00:22 (0.5341 s / it)
[10:42:13.843000] Averaged stats: lr: 0.000661  loss: 0.4783 (0.4845)
[10:42:18.479770] {"train_lr": 0.000662621103425124, "train_loss": 0.48446505569985937, "epoch": 532}
[10:42:18.480181] [10:42:18.480268] Training epoch 532 for 0:00:27
[10:42:18.480323] [10:42:18.484980] log_dir: ./exp/debug/cifar100-LT/debug
[10:42:20.058396] Epoch: [533]  [ 0/42]  eta: 0:01:06  lr: 0.000660  loss: 0.4590 (0.4590)  time: 1.5722  data: 1.0607  max mem: 9341
[10:42:30.315560] Epoch: [533]  [20/42]  eta: 0:00:12  lr: 0.000658  loss: 0.4670 (0.4695)  time: 0.5128  data: 0.0001  max mem: 9341
[10:42:40.495003] Epoch: [533]  [40/42]  eta: 0:00:01  lr: 0.000656  loss: 0.4875 (0.4769)  time: 0.5089  data: 0.0001  max mem: 9341
[10:42:41.000920] Epoch: [533]  [41/42]  eta: 0:00:00  lr: 0.000656  loss: 0.4909 (0.4774)  time: 0.5088  data: 0.0001  max mem: 9341
[10:42:41.162759] Epoch: [533] Total time: 0:00:22 (0.5399 s / it)
[10:42:41.172725] Averaged stats: lr: 0.000656  loss: 0.4909 (0.4844)
[10:42:45.666422] {"train_lr": 0.0006581904809585408, "train_loss": 0.48437336690369104, "epoch": 533}
[10:42:45.666791] [10:42:45.666871] Training epoch 533 for 0:00:27
[10:42:45.666922] [10:42:45.671321] log_dir: ./exp/debug/cifar100-LT/debug
[10:42:47.208352] Epoch: [534]  [ 0/42]  eta: 0:01:04  lr: 0.000655  loss: 0.5068 (0.5068)  time: 1.5359  data: 1.0290  max mem: 9341
[10:42:57.393067] Epoch: [534]  [20/42]  eta: 0:00:12  lr: 0.000654  loss: 0.4760 (0.4819)  time: 0.5092  data: 0.0001  max mem: 9341
[10:43:07.544727] Epoch: [534]  [40/42]  eta: 0:00:01  lr: 0.000652  loss: 0.4864 (0.4847)  time: 0.5075  data: 0.0001  max mem: 9341
[10:43:08.051136] Epoch: [534]  [41/42]  eta: 0:00:00  lr: 0.000652  loss: 0.4864 (0.4843)  time: 0.5075  data: 0.0001  max mem: 9341
[10:43:08.219194] Epoch: [534] Total time: 0:00:22 (0.5369 s / it)
[10:43:08.220053] Averaged stats: lr: 0.000652  loss: 0.4864 (0.4841)
[10:43:12.886854] {"train_lr": 0.0006537691165217337, "train_loss": 0.4841327072963828, "epoch": 534}
[10:43:12.887317] [10:43:12.887409] Training epoch 534 for 0:00:27
[10:43:12.887460] [10:43:12.892692] log_dir: ./exp/debug/cifar100-LT/debug
[10:43:14.369069] Epoch: [535]  [ 0/42]  eta: 0:01:01  lr: 0.000651  loss: 0.5162 (0.5162)  time: 1.4749  data: 0.9765  max mem: 9341
[10:43:24.544715] Epoch: [535]  [20/42]  eta: 0:00:12  lr: 0.000649  loss: 0.4653 (0.4833)  time: 0.5087  data: 0.0002  max mem: 9341
[10:43:34.694619] Epoch: [535]  [40/42]  eta: 0:00:01  lr: 0.000647  loss: 0.4820 (0.4811)  time: 0.5074  data: 0.0001  max mem: 9341
[10:43:35.201836] Epoch: [535]  [41/42]  eta: 0:00:00  lr: 0.000647  loss: 0.4820 (0.4811)  time: 0.5075  data: 0.0001  max mem: 9341
[10:43:35.381782] Epoch: [535] Total time: 0:00:22 (0.5355 s / it)
[10:43:35.398338] Averaged stats: lr: 0.000647  loss: 0.4820 (0.4835)
[10:43:40.087810] {"train_lr": 0.0006493570856636229, "train_loss": 0.48352865661893574, "epoch": 535}
[10:43:40.088251] [10:43:40.088341] Training epoch 535 for 0:00:27
[10:43:40.088413] [10:43:40.092905] log_dir: ./exp/debug/cifar100-LT/debug
[10:43:41.798663] Epoch: [536]  [ 0/42]  eta: 0:01:11  lr: 0.000646  loss: 0.4808 (0.4808)  time: 1.7045  data: 1.2009  max mem: 9341
[10:43:51.964225] Epoch: [536]  [20/42]  eta: 0:00:12  lr: 0.000645  loss: 0.4837 (0.4829)  time: 0.5082  data: 0.0001  max mem: 9341
[10:44:02.113582] Epoch: [536]  [40/42]  eta: 0:00:01  lr: 0.000643  loss: 0.4731 (0.4816)  time: 0.5074  data: 0.0001  max mem: 9341
[10:44:02.619201] Epoch: [536]  [41/42]  eta: 0:00:00  lr: 0.000643  loss: 0.4731 (0.4823)  time: 0.5075  data: 0.0001  max mem: 9341
[10:44:02.793357] Epoch: [536] Total time: 0:00:22 (0.5405 s / it)
[10:44:02.794064] Averaged stats: lr: 0.000643  loss: 0.4731 (0.4832)
[10:44:07.310497] {"train_lr": 0.0006449544637736439, "train_loss": 0.4832301883115655, "epoch": 536}
[10:44:07.310853] [10:44:07.310935] Training epoch 536 for 0:00:27
[10:44:07.310986] [10:44:07.315257] log_dir: ./exp/debug/cifar100-LT/debug
[10:44:09.039224] Epoch: [537]  [ 0/42]  eta: 0:01:12  lr: 0.000642  loss: 0.4716 (0.4716)  time: 1.7229  data: 1.2300  max mem: 9341
[10:44:19.201950] Epoch: [537]  [20/42]  eta: 0:00:12  lr: 0.000640  loss: 0.4798 (0.4816)  time: 0.5081  data: 0.0001  max mem: 9341
[10:44:29.351257] Epoch: [537]  [40/42]  eta: 0:00:01  lr: 0.000639  loss: 0.4785 (0.4812)  time: 0.5074  data: 0.0001  max mem: 9341
[10:44:29.857500] Epoch: [537]  [41/42]  eta: 0:00:00  lr: 0.000639  loss: 0.4785 (0.4808)  time: 0.5074  data: 0.0001  max mem: 9341
[10:44:30.019997] Epoch: [537] Total time: 0:00:22 (0.5406 s / it)
[10:44:30.027837] Averaged stats: lr: 0.000639  loss: 0.4785 (0.4831)
[10:44:34.632840] {"train_lr": 0.0006405613260804551, "train_loss": 0.4831052314312685, "epoch": 537}
[10:44:34.633198] [10:44:34.633278] Training epoch 537 for 0:00:27
[10:44:34.633328] [10:44:34.637732] log_dir: ./exp/debug/cifar100-LT/debug
[10:44:36.095461] Epoch: [538]  [ 0/42]  eta: 0:01:01  lr: 0.000638  loss: 0.5036 (0.5036)  time: 1.4564  data: 0.9465  max mem: 9341
[10:44:46.254736] Epoch: [538]  [20/42]  eta: 0:00:12  lr: 0.000636  loss: 0.4970 (0.4972)  time: 0.5079  data: 0.0001  max mem: 9341
[10:44:56.402393] Epoch: [538]  [40/42]  eta: 0:00:01  lr: 0.000634  loss: 0.4853 (0.4918)  time: 0.5073  data: 0.0001  max mem: 9341
[10:44:56.908370] Epoch: [538]  [41/42]  eta: 0:00:00  lr: 0.000634  loss: 0.4880 (0.4921)  time: 0.5074  data: 0.0001  max mem: 9341
[10:44:57.077380] Epoch: [538] Total time: 0:00:22 (0.5343 s / it)
[10:44:57.082124] Averaged stats: lr: 0.000634  loss: 0.4880 (0.4845)
[10:45:01.721181] {"train_lr": 0.0006361777476506606, "train_loss": 0.48447647495638757, "epoch": 538}
[10:45:01.721524] [10:45:01.721604] Training epoch 538 for 0:00:27
[10:45:01.721653] [10:45:01.725879] log_dir: ./exp/debug/cifar100-LT/debug
[10:45:03.346048] Epoch: [539]  [ 0/42]  eta: 0:01:07  lr: 0.000633  loss: 0.4562 (0.4562)  time: 1.6189  data: 1.1049  max mem: 9341
[10:45:13.503173] Epoch: [539]  [20/42]  eta: 0:00:12  lr: 0.000632  loss: 0.4851 (0.4869)  time: 0.5078  data: 0.0001  max mem: 9341
[10:45:23.647103] Epoch: [539]  [40/42]  eta: 0:00:01  lr: 0.000630  loss: 0.4953 (0.4909)  time: 0.5072  data: 0.0001  max mem: 9341
[10:45:24.152602] Epoch: [539]  [41/42]  eta: 0:00:00  lr: 0.000630  loss: 0.4953 (0.4915)  time: 0.5071  data: 0.0001  max mem: 9341
[10:45:24.319724] Epoch: [539] Total time: 0:00:22 (0.5379 s / it)
[10:45:24.320794] Averaged stats: lr: 0.000630  loss: 0.4953 (0.4853)
[10:45:28.800501] {"train_lr": 0.0006318038033875208, "train_loss": 0.4852687544411137, "epoch": 539}
[10:45:28.800896] [10:45:28.800995] Training epoch 539 for 0:00:27
[10:45:28.801046] [10:45:28.805286] log_dir: ./exp/debug/cifar100-LT/debug
[10:45:30.417180] Epoch: [540]  [ 0/42]  eta: 0:01:07  lr: 0.000629  loss: 0.4634 (0.4634)  time: 1.6104  data: 1.1096  max mem: 9341
[10:45:40.600664] Epoch: [540]  [20/42]  eta: 0:00:12  lr: 0.000627  loss: 0.4737 (0.4725)  time: 0.5091  data: 0.0001  max mem: 9341
[10:45:50.767920] Epoch: [540]  [40/42]  eta: 0:00:01  lr: 0.000626  loss: 0.4784 (0.4758)  time: 0.5083  data: 0.0001  max mem: 9341
[10:45:51.273526] Epoch: [540]  [41/42]  eta: 0:00:00  lr: 0.000626  loss: 0.4784 (0.4763)  time: 0.5082  data: 0.0001  max mem: 9341
[10:45:51.432227] Epoch: [540] Total time: 0:00:22 (0.5387 s / it)
[10:45:51.438573] Averaged stats: lr: 0.000626  loss: 0.4784 (0.4815)
[10:45:55.966414] {"train_lr": 0.0006274395680296731, "train_loss": 0.48154892151554424, "epoch": 540}
[10:45:55.966798] [10:45:55.966882] Training epoch 540 for 0:00:27
[10:45:55.966935] [10:45:55.971304] log_dir: ./exp/debug/cifar100-LT/debug
[10:45:57.487515] Epoch: [541]  [ 0/42]  eta: 0:01:03  lr: 0.000625  loss: 0.4975 (0.4975)  time: 1.5149  data: 1.0022  max mem: 9341
[10:46:07.648541] Epoch: [541]  [20/42]  eta: 0:00:12  lr: 0.000623  loss: 0.4973 (0.4953)  time: 0.5080  data: 0.0001  max mem: 9341
[10:46:17.793187] Epoch: [541]  [40/42]  eta: 0:00:01  lr: 0.000621  loss: 0.4793 (0.4866)  time: 0.5072  data: 0.0001  max mem: 9341
[10:46:18.298908] Epoch: [541]  [41/42]  eta: 0:00:00  lr: 0.000621  loss: 0.4795 (0.4865)  time: 0.5072  data: 0.0001  max mem: 9341
[10:46:18.463713] Epoch: [541] Total time: 0:00:22 (0.5355 s / it)
[10:46:18.469982] Averaged stats: lr: 0.000621  loss: 0.4795 (0.4844)
[10:46:23.106985] {"train_lr": 0.00062308511614986, "train_loss": 0.484429944306612, "epoch": 541}
[10:46:23.107324] [10:46:23.107403] Training epoch 541 for 0:00:27
[10:46:23.107454] [10:46:23.111813] log_dir: ./exp/debug/cifar100-LT/debug
[10:46:24.805978] Epoch: [542]  [ 0/42]  eta: 0:01:11  lr: 0.000620  loss: 0.4925 (0.4925)  time: 1.6931  data: 1.1906  max mem: 9341
[10:46:34.984025] Epoch: [542]  [20/42]  eta: 0:00:12  lr: 0.000619  loss: 0.4919 (0.4909)  time: 0.5088  data: 0.0001  max mem: 9341
[10:46:45.139601] Epoch: [542]  [40/42]  eta: 0:00:01  lr: 0.000617  loss: 0.4840 (0.4887)  time: 0.5077  data: 0.0001  max mem: 9341
[10:46:45.645520] Epoch: [542]  [41/42]  eta: 0:00:00  lr: 0.000617  loss: 0.4840 (0.4895)  time: 0.5078  data: 0.0001  max mem: 9341
[10:46:45.824167] Epoch: [542] Total time: 0:00:22 (0.5408 s / it)
[10:46:45.837217] Averaged stats: lr: 0.000617  loss: 0.4840 (0.4827)
[10:46:50.496994] {"train_lr": 0.0006187405221536506, "train_loss": 0.48274007847621325, "epoch": 542}
[10:46:50.497331] [10:46:50.497415] Training epoch 542 for 0:00:27
[10:46:50.497467] [10:46:50.501955] log_dir: ./exp/debug/cifar100-LT/debug
[10:46:52.142141] Epoch: [543]  [ 0/42]  eta: 0:01:08  lr: 0.000616  loss: 0.4913 (0.4913)  time: 1.6391  data: 1.1255  max mem: 9341
[10:47:02.330169] Epoch: [543]  [20/42]  eta: 0:00:12  lr: 0.000614  loss: 0.4759 (0.4716)  time: 0.5093  data: 0.0001  max mem: 9341
[10:47:12.496252] Epoch: [543]  [40/42]  eta: 0:00:01  lr: 0.000613  loss: 0.4850 (0.4764)  time: 0.5083  data: 0.0001  max mem: 9341
[10:47:13.002312] Epoch: [543]  [41/42]  eta: 0:00:00  lr: 0.000613  loss: 0.4848 (0.4765)  time: 0.5083  data: 0.0001  max mem: 9341
[10:47:13.172179] Epoch: [543] Total time: 0:00:22 (0.5398 s / it)
[10:47:13.180328] Averaged stats: lr: 0.000613  loss: 0.4848 (0.4813)
[10:47:17.750702] {"train_lr": 0.0006144058602781678, "train_loss": 0.4813246780208179, "epoch": 543}
[10:47:17.751056] [10:47:17.751137] Training epoch 543 for 0:00:27
[10:47:17.751189] [10:47:17.755548] log_dir: ./exp/debug/cifar100-LT/debug
[10:47:19.479666] Epoch: [544]  [ 0/42]  eta: 0:01:12  lr: 0.000611  loss: 0.4417 (0.4417)  time: 1.7231  data: 1.2188  max mem: 9341
[10:47:29.652338] Epoch: [544]  [20/42]  eta: 0:00:12  lr: 0.000610  loss: 0.4750 (0.4783)  time: 0.5086  data: 0.0001  max mem: 9341
[10:47:39.800385] Epoch: [544]  [40/42]  eta: 0:00:01  lr: 0.000608  loss: 0.4853 (0.4803)  time: 0.5073  data: 0.0001  max mem: 9341
[10:47:40.305872] Epoch: [544]  [41/42]  eta: 0:00:00  lr: 0.000608  loss: 0.4832 (0.4795)  time: 0.5073  data: 0.0001  max mem: 9341
[10:47:40.454699] Epoch: [544] Total time: 0:00:22 (0.5405 s / it)
[10:47:40.474131] Averaged stats: lr: 0.000608  loss: 0.4832 (0.4838)
[10:47:45.089282] {"train_lr": 0.0006100812045908224, "train_loss": 0.48381264525510015, "epoch": 544}
[10:47:45.089698] [10:47:45.089803] Training epoch 544 for 0:00:27
[10:47:45.089855] [10:47:45.094380] log_dir: ./exp/debug/cifar100-LT/debug
[10:47:46.829884] Epoch: [545]  [ 0/42]  eta: 0:01:12  lr: 0.000607  loss: 0.4396 (0.4396)  time: 1.7335  data: 1.2286  max mem: 9341
[10:47:56.995404] Epoch: [545]  [20/42]  eta: 0:00:12  lr: 0.000606  loss: 0.4827 (0.4814)  time: 0.5082  data: 0.0001  max mem: 9341
[10:48:07.149922] Epoch: [545]  [40/42]  eta: 0:00:01  lr: 0.000604  loss: 0.4834 (0.4811)  time: 0.5077  data: 0.0001  max mem: 9341
[10:48:07.655189] Epoch: [545]  [41/42]  eta: 0:00:00  lr: 0.000604  loss: 0.4864 (0.4813)  time: 0.5076  data: 0.0001  max mem: 9341
[10:48:07.824341] Epoch: [545] Total time: 0:00:22 (0.5412 s / it)
[10:48:07.828583] Averaged stats: lr: 0.000604  loss: 0.4864 (0.4848)
[10:48:12.406998] {"train_lr": 0.0006057666289880502, "train_loss": 0.4847895293718293, "epoch": 545}
[10:48:12.407361] [10:48:12.407445] Training epoch 545 for 0:00:27
[10:48:12.407496] [10:48:12.411960] log_dir: ./exp/debug/cifar100-LT/debug
[10:48:14.065405] Epoch: [546]  [ 0/42]  eta: 0:01:09  lr: 0.000603  loss: 0.4487 (0.4487)  time: 1.6525  data: 1.1532  max mem: 9341
[10:48:24.250613] Epoch: [546]  [20/42]  eta: 0:00:12  lr: 0.000601  loss: 0.4774 (0.4778)  time: 0.5092  data: 0.0001  max mem: 9341
[10:48:34.421080] Epoch: [546]  [40/42]  eta: 0:00:01  lr: 0.000600  loss: 0.4849 (0.4804)  time: 0.5085  data: 0.0001  max mem: 9341
[10:48:34.927475] Epoch: [546]  [41/42]  eta: 0:00:00  lr: 0.000600  loss: 0.4849 (0.4816)  time: 0.5085  data: 0.0001  max mem: 9341
[10:48:35.083393] Epoch: [546] Total time: 0:00:22 (0.5398 s / it)
[10:48:35.102199] Averaged stats: lr: 0.000600  loss: 0.4849 (0.4802)
[10:48:39.673406] {"train_lr": 0.0006014622071940427, "train_loss": 0.4801549540743941, "epoch": 546}
[10:48:39.673756] [10:48:39.673841] Training epoch 546 for 0:00:27
[10:48:39.673893] [10:48:39.678421] log_dir: ./exp/debug/cifar100-LT/debug
[10:48:41.461589] Epoch: [547]  [ 0/42]  eta: 0:01:14  lr: 0.000599  loss: 0.4374 (0.4374)  time: 1.7817  data: 1.2602  max mem: 9341
[10:48:51.642659] Epoch: [547]  [20/42]  eta: 0:00:12  lr: 0.000597  loss: 0.4841 (0.4825)  time: 0.5090  data: 0.0001  max mem: 9341
[10:49:01.791979] Epoch: [547]  [40/42]  eta: 0:00:01  lr: 0.000595  loss: 0.4728 (0.4796)  time: 0.5074  data: 0.0001  max mem: 9341
[10:49:02.298691] Epoch: [547]  [41/42]  eta: 0:00:00  lr: 0.000595  loss: 0.4742 (0.4795)  time: 0.5074  data: 0.0001  max mem: 9341
[10:49:02.457742] Epoch: [547] Total time: 0:00:22 (0.5424 s / it)
[10:49:02.465356] Averaged stats: lr: 0.000595  loss: 0.4742 (0.4835)
[10:49:07.067639] {"train_lr": 0.0005971680127594928, "train_loss": 0.4835364355572632, "epoch": 547}
[10:49:07.067988] [10:49:07.068070] Training epoch 547 for 0:00:27
[10:49:07.068168] [10:49:07.072543] log_dir: ./exp/debug/cifar100-LT/debug
[10:49:08.650671] Epoch: [548]  [ 0/42]  eta: 0:01:06  lr: 0.000594  loss: 0.5093 (0.5093)  time: 1.5768  data: 1.0628  max mem: 9341
[10:49:18.818632] Epoch: [548]  [20/42]  eta: 0:00:12  lr: 0.000593  loss: 0.4902 (0.4873)  time: 0.5083  data: 0.0001  max mem: 9341
[10:49:28.969325] Epoch: [548]  [40/42]  eta: 0:00:01  lr: 0.000591  loss: 0.4824 (0.4854)  time: 0.5075  data: 0.0001  max mem: 9341
[10:49:29.474691] Epoch: [548]  [41/42]  eta: 0:00:00  lr: 0.000591  loss: 0.4824 (0.4847)  time: 0.5074  data: 0.0001  max mem: 9341
[10:49:29.635422] Epoch: [548] Total time: 0:00:22 (0.5372 s / it)
[10:49:29.651439] Averaged stats: lr: 0.000591  loss: 0.4824 (0.4827)
[10:49:34.128002] {"train_lr": 0.0005928841190603343, "train_loss": 0.48267265497928574, "epoch": 548}
[10:49:34.128387] [10:49:34.128489] Training epoch 548 for 0:00:27
[10:49:34.128538] [10:49:34.132959] log_dir: ./exp/debug/cifar100-LT/debug
[10:49:35.594651] Epoch: [549]  [ 0/42]  eta: 0:01:01  lr: 0.000590  loss: 0.4857 (0.4857)  time: 1.4605  data: 0.9437  max mem: 9341
[10:49:45.766160] Epoch: [549]  [20/42]  eta: 0:00:12  lr: 0.000588  loss: 0.4785 (0.4846)  time: 0.5085  data: 0.0001  max mem: 9341
[10:49:55.917058] Epoch: [549]  [40/42]  eta: 0:00:01  lr: 0.000587  loss: 0.4930 (0.4895)  time: 0.5075  data: 0.0001  max mem: 9341
[10:49:56.423519] Epoch: [549]  [41/42]  eta: 0:00:00  lr: 0.000587  loss: 0.4942 (0.4900)  time: 0.5075  data: 0.0001  max mem: 9341
[10:49:56.578361] Epoch: [549] Total time: 0:00:22 (0.5344 s / it)
[10:49:56.596564] Averaged stats: lr: 0.000587  loss: 0.4942 (0.4875)
[10:50:01.120909] {"train_lr": 0.0005886105992964912, "train_loss": 0.4875420906714031, "epoch": 549}
[10:50:01.121273] [10:50:01.121356] Training epoch 549 for 0:00:26
[10:50:01.121408] [10:50:01.125855] log_dir: ./exp/debug/cifar100-LT/debug
[10:50:02.586045] Epoch: [550]  [ 0/42]  eta: 0:01:01  lr: 0.000586  loss: 0.4738 (0.4738)  time: 1.4590  data: 0.9436  max mem: 9341
[10:50:12.747798] Epoch: [550]  [20/42]  eta: 0:00:12  lr: 0.000584  loss: 0.4754 (0.4806)  time: 0.5080  data: 0.0001  max mem: 9341
[10:50:22.895948] Epoch: [550]  [40/42]  eta: 0:00:01  lr: 0.000582  loss: 0.4791 (0.4813)  time: 0.5074  data: 0.0001  max mem: 9341
[10:50:23.402106] Epoch: [550]  [41/42]  eta: 0:00:00  lr: 0.000582  loss: 0.4825 (0.4814)  time: 0.5074  data: 0.0001  max mem: 9341
[10:50:23.575416] Epoch: [550] Total time: 0:00:22 (0.5345 s / it)
[10:50:23.579235] Averaged stats: lr: 0.000582  loss: 0.4825 (0.4853)
[10:50:28.083337] {"train_lr": 0.0005843475264906262, "train_loss": 0.4853286778643018, "epoch": 550}
[10:50:28.083711] [10:50:28.083798] Training epoch 550 for 0:00:26
[10:50:28.083851] [10:50:28.088259] log_dir: ./exp/debug/cifar100-LT/debug
[10:50:29.563147] Epoch: [551]  [ 0/42]  eta: 0:01:01  lr: 0.000581  loss: 0.5094 (0.5094)  time: 1.4734  data: 0.9652  max mem: 9341
[10:50:39.724018] Epoch: [551]  [20/42]  eta: 0:00:12  lr: 0.000580  loss: 0.4927 (0.4912)  time: 0.5080  data: 0.0001  max mem: 9341
[10:50:49.876654] Epoch: [551]  [40/42]  eta: 0:00:01  lr: 0.000578  loss: 0.4893 (0.4905)  time: 0.5076  data: 0.0001  max mem: 9341
[10:50:50.380440] Epoch: [551]  [41/42]  eta: 0:00:00  lr: 0.000578  loss: 0.4885 (0.4897)  time: 0.5075  data: 0.0001  max mem: 9341
[10:50:50.554913] Epoch: [551] Total time: 0:00:22 (0.5349 s / it)
[10:50:50.565584] Averaged stats: lr: 0.000578  loss: 0.4885 (0.4849)
[10:50:55.142402] {"train_lr": 0.0005800949734868903, "train_loss": 0.4848687451864992, "epoch": 551}
[10:50:55.142775] [10:50:55.142860] Training epoch 551 for 0:00:27
[10:50:55.142913] [10:50:55.147196] log_dir: ./exp/debug/cifar100-LT/debug
[10:50:56.979824] Epoch: [552]  [ 0/42]  eta: 0:01:16  lr: 0.000577  loss: 0.4844 (0.4844)  time: 1.8316  data: 1.3283  max mem: 9341
[10:51:07.145631] Epoch: [552]  [20/42]  eta: 0:00:12  lr: 0.000576  loss: 0.4829 (0.4765)  time: 0.5082  data: 0.0001  max mem: 9341
[10:51:17.292366] Epoch: [552]  [40/42]  eta: 0:00:01  lr: 0.000574  loss: 0.4828 (0.4800)  time: 0.5073  data: 0.0001  max mem: 9341
[10:51:17.798141] Epoch: [552]  [41/42]  eta: 0:00:00  lr: 0.000574  loss: 0.4828 (0.4795)  time: 0.5073  data: 0.0001  max mem: 9341
[10:51:17.968123] Epoch: [552] Total time: 0:00:22 (0.5434 s / it)
[10:51:17.979593] Averaged stats: lr: 0.000574  loss: 0.4828 (0.4803)
[10:51:22.523754] {"train_lr": 0.0005758530129496812, "train_loss": 0.4802861822148164, "epoch": 552}
[10:51:22.524145] [10:51:22.524232] Training epoch 552 for 0:00:27
[10:51:22.524287] [10:51:22.528608] log_dir: ./exp/debug/cifar100-LT/debug
[10:51:24.144815] Epoch: [553]  [ 0/42]  eta: 0:01:07  lr: 0.000573  loss: 0.4399 (0.4399)  time: 1.6151  data: 1.1122  max mem: 9341
[10:51:34.316996] Epoch: [553]  [20/42]  eta: 0:00:12  lr: 0.000571  loss: 0.4798 (0.4748)  time: 0.5085  data: 0.0001  max mem: 9341
[10:51:44.471531] Epoch: [553]  [40/42]  eta: 0:00:01  lr: 0.000570  loss: 0.4798 (0.4772)  time: 0.5077  data: 0.0001  max mem: 9341
[10:51:44.976631] Epoch: [553]  [41/42]  eta: 0:00:00  lr: 0.000570  loss: 0.4792 (0.4757)  time: 0.5076  data: 0.0001  max mem: 9341
[10:51:45.144922] Epoch: [553] Total time: 0:00:22 (0.5385 s / it)
[10:51:45.148914] Averaged stats: lr: 0.000570  loss: 0.4792 (0.4818)
[10:51:49.727480] {"train_lr": 0.0005716217173624002, "train_loss": 0.4817872015493257, "epoch": 553}
[10:51:49.727801] [10:51:49.727883] Training epoch 553 for 0:00:27
[10:51:49.727933] [10:51:49.732232] log_dir: ./exp/debug/cifar100-LT/debug
[10:51:51.137205] Epoch: [554]  [ 0/42]  eta: 0:00:58  lr: 0.000569  loss: 0.4937 (0.4937)  time: 1.4037  data: 0.8932  max mem: 9341
[10:52:01.291579] Epoch: [554]  [20/42]  eta: 0:00:12  lr: 0.000567  loss: 0.4732 (0.4770)  time: 0.5077  data: 0.0001  max mem: 9341
[10:52:11.432954] Epoch: [554]  [40/42]  eta: 0:00:01  lr: 0.000566  loss: 0.4833 (0.4742)  time: 0.5070  data: 0.0001  max mem: 9341
[10:52:11.937986] Epoch: [554]  [41/42]  eta: 0:00:00  lr: 0.000566  loss: 0.4833 (0.4752)  time: 0.5071  data: 0.0001  max mem: 9341
[10:52:12.104614] Epoch: [554] Total time: 0:00:22 (0.5327 s / it)
[10:52:12.105410] Averaged stats: lr: 0.000566  loss: 0.4833 (0.4811)
[10:52:16.731617] {"train_lr": 0.0005674011590262147, "train_loss": 0.48113416507840157, "epoch": 554}
[10:52:16.731991] [10:52:16.732103] Training epoch 554 for 0:00:27
[10:52:16.732178] [10:52:16.736652] log_dir: ./exp/debug/cifar100-LT/debug
[10:52:18.192099] Epoch: [555]  [ 0/42]  eta: 0:01:01  lr: 0.000565  loss: 0.4897 (0.4897)  time: 1.4540  data: 0.9272  max mem: 9341
[10:52:28.360839] Epoch: [555]  [20/42]  eta: 0:00:12  lr: 0.000563  loss: 0.4826 (0.4871)  time: 0.5084  data: 0.0001  max mem: 9341
[10:52:38.515169] Epoch: [555]  [40/42]  eta: 0:00:01  lr: 0.000561  loss: 0.4828 (0.4860)  time: 0.5077  data: 0.0001  max mem: 9341
[10:52:39.022582] Epoch: [555]  [41/42]  eta: 0:00:00  lr: 0.000561  loss: 0.4828 (0.4860)  time: 0.5077  data: 0.0001  max mem: 9341
[10:52:39.186274] Epoch: [555] Total time: 0:00:22 (0.5345 s / it)
[10:52:39.190623] Averaged stats: lr: 0.000561  loss: 0.4828 (0.4811)
[10:52:43.727221] {"train_lr": 0.0005631914100588222, "train_loss": 0.4811190902477219, "epoch": 555}
[10:52:43.727572] [10:52:43.727654] Training epoch 555 for 0:00:26
[10:52:43.727704] [10:52:43.732525] log_dir: ./exp/debug/cifar100-LT/debug
[10:52:45.478006] Epoch: [556]  [ 0/42]  eta: 0:01:13  lr: 0.000560  loss: 0.4661 (0.4661)  time: 1.7442  data: 1.2361  max mem: 9341
[10:52:55.644629] Epoch: [556]  [20/42]  eta: 0:00:12  lr: 0.000559  loss: 0.4815 (0.4834)  time: 0.5083  data: 0.0001  max mem: 9341
[10:53:05.800116] Epoch: [556]  [40/42]  eta: 0:00:01  lr: 0.000557  loss: 0.4759 (0.4807)  time: 0.5077  data: 0.0001  max mem: 9341
[10:53:06.307049] Epoch: [556]  [41/42]  eta: 0:00:00  lr: 0.000557  loss: 0.4759 (0.4809)  time: 0.5077  data: 0.0001  max mem: 9341
[10:53:06.476463] Epoch: [556] Total time: 0:00:22 (0.5415 s / it)
[10:53:06.481935] Averaged stats: lr: 0.000557  loss: 0.4759 (0.4814)
[10:53:10.998143] {"train_lr": 0.0005589925423932176, "train_loss": 0.4813999411250864, "epoch": 556}
[10:53:10.998589] [10:53:10.998683] Training epoch 556 for 0:00:27
[10:53:10.998736] [10:53:11.003194] log_dir: ./exp/debug/cifar100-LT/debug
[10:53:12.500717] Epoch: [557]  [ 0/42]  eta: 0:01:02  lr: 0.000556  loss: 0.4710 (0.4710)  time: 1.4962  data: 0.9828  max mem: 9341
[10:53:22.661841] Epoch: [557]  [20/42]  eta: 0:00:12  lr: 0.000555  loss: 0.4792 (0.4823)  time: 0.5080  data: 0.0001  max mem: 9341
[10:53:32.807432] Epoch: [557]  [40/42]  eta: 0:00:01  lr: 0.000553  loss: 0.4779 (0.4798)  time: 0.5072  data: 0.0001  max mem: 9341
[10:53:33.313147] Epoch: [557]  [41/42]  eta: 0:00:00  lr: 0.000553  loss: 0.4741 (0.4794)  time: 0.5072  data: 0.0001  max mem: 9341
[10:53:33.474477] Epoch: [557] Total time: 0:00:22 (0.5350 s / it)
[10:53:33.480985] Averaged stats: lr: 0.000553  loss: 0.4741 (0.4796)
[10:53:38.112646] {"train_lr": 0.0005548046277764638, "train_loss": 0.479637202052843, "epoch": 557}
[10:53:38.113253] [10:53:38.113355] Training epoch 557 for 0:00:27
[10:53:38.113405] [10:53:38.117766] log_dir: ./exp/debug/cifar100-LT/debug
[10:53:39.658909] Epoch: [558]  [ 0/42]  eta: 0:01:04  lr: 0.000552  loss: 0.4719 (0.4719)  time: 1.5397  data: 1.0387  max mem: 9341
[10:53:49.827929] Epoch: [558]  [20/42]  eta: 0:00:12  lr: 0.000550  loss: 0.4781 (0.4831)  time: 0.5084  data: 0.0001  max mem: 9341
[10:53:59.969770] Epoch: [558]  [40/42]  eta: 0:00:01  lr: 0.000549  loss: 0.4798 (0.4817)  time: 0.5070  data: 0.0001  max mem: 9341
[10:54:00.474828] Epoch: [558]  [41/42]  eta: 0:00:00  lr: 0.000549  loss: 0.4798 (0.4808)  time: 0.5070  data: 0.0001  max mem: 9341
[10:54:00.640843] Epoch: [558] Total time: 0:00:22 (0.5363 s / it)
[10:54:00.646508] Averaged stats: lr: 0.000549  loss: 0.4798 (0.4816)
[10:54:05.244070] {"train_lr": 0.0005506277377684656, "train_loss": 0.4815994305979638, "epoch": 558}
[10:54:05.244435] [10:54:05.244519] Training epoch 558 for 0:00:27
[10:54:05.244570] [10:54:05.248722] log_dir: ./exp/debug/cifar100-LT/debug
[10:54:06.827774] Epoch: [559]  [ 0/42]  eta: 0:01:06  lr: 0.000548  loss: 0.4515 (0.4515)  time: 1.5768  data: 1.0802  max mem: 9341
[10:54:17.013216] Epoch: [559]  [20/42]  eta: 0:00:12  lr: 0.000546  loss: 0.4812 (0.4837)  time: 0.5092  data: 0.0001  max mem: 9341
[10:54:27.185253] Epoch: [559]  [40/42]  eta: 0:00:01  lr: 0.000545  loss: 0.4852 (0.4851)  time: 0.5086  data: 0.0001  max mem: 9341
[10:54:27.692772] Epoch: [559]  [41/42]  eta: 0:00:00  lr: 0.000545  loss: 0.4852 (0.4850)  time: 0.5087  data: 0.0001  max mem: 9341
[10:54:27.852506] Epoch: [559] Total time: 0:00:22 (0.5382 s / it)
[10:54:27.856126] Averaged stats: lr: 0.000545  loss: 0.4852 (0.4798)
[10:54:32.434073] {"train_lr": 0.0005464619437407508, "train_loss": 0.4798474008483546, "epoch": 559}
[10:54:32.434383] [10:54:32.434464] Training epoch 559 for 0:00:27
[10:54:32.434577] [10:54:32.438974] log_dir: ./exp/debug/cifar100-LT/debug
[10:54:33.918066] Epoch: [560]  [ 0/42]  eta: 0:01:02  lr: 0.000544  loss: 0.4257 (0.4257)  time: 1.4777  data: 0.9582  max mem: 9341
[10:54:44.087199] Epoch: [560]  [20/42]  eta: 0:00:12  lr: 0.000542  loss: 0.4762 (0.4716)  time: 0.5084  data: 0.0001  max mem: 9341
[10:54:54.236712] Epoch: [560]  [40/42]  eta: 0:00:01  lr: 0.000541  loss: 0.4897 (0.4796)  time: 0.5074  data: 0.0001  max mem: 9341
[10:54:54.744226] Epoch: [560]  [41/42]  eta: 0:00:00  lr: 0.000541  loss: 0.4897 (0.4792)  time: 0.5075  data: 0.0001  max mem: 9341
[10:54:54.910953] Epoch: [560] Total time: 0:00:22 (0.5350 s / it)
[10:54:54.911647] Averaged stats: lr: 0.000541  loss: 0.4897 (0.4797)
[10:54:59.461124] {"train_lr": 0.0005423073168752445, "train_loss": 0.4797360815462612, "epoch": 560}
[10:54:59.461471] [10:54:59.461555] Training epoch 560 for 0:00:27
[10:54:59.461605] [10:54:59.465906] log_dir: ./exp/debug/cifar100-LT/debug
[10:55:00.912521] Epoch: [561]  [ 0/42]  eta: 0:01:00  lr: 0.000540  loss: 0.5012 (0.5012)  time: 1.4451  data: 0.9455  max mem: 9341
[10:55:11.089698] Epoch: [561]  [20/42]  eta: 0:00:12  lr: 0.000538  loss: 0.4920 (0.4922)  time: 0.5088  data: 0.0001  max mem: 9341
[10:55:21.256405] Epoch: [561]  [40/42]  eta: 0:00:01  lr: 0.000536  loss: 0.4853 (0.4867)  time: 0.5083  data: 0.0001  max mem: 9341
[10:55:21.762621] Epoch: [561]  [41/42]  eta: 0:00:00  lr: 0.000536  loss: 0.4857 (0.4867)  time: 0.5082  data: 0.0001  max mem: 9341
[10:55:21.930213] Epoch: [561] Total time: 0:00:22 (0.5349 s / it)
[10:55:21.930927] Averaged stats: lr: 0.000536  loss: 0.4857 (0.4802)
[10:55:26.433540] {"train_lr": 0.0005381639281630569, "train_loss": 0.4802176845925195, "epoch": 561}
[10:55:26.433955] [10:55:26.434042] Training epoch 561 for 0:00:26
[10:55:26.434095] [10:55:26.438615] log_dir: ./exp/debug/cifar100-LT/debug
[10:55:27.994794] Epoch: [562]  [ 0/42]  eta: 0:01:05  lr: 0.000535  loss: 0.4956 (0.4956)  time: 1.5550  data: 1.0436  max mem: 9341
[10:55:38.165703] Epoch: [562]  [20/42]  eta: 0:00:12  lr: 0.000534  loss: 0.4831 (0.4824)  time: 0.5085  data: 0.0002  max mem: 9341
[10:55:48.329675] Epoch: [562]  [40/42]  eta: 0:00:01  lr: 0.000532  loss: 0.4776 (0.4819)  time: 0.5081  data: 0.0001  max mem: 9341
[10:55:48.834548] Epoch: [562]  [41/42]  eta: 0:00:00  lr: 0.000532  loss: 0.4774 (0.4814)  time: 0.5081  data: 0.0001  max mem: 9341
[10:55:48.995236] Epoch: [562] Total time: 0:00:22 (0.5371 s / it)
[10:55:49.002488] Averaged stats: lr: 0.000532  loss: 0.4774 (0.4783)
[10:55:53.539709] {"train_lr": 0.0005340318484032695, "train_loss": 0.47832494512909934, "epoch": 562}
[10:55:53.540108] [10:55:53.540196] Training epoch 562 for 0:00:27
[10:55:53.540249] [10:55:53.544630] log_dir: ./exp/debug/cifar100-LT/debug
[10:55:55.053091] Epoch: [563]  [ 0/42]  eta: 0:01:03  lr: 0.000531  loss: 0.5312 (0.5312)  time: 1.5072  data: 0.9902  max mem: 9341
[10:56:05.222674] Epoch: [563]  [20/42]  eta: 0:00:12  lr: 0.000530  loss: 0.4751 (0.4770)  time: 0.5084  data: 0.0001  max mem: 9341
[10:56:15.365853] Epoch: [563]  [40/42]  eta: 0:00:01  lr: 0.000528  loss: 0.4765 (0.4814)  time: 0.5071  data: 0.0001  max mem: 9341
[10:56:15.872919] Epoch: [563]  [41/42]  eta: 0:00:00  lr: 0.000528  loss: 0.4741 (0.4811)  time: 0.5071  data: 0.0001  max mem: 9341
[10:56:16.042863] Epoch: [563] Total time: 0:00:22 (0.5357 s / it)
[10:56:16.059408] Averaged stats: lr: 0.000528  loss: 0.4741 (0.4793)
[10:56:20.597527] {"train_lr": 0.0005299111482017239, "train_loss": 0.47931040078401566, "epoch": 563}
[10:56:20.597873] [10:56:20.597953] Training epoch 563 for 0:00:27
[10:56:20.598003] [10:56:20.602318] log_dir: ./exp/debug/cifar100-LT/debug
[10:56:22.155387] Epoch: [564]  [ 0/42]  eta: 0:01:05  lr: 0.000527  loss: 0.4973 (0.4973)  time: 1.5519  data: 1.0435  max mem: 9341
[10:56:32.338675] Epoch: [564]  [20/42]  eta: 0:00:12  lr: 0.000526  loss: 0.4713 (0.4774)  time: 0.5091  data: 0.0001  max mem: 9341
[10:56:42.508960] Epoch: [564]  [40/42]  eta: 0:00:01  lr: 0.000524  loss: 0.4813 (0.4822)  time: 0.5085  data: 0.0001  max mem: 9341
[10:56:43.015799] Epoch: [564]  [41/42]  eta: 0:00:00  lr: 0.000524  loss: 0.4808 (0.4822)  time: 0.5085  data: 0.0001  max mem: 9341
[10:56:43.177053] Epoch: [564] Total time: 0:00:22 (0.5375 s / it)
[10:56:43.182867] Averaged stats: lr: 0.000524  loss: 0.4808 (0.4807)
[10:56:47.850436] {"train_lr": 0.0005258018979698173, "train_loss": 0.480689273703666, "epoch": 564}
[10:56:47.850744] [10:56:47.850828] Training epoch 564 for 0:00:27
[10:56:47.850879] [10:56:47.855320] log_dir: ./exp/debug/cifar100-LT/debug
[10:56:49.388687] Epoch: [565]  [ 0/42]  eta: 0:01:04  lr: 0.000523  loss: 0.5361 (0.5361)  time: 1.5319  data: 1.0222  max mem: 9341
[10:56:59.558563] Epoch: [565]  [20/42]  eta: 0:00:12  lr: 0.000521  loss: 0.4890 (0.4870)  time: 0.5084  data: 0.0001  max mem: 9341
[10:57:09.714913] Epoch: [565]  [40/42]  eta: 0:00:01  lr: 0.000520  loss: 0.4793 (0.4868)  time: 0.5078  data: 0.0001  max mem: 9341
[10:57:10.220741] Epoch: [565]  [41/42]  eta: 0:00:00  lr: 0.000520  loss: 0.4793 (0.4878)  time: 0.5078  data: 0.0001  max mem: 9341
[10:57:10.380500] Epoch: [565] Total time: 0:00:22 (0.5363 s / it)
[10:57:10.387712] Averaged stats: lr: 0.000520  loss: 0.4793 (0.4806)
[10:57:14.878299] {"train_lr": 0.0005217041679232982, "train_loss": 0.48058454869758516, "epoch": 565}
[10:57:14.878625] [10:57:14.878707] Training epoch 565 for 0:00:27
[10:57:14.878757] [10:57:14.883018] log_dir: ./exp/debug/cifar100-LT/debug
[10:57:16.559483] Epoch: [566]  [ 0/42]  eta: 0:01:10  lr: 0.000519  loss: 0.4257 (0.4257)  time: 1.6757  data: 1.1697  max mem: 9341
[10:57:26.723270] Epoch: [566]  [20/42]  eta: 0:00:12  lr: 0.000517  loss: 0.4799 (0.4851)  time: 0.5081  data: 0.0001  max mem: 9341
[10:57:36.875610] Epoch: [566]  [40/42]  eta: 0:00:01  lr: 0.000516  loss: 0.4761 (0.4843)  time: 0.5076  data: 0.0001  max mem: 9341
[10:57:37.381939] Epoch: [566]  [41/42]  eta: 0:00:00  lr: 0.000516  loss: 0.4832 (0.4845)  time: 0.5076  data: 0.0001  max mem: 9341
[10:57:37.553680] Epoch: [566] Total time: 0:00:22 (0.5398 s / it)
[10:57:37.558142] Averaged stats: lr: 0.000516  loss: 0.4832 (0.4818)
[10:57:42.133928] {"train_lr": 0.0005176180280810685, "train_loss": 0.48183192702985944, "epoch": 566}
[10:57:42.134320] [10:57:42.134408] Training epoch 566 for 0:00:27
[10:57:42.134459] [10:57:42.139390] log_dir: ./exp/debug/cifar100-LT/debug
[10:57:43.696912] Epoch: [567]  [ 0/42]  eta: 0:01:05  lr: 0.000515  loss: 0.4671 (0.4671)  time: 1.5561  data: 1.0532  max mem: 9341
[10:57:53.864425] Epoch: [567]  [20/42]  eta: 0:00:12  lr: 0.000513  loss: 0.4710 (0.4789)  time: 0.5083  data: 0.0001  max mem: 9341
[10:58:04.019056] Epoch: [567]  [40/42]  eta: 0:00:01  lr: 0.000512  loss: 0.4716 (0.4778)  time: 0.5077  data: 0.0001  max mem: 9341
[10:58:04.525064] Epoch: [567]  [41/42]  eta: 0:00:00  lr: 0.000512  loss: 0.4686 (0.4775)  time: 0.5077  data: 0.0001  max mem: 9341
[10:58:04.685259] Epoch: [567] Total time: 0:00:22 (0.5368 s / it)
[10:58:04.698982] Averaged stats: lr: 0.000512  loss: 0.4686 (0.4772)
[10:58:09.297090] {"train_lr": 0.0005135435482639822, "train_loss": 0.4772060165802638, "epoch": 567}
[10:58:09.297480] [10:58:09.297573] Training epoch 567 for 0:00:27
[10:58:09.297686] [10:58:09.302620] log_dir: ./exp/debug/cifar100-LT/debug
[10:58:10.908782] Epoch: [568]  [ 0/42]  eta: 0:01:07  lr: 0.000511  loss: 0.4800 (0.4800)  time: 1.6049  data: 1.0973  max mem: 9341
[10:58:21.076565] Epoch: [568]  [20/42]  eta: 0:00:12  lr: 0.000509  loss: 0.4734 (0.4782)  time: 0.5083  data: 0.0001  max mem: 9341
[10:58:31.231663] Epoch: [568]  [40/42]  eta: 0:00:01  lr: 0.000508  loss: 0.4809 (0.4797)  time: 0.5077  data: 0.0001  max mem: 9341
[10:58:31.736980] Epoch: [568]  [41/42]  eta: 0:00:00  lr: 0.000508  loss: 0.4746 (0.4796)  time: 0.5076  data: 0.0001  max mem: 9341
[10:58:31.902704] Epoch: [568] Total time: 0:00:22 (0.5381 s / it)
[10:58:31.903474] Averaged stats: lr: 0.000508  loss: 0.4746 (0.4787)
[10:58:36.501374] {"train_lr": 0.0005094807980936584, "train_loss": 0.47866407072260264, "epoch": 568}
[10:58:36.501745] [10:58:36.501835] Training epoch 568 for 0:00:27
[10:58:36.501886] [10:58:36.506988] log_dir: ./exp/debug/cifar100-LT/debug
[10:58:37.996604] Epoch: [569]  [ 0/42]  eta: 0:01:02  lr: 0.000507  loss: 0.4815 (0.4815)  time: 1.4888  data: 0.9795  max mem: 9341
[10:58:48.156835] Epoch: [569]  [20/42]  eta: 0:00:12  lr: 0.000505  loss: 0.4732 (0.4770)  time: 0.5080  data: 0.0001  max mem: 9341
[10:58:58.306522] Epoch: [569]  [40/42]  eta: 0:00:01  lr: 0.000504  loss: 0.4868 (0.4798)  time: 0.5074  data: 0.0001  max mem: 9341
[10:58:58.813307] Epoch: [569]  [41/42]  eta: 0:00:00  lr: 0.000504  loss: 0.4886 (0.4801)  time: 0.5075  data: 0.0001  max mem: 9341
[10:58:58.979051] Epoch: [569] Total time: 0:00:22 (0.5350 s / it)
[10:58:58.986121] Averaged stats: lr: 0.000504  loss: 0.4886 (0.4826)
[10:59:03.641010] {"train_lr": 0.0005054298469912883, "train_loss": 0.48255773367626326, "epoch": 569}
[10:59:03.641370] [10:59:03.641453] Training epoch 569 for 0:00:27
[10:59:03.641505] [10:59:03.645820] log_dir: ./exp/debug/cifar100-LT/debug
[10:59:05.299688] Epoch: [570]  [ 0/42]  eta: 0:01:09  lr: 0.000503  loss: 0.5054 (0.5054)  time: 1.6528  data: 1.1449  max mem: 9341
[10:59:15.467063] Epoch: [570]  [20/42]  eta: 0:00:12  lr: 0.000501  loss: 0.4838 (0.4840)  time: 0.5083  data: 0.0001  max mem: 9341
[10:59:25.611295] Epoch: [570]  [40/42]  eta: 0:00:01  lr: 0.000500  loss: 0.4809 (0.4830)  time: 0.5072  data: 0.0001  max mem: 9341
[10:59:26.117205] Epoch: [570]  [41/42]  eta: 0:00:00  lr: 0.000500  loss: 0.4809 (0.4831)  time: 0.5072  data: 0.0001  max mem: 9341
[10:59:26.287107] Epoch: [570] Total time: 0:00:22 (0.5391 s / it)
[10:59:26.292905] Averaged stats: lr: 0.000500  loss: 0.4809 (0.4827)
[10:59:30.956527] {"train_lr": 0.0005013907641764481, "train_loss": 0.4826637701619239, "epoch": 570}
[10:59:30.956904] [10:59:30.957006] Training epoch 570 for 0:00:27
[10:59:30.957056] [10:59:30.961277] log_dir: ./exp/debug/cifar100-LT/debug
[10:59:32.492844] Epoch: [571]  [ 0/42]  eta: 0:01:04  lr: 0.000499  loss: 0.5040 (0.5040)  time: 1.5303  data: 1.0245  max mem: 9341
[10:59:42.652369] Epoch: [571]  [20/42]  eta: 0:00:12  lr: 0.000497  loss: 0.4786 (0.4862)  time: 0.5079  data: 0.0001  max mem: 9341
[10:59:52.802693] Epoch: [571]  [40/42]  eta: 0:00:01  lr: 0.000496  loss: 0.4732 (0.4830)  time: 0.5075  data: 0.0001  max mem: 9341
[10:59:53.308502] Epoch: [571]  [41/42]  eta: 0:00:00  lr: 0.000496  loss: 0.4732 (0.4836)  time: 0.5075  data: 0.0001  max mem: 9341
[10:59:53.474947] Epoch: [571] Total time: 0:00:22 (0.5360 s / it)
[10:59:53.479411] Averaged stats: lr: 0.000496  loss: 0.4732 (0.4819)
[10:59:58.011189] {"train_lr": 0.0004973636186659214, "train_loss": 0.4818558403778644, "epoch": 571}
[10:59:58.011549] [10:59:58.011630] Training epoch 571 for 0:00:27
[10:59:58.011681] [10:59:58.016072] log_dir: ./exp/debug/cifar100-LT/debug
[10:59:59.547460] Epoch: [572]  [ 0/42]  eta: 0:01:04  lr: 0.000495  loss: 0.4731 (0.4731)  time: 1.5302  data: 1.0198  max mem: 9341
[11:00:09.722238] Epoch: [572]  [20/42]  eta: 0:00:12  lr: 0.000493  loss: 0.4691 (0.4753)  time: 0.5087  data: 0.0001  max mem: 9341
[11:00:19.871811] Epoch: [572]  [40/42]  eta: 0:00:01  lr: 0.000492  loss: 0.4817 (0.4798)  time: 0.5074  data: 0.0001  max mem: 9341
[11:00:20.377655] Epoch: [572]  [41/42]  eta: 0:00:00  lr: 0.000492  loss: 0.4820 (0.4799)  time: 0.5074  data: 0.0001  max mem: 9341
[11:00:20.543955] Epoch: [572] Total time: 0:00:22 (0.5364 s / it)
[11:00:20.555610] Averaged stats: lr: 0.000492  loss: 0.4820 (0.4805)
[11:00:25.054633] {"train_lr": 0.0004933484792725099, "train_loss": 0.48049748848591534, "epoch": 572}
[11:00:25.054984] [11:00:25.055067] Training epoch 572 for 0:00:27
[11:00:25.055118] [11:00:25.059418] log_dir: ./exp/debug/cifar100-LT/debug
[11:00:26.651817] Epoch: [573]  [ 0/42]  eta: 0:01:06  lr: 0.000491  loss: 0.4669 (0.4669)  time: 1.5912  data: 1.0727  max mem: 9341
[11:00:36.819962] Epoch: [573]  [20/42]  eta: 0:00:12  lr: 0.000489  loss: 0.4853 (0.4774)  time: 0.5084  data: 0.0001  max mem: 9341
[11:00:46.981458] Epoch: [573]  [40/42]  eta: 0:00:01  lr: 0.000488  loss: 0.4847 (0.4801)  time: 0.5080  data: 0.0001  max mem: 9341
[11:00:47.487069] Epoch: [573]  [41/42]  eta: 0:00:00  lr: 0.000488  loss: 0.4847 (0.4799)  time: 0.5080  data: 0.0001  max mem: 9341
[11:00:47.648868] Epoch: [573] Total time: 0:00:22 (0.5378 s / it)
[11:00:47.650599] Averaged stats: lr: 0.000488  loss: 0.4847 (0.4755)
[11:00:52.177069] {"train_lr": 0.0004893454146038707, "train_loss": 0.4754704206827141, "epoch": 573}
[11:00:52.177442] [11:00:52.177541] Training epoch 573 for 0:00:27
[11:00:52.177593] [11:00:52.181873] log_dir: ./exp/debug/cifar100-LT/debug
[11:00:53.705942] Epoch: [574]  [ 0/42]  eta: 0:01:03  lr: 0.000487  loss: 0.5090 (0.5090)  time: 1.5225  data: 1.0291  max mem: 9341
[11:01:03.871029] Epoch: [574]  [20/42]  eta: 0:00:12  lr: 0.000485  loss: 0.4787 (0.4840)  time: 0.5082  data: 0.0001  max mem: 9341
[11:01:14.018602] Epoch: [574]  [40/42]  eta: 0:00:01  lr: 0.000484  loss: 0.4690 (0.4817)  time: 0.5073  data: 0.0001  max mem: 9341
[11:01:14.524861] Epoch: [574]  [41/42]  eta: 0:00:00  lr: 0.000484  loss: 0.4689 (0.4814)  time: 0.5074  data: 0.0001  max mem: 9341
[11:01:14.699434] Epoch: [574] Total time: 0:00:22 (0.5361 s / it)
[11:01:14.700145] Averaged stats: lr: 0.000484  loss: 0.4689 (0.4786)
[11:01:19.170215] {"train_lr": 0.000485354493061333, "train_loss": 0.47860241486203103, "epoch": 574}
[11:01:19.170674] [11:01:19.170791] Training epoch 574 for 0:00:26
[11:01:19.170844] [11:01:19.175959] log_dir: ./exp/debug/cifar100-LT/debug
[11:01:20.784861] Epoch: [575]  [ 0/42]  eta: 0:01:07  lr: 0.000483  loss: 0.4685 (0.4685)  time: 1.6076  data: 1.0922  max mem: 9341
[11:01:30.955520] Epoch: [575]  [20/42]  eta: 0:00:12  lr: 0.000481  loss: 0.4757 (0.4787)  time: 0.5085  data: 0.0001  max mem: 9341
[11:01:41.107223] Epoch: [575]  [40/42]  eta: 0:00:01  lr: 0.000480  loss: 0.4614 (0.4738)  time: 0.5075  data: 0.0001  max mem: 9341
[11:01:41.612613] Epoch: [575]  [41/42]  eta: 0:00:00  lr: 0.000480  loss: 0.4614 (0.4752)  time: 0.5075  data: 0.0001  max mem: 9341
[11:01:41.781968] Epoch: [575] Total time: 0:00:22 (0.5382 s / it)
[11:01:41.784797] Averaged stats: lr: 0.000480  loss: 0.4614 (0.4767)
[11:01:46.402601] {"train_lr": 0.00048137578283873473, "train_loss": 0.47673054553923155, "epoch": 575}
[11:01:46.402924] [11:01:46.403007] Training epoch 575 for 0:00:27
[11:01:46.403058] [11:01:46.407376] log_dir: ./exp/debug/cifar100-LT/debug
[11:01:47.837336] Epoch: [576]  [ 0/42]  eta: 0:00:59  lr: 0.000479  loss: 0.4761 (0.4761)  time: 1.4285  data: 0.9198  max mem: 9341
[11:01:58.007336] Epoch: [576]  [20/42]  eta: 0:00:12  lr: 0.000477  loss: 0.4830 (0.4863)  time: 0.5084  data: 0.0001  max mem: 9341
[11:02:08.156023] Epoch: [576]  [40/42]  eta: 0:00:01  lr: 0.000476  loss: 0.4810 (0.4831)  time: 0.5074  data: 0.0001  max mem: 9341
[11:02:08.661318] Epoch: [576]  [41/42]  eta: 0:00:00  lr: 0.000476  loss: 0.4821 (0.4830)  time: 0.5073  data: 0.0001  max mem: 9341
[11:02:08.842029] Epoch: [576] Total time: 0:00:22 (0.5342 s / it)
[11:02:08.849426] Averaged stats: lr: 0.000476  loss: 0.4821 (0.4798)
[11:02:13.391416] {"train_lr": 0.00047740935192125633, "train_loss": 0.4798353724181652, "epoch": 576}
[11:02:13.391808] [11:02:13.391891] Training epoch 576 for 0:00:26
[11:02:13.391942] [11:02:13.396340] log_dir: ./exp/debug/cifar100-LT/debug
[11:02:14.982835] Epoch: [577]  [ 0/42]  eta: 0:01:06  lr: 0.000475  loss: 0.4633 (0.4633)  time: 1.5850  data: 1.0792  max mem: 9341
[11:02:25.146866] Epoch: [577]  [20/42]  eta: 0:00:12  lr: 0.000473  loss: 0.4833 (0.4801)  time: 0.5081  data: 0.0001  max mem: 9341
[11:02:35.290430] Epoch: [577]  [40/42]  eta: 0:00:01  lr: 0.000472  loss: 0.4961 (0.4851)  time: 0.5071  data: 0.0001  max mem: 9341
[11:02:35.794152] Epoch: [577]  [41/42]  eta: 0:00:00  lr: 0.000472  loss: 0.4940 (0.4851)  time: 0.5071  data: 0.0001  max mem: 9341
[11:02:35.955540] Epoch: [577] Total time: 0:00:22 (0.5371 s / it)
[11:02:35.965163] Averaged stats: lr: 0.000472  loss: 0.4940 (0.4784)
[11:02:40.502808] {"train_lr": 0.0004734552680842581, "train_loss": 0.478382955349627, "epoch": 577}
[11:02:40.503191] [11:02:40.503275] Training epoch 577 for 0:00:27
[11:02:40.503327] [11:02:40.507779] log_dir: ./exp/debug/cifar100-LT/debug
[11:02:41.959936] Epoch: [578]  [ 0/42]  eta: 0:01:00  lr: 0.000471  loss: 0.5230 (0.5230)  time: 1.4510  data: 0.9382  max mem: 9341
[11:02:52.142522] Epoch: [578]  [20/42]  eta: 0:00:12  lr: 0.000469  loss: 0.4753 (0.4752)  time: 0.5091  data: 0.0001  max mem: 9341
[11:03:02.314862] Epoch: [578]  [40/42]  eta: 0:00:01  lr: 0.000468  loss: 0.4779 (0.4767)  time: 0.5086  data: 0.0001  max mem: 9341
[11:03:02.821767] Epoch: [578]  [41/42]  eta: 0:00:00  lr: 0.000468  loss: 0.4793 (0.4780)  time: 0.5086  data: 0.0001  max mem: 9341
[11:03:02.992813] Epoch: [578] Total time: 0:00:22 (0.5354 s / it)
[11:03:03.011441] Averaged stats: lr: 0.000468  loss: 0.4793 (0.4796)
[11:03:07.632579] {"train_lr": 0.0004695135988921234, "train_loss": 0.4795706722707975, "epoch": 578}
[11:03:07.633031] [11:03:07.633129] Training epoch 578 for 0:00:27
[11:03:07.633181] [11:03:07.638103] log_dir: ./exp/debug/cifar100-LT/debug
[11:03:09.313763] Epoch: [579]  [ 0/42]  eta: 0:01:10  lr: 0.000467  loss: 0.4876 (0.4876)  time: 1.6745  data: 1.1677  max mem: 9341
[11:03:19.483738] Epoch: [579]  [20/42]  eta: 0:00:12  lr: 0.000465  loss: 0.4740 (0.4742)  time: 0.5084  data: 0.0001  max mem: 9341
[11:03:29.627869] Epoch: [579]  [40/42]  eta: 0:00:01  lr: 0.000464  loss: 0.4779 (0.4769)  time: 0.5072  data: 0.0001  max mem: 9341
[11:03:30.132016] Epoch: [579]  [41/42]  eta: 0:00:00  lr: 0.000464  loss: 0.4721 (0.4767)  time: 0.5071  data: 0.0001  max mem: 9341
[11:03:30.300767] Epoch: [579] Total time: 0:00:22 (0.5396 s / it)
[11:03:30.305979] Averaged stats: lr: 0.000464  loss: 0.4721 (0.4750)
[11:03:34.859439] {"train_lr": 0.0004655844116971036, "train_loss": 0.47499814806949525, "epoch": 579}
[11:03:34.859793] [11:03:34.859875] Training epoch 579 for 0:00:27
[11:03:34.859926] [11:03:34.864431] log_dir: ./exp/debug/cifar100-LT/debug
[11:03:36.465875] Epoch: [580]  [ 0/42]  eta: 0:01:07  lr: 0.000463  loss: 0.5046 (0.5046)  time: 1.6003  data: 1.0917  max mem: 9341
[11:03:46.648341] Epoch: [580]  [20/42]  eta: 0:00:12  lr: 0.000461  loss: 0.4748 (0.4752)  time: 0.5091  data: 0.0001  max mem: 9341
[11:03:56.801813] Epoch: [580]  [40/42]  eta: 0:00:01  lr: 0.000460  loss: 0.4803 (0.4767)  time: 0.5076  data: 0.0001  max mem: 9341
[11:03:57.307442] Epoch: [580]  [41/42]  eta: 0:00:00  lr: 0.000460  loss: 0.4751 (0.4759)  time: 0.5076  data: 0.0001  max mem: 9341
[11:03:57.467710] Epoch: [580] Total time: 0:00:22 (0.5382 s / it)
[11:03:57.468509] Averaged stats: lr: 0.000460  loss: 0.4751 (0.4777)
[11:04:01.937534] {"train_lr": 0.00046166777363816495, "train_loss": 0.47767468275768415, "epoch": 580}
[11:04:01.937867] [11:04:01.937948] Training epoch 580 for 0:00:27
[11:04:01.937999] [11:04:01.942246] log_dir: ./exp/debug/cifar100-LT/debug
[11:04:03.563138] Epoch: [581]  [ 0/42]  eta: 0:01:08  lr: 0.000459  loss: 0.5070 (0.5070)  time: 1.6196  data: 1.1011  max mem: 9341
[11:04:13.726442] Epoch: [581]  [20/42]  eta: 0:00:12  lr: 0.000458  loss: 0.4757 (0.4801)  time: 0.5081  data: 0.0001  max mem: 9341
[11:04:23.870946] Epoch: [581]  [40/42]  eta: 0:00:01  lr: 0.000456  loss: 0.4792 (0.4822)  time: 0.5072  data: 0.0001  max mem: 9341
[11:04:24.376448] Epoch: [581]  [41/42]  eta: 0:00:00  lr: 0.000456  loss: 0.4792 (0.4826)  time: 0.5072  data: 0.0001  max mem: 9341
[11:04:24.542007] Epoch: [581] Total time: 0:00:22 (0.5381 s / it)
[11:04:24.547401] Averaged stats: lr: 0.000456  loss: 0.4792 (0.4795)
[11:04:29.066777] {"train_lr": 0.0004577637516398478, "train_loss": 0.4794509806448505, "epoch": 581}
[11:04:29.067130] [11:04:29.067211] Training epoch 581 for 0:00:27
[11:04:29.067262] [11:04:29.071568] log_dir: ./exp/debug/cifar100-LT/debug
[11:04:30.579179] Epoch: [582]  [ 0/42]  eta: 0:01:03  lr: 0.000455  loss: 0.4877 (0.4877)  time: 1.5061  data: 1.0079  max mem: 9341
[11:04:40.743346] Epoch: [582]  [20/42]  eta: 0:00:12  lr: 0.000454  loss: 0.4697 (0.4713)  time: 0.5082  data: 0.0001  max mem: 9341
[11:04:50.889055] Epoch: [582]  [40/42]  eta: 0:00:01  lr: 0.000452  loss: 0.4860 (0.4779)  time: 0.5072  data: 0.0001  max mem: 9341
[11:04:51.393515] Epoch: [582]  [41/42]  eta: 0:00:00  lr: 0.000452  loss: 0.4860 (0.4769)  time: 0.5072  data: 0.0001  max mem: 9341
[11:04:51.565095] Epoch: [582] Total time: 0:00:22 (0.5356 s / it)
[11:04:51.582763] Averaged stats: lr: 0.000452  loss: 0.4860 (0.4757)
[11:04:56.126676] {"train_lr": 0.000453872412411116, "train_loss": 0.4756593879844461, "epoch": 582}
[11:04:56.127036] [11:04:56.127117] Training epoch 582 for 0:00:27
[11:04:56.127168] [11:04:56.131476] log_dir: ./exp/debug/cifar100-LT/debug
[11:04:57.874310] Epoch: [583]  [ 0/42]  eta: 0:01:13  lr: 0.000451  loss: 0.4873 (0.4873)  time: 1.7418  data: 1.2361  max mem: 9341
[11:05:08.057792] Epoch: [583]  [20/42]  eta: 0:00:12  lr: 0.000450  loss: 0.4749 (0.4735)  time: 0.5091  data: 0.0001  max mem: 9341
[11:05:18.228449] Epoch: [583]  [40/42]  eta: 0:00:01  lr: 0.000448  loss: 0.4735 (0.4758)  time: 0.5085  data: 0.0001  max mem: 9341
[11:05:18.735350] Epoch: [583]  [41/42]  eta: 0:00:00  lr: 0.000448  loss: 0.4735 (0.4760)  time: 0.5085  data: 0.0001  max mem: 9341
[11:05:18.903478] Epoch: [583] Total time: 0:00:22 (0.5422 s / it)
[11:05:18.910056] Averaged stats: lr: 0.000448  loss: 0.4735 (0.4761)
[11:05:23.536641] {"train_lr": 0.00044999382244421883, "train_loss": 0.4760860726237297, "epoch": 583}
[11:05:23.536920] [11:05:23.536997] Training epoch 583 for 0:00:27
[11:05:23.537105] [11:05:23.541492] log_dir: ./exp/debug/cifar100-LT/debug
[11:05:25.027204] Epoch: [584]  [ 0/42]  eta: 0:01:02  lr: 0.000447  loss: 0.4814 (0.4814)  time: 1.4844  data: 0.9748  max mem: 9341
[11:05:35.190615] Epoch: [584]  [20/42]  eta: 0:00:12  lr: 0.000446  loss: 0.4836 (0.4756)  time: 0.5081  data: 0.0001  max mem: 9341
[11:05:45.340298] Epoch: [584]  [40/42]  eta: 0:00:01  lr: 0.000444  loss: 0.4782 (0.4797)  time: 0.5074  data: 0.0001  max mem: 9341
[11:05:45.847050] Epoch: [584]  [41/42]  eta: 0:00:00  lr: 0.000444  loss: 0.4760 (0.4795)  time: 0.5074  data: 0.0001  max mem: 9341
[11:05:46.026928] Epoch: [584] Total time: 0:00:22 (0.5354 s / it)
[11:05:46.027658] Averaged stats: lr: 0.000444  loss: 0.4760 (0.4779)
[11:05:50.593553] {"train_lr": 0.0004461280480135595, "train_loss": 0.4779203198850155, "epoch": 584}
[11:05:50.593911] [11:05:50.593995] Training epoch 584 for 0:00:27
[11:05:50.594046] [11:05:50.598489] log_dir: ./exp/debug/cifar100-LT/debug
[11:05:52.116772] Epoch: [585]  [ 0/42]  eta: 0:01:03  lr: 0.000444  loss: 0.4655 (0.4655)  time: 1.5167  data: 1.0058  max mem: 9341
[11:06:02.320948] Epoch: [585]  [20/42]  eta: 0:00:12  lr: 0.000442  loss: 0.4846 (0.4850)  time: 0.5102  data: 0.0001  max mem: 9341
[11:06:12.478871] Epoch: [585]  [40/42]  eta: 0:00:01  lr: 0.000441  loss: 0.4680 (0.4814)  time: 0.5079  data: 0.0001  max mem: 9341
[11:06:12.984514] Epoch: [585]  [41/42]  eta: 0:00:00  lr: 0.000441  loss: 0.4828 (0.4816)  time: 0.5077  data: 0.0001  max mem: 9341
[11:06:13.148951] Epoch: [585] Total time: 0:00:22 (0.5369 s / it)
[11:06:13.149750] Averaged stats: lr: 0.000441  loss: 0.4828 (0.4772)
[11:06:17.703576] {"train_lr": 0.0004422751551745566, "train_loss": 0.47718754605877967, "epoch": 585}
[11:06:17.703927] [11:06:17.704019] Training epoch 585 for 0:00:27
[11:06:17.704071] [11:06:17.708439] log_dir: ./exp/debug/cifar100-LT/debug
[11:06:19.259817] Epoch: [586]  [ 0/42]  eta: 0:01:05  lr: 0.000440  loss: 0.5030 (0.5030)  time: 1.5497  data: 1.0408  max mem: 9341
[11:06:29.413728] Epoch: [586]  [20/42]  eta: 0:00:12  lr: 0.000438  loss: 0.4757 (0.4833)  time: 0.5076  data: 0.0002  max mem: 9341
[11:06:39.560790] Epoch: [586]  [40/42]  eta: 0:00:01  lr: 0.000437  loss: 0.4693 (0.4773)  time: 0.5073  data: 0.0001  max mem: 9341
[11:06:40.066369] Epoch: [586]  [41/42]  eta: 0:00:00  lr: 0.000437  loss: 0.4699 (0.4771)  time: 0.5073  data: 0.0001  max mem: 9341
[11:06:40.244513] Epoch: [586] Total time: 0:00:22 (0.5366 s / it)
[11:06:40.256649] Averaged stats: lr: 0.000437  loss: 0.4699 (0.4771)
[11:06:44.917080] {"train_lr": 0.0004384352097625168, "train_loss": 0.47706614505677, "epoch": 586}
[11:06:44.917404] [11:06:44.917490] Training epoch 586 for 0:00:27
[11:06:44.917543] [11:06:44.922346] log_dir: ./exp/debug/cifar100-LT/debug
[11:06:46.521009] Epoch: [587]  [ 0/42]  eta: 0:01:07  lr: 0.000436  loss: 0.4461 (0.4461)  time: 1.5974  data: 1.0902  max mem: 9341
[11:06:56.700458] Epoch: [587]  [20/42]  eta: 0:00:12  lr: 0.000434  loss: 0.4819 (0.4769)  time: 0.5089  data: 0.0001  max mem: 9341
[11:07:06.855906] Epoch: [587]  [40/42]  eta: 0:00:01  lr: 0.000433  loss: 0.4770 (0.4756)  time: 0.5077  data: 0.0001  max mem: 9341
[11:07:07.361092] Epoch: [587]  [41/42]  eta: 0:00:00  lr: 0.000433  loss: 0.4815 (0.4761)  time: 0.5077  data: 0.0001  max mem: 9341
[11:07:07.519565] Epoch: [587] Total time: 0:00:22 (0.5380 s / it)
[11:07:07.539908] Averaged stats: lr: 0.000433  loss: 0.4815 (0.4795)
[11:07:12.115126] {"train_lr": 0.00043460827739151336, "train_loss": 0.4795087496084826, "epoch": 587}
[11:07:12.115478] [11:07:12.115570] Training epoch 587 for 0:00:27
[11:07:12.115621] [11:07:12.120484] log_dir: ./exp/debug/cifar100-LT/debug
[11:07:13.823641] Epoch: [588]  [ 0/42]  eta: 0:01:11  lr: 0.000432  loss: 0.4327 (0.4327)  time: 1.7020  data: 1.2078  max mem: 9341
[11:07:23.987278] Epoch: [588]  [20/42]  eta: 0:00:12  lr: 0.000431  loss: 0.4943 (0.4915)  time: 0.5081  data: 0.0001  max mem: 9341
[11:07:34.133766] Epoch: [588]  [40/42]  eta: 0:00:01  lr: 0.000429  loss: 0.4848 (0.4867)  time: 0.5073  data: 0.0001  max mem: 9341
[11:07:34.642067] Epoch: [588]  [41/42]  eta: 0:00:00  lr: 0.000429  loss: 0.4827 (0.4866)  time: 0.5073  data: 0.0001  max mem: 9341
[11:07:34.804990] Epoch: [588] Total time: 0:00:22 (0.5401 s / it)
[11:07:34.816263] Averaged stats: lr: 0.000429  loss: 0.4827 (0.4802)
[11:07:39.377677] {"train_lr": 0.00043079442345326233, "train_loss": 0.48018702190546764, "epoch": 588}
[11:07:39.378030] [11:07:39.378116] Training epoch 588 for 0:00:27
[11:07:39.378168] [11:07:39.382585] log_dir: ./exp/debug/cifar100-LT/debug
[11:07:40.919522] Epoch: [589]  [ 0/42]  eta: 0:01:04  lr: 0.000428  loss: 0.4517 (0.4517)  time: 1.5356  data: 1.0359  max mem: 9341
[11:07:51.099314] Epoch: [589]  [20/42]  eta: 0:00:12  lr: 0.000427  loss: 0.4696 (0.4671)  time: 0.5089  data: 0.0001  max mem: 9341
[11:08:01.263826] Epoch: [589]  [40/42]  eta: 0:00:01  lr: 0.000425  loss: 0.4666 (0.4704)  time: 0.5082  data: 0.0001  max mem: 9341
[11:08:01.770153] Epoch: [589]  [41/42]  eta: 0:00:00  lr: 0.000425  loss: 0.4735 (0.4708)  time: 0.5082  data: 0.0001  max mem: 9341
[11:08:01.942989] Epoch: [589] Total time: 0:00:22 (0.5372 s / it)
[11:08:01.951536] Averaged stats: lr: 0.000425  loss: 0.4735 (0.4742)
[11:08:06.544882] {"train_lr": 0.0004269937131160046, "train_loss": 0.47424762288019773, "epoch": 589}
[11:08:06.545310] [11:08:06.545395] Training epoch 589 for 0:00:27
[11:08:06.545505] [11:08:06.549993] log_dir: ./exp/debug/cifar100-LT/debug
[11:08:08.047075] Epoch: [590]  [ 0/42]  eta: 0:01:02  lr: 0.000424  loss: 0.4788 (0.4788)  time: 1.4958  data: 0.9891  max mem: 9341
[11:08:18.216308] Epoch: [590]  [20/42]  eta: 0:00:12  lr: 0.000423  loss: 0.4731 (0.4763)  time: 0.5084  data: 0.0001  max mem: 9341
[11:08:28.364840] Epoch: [590]  [40/42]  eta: 0:00:01  lr: 0.000422  loss: 0.4647 (0.4776)  time: 0.5074  data: 0.0001  max mem: 9341
[11:08:28.870008] Epoch: [590]  [41/42]  eta: 0:00:00  lr: 0.000422  loss: 0.4647 (0.4774)  time: 0.5074  data: 0.0001  max mem: 9341
[11:08:29.038508] Epoch: [590] Total time: 0:00:22 (0.5354 s / it)
[11:08:29.047013] Averaged stats: lr: 0.000422  loss: 0.4647 (0.4761)
[11:08:33.647438] {"train_lr": 0.00042320621132339154, "train_loss": 0.47612339364630835, "epoch": 590}
[11:08:33.647804] [11:08:33.647889] Training epoch 590 for 0:00:27
[11:08:33.647941] [11:08:33.652356] log_dir: ./exp/debug/cifar100-LT/debug
[11:08:35.135924] Epoch: [591]  [ 0/42]  eta: 0:01:02  lr: 0.000421  loss: 0.4431 (0.4431)  time: 1.4823  data: 0.9660  max mem: 9341
[11:08:45.302184] Epoch: [591]  [20/42]  eta: 0:00:12  lr: 0.000419  loss: 0.4659 (0.4713)  time: 0.5083  data: 0.0001  max mem: 9341
[11:08:55.446890] Epoch: [591]  [40/42]  eta: 0:00:01  lr: 0.000418  loss: 0.4724 (0.4712)  time: 0.5072  data: 0.0001  max mem: 9341
[11:08:55.951600] Epoch: [591]  [41/42]  eta: 0:00:00  lr: 0.000418  loss: 0.4738 (0.4720)  time: 0.5072  data: 0.0001  max mem: 9341
[11:08:56.111968] Epoch: [591] Total time: 0:00:22 (0.5348 s / it)
[11:08:56.117459] Averaged stats: lr: 0.000418  loss: 0.4738 (0.4769)
[11:09:00.640219] {"train_lr": 0.00041943198279338086, "train_loss": 0.47692631859154927, "epoch": 591}
[11:09:00.640613] [11:09:00.640694] Training epoch 591 for 0:00:26
[11:09:00.640744] [11:09:00.644940] log_dir: ./exp/debug/cifar100-LT/debug
[11:09:02.043406] Epoch: [592]  [ 0/42]  eta: 0:00:58  lr: 0.000417  loss: 0.4855 (0.4855)  time: 1.3972  data: 0.8894  max mem: 9341
[11:09:12.203321] Epoch: [592]  [20/42]  eta: 0:00:12  lr: 0.000415  loss: 0.4727 (0.4714)  time: 0.5079  data: 0.0001  max mem: 9341
[11:09:22.351467] Epoch: [592]  [40/42]  eta: 0:00:01  lr: 0.000414  loss: 0.4678 (0.4714)  time: 0.5074  data: 0.0001  max mem: 9341
[11:09:22.856867] Epoch: [592]  [41/42]  eta: 0:00:00  lr: 0.000414  loss: 0.4635 (0.4712)  time: 0.5073  data: 0.0001  max mem: 9341
[11:09:23.017124] Epoch: [592] Total time: 0:00:22 (0.5327 s / it)
[11:09:23.017879] Averaged stats: lr: 0.000414  loss: 0.4635 (0.4764)
[11:09:27.640736] {"train_lr": 0.00041567109201712325, "train_loss": 0.47639110932747525, "epoch": 592}
[11:09:27.641118] [11:09:27.641221] Training epoch 592 for 0:00:27
[11:09:27.641272] [11:09:27.645741] log_dir: ./exp/debug/cifar100-LT/debug
[11:09:29.230820] Epoch: [593]  [ 0/42]  eta: 0:01:06  lr: 0.000413  loss: 0.5036 (0.5036)  time: 1.5836  data: 1.0600  max mem: 9341
[11:09:39.400486] Epoch: [593]  [20/42]  eta: 0:00:12  lr: 0.000412  loss: 0.4663 (0.4661)  time: 0.5084  data: 0.0001  max mem: 9341
[11:09:49.552389] Epoch: [593]  [40/42]  eta: 0:00:01  lr: 0.000410  loss: 0.4670 (0.4707)  time: 0.5075  data: 0.0001  max mem: 9341
[11:09:50.059329] Epoch: [593]  [41/42]  eta: 0:00:00  lr: 0.000410  loss: 0.4750 (0.4719)  time: 0.5076  data: 0.0001  max mem: 9341
[11:09:50.218017] Epoch: [593] Total time: 0:00:22 (0.5374 s / it)
[11:09:50.221181] Averaged stats: lr: 0.000410  loss: 0.4750 (0.4755)
[11:09:54.741756] {"train_lr": 0.0004119236032578671, "train_loss": 0.47554762608238627, "epoch": 593}
[11:09:54.742143] [11:09:54.742230] Training epoch 593 for 0:00:27
[11:09:54.742281] [11:09:54.746788] log_dir: ./exp/debug/cifar100-LT/debug
[11:09:56.412989] Epoch: [594]  [ 0/42]  eta: 0:01:09  lr: 0.000409  loss: 0.4865 (0.4865)  time: 1.6650  data: 1.1661  max mem: 9341
[11:10:06.574215] Epoch: [594]  [20/42]  eta: 0:00:12  lr: 0.000408  loss: 0.4690 (0.4719)  time: 0.5080  data: 0.0001  max mem: 9341
[11:10:16.721295] Epoch: [594]  [40/42]  eta: 0:00:01  lr: 0.000407  loss: 0.4770 (0.4730)  time: 0.5073  data: 0.0001  max mem: 9341
[11:10:17.228285] Epoch: [594]  [41/42]  eta: 0:00:00  lr: 0.000407  loss: 0.4812 (0.4739)  time: 0.5074  data: 0.0001  max mem: 9341
[11:10:17.382106] Epoch: [594] Total time: 0:00:22 (0.5389 s / it)
[11:10:17.397487] Averaged stats: lr: 0.000407  loss: 0.4812 (0.4775)
[11:10:21.945932] {"train_lr": 0.00040818958054985324, "train_loss": 0.4774527847766876, "epoch": 594}
[11:10:21.946258] [11:10:21.946338] Training epoch 594 for 0:00:27
[11:10:21.946389] [11:10:21.950781] log_dir: ./exp/debug/cifar100-LT/debug
[11:10:23.468490] Epoch: [595]  [ 0/42]  eta: 0:01:03  lr: 0.000406  loss: 0.4973 (0.4973)  time: 1.5169  data: 1.0097  max mem: 9341
[11:10:33.658204] Epoch: [595]  [20/42]  eta: 0:00:12  lr: 0.000404  loss: 0.4766 (0.4807)  time: 0.5094  data: 0.0001  max mem: 9341
[11:10:43.791761] Epoch: [595]  [40/42]  eta: 0:00:01  lr: 0.000403  loss: 0.4818 (0.4833)  time: 0.5066  data: 0.0001  max mem: 9341
[11:10:44.298039] Epoch: [595]  [41/42]  eta: 0:00:00  lr: 0.000403  loss: 0.4845 (0.4847)  time: 0.5067  data: 0.0001  max mem: 9341
[11:10:44.469730] Epoch: [595] Total time: 0:00:22 (0.5362 s / it)
[11:10:44.472662] Averaged stats: lr: 0.000403  loss: 0.4845 (0.4777)
[11:10:49.013859] {"train_lr": 0.00040446908769722884, "train_loss": 0.4777083710900375, "epoch": 595}
[11:10:49.014232] [11:10:49.014314] Training epoch 595 for 0:00:27
[11:10:49.014365] [11:10:49.018781] log_dir: ./exp/debug/cifar100-LT/debug
[11:10:50.495989] Epoch: [596]  [ 0/42]  eta: 0:01:01  lr: 0.000402  loss: 0.4760 (0.4760)  time: 1.4759  data: 0.9611  max mem: 9341
[11:11:00.658743] Epoch: [596]  [20/42]  eta: 0:00:12  lr: 0.000401  loss: 0.4812 (0.4794)  time: 0.5081  data: 0.0001  max mem: 9341
[11:11:10.806235] Epoch: [596]  [40/42]  eta: 0:00:01  lr: 0.000399  loss: 0.4736 (0.4755)  time: 0.5073  data: 0.0001  max mem: 9341
[11:11:11.312693] Epoch: [596]  [41/42]  eta: 0:00:00  lr: 0.000399  loss: 0.4772 (0.4760)  time: 0.5074  data: 0.0001  max mem: 9341
[11:11:11.472333] Epoch: [596] Total time: 0:00:22 (0.5346 s / it)
[11:11:11.484569] Averaged stats: lr: 0.000399  loss: 0.4772 (0.4741)
[11:11:16.022165] {"train_lr": 0.00040076218827295146, "train_loss": 0.4741436037279311, "epoch": 596}
[11:11:16.022508] [11:11:16.022589] Training epoch 596 for 0:00:27
[11:11:16.022639] [11:11:16.027064] log_dir: ./exp/debug/cifar100-LT/debug
[11:11:17.712971] Epoch: [597]  [ 0/42]  eta: 0:01:10  lr: 0.000398  loss: 0.4602 (0.4602)  time: 1.6846  data: 1.1853  max mem: 9341
[11:11:27.899400] Epoch: [597]  [20/42]  eta: 0:00:12  lr: 0.000397  loss: 0.4850 (0.4826)  time: 0.5093  data: 0.0001  max mem: 9341
[11:11:38.052766] Epoch: [597]  [40/42]  eta: 0:00:01  lr: 0.000395  loss: 0.4799 (0.4821)  time: 0.5076  data: 0.0001  max mem: 9341
[11:11:38.559624] Epoch: [597]  [41/42]  eta: 0:00:00  lr: 0.000395  loss: 0.4857 (0.4830)  time: 0.5076  data: 0.0001  max mem: 9341
[11:11:38.724734] Epoch: [597] Total time: 0:00:22 (0.5404 s / it)
[11:11:38.727582] Averaged stats: lr: 0.000395  loss: 0.4857 (0.4774)
[11:11:43.417899] {"train_lr": 0.0003970689456177041, "train_loss": 0.47744666288296383, "epoch": 597}
[11:11:43.418267] [11:11:43.418366] Training epoch 597 for 0:00:27
[11:11:43.418420] [11:11:43.422673] log_dir: ./exp/debug/cifar100-LT/debug
[11:11:44.965013] Epoch: [598]  [ 0/42]  eta: 0:01:04  lr: 0.000395  loss: 0.4958 (0.4958)  time: 1.5411  data: 1.0322  max mem: 9341
[11:11:55.133140] Epoch: [598]  [20/42]  eta: 0:00:12  lr: 0.000393  loss: 0.4764 (0.4713)  time: 0.5083  data: 0.0001  max mem: 9341
[11:12:05.283797] Epoch: [598]  [40/42]  eta: 0:00:01  lr: 0.000392  loss: 0.4766 (0.4725)  time: 0.5075  data: 0.0001  max mem: 9341
[11:12:05.790090] Epoch: [598]  [41/42]  eta: 0:00:00  lr: 0.000392  loss: 0.4776 (0.4737)  time: 0.5075  data: 0.0001  max mem: 9341
[11:12:05.975548] Epoch: [598] Total time: 0:00:22 (0.5370 s / it)
[11:12:05.976360] Averaged stats: lr: 0.000392  loss: 0.4776 (0.4734)
[11:12:10.523618] {"train_lr": 0.0003933894228388143, "train_loss": 0.47337492200590314, "epoch": 598}
[11:12:10.524003] [11:12:10.524121] Training epoch 598 for 0:00:27
[11:12:10.524177] [11:12:10.528745] log_dir: ./exp/debug/cifar100-LT/debug
[11:12:12.215103] Epoch: [599]  [ 0/42]  eta: 0:01:10  lr: 0.000391  loss: 0.4925 (0.4925)  time: 1.6852  data: 1.1553  max mem: 9341
[11:12:22.405344] Epoch: [599]  [20/42]  eta: 0:00:12  lr: 0.000390  loss: 0.4637 (0.4743)  time: 0.5095  data: 0.0001  max mem: 9341
[11:12:32.572201] Epoch: [599]  [40/42]  eta: 0:00:01  lr: 0.000388  loss: 0.4692 (0.4712)  time: 0.5083  data: 0.0001  max mem: 9341
[11:12:33.077968] Epoch: [599]  [41/42]  eta: 0:00:00  lr: 0.000388  loss: 0.4678 (0.4711)  time: 0.5083  data: 0.0001  max mem: 9341
[11:12:33.252877] Epoch: [599] Total time: 0:00:22 (0.5410 s / it)
[11:12:33.256040] Averaged stats: lr: 0.000388  loss: 0.4678 (0.4751)
[11:12:37.873636] {"train_lr": 0.0003897236828091737, "train_loss": 0.47505942590179895, "epoch": 599}
[11:12:37.873987] [11:12:37.874070] Training epoch 599 for 0:00:27
[11:12:37.874139] [11:12:37.878529] log_dir: ./exp/debug/cifar100-LT/debug
[11:12:39.388227] Epoch: [600]  [ 0/42]  eta: 0:01:03  lr: 0.000387  loss: 0.4728 (0.4728)  time: 1.5081  data: 0.9896  max mem: 9341
[11:12:49.542975] Epoch: [600]  [20/42]  eta: 0:00:12  lr: 0.000386  loss: 0.4728 (0.4711)  time: 0.5077  data: 0.0001  max mem: 9341
[11:12:59.679183] Epoch: [600]  [40/42]  eta: 0:00:01  lr: 0.000384  loss: 0.4881 (0.4790)  time: 0.5068  data: 0.0001  max mem: 9341
[11:13:00.183448] Epoch: [600]  [41/42]  eta: 0:00:00  lr: 0.000384  loss: 0.4821 (0.4780)  time: 0.5067  data: 0.0001  max mem: 9341
[11:13:00.355295] Epoch: [600] Total time: 0:00:22 (0.5352 s / it)
[11:13:00.359316] Averaged stats: lr: 0.000384  loss: 0.4821 (0.4766)
[11:13:05.003542] {"train_lr": 0.00038607178816616556, "train_loss": 0.47659220901273547, "epoch": 600}
[11:13:05.003862] [11:13:05.003953] Training epoch 600 for 0:00:27
[11:13:05.004003] [11:13:05.009014] log_dir: ./exp/debug/cifar100-LT/debug
[11:13:06.515985] Epoch: [601]  [ 0/42]  eta: 0:01:03  lr: 0.000384  loss: 0.4789 (0.4789)  time: 1.5058  data: 0.9943  max mem: 9341
[11:13:16.715023] Epoch: [601]  [20/42]  eta: 0:00:12  lr: 0.000382  loss: 0.4724 (0.4720)  time: 0.5099  data: 0.0002  max mem: 9341
[11:13:26.863160] Epoch: [601]  [40/42]  eta: 0:00:01  lr: 0.000381  loss: 0.4653 (0.4702)  time: 0.5074  data: 0.0001  max mem: 9341
[11:13:27.368307] Epoch: [601]  [41/42]  eta: 0:00:00  lr: 0.000381  loss: 0.4653 (0.4707)  time: 0.5073  data: 0.0001  max mem: 9341
[11:13:27.531535] Epoch: [601] Total time: 0:00:22 (0.5362 s / it)
[11:13:27.539836] Averaged stats: lr: 0.000381  loss: 0.4653 (0.4773)
[11:13:32.051930] {"train_lr": 0.00038243380131059335, "train_loss": 0.47728039182367776, "epoch": 601}
[11:13:32.052336] [11:13:32.052457] Training epoch 601 for 0:00:27
[11:13:32.052509] [11:13:32.056756] log_dir: ./exp/debug/cifar100-LT/debug
[11:13:33.594745] Epoch: [602]  [ 0/42]  eta: 0:01:04  lr: 0.000380  loss: 0.5013 (0.5013)  time: 1.5367  data: 1.0236  max mem: 9341
[11:13:43.791003] Epoch: [602]  [20/42]  eta: 0:00:12  lr: 0.000379  loss: 0.4730 (0.4750)  time: 0.5098  data: 0.0001  max mem: 9341
[11:13:53.958678] Epoch: [602]  [40/42]  eta: 0:00:01  lr: 0.000377  loss: 0.4734 (0.4749)  time: 0.5083  data: 0.0001  max mem: 9341
[11:13:54.465258] Epoch: [602]  [41/42]  eta: 0:00:00  lr: 0.000377  loss: 0.4714 (0.4746)  time: 0.5084  data: 0.0001  max mem: 9341
[11:13:54.639936] Epoch: [602] Total time: 0:00:22 (0.5377 s / it)
[11:13:54.643915] Averaged stats: lr: 0.000377  loss: 0.4714 (0.4755)
[11:13:59.260868] {"train_lr": 0.0003788097844056148, "train_loss": 0.4754966887689772, "epoch": 602}
[11:13:59.261224] [11:13:59.261308] Training epoch 602 for 0:00:27
[11:13:59.261359] [11:13:59.265711] log_dir: ./exp/debug/cifar100-LT/debug
[11:14:00.746360] Epoch: [603]  [ 0/42]  eta: 0:01:02  lr: 0.000376  loss: 0.4429 (0.4429)  time: 1.4793  data: 0.9584  max mem: 9341
[11:14:10.912596] Epoch: [603]  [20/42]  eta: 0:00:12  lr: 0.000375  loss: 0.4817 (0.4774)  time: 0.5083  data: 0.0001  max mem: 9341
[11:14:21.064525] Epoch: [603]  [40/42]  eta: 0:00:01  lr: 0.000374  loss: 0.4738 (0.4767)  time: 0.5075  data: 0.0001  max mem: 9341
[11:14:21.568717] Epoch: [603]  [41/42]  eta: 0:00:00  lr: 0.000374  loss: 0.4738 (0.4763)  time: 0.5074  data: 0.0001  max mem: 9341
[11:14:21.747236] Epoch: [603] Total time: 0:00:22 (0.5353 s / it)
[11:14:21.749571] Averaged stats: lr: 0.000374  loss: 0.4738 (0.4770)
[11:14:26.363061] {"train_lr": 0.00037519979937568, "train_loss": 0.4769844960953508, "epoch": 603}
[11:14:26.363404] [11:14:26.363487] Training epoch 603 for 0:00:27
[11:14:26.363540] [11:14:26.368110] log_dir: ./exp/debug/cifar100-LT/debug
[11:14:27.890795] Epoch: [604]  [ 0/42]  eta: 0:01:03  lr: 0.000373  loss: 0.4458 (0.4458)  time: 1.5215  data: 1.0096  max mem: 9341
[11:14:38.075424] Epoch: [604]  [20/42]  eta: 0:00:12  lr: 0.000371  loss: 0.4846 (0.4820)  time: 0.5092  data: 0.0001  max mem: 9341
[11:14:48.241707] Epoch: [604]  [40/42]  eta: 0:00:01  lr: 0.000370  loss: 0.4754 (0.4825)  time: 0.5083  data: 0.0001  max mem: 9341
[11:14:48.747001] Epoch: [604]  [41/42]  eta: 0:00:00  lr: 0.000370  loss: 0.4754 (0.4824)  time: 0.5083  data: 0.0001  max mem: 9341
[11:14:48.906835] Epoch: [604] Total time: 0:00:22 (0.5366 s / it)
[11:14:48.910913] Averaged stats: lr: 0.000370  loss: 0.4754 (0.4764)
[11:14:53.531574] {"train_lr": 0.00037160390790547267, "train_loss": 0.47642551735043526, "epoch": 604}
[11:14:53.531934] [11:14:53.532017] Training epoch 604 for 0:00:27
[11:14:53.532068] [11:14:53.536651] log_dir: ./exp/debug/cifar100-LT/debug
[11:14:55.178550] Epoch: [605]  [ 0/42]  eta: 0:01:08  lr: 0.000369  loss: 0.4456 (0.4456)  time: 1.6403  data: 1.1477  max mem: 9341
[11:15:05.369250] Epoch: [605]  [20/42]  eta: 0:00:12  lr: 0.000368  loss: 0.4626 (0.4636)  time: 0.5095  data: 0.0001  max mem: 9341
[11:15:15.537504] Epoch: [605]  [40/42]  eta: 0:00:01  lr: 0.000366  loss: 0.4860 (0.4723)  time: 0.5084  data: 0.0001  max mem: 9341
[11:15:16.042786] Epoch: [605]  [41/42]  eta: 0:00:00  lr: 0.000366  loss: 0.4860 (0.4724)  time: 0.5083  data: 0.0001  max mem: 9341
[11:15:16.211475] Epoch: [605] Total time: 0:00:22 (0.5399 s / it)
[11:15:16.217125] Averaged stats: lr: 0.000366  loss: 0.4860 (0.4732)
[11:15:20.791571] {"train_lr": 0.0003680221714388567, "train_loss": 0.4732349555762041, "epoch": 605}
[11:15:20.791964] [11:15:20.792048] Training epoch 605 for 0:00:27
[11:15:20.792145] [11:15:20.796952] log_dir: ./exp/debug/cifar100-LT/debug
[11:15:22.389654] Epoch: [606]  [ 0/42]  eta: 0:01:06  lr: 0.000366  loss: 0.4689 (0.4689)  time: 1.5915  data: 1.0802  max mem: 9341
[11:15:32.559895] Epoch: [606]  [20/42]  eta: 0:00:12  lr: 0.000364  loss: 0.4733 (0.4721)  time: 0.5085  data: 0.0001  max mem: 9341
[11:15:42.702815] Epoch: [606]  [40/42]  eta: 0:00:01  lr: 0.000363  loss: 0.4774 (0.4739)  time: 0.5071  data: 0.0001  max mem: 9341
[11:15:43.210698] Epoch: [606]  [41/42]  eta: 0:00:00  lr: 0.000363  loss: 0.4792 (0.4741)  time: 0.5073  data: 0.0001  max mem: 9341
[11:15:43.380681] Epoch: [606] Total time: 0:00:22 (0.5377 s / it)
[11:15:43.384678] Averaged stats: lr: 0.000363  loss: 0.4792 (0.4745)
[11:15:48.074733] {"train_lr": 0.00036445465117782553, "train_loss": 0.4745268596424943, "epoch": 606}
[11:15:48.075181] [11:15:48.075270] Training epoch 606 for 0:00:27
[11:15:48.075322] [11:15:48.080298] log_dir: ./exp/debug/cifar100-LT/debug
[11:15:49.534909] Epoch: [607]  [ 0/42]  eta: 0:01:01  lr: 0.000362  loss: 0.4076 (0.4076)  time: 1.4533  data: 0.9482  max mem: 9341
[11:15:59.701349] Epoch: [607]  [20/42]  eta: 0:00:12  lr: 0.000361  loss: 0.4762 (0.4699)  time: 0.5083  data: 0.0001  max mem: 9341
[11:16:09.855216] Epoch: [607]  [40/42]  eta: 0:00:01  lr: 0.000359  loss: 0.4812 (0.4729)  time: 0.5076  data: 0.0001  max mem: 9341
[11:16:10.361272] Epoch: [607]  [41/42]  eta: 0:00:00  lr: 0.000359  loss: 0.4812 (0.4728)  time: 0.5076  data: 0.0001  max mem: 9341
[11:16:10.523694] Epoch: [607] Total time: 0:00:22 (0.5344 s / it)
[11:16:10.525938] Averaged stats: lr: 0.000359  loss: 0.4812 (0.4762)
[11:16:15.016142] {"train_lr": 0.0003609014080814576, "train_loss": 0.4761954978817985, "epoch": 607}
[11:16:15.016652] [11:16:15.016743] Training epoch 607 for 0:00:26
[11:16:15.016795] [11:16:15.022054] log_dir: ./exp/debug/cifar100-LT/debug
[11:16:16.494231] Epoch: [608]  [ 0/42]  eta: 0:01:01  lr: 0.000359  loss: 0.4591 (0.4591)  time: 1.4709  data: 0.9650  max mem: 9341
[11:16:26.661841] Epoch: [608]  [20/42]  eta: 0:00:12  lr: 0.000357  loss: 0.4485 (0.4634)  time: 0.5083  data: 0.0001  max mem: 9341
[11:16:36.812444] Epoch: [608]  [40/42]  eta: 0:00:01  lr: 0.000356  loss: 0.4946 (0.4745)  time: 0.5075  data: 0.0001  max mem: 9341
[11:16:37.318479] Epoch: [608]  [41/42]  eta: 0:00:00  lr: 0.000356  loss: 0.4886 (0.4743)  time: 0.5076  data: 0.0001  max mem: 9341
[11:16:37.491201] Epoch: [608] Total time: 0:00:22 (0.5350 s / it)
[11:16:37.497933] Averaged stats: lr: 0.000356  loss: 0.4886 (0.4760)
[11:16:42.217485] {"train_lr": 0.00035736250286487475, "train_loss": 0.4759758574267228, "epoch": 608}
[11:16:42.217783] [11:16:42.217865] Training epoch 608 for 0:00:27
[11:16:42.217938] [11:16:42.222323] log_dir: ./exp/debug/cifar100-LT/debug
[11:16:43.851434] Epoch: [609]  [ 0/42]  eta: 0:01:08  lr: 0.000355  loss: 0.4817 (0.4817)  time: 1.6280  data: 1.1123  max mem: 9341
[11:16:54.056847] Epoch: [609]  [20/42]  eta: 0:00:12  lr: 0.000354  loss: 0.4590 (0.4659)  time: 0.5102  data: 0.0001  max mem: 9341
[11:17:04.209401] Epoch: [609]  [40/42]  eta: 0:00:01  lr: 0.000352  loss: 0.4716 (0.4693)  time: 0.5076  data: 0.0001  max mem: 9341
[11:17:04.715773] Epoch: [609]  [41/42]  eta: 0:00:00  lr: 0.000352  loss: 0.4716 (0.4693)  time: 0.5077  data: 0.0001  max mem: 9341
[11:17:04.883485] Epoch: [609] Total time: 0:00:22 (0.5395 s / it)
[11:17:04.896745] Averaged stats: lr: 0.000352  loss: 0.4716 (0.4701)
[11:17:09.429585] {"train_lr": 0.0003538379959982016, "train_loss": 0.4701171052597818, "epoch": 609}
[11:17:09.429889] [11:17:09.429972] Training epoch 609 for 0:00:27
[11:17:09.430081] [11:17:09.434462] log_dir: ./exp/debug/cifar100-LT/debug
[11:17:10.853582] Epoch: [610]  [ 0/42]  eta: 0:00:59  lr: 0.000351  loss: 0.5072 (0.5072)  time: 1.4176  data: 0.9206  max mem: 9341
[11:17:21.020738] Epoch: [610]  [20/42]  eta: 0:00:12  lr: 0.000350  loss: 0.4757 (0.4753)  time: 0.5083  data: 0.0001  max mem: 9341
[11:17:31.167097] Epoch: [610]  [40/42]  eta: 0:00:01  lr: 0.000349  loss: 0.4754 (0.4760)  time: 0.5073  data: 0.0001  max mem: 9341
[11:17:31.673215] Epoch: [610]  [41/42]  eta: 0:00:00  lr: 0.000349  loss: 0.4754 (0.4763)  time: 0.5074  data: 0.0001  max mem: 9341
[11:17:31.846458] Epoch: [610] Total time: 0:00:22 (0.5336 s / it)
[11:17:31.847283] Averaged stats: lr: 0.000349  loss: 0.4754 (0.4714)
[11:17:36.418616] {"train_lr": 0.0003503279477055358, "train_loss": 0.47136595224340755, "epoch": 610}
[11:17:36.418992] [11:17:36.419078] Training epoch 610 for 0:00:26
[11:17:36.419129] [11:17:36.423408] log_dir: ./exp/debug/cifar100-LT/debug
[11:17:37.959456] Epoch: [611]  [ 0/42]  eta: 0:01:04  lr: 0.000348  loss: 0.4793 (0.4793)  time: 1.5347  data: 1.0312  max mem: 9341
[11:17:48.121089] Epoch: [611]  [20/42]  eta: 0:00:12  lr: 0.000347  loss: 0.4806 (0.4756)  time: 0.5080  data: 0.0001  max mem: 9341
[11:17:58.275010] Epoch: [611]  [40/42]  eta: 0:00:01  lr: 0.000345  loss: 0.4771 (0.4752)  time: 0.5077  data: 0.0001  max mem: 9341
[11:17:58.780227] Epoch: [611]  [41/42]  eta: 0:00:00  lr: 0.000345  loss: 0.4771 (0.4747)  time: 0.5076  data: 0.0001  max mem: 9341
[11:17:58.935704] Epoch: [611] Total time: 0:00:22 (0.5360 s / it)
[11:17:58.944669] Averaged stats: lr: 0.000345  loss: 0.4771 (0.4732)
[11:18:03.485773] {"train_lr": 0.00034683241796391916, "train_loss": 0.4731844931486107, "epoch": 611}
[11:18:03.486103] [11:18:03.486184] Training epoch 611 for 0:00:27
[11:18:03.486233] [11:18:03.490622] log_dir: ./exp/debug/cifar100-LT/debug
[11:18:05.114777] Epoch: [612]  [ 0/42]  eta: 0:01:08  lr: 0.000344  loss: 0.4857 (0.4857)  time: 1.6231  data: 1.1260  max mem: 9341
[11:18:15.283623] Epoch: [612]  [20/42]  eta: 0:00:12  lr: 0.000343  loss: 0.4821 (0.4836)  time: 0.5084  data: 0.0001  max mem: 9341
[11:18:25.438623] Epoch: [612]  [40/42]  eta: 0:00:01  lr: 0.000342  loss: 0.4677 (0.4767)  time: 0.5077  data: 0.0001  max mem: 9341
[11:18:25.944706] Epoch: [612]  [41/42]  eta: 0:00:00  lr: 0.000342  loss: 0.4742 (0.4775)  time: 0.5077  data: 0.0001  max mem: 9341
[11:18:26.115638] Epoch: [612] Total time: 0:00:22 (0.5387 s / it)
[11:18:26.116501] Averaged stats: lr: 0.000342  loss: 0.4742 (0.4757)
[11:18:30.577896] {"train_lr": 0.00034335146650231013, "train_loss": 0.47573774254747797, "epoch": 612}
[11:18:30.578247] [11:18:30.578331] Training epoch 612 for 0:00:27
[11:18:30.578383] [11:18:30.582709] log_dir: ./exp/debug/cifar100-LT/debug
[11:18:32.038877] Epoch: [613]  [ 0/42]  eta: 0:01:01  lr: 0.000341  loss: 0.4580 (0.4580)  time: 1.4546  data: 0.9426  max mem: 9341
[11:18:42.196827] Epoch: [613]  [20/42]  eta: 0:00:12  lr: 0.000340  loss: 0.4795 (0.4826)  time: 0.5078  data: 0.0001  max mem: 9341
[11:18:52.338224] Epoch: [613]  [40/42]  eta: 0:00:01  lr: 0.000338  loss: 0.4722 (0.4807)  time: 0.5070  data: 0.0001  max mem: 9341
[11:18:52.841969] Epoch: [613]  [41/42]  eta: 0:00:00  lr: 0.000338  loss: 0.4750 (0.4807)  time: 0.5070  data: 0.0001  max mem: 9341
[11:18:53.011763] Epoch: [613] Total time: 0:00:22 (0.5340 s / it)
[11:18:53.020679] Averaged stats: lr: 0.000338  loss: 0.4750 (0.4779)
[11:18:57.558384] {"train_lr": 0.00033988515280056474, "train_loss": 0.4779174120298454, "epoch": 613}
[11:18:57.558760] [11:18:57.558843] Training epoch 613 for 0:00:26
[11:18:57.558894] [11:18:57.563188] log_dir: ./exp/debug/cifar100-LT/debug
[11:18:59.132273] Epoch: [614]  [ 0/42]  eta: 0:01:05  lr: 0.000338  loss: 0.4968 (0.4968)  time: 1.5678  data: 1.0643  max mem: 9341
[11:19:09.286293] Epoch: [614]  [20/42]  eta: 0:00:12  lr: 0.000336  loss: 0.4805 (0.4778)  time: 0.5076  data: 0.0001  max mem: 9341
[11:19:19.444361] Epoch: [614]  [40/42]  eta: 0:00:01  lr: 0.000335  loss: 0.4826 (0.4797)  time: 0.5079  data: 0.0001  max mem: 9341
[11:19:19.948930] Epoch: [614]  [41/42]  eta: 0:00:00  lr: 0.000335  loss: 0.4830 (0.4801)  time: 0.5078  data: 0.0001  max mem: 9341
[11:19:20.113624] Epoch: [614] Total time: 0:00:22 (0.5369 s / it)
[11:19:20.120058] Averaged stats: lr: 0.000335  loss: 0.4830 (0.4740)
[11:19:24.663262] {"train_lr": 0.00033643353608842086, "train_loss": 0.4739513257074924, "epoch": 614}
[11:19:24.663601] [11:19:24.663678] Training epoch 614 for 0:00:27
[11:19:24.663729] [11:19:24.667964] log_dir: ./exp/debug/cifar100-LT/debug
[11:19:26.323580] Epoch: [615]  [ 0/42]  eta: 0:01:09  lr: 0.000334  loss: 0.4696 (0.4696)  time: 1.6543  data: 1.1449  max mem: 9341
[11:19:36.482875] Epoch: [615]  [20/42]  eta: 0:00:12  lr: 0.000333  loss: 0.4682 (0.4710)  time: 0.5079  data: 0.0001  max mem: 9341
[11:19:46.612097] Epoch: [615]  [40/42]  eta: 0:00:01  lr: 0.000332  loss: 0.4701 (0.4763)  time: 0.5064  data: 0.0001  max mem: 9341
[11:19:47.117085] Epoch: [615]  [41/42]  eta: 0:00:00  lr: 0.000332  loss: 0.4701 (0.4757)  time: 0.5063  data: 0.0001  max mem: 9341
[11:19:47.287658] Epoch: [615] Total time: 0:00:22 (0.5386 s / it)
[11:19:47.290767] Averaged stats: lr: 0.000332  loss: 0.4701 (0.4724)
[11:19:51.942856] {"train_lr": 0.0003329966753444841, "train_loss": 0.4723623673475924, "epoch": 615}
[11:19:51.943209] [11:19:51.943291] Training epoch 615 for 0:00:27
[11:19:51.943341] [11:19:51.947866] log_dir: ./exp/debug/cifar100-LT/debug
[11:19:53.629685] Epoch: [616]  [ 0/42]  eta: 0:01:10  lr: 0.000331  loss: 0.5223 (0.5223)  time: 1.6806  data: 1.1661  max mem: 9341
[11:20:03.808515] Epoch: [616]  [20/42]  eta: 0:00:12  lr: 0.000329  loss: 0.4609 (0.4684)  time: 0.5089  data: 0.0001  max mem: 9341
[11:20:13.959415] Epoch: [616]  [40/42]  eta: 0:00:01  lr: 0.000328  loss: 0.4792 (0.4726)  time: 0.5075  data: 0.0001  max mem: 9341
[11:20:14.466298] Epoch: [616]  [41/42]  eta: 0:00:00  lr: 0.000328  loss: 0.4792 (0.4726)  time: 0.5075  data: 0.0001  max mem: 9341
[11:20:14.639680] Epoch: [616] Total time: 0:00:22 (0.5403 s / it)
[11:20:14.645623] Averaged stats: lr: 0.000328  loss: 0.4792 (0.4730)
[11:20:19.151884] {"train_lr": 0.0003295746292952219, "train_loss": 0.47295897400804926, "epoch": 616}
[11:20:19.152274] [11:20:19.152359] Training epoch 616 for 0:00:27
[11:20:19.152412] [11:20:19.156649] log_dir: ./exp/debug/cifar100-LT/debug
[11:20:20.752028] Epoch: [617]  [ 0/42]  eta: 0:01:06  lr: 0.000327  loss: 0.5107 (0.5107)  time: 1.5941  data: 1.0766  max mem: 9341
[11:20:30.907738] Epoch: [617]  [20/42]  eta: 0:00:12  lr: 0.000326  loss: 0.4778 (0.4750)  time: 0.5077  data: 0.0001  max mem: 9341
[11:20:41.052639] Epoch: [617]  [40/42]  eta: 0:00:01  lr: 0.000325  loss: 0.4658 (0.4726)  time: 0.5072  data: 0.0001  max mem: 9341
[11:20:41.558904] Epoch: [617]  [41/42]  eta: 0:00:00  lr: 0.000325  loss: 0.4629 (0.4722)  time: 0.5073  data: 0.0001  max mem: 9341
[11:20:41.722577] Epoch: [617] Total time: 0:00:22 (0.5373 s / it)
[11:20:41.732443] Averaged stats: lr: 0.000325  loss: 0.4629 (0.4748)
[11:20:46.272657] {"train_lr": 0.00032616745641395983, "train_loss": 0.47481971074427876, "epoch": 617}
[11:20:46.273071] [11:20:46.273160] Training epoch 617 for 0:00:27
[11:20:46.273212] [11:20:46.278097] log_dir: ./exp/debug/cifar100-LT/debug
[11:20:47.712916] Epoch: [618]  [ 0/42]  eta: 0:01:00  lr: 0.000324  loss: 0.4588 (0.4588)  time: 1.4334  data: 0.9095  max mem: 9341
[11:20:57.908948] Epoch: [618]  [20/42]  eta: 0:00:12  lr: 0.000323  loss: 0.4861 (0.4800)  time: 0.5097  data: 0.0001  max mem: 9341
[11:21:08.054876] Epoch: [618]  [40/42]  eta: 0:00:01  lr: 0.000321  loss: 0.4723 (0.4791)  time: 0.5073  data: 0.0001  max mem: 9341
[11:21:08.559969] Epoch: [618]  [41/42]  eta: 0:00:00  lr: 0.000321  loss: 0.4723 (0.4793)  time: 0.5072  data: 0.0001  max mem: 9341
[11:21:08.721171] Epoch: [618] Total time: 0:00:22 (0.5344 s / it)
[11:21:08.731901] Averaged stats: lr: 0.000321  loss: 0.4723 (0.4738)
[11:21:13.320374] {"train_lr": 0.00032277521491988123, "train_loss": 0.4737883884282339, "epoch": 618}
[11:21:13.320741] [11:21:13.320824] Training epoch 618 for 0:00:27
[11:21:13.320878] [11:21:13.325168] log_dir: ./exp/debug/cifar100-LT/debug
[11:21:14.775354] Epoch: [619]  [ 0/42]  eta: 0:01:00  lr: 0.000320  loss: 0.4767 (0.4767)  time: 1.4490  data: 0.9441  max mem: 9341
[11:21:24.958348] Epoch: [619]  [20/42]  eta: 0:00:12  lr: 0.000319  loss: 0.4791 (0.4793)  time: 0.5091  data: 0.0001  max mem: 9341
[11:21:35.118688] Epoch: [619]  [40/42]  eta: 0:00:01  lr: 0.000318  loss: 0.4630 (0.4772)  time: 0.5080  data: 0.0001  max mem: 9341
[11:21:35.624623] Epoch: [619]  [41/42]  eta: 0:00:00  lr: 0.000318  loss: 0.4596 (0.4767)  time: 0.5081  data: 0.0001  max mem: 9341
[11:21:35.789426] Epoch: [619] Total time: 0:00:22 (0.5349 s / it)
[11:21:35.790971] Averaged stats: lr: 0.000318  loss: 0.4596 (0.4750)
[11:21:40.464892] {"train_lr": 0.00031939796277703476, "train_loss": 0.4750144345064958, "epoch": 619}
[11:21:40.465209] [11:21:40.465293] Training epoch 619 for 0:00:27
[11:21:40.465344] [11:21:40.469745] log_dir: ./exp/debug/cifar100-LT/debug
[11:21:42.042841] Epoch: [620]  [ 0/42]  eta: 0:01:06  lr: 0.000317  loss: 0.5018 (0.5018)  time: 1.5720  data: 1.0699  max mem: 9341
[11:21:52.229664] Epoch: [620]  [20/42]  eta: 0:00:12  lr: 0.000316  loss: 0.4638 (0.4687)  time: 0.5093  data: 0.0001  max mem: 9341
[11:22:02.399500] Epoch: [620]  [40/42]  eta: 0:00:01  lr: 0.000315  loss: 0.4703 (0.4687)  time: 0.5084  data: 0.0001  max mem: 9341
[11:22:02.905561] Epoch: [620]  [41/42]  eta: 0:00:00  lr: 0.000315  loss: 0.4703 (0.4680)  time: 0.5085  data: 0.0001  max mem: 9341
[11:22:03.078916] Epoch: [620] Total time: 0:00:22 (0.5383 s / it)
[11:22:03.079735] Averaged stats: lr: 0.000315  loss: 0.4703 (0.4742)
[11:22:07.779384] {"train_lr": 0.00031603575769334057, "train_loss": 0.47420683982116835, "epoch": 620}
[11:22:07.779783] [11:22:07.779889] Training epoch 620 for 0:00:27
[11:22:07.779942] [11:22:07.784810] log_dir: ./exp/debug/cifar100-LT/debug
[11:22:09.289794] Epoch: [621]  [ 0/42]  eta: 0:01:03  lr: 0.000314  loss: 0.4490 (0.4490)  time: 1.5037  data: 0.9863  max mem: 9341
[11:22:19.520190] Epoch: [621]  [20/42]  eta: 0:00:12  lr: 0.000313  loss: 0.4660 (0.4673)  time: 0.5115  data: 0.0001  max mem: 9341
[11:22:29.693298] Epoch: [621]  [40/42]  eta: 0:00:01  lr: 0.000311  loss: 0.4710 (0.4706)  time: 0.5086  data: 0.0001  max mem: 9341
[11:22:30.199716] Epoch: [621]  [41/42]  eta: 0:00:00  lr: 0.000311  loss: 0.4735 (0.4710)  time: 0.5085  data: 0.0001  max mem: 9341
[11:22:30.364775] Epoch: [621] Total time: 0:00:22 (0.5376 s / it)
[11:22:30.372937] Averaged stats: lr: 0.000311  loss: 0.4735 (0.4736)
[11:22:34.991141] {"train_lr": 0.0003126886571196069, "train_loss": 0.47357758649048354, "epoch": 621}
[11:22:34.991475] [11:22:34.991563] Training epoch 621 for 0:00:27
[11:22:34.991613] [11:22:34.996293] log_dir: ./exp/debug/cifar100-LT/debug
[11:22:36.719976] Epoch: [622]  [ 0/42]  eta: 0:01:12  lr: 0.000310  loss: 0.4571 (0.4571)  time: 1.7229  data: 1.2268  max mem: 9341
[11:22:46.876918] Epoch: [622]  [20/42]  eta: 0:00:12  lr: 0.000309  loss: 0.4744 (0.4746)  time: 0.5078  data: 0.0001  max mem: 9341
[11:22:57.019418] Epoch: [622]  [40/42]  eta: 0:00:01  lr: 0.000308  loss: 0.4676 (0.4715)  time: 0.5071  data: 0.0001  max mem: 9341
[11:22:57.521830] Epoch: [622]  [41/42]  eta: 0:00:00  lr: 0.000308  loss: 0.4654 (0.4714)  time: 0.5069  data: 0.0001  max mem: 9341
[11:22:57.684792] Epoch: [622] Total time: 0:00:22 (0.5402 s / it)
[11:22:57.693470] Averaged stats: lr: 0.000308  loss: 0.4654 (0.4722)
[11:23:02.320056] {"train_lr": 0.00030935671824854793, "train_loss": 0.4722075653927667, "epoch": 622}
[11:23:02.320540] [11:23:02.320634] Training epoch 622 for 0:00:27
[11:23:02.320686] [11:23:02.325626] log_dir: ./exp/debug/cifar100-LT/debug
[11:23:04.053905] Epoch: [623]  [ 0/42]  eta: 0:01:12  lr: 0.000307  loss: 0.4779 (0.4779)  time: 1.7270  data: 1.2284  max mem: 9341
[11:23:14.223877] Epoch: [623]  [20/42]  eta: 0:00:12  lr: 0.000306  loss: 0.4657 (0.4657)  time: 0.5084  data: 0.0001  max mem: 9341
[11:23:24.372574] Epoch: [623]  [40/42]  eta: 0:00:01  lr: 0.000305  loss: 0.4558 (0.4633)  time: 0.5074  data: 0.0001  max mem: 9341
[11:23:24.878387] Epoch: [623]  [41/42]  eta: 0:00:00  lr: 0.000305  loss: 0.4558 (0.4637)  time: 0.5075  data: 0.0001  max mem: 9341
[11:23:25.035919] Epoch: [623] Total time: 0:00:22 (0.5407 s / it)
[11:23:25.043361] Averaged stats: lr: 0.000305  loss: 0.4558 (0.4716)
[11:23:29.540017] {"train_lr": 0.00030603999801380585, "train_loss": 0.47159130356851076, "epoch": 623}
[11:23:29.540419] [11:23:29.540515] Training epoch 623 for 0:00:27
[11:23:29.540565] [11:23:29.544832] log_dir: ./exp/debug/cifar100-LT/debug
[11:23:31.227800] Epoch: [624]  [ 0/42]  eta: 0:01:10  lr: 0.000304  loss: 0.5039 (0.5039)  time: 1.6818  data: 1.1889  max mem: 9341
[11:23:41.390072] Epoch: [624]  [20/42]  eta: 0:00:12  lr: 0.000303  loss: 0.4739 (0.4732)  time: 0.5081  data: 0.0001  max mem: 9341
[11:23:51.545888] Epoch: [624]  [40/42]  eta: 0:00:01  lr: 0.000301  loss: 0.4594 (0.4682)  time: 0.5077  data: 0.0001  max mem: 9341
[11:23:52.051264] Epoch: [624]  [41/42]  eta: 0:00:00  lr: 0.000301  loss: 0.4594 (0.4674)  time: 0.5077  data: 0.0001  max mem: 9341
[11:23:52.217755] Epoch: [624] Total time: 0:00:22 (0.5398 s / it)
[11:23:52.220190] Averaged stats: lr: 0.000301  loss: 0.4594 (0.4714)
[11:23:56.817292] {"train_lr": 0.0003027385530889782, "train_loss": 0.47142949735834483, "epoch": 624}
[11:23:56.817662] [11:23:56.817748] Training epoch 624 for 0:00:27
[11:23:56.817815] [11:23:56.822186] log_dir: ./exp/debug/cifar100-LT/debug
[11:23:58.250425] Epoch: [625]  [ 0/42]  eta: 0:00:59  lr: 0.000301  loss: 0.4729 (0.4729)  time: 1.4270  data: 0.9102  max mem: 9341
[11:24:08.421591] Epoch: [625]  [20/42]  eta: 0:00:12  lr: 0.000299  loss: 0.4744 (0.4696)  time: 0.5085  data: 0.0001  max mem: 9341
[11:24:18.571140] Epoch: [625]  [40/42]  eta: 0:00:01  lr: 0.000298  loss: 0.4702 (0.4719)  time: 0.5074  data: 0.0001  max mem: 9341
[11:24:19.078050] Epoch: [625]  [41/42]  eta: 0:00:00  lr: 0.000298  loss: 0.4693 (0.4715)  time: 0.5074  data: 0.0001  max mem: 9341
[11:24:19.240783] Epoch: [625] Total time: 0:00:22 (0.5338 s / it)
[11:24:19.241496] Averaged stats: lr: 0.000298  loss: 0.4693 (0.4727)
[11:24:23.829619] {"train_lr": 0.00029945243988664894, "train_loss": 0.4726944189696085, "epoch": 625}
[11:24:23.829969] [11:24:23.830051] Training epoch 625 for 0:00:27
[11:24:23.830101] [11:24:23.834451] log_dir: ./exp/debug/cifar100-LT/debug
[11:24:25.507298] Epoch: [626]  [ 0/42]  eta: 0:01:10  lr: 0.000297  loss: 0.4592 (0.4592)  time: 1.6715  data: 1.1678  max mem: 9341
[11:24:35.717045] Epoch: [626]  [20/42]  eta: 0:00:12  lr: 0.000296  loss: 0.4726 (0.4758)  time: 0.5104  data: 0.0001  max mem: 9341
[11:24:45.854293] Epoch: [626]  [40/42]  eta: 0:00:01  lr: 0.000295  loss: 0.4800 (0.4766)  time: 0.5068  data: 0.0001  max mem: 9341
[11:24:46.360207] Epoch: [626]  [41/42]  eta: 0:00:00  lr: 0.000295  loss: 0.4812 (0.4770)  time: 0.5068  data: 0.0001  max mem: 9341
[11:24:46.533126] Epoch: [626] Total time: 0:00:22 (0.5404 s / it)
[11:24:46.533841] Averaged stats: lr: 0.000295  loss: 0.4812 (0.4754)
[11:24:51.109037] {"train_lr": 0.00029618171455742773, "train_loss": 0.47541670377055806, "epoch": 626}
[11:24:51.109473] [11:24:51.109567] Training epoch 626 for 0:00:27
[11:24:51.109620] [11:24:51.114625] log_dir: ./exp/debug/cifar100-LT/debug
[11:24:52.712832] Epoch: [627]  [ 0/42]  eta: 0:01:07  lr: 0.000294  loss: 0.4908 (0.4908)  time: 1.5965  data: 1.0901  max mem: 9341
[11:25:02.874084] Epoch: [627]  [20/42]  eta: 0:00:12  lr: 0.000293  loss: 0.4783 (0.4806)  time: 0.5080  data: 0.0001  max mem: 9341
[11:25:13.027501] Epoch: [627]  [40/42]  eta: 0:00:01  lr: 0.000292  loss: 0.4842 (0.4794)  time: 0.5076  data: 0.0001  max mem: 9341
[11:25:13.534037] Epoch: [627]  [41/42]  eta: 0:00:00  lr: 0.000292  loss: 0.4842 (0.4785)  time: 0.5076  data: 0.0001  max mem: 9341
[11:25:13.689149] Epoch: [627] Total time: 0:00:22 (0.5375 s / it)
[11:25:13.699075] Averaged stats: lr: 0.000292  loss: 0.4842 (0.4715)
[11:25:18.304971] {"train_lr": 0.0002929264329889846, "train_loss": 0.47154811432673815, "epoch": 627}
[11:25:18.305320] [11:25:18.305400] Training epoch 627 for 0:00:27
[11:25:18.305450] [11:25:18.309860] log_dir: ./exp/debug/cifar100-LT/debug
[11:25:19.910082] Epoch: [628]  [ 0/42]  eta: 0:01:07  lr: 0.000291  loss: 0.4869 (0.4869)  time: 1.5987  data: 1.0861  max mem: 9341
[11:25:30.132184] Epoch: [628]  [20/42]  eta: 0:00:12  lr: 0.000290  loss: 0.4786 (0.4763)  time: 0.5110  data: 0.0001  max mem: 9341
[11:25:40.299610] Epoch: [628]  [40/42]  eta: 0:00:01  lr: 0.000288  loss: 0.4730 (0.4753)  time: 0.5083  data: 0.0001  max mem: 9341
[11:25:40.805407] Epoch: [628]  [41/42]  eta: 0:00:00  lr: 0.000288  loss: 0.4718 (0.4747)  time: 0.5083  data: 0.0001  max mem: 9341
[11:25:40.968904] Epoch: [628] Total time: 0:00:22 (0.5395 s / it)
[11:25:40.969599] Averaged stats: lr: 0.000288  loss: 0.4718 (0.4756)
[11:25:45.596994] {"train_lr": 0.000289686650805101, "train_loss": 0.4756135151145004, "epoch": 628}
[11:25:45.597331] [11:25:45.597412] Training epoch 628 for 0:00:27
[11:25:45.597461] [11:25:45.601718] log_dir: ./exp/debug/cifar100-LT/debug
[11:25:47.304024] Epoch: [629]  [ 0/42]  eta: 0:01:11  lr: 0.000288  loss: 0.4392 (0.4392)  time: 1.7011  data: 1.1879  max mem: 9341
[11:25:57.469029] Epoch: [629]  [20/42]  eta: 0:00:12  lr: 0.000286  loss: 0.4651 (0.4605)  time: 0.5082  data: 0.0001  max mem: 9341
[11:26:07.617253] Epoch: [629]  [40/42]  eta: 0:00:01  lr: 0.000285  loss: 0.4806 (0.4670)  time: 0.5074  data: 0.0001  max mem: 9341
[11:26:08.121944] Epoch: [629]  [41/42]  eta: 0:00:00  lr: 0.000285  loss: 0.4806 (0.4691)  time: 0.5074  data: 0.0001  max mem: 9341
[11:26:08.275573] Epoch: [629] Total time: 0:00:22 (0.5399 s / it)
[11:26:08.290202] Averaged stats: lr: 0.000285  loss: 0.4806 (0.4702)
[11:26:12.855551] {"train_lr": 0.0002864624233647148, "train_loss": 0.47023647668815793, "epoch": 629}
[11:26:12.855898] [11:26:12.855985] Training epoch 629 for 0:00:27
[11:26:12.856037] [11:26:12.860841] log_dir: ./exp/debug/cifar100-LT/debug
[11:26:14.541075] Epoch: [630]  [ 0/42]  eta: 0:01:10  lr: 0.000284  loss: 0.4345 (0.4345)  time: 1.6793  data: 1.1727  max mem: 9341
[11:26:24.699477] Epoch: [630]  [20/42]  eta: 0:00:12  lr: 0.000283  loss: 0.4837 (0.4835)  time: 0.5079  data: 0.0001  max mem: 9341
[11:26:34.855535] Epoch: [630]  [40/42]  eta: 0:00:01  lr: 0.000282  loss: 0.4809 (0.4810)  time: 0.5078  data: 0.0001  max mem: 9341
[11:26:35.360236] Epoch: [630]  [41/42]  eta: 0:00:00  lr: 0.000282  loss: 0.4783 (0.4802)  time: 0.5077  data: 0.0001  max mem: 9341
[11:26:35.515696] Epoch: [630] Total time: 0:00:22 (0.5394 s / it)
[11:26:35.526052] Averaged stats: lr: 0.000282  loss: 0.4783 (0.4747)
[11:26:40.157903] {"train_lr": 0.00028325380576097755, "train_loss": 0.47468313663488343, "epoch": 630}
[11:26:40.158264] [11:26:40.158347] Training epoch 630 for 0:00:27
[11:26:40.158400] [11:26:40.162627] log_dir: ./exp/debug/cifar100-LT/debug
[11:26:41.839425] Epoch: [631]  [ 0/42]  eta: 0:01:10  lr: 0.000281  loss: 0.4360 (0.4360)  time: 1.6758  data: 1.1799  max mem: 9341
[11:26:52.004145] Epoch: [631]  [20/42]  eta: 0:00:12  lr: 0.000280  loss: 0.4796 (0.4692)  time: 0.5082  data: 0.0001  max mem: 9341
[11:27:02.158321] Epoch: [631]  [40/42]  eta: 0:00:01  lr: 0.000279  loss: 0.4591 (0.4689)  time: 0.5077  data: 0.0001  max mem: 9341
[11:27:02.664509] Epoch: [631]  [41/42]  eta: 0:00:00  lr: 0.000279  loss: 0.4653 (0.4696)  time: 0.5076  data: 0.0001  max mem: 9341
[11:27:02.833476] Epoch: [631] Total time: 0:00:22 (0.5398 s / it)
[11:27:02.841951] Averaged stats: lr: 0.000279  loss: 0.4653 (0.4734)
[11:27:07.316187] {"train_lr": 0.00028006085282031144, "train_loss": 0.4733905487117313, "epoch": 631}
[11:27:07.316577] [11:27:07.316663] Training epoch 631 for 0:00:27
[11:27:07.316715] [11:27:07.320933] log_dir: ./exp/debug/cifar100-LT/debug
[11:27:09.024605] Epoch: [632]  [ 0/42]  eta: 0:01:11  lr: 0.000278  loss: 0.4891 (0.4891)  time: 1.7013  data: 1.1837  max mem: 9341
[11:27:19.183491] Epoch: [632]  [20/42]  eta: 0:00:12  lr: 0.000277  loss: 0.4850 (0.4827)  time: 0.5079  data: 0.0001  max mem: 9341
[11:27:29.323966] Epoch: [632]  [40/42]  eta: 0:00:01  lr: 0.000276  loss: 0.4745 (0.4761)  time: 0.5070  data: 0.0001  max mem: 9341
[11:27:29.831963] Epoch: [632]  [41/42]  eta: 0:00:00  lr: 0.000276  loss: 0.4728 (0.4755)  time: 0.5070  data: 0.0001  max mem: 9341
[11:27:30.003090] Epoch: [632] Total time: 0:00:22 (0.5400 s / it)
[11:27:30.004028] Averaged stats: lr: 0.000276  loss: 0.4728 (0.4729)
[11:27:34.666027] {"train_lr": 0.0002768836191014714, "train_loss": 0.47292795458010267, "epoch": 632}
[11:27:34.666484] [11:27:34.666579] Training epoch 632 for 0:00:27
[11:27:34.666631] [11:27:34.671718] log_dir: ./exp/debug/cifar100-LT/debug
[11:27:36.372589] Epoch: [633]  [ 0/42]  eta: 0:01:11  lr: 0.000275  loss: 0.4853 (0.4853)  time: 1.6997  data: 1.1933  max mem: 9341
[11:27:46.535980] Epoch: [633]  [20/42]  eta: 0:00:12  lr: 0.000274  loss: 0.4607 (0.4663)  time: 0.5081  data: 0.0001  max mem: 9341
[11:27:56.686955] Epoch: [633]  [40/42]  eta: 0:00:01  lr: 0.000272  loss: 0.4791 (0.4709)  time: 0.5075  data: 0.0001  max mem: 9341
[11:27:57.191416] Epoch: [633]  [41/42]  eta: 0:00:00  lr: 0.000272  loss: 0.4719 (0.4709)  time: 0.5074  data: 0.0001  max mem: 9341
[11:27:57.361996] Epoch: [633] Total time: 0:00:22 (0.5402 s / it)
[11:27:57.379415] Averaged stats: lr: 0.000272  loss: 0.4719 (0.4721)
[11:28:01.988371] {"train_lr": 0.00027372215889461665, "train_loss": 0.472069769742943, "epoch": 633}
[11:28:01.988744] [11:28:01.988825] Training epoch 633 for 0:00:27
[11:28:01.988893] [11:28:01.993252] log_dir: ./exp/debug/cifar100-LT/debug
[11:28:03.462792] Epoch: [634]  [ 0/42]  eta: 0:01:01  lr: 0.000272  loss: 0.4809 (0.4809)  time: 1.4682  data: 0.9684  max mem: 9341
[11:28:13.629266] Epoch: [634]  [20/42]  eta: 0:00:12  lr: 0.000270  loss: 0.4608 (0.4665)  time: 0.5083  data: 0.0001  max mem: 9341
[11:28:23.774049] Epoch: [634]  [40/42]  eta: 0:00:01  lr: 0.000269  loss: 0.4852 (0.4753)  time: 0.5072  data: 0.0001  max mem: 9341
[11:28:24.279361] Epoch: [634]  [41/42]  eta: 0:00:00  lr: 0.000269  loss: 0.4839 (0.4755)  time: 0.5072  data: 0.0001  max mem: 9341
[11:28:24.441749] Epoch: [634] Total time: 0:00:22 (0.5345 s / it)
[11:28:24.455148] Averaged stats: lr: 0.000269  loss: 0.4839 (0.4737)
[11:28:28.941149] {"train_lr": 0.000270576526220379, "train_loss": 0.4737470332710516, "epoch": 634}
[11:28:28.941527] [11:28:28.941611] Training epoch 634 for 0:00:26
[11:28:28.941662] [11:28:28.946046] log_dir: ./exp/debug/cifar100-LT/debug
[11:28:30.411603] Epoch: [635]  [ 0/42]  eta: 0:01:01  lr: 0.000268  loss: 0.4583 (0.4583)  time: 1.4639  data: 0.9492  max mem: 9341
[11:28:40.575704] Epoch: [635]  [20/42]  eta: 0:00:12  lr: 0.000267  loss: 0.4793 (0.4734)  time: 0.5082  data: 0.0001  max mem: 9341
[11:28:50.724775] Epoch: [635]  [40/42]  eta: 0:00:01  lr: 0.000266  loss: 0.4621 (0.4715)  time: 0.5074  data: 0.0001  max mem: 9341
[11:28:51.230324] Epoch: [635]  [41/42]  eta: 0:00:00  lr: 0.000266  loss: 0.4621 (0.4715)  time: 0.5075  data: 0.0001  max mem: 9341
[11:28:51.400598] Epoch: [635] Total time: 0:00:22 (0.5346 s / it)
[11:28:51.413219] Averaged stats: lr: 0.000266  loss: 0.4621 (0.4726)
[11:28:55.980601] {"train_lr": 0.0002674467748289416, "train_loss": 0.4725676668541772, "epoch": 635}
[11:28:55.980921] [11:28:55.981008] Training epoch 635 for 0:00:27
[11:28:55.981058] [11:28:55.985915] log_dir: ./exp/debug/cifar100-LT/debug
[11:28:57.607612] Epoch: [636]  [ 0/42]  eta: 0:01:08  lr: 0.000265  loss: 0.4597 (0.4597)  time: 1.6207  data: 1.1024  max mem: 9341
[11:29:07.762101] Epoch: [636]  [20/42]  eta: 0:00:12  lr: 0.000264  loss: 0.4631 (0.4758)  time: 0.5077  data: 0.0001  max mem: 9341
[11:29:17.904859] Epoch: [636]  [40/42]  eta: 0:00:01  lr: 0.000263  loss: 0.4800 (0.4763)  time: 0.5071  data: 0.0001  max mem: 9341
[11:29:18.411980] Epoch: [636]  [41/42]  eta: 0:00:00  lr: 0.000263  loss: 0.4800 (0.4760)  time: 0.5071  data: 0.0001  max mem: 9341
[11:29:18.589065] Epoch: [636] Total time: 0:00:22 (0.5382 s / it)
[11:29:18.594932] Averaged stats: lr: 0.000263  loss: 0.4800 (0.4701)
[11:29:23.093897] {"train_lr": 0.00026433295819912003, "train_loss": 0.47013255598999204, "epoch": 636}
[11:29:23.094251] [11:29:23.094342] Training epoch 636 for 0:00:27
[11:29:23.094392] [11:29:23.098743] log_dir: ./exp/debug/cifar100-LT/debug
[11:29:24.661364] Epoch: [637]  [ 0/42]  eta: 0:01:05  lr: 0.000262  loss: 0.4801 (0.4801)  time: 1.5615  data: 1.0591  max mem: 9341
[11:29:34.846686] Epoch: [637]  [20/42]  eta: 0:00:12  lr: 0.000261  loss: 0.4762 (0.4818)  time: 0.5092  data: 0.0001  max mem: 9341
[11:29:45.009410] Epoch: [637]  [40/42]  eta: 0:00:01  lr: 0.000260  loss: 0.4855 (0.4803)  time: 0.5081  data: 0.0001  max mem: 9341
[11:29:45.515830] Epoch: [637]  [41/42]  eta: 0:00:00  lr: 0.000260  loss: 0.4855 (0.4806)  time: 0.5081  data: 0.0001  max mem: 9341
[11:29:45.683896] Epoch: [637] Total time: 0:00:22 (0.5377 s / it)
[11:29:45.695533] Averaged stats: lr: 0.000260  loss: 0.4855 (0.4724)
[11:29:50.324557] {"train_lr": 0.00026123512953745, "train_loss": 0.472365696337961, "epoch": 637}
[11:29:50.324894] [11:29:50.324974] Training epoch 637 for 0:00:27
[11:29:50.325024] [11:29:50.329342] log_dir: ./exp/debug/cifar100-LT/debug
[11:29:51.997588] Epoch: [638]  [ 0/42]  eta: 0:01:10  lr: 0.000259  loss: 0.4574 (0.4574)  time: 1.6672  data: 1.1672  max mem: 9341
[11:30:02.173831] Epoch: [638]  [20/42]  eta: 0:00:12  lr: 0.000258  loss: 0.4773 (0.4752)  time: 0.5088  data: 0.0001  max mem: 9341
[11:30:12.321415] Epoch: [638]  [40/42]  eta: 0:00:01  lr: 0.000257  loss: 0.4733 (0.4763)  time: 0.5073  data: 0.0001  max mem: 9341
[11:30:12.827354] Epoch: [638]  [41/42]  eta: 0:00:00  lr: 0.000257  loss: 0.4733 (0.4760)  time: 0.5073  data: 0.0001  max mem: 9341
[11:30:13.002088] Epoch: [638] Total time: 0:00:22 (0.5398 s / it)
[11:30:13.002912] Averaged stats: lr: 0.000257  loss: 0.4733 (0.4726)
[11:30:17.554387] {"train_lr": 0.0002581533417772758, "train_loss": 0.4726398796552703, "epoch": 638}
[11:30:17.555091] [11:30:17.555208] Training epoch 638 for 0:00:27
[11:30:17.555279] [11:30:17.561651] log_dir: ./exp/debug/cifar100-LT/debug
[11:30:19.317102] Epoch: [639]  [ 0/42]  eta: 0:01:13  lr: 0.000256  loss: 0.4496 (0.4496)  time: 1.7543  data: 1.2595  max mem: 9341
[11:30:29.481263] Epoch: [639]  [20/42]  eta: 0:00:12  lr: 0.000255  loss: 0.4703 (0.4697)  time: 0.5081  data: 0.0001  max mem: 9341
[11:30:39.630215] Epoch: [639]  [40/42]  eta: 0:00:01  lr: 0.000254  loss: 0.4650 (0.4676)  time: 0.5074  data: 0.0001  max mem: 9341
[11:30:40.135638] Epoch: [639]  [41/42]  eta: 0:00:00  lr: 0.000254  loss: 0.4549 (0.4672)  time: 0.5074  data: 0.0001  max mem: 9341
[11:30:40.295550] Epoch: [639] Total time: 0:00:22 (0.5413 s / it)
[11:30:40.306159] Averaged stats: lr: 0.000254  loss: 0.4549 (0.4703)
[11:30:44.907665] {"train_lr": 0.0002550876475778472, "train_loss": 0.4702791048302537, "epoch": 639}
[11:30:44.908031] [11:30:44.908158] Training epoch 639 for 0:00:27
[11:30:44.908214] [11:30:44.912547] log_dir: ./exp/debug/cifar100-LT/debug
[11:30:46.624964] Epoch: [640]  [ 0/42]  eta: 0:01:11  lr: 0.000253  loss: 0.5143 (0.5143)  time: 1.7112  data: 1.2168  max mem: 9341
[11:30:56.810578] Epoch: [640]  [20/42]  eta: 0:00:12  lr: 0.000252  loss: 0.4598 (0.4644)  time: 0.5092  data: 0.0001  max mem: 9341
[11:31:06.979599] Epoch: [640]  [40/42]  eta: 0:00:01  lr: 0.000251  loss: 0.4661 (0.4648)  time: 0.5084  data: 0.0001  max mem: 9341
[11:31:07.485373] Epoch: [640]  [41/42]  eta: 0:00:00  lr: 0.000251  loss: 0.4661 (0.4645)  time: 0.5084  data: 0.0001  max mem: 9341
[11:31:07.648296] Epoch: [640] Total time: 0:00:22 (0.5413 s / it)
[11:31:07.654085] Averaged stats: lr: 0.000251  loss: 0.4661 (0.4677)
[11:31:12.294787] {"train_lr": 0.0002520380993234198, "train_loss": 0.46766774728894234, "epoch": 640}
[11:31:12.295165] [11:31:12.295245] Training epoch 640 for 0:00:27
[11:31:12.295297] [11:31:12.299698] log_dir: ./exp/debug/cifar100-LT/debug
[11:31:13.892011] Epoch: [641]  [ 0/42]  eta: 0:01:06  lr: 0.000250  loss: 0.4834 (0.4834)  time: 1.5911  data: 1.0776  max mem: 9341
[11:31:24.060103] Epoch: [641]  [20/42]  eta: 0:00:12  lr: 0.000249  loss: 0.4718 (0.4701)  time: 0.5083  data: 0.0001  max mem: 9341
[11:31:34.211844] Epoch: [641]  [40/42]  eta: 0:00:01  lr: 0.000248  loss: 0.4698 (0.4706)  time: 0.5075  data: 0.0001  max mem: 9341
[11:31:34.719221] Epoch: [641]  [41/42]  eta: 0:00:00  lr: 0.000248  loss: 0.4688 (0.4702)  time: 0.5076  data: 0.0001  max mem: 9341
[11:31:34.908263] Epoch: [641] Total time: 0:00:22 (0.5383 s / it)
[11:31:34.908993] Averaged stats: lr: 0.000248  loss: 0.4688 (0.4712)
[11:31:39.577215] {"train_lr": 0.0002490047491223585, "train_loss": 0.4711948780431634, "epoch": 641}
[11:31:39.577529] [11:31:39.577611] Training epoch 641 for 0:00:27
[11:31:39.577719] [11:31:39.582109] log_dir: ./exp/debug/cifar100-LT/debug
[11:31:41.172396] Epoch: [642]  [ 0/42]  eta: 0:01:06  lr: 0.000247  loss: 0.5096 (0.5096)  time: 1.5891  data: 1.0794  max mem: 9341
[11:31:51.339868] Epoch: [642]  [20/42]  eta: 0:00:12  lr: 0.000246  loss: 0.4632 (0.4660)  time: 0.5083  data: 0.0001  max mem: 9341
[11:32:01.485000] Epoch: [642]  [40/42]  eta: 0:00:01  lr: 0.000245  loss: 0.4694 (0.4697)  time: 0.5072  data: 0.0001  max mem: 9341
[11:32:01.989882] Epoch: [642]  [41/42]  eta: 0:00:00  lr: 0.000245  loss: 0.4694 (0.4691)  time: 0.5072  data: 0.0001  max mem: 9341
[11:32:02.155045] Epoch: [642] Total time: 0:00:22 (0.5374 s / it)
[11:32:02.158990] Averaged stats: lr: 0.000245  loss: 0.4694 (0.4707)
[11:32:06.689264] {"train_lr": 0.0002459876488062502, "train_loss": 0.4707313650065944, "epoch": 642}
[11:32:06.689583] [11:32:06.689663] Training epoch 642 for 0:00:27
[11:32:06.689769] [11:32:06.694140] log_dir: ./exp/debug/cifar100-LT/debug
[11:32:08.427195] Epoch: [643]  [ 0/42]  eta: 0:01:12  lr: 0.000244  loss: 0.4978 (0.4978)  time: 1.7320  data: 1.2283  max mem: 9341
[11:32:18.595454] Epoch: [643]  [20/42]  eta: 0:00:12  lr: 0.000243  loss: 0.4807 (0.4771)  time: 0.5084  data: 0.0001  max mem: 9341
[11:32:28.753984] Epoch: [643]  [40/42]  eta: 0:00:01  lr: 0.000242  loss: 0.4570 (0.4721)  time: 0.5079  data: 0.0001  max mem: 9341
[11:32:29.259409] Epoch: [643]  [41/42]  eta: 0:00:00  lr: 0.000242  loss: 0.4570 (0.4726)  time: 0.5078  data: 0.0001  max mem: 9341
[11:32:29.414788] Epoch: [643] Total time: 0:00:22 (0.5410 s / it)
[11:32:29.428686] Averaged stats: lr: 0.000242  loss: 0.4570 (0.4702)
[11:32:34.071333] {"train_lr": 0.0002429868499290139, "train_loss": 0.4702305557827155, "epoch": 643}
[11:32:34.071698] [11:32:34.071778] Training epoch 643 for 0:00:27
[11:32:34.071828] [11:32:34.076186] log_dir: ./exp/debug/cifar100-LT/debug
[11:32:35.723640] Epoch: [644]  [ 0/42]  eta: 0:01:09  lr: 0.000241  loss: 0.4529 (0.4529)  time: 1.6461  data: 1.1297  max mem: 9341
[11:32:45.889887] Epoch: [644]  [20/42]  eta: 0:00:12  lr: 0.000240  loss: 0.4652 (0.4646)  time: 0.5083  data: 0.0001  max mem: 9341
[11:32:56.039041] Epoch: [644]  [40/42]  eta: 0:00:01  lr: 0.000239  loss: 0.4598 (0.4660)  time: 0.5074  data: 0.0001  max mem: 9341
[11:32:56.544014] Epoch: [644]  [41/42]  eta: 0:00:00  lr: 0.000239  loss: 0.4598 (0.4655)  time: 0.5074  data: 0.0001  max mem: 9341
[11:32:56.709589] Epoch: [644] Total time: 0:00:22 (0.5389 s / it)
[11:32:56.719406] Averaged stats: lr: 0.000239  loss: 0.4598 (0.4710)
[11:33:01.321368] {"train_lr": 0.0002400024037660235, "train_loss": 0.4710040060537202, "epoch": 644}
[11:33:01.321719] [11:33:01.321797] Training epoch 644 for 0:00:27
[11:33:01.321848] [11:33:01.326189] log_dir: ./exp/debug/cifar100-LT/debug
[11:33:02.930145] Epoch: [645]  [ 0/42]  eta: 0:01:07  lr: 0.000238  loss: 0.4817 (0.4817)  time: 1.6027  data: 1.0951  max mem: 9341
[11:33:13.081149] Epoch: [645]  [20/42]  eta: 0:00:12  lr: 0.000237  loss: 0.4675 (0.4679)  time: 0.5075  data: 0.0002  max mem: 9341
[11:33:23.220885] Epoch: [645]  [40/42]  eta: 0:00:01  lr: 0.000236  loss: 0.4750 (0.4704)  time: 0.5069  data: 0.0001  max mem: 9341
[11:33:23.726925] Epoch: [645]  [41/42]  eta: 0:00:00  lr: 0.000236  loss: 0.4750 (0.4711)  time: 0.5070  data: 0.0001  max mem: 9341
[11:33:23.884490] Epoch: [645] Total time: 0:00:22 (0.5371 s / it)
[11:33:23.888481] Averaged stats: lr: 0.000236  loss: 0.4750 (0.4726)
[11:33:28.505716] {"train_lr": 0.00023703436131322888, "train_loss": 0.47262039071037654, "epoch": 645}
[11:33:28.506098] [11:33:28.506180] Training epoch 645 for 0:00:27
[11:33:28.506229] [11:33:28.510467] log_dir: ./exp/debug/cifar100-LT/debug
[11:33:30.087970] Epoch: [646]  [ 0/42]  eta: 0:01:06  lr: 0.000235  loss: 0.5028 (0.5028)  time: 1.5763  data: 1.0804  max mem: 9341
[11:33:40.253133] Epoch: [646]  [20/42]  eta: 0:00:12  lr: 0.000234  loss: 0.4785 (0.4763)  time: 0.5082  data: 0.0001  max mem: 9341
[11:33:50.411850] Epoch: [646]  [40/42]  eta: 0:00:01  lr: 0.000233  loss: 0.4669 (0.4755)  time: 0.5079  data: 0.0001  max mem: 9341
[11:33:50.919001] Epoch: [646]  [41/42]  eta: 0:00:00  lr: 0.000233  loss: 0.4671 (0.4757)  time: 0.5080  data: 0.0001  max mem: 9341
[11:33:51.086127] Epoch: [646] Total time: 0:00:22 (0.5375 s / it)
[11:33:51.087047] Averaged stats: lr: 0.000233  loss: 0.4671 (0.4722)
[11:33:55.780501] {"train_lr": 0.00023408277328628718, "train_loss": 0.47218324226282893, "epoch": 646}
[11:33:55.780986] [11:33:55.781080] Training epoch 646 for 0:00:27
[11:33:55.781133] [11:33:55.786193] log_dir: ./exp/debug/cifar100-LT/debug
[11:33:57.425760] Epoch: [647]  [ 0/42]  eta: 0:01:08  lr: 0.000232  loss: 0.4602 (0.4602)  time: 1.6384  data: 1.1327  max mem: 9341
[11:34:07.581207] Epoch: [647]  [20/42]  eta: 0:00:12  lr: 0.000231  loss: 0.4725 (0.4833)  time: 0.5077  data: 0.0001  max mem: 9341
[11:34:17.720944] Epoch: [647]  [40/42]  eta: 0:00:01  lr: 0.000230  loss: 0.4655 (0.4776)  time: 0.5069  data: 0.0001  max mem: 9341
[11:34:18.225093] Epoch: [647]  [41/42]  eta: 0:00:00  lr: 0.000230  loss: 0.4633 (0.4771)  time: 0.5069  data: 0.0001  max mem: 9341
[11:34:18.397711] Epoch: [647] Total time: 0:00:22 (0.5384 s / it)
[11:34:18.398469] Averaged stats: lr: 0.000230  loss: 0.4633 (0.4712)
[11:34:22.939176] {"train_lr": 0.00023114769011969353, "train_loss": 0.4712149305712609, "epoch": 647}
[11:34:22.939530] [11:34:22.939611] Training epoch 647 for 0:00:27
[11:34:22.939663] [11:34:22.943985] log_dir: ./exp/debug/cifar100-LT/debug
[11:34:24.360209] Epoch: [648]  [ 0/42]  eta: 0:00:59  lr: 0.000229  loss: 0.4410 (0.4410)  time: 1.4149  data: 0.9151  max mem: 9341
[11:34:34.513190] Epoch: [648]  [20/42]  eta: 0:00:12  lr: 0.000228  loss: 0.4720 (0.4713)  time: 0.5076  data: 0.0001  max mem: 9341
[11:34:44.653685] Epoch: [648]  [40/42]  eta: 0:00:01  lr: 0.000227  loss: 0.4821 (0.4738)  time: 0.5070  data: 0.0001  max mem: 9341
[11:34:45.159038] Epoch: [648]  [41/42]  eta: 0:00:00  lr: 0.000227  loss: 0.4821 (0.4734)  time: 0.5070  data: 0.0001  max mem: 9341
[11:34:45.318130] Epoch: [648] Total time: 0:00:22 (0.5327 s / it)
[11:34:45.325106] Averaged stats: lr: 0.000227  loss: 0.4821 (0.4704)
[11:34:49.951655] {"train_lr": 0.0002282291619659213, "train_loss": 0.47036404872224447, "epoch": 648}
[11:34:49.952036] [11:34:49.952170] Training epoch 648 for 0:00:27
[11:34:49.952225] [11:34:49.956504] log_dir: ./exp/debug/cifar100-LT/debug
[11:34:51.629026] Epoch: [649]  [ 0/42]  eta: 0:01:10  lr: 0.000226  loss: 0.4905 (0.4905)  time: 1.6714  data: 1.1630  max mem: 9341
[11:35:01.801621] Epoch: [649]  [20/42]  eta: 0:00:12  lr: 0.000225  loss: 0.4722 (0.4776)  time: 0.5086  data: 0.0001  max mem: 9341
[11:35:11.942506] Epoch: [649]  [40/42]  eta: 0:00:01  lr: 0.000224  loss: 0.4634 (0.4730)  time: 0.5070  data: 0.0001  max mem: 9341
[11:35:12.445745] Epoch: [649]  [41/42]  eta: 0:00:00  lr: 0.000224  loss: 0.4627 (0.4721)  time: 0.5069  data: 0.0001  max mem: 9341
[11:35:12.617736] Epoch: [649] Total time: 0:00:22 (0.5396 s / it)
[11:35:12.628442] Averaged stats: lr: 0.000224  loss: 0.4627 (0.4733)
[11:35:17.293736] {"train_lr": 0.00022532723869456424, "train_loss": 0.473266041349797, "epoch": 649}
[11:35:17.294153] [11:35:17.294241] Training epoch 649 for 0:00:27
[11:35:17.294293] [11:35:17.299031] log_dir: ./exp/debug/cifar100-LT/debug
[11:35:18.946954] Epoch: [650]  [ 0/42]  eta: 0:01:09  lr: 0.000223  loss: 0.4622 (0.4622)  time: 1.6468  data: 1.1354  max mem: 9341
[11:35:29.117853] Epoch: [650]  [20/42]  eta: 0:00:12  lr: 0.000222  loss: 0.4688 (0.4710)  time: 0.5085  data: 0.0001  max mem: 9341
[11:35:39.270376] Epoch: [650]  [40/42]  eta: 0:00:01  lr: 0.000221  loss: 0.4729 (0.4705)  time: 0.5076  data: 0.0001  max mem: 9341
[11:35:39.774950] Epoch: [650]  [41/42]  eta: 0:00:00  lr: 0.000221  loss: 0.4631 (0.4701)  time: 0.5074  data: 0.0001  max mem: 9341
[11:35:39.945324] Epoch: [650] Total time: 0:00:22 (0.5392 s / it)
[11:35:39.946143] Averaged stats: lr: 0.000221  loss: 0.4631 (0.4724)
[11:35:44.494159] {"train_lr": 0.0002224419698914837, "train_loss": 0.47239045710081146, "epoch": 650}
[11:35:44.494616] [11:35:44.494705] Training epoch 650 for 0:00:27
[11:35:44.494758] [11:35:44.499198] log_dir: ./exp/debug/cifar100-LT/debug
[11:35:46.078310] Epoch: [651]  [ 0/42]  eta: 0:01:06  lr: 0.000221  loss: 0.4707 (0.4707)  time: 1.5779  data: 1.0634  max mem: 9341
[11:35:56.265589] Epoch: [651]  [20/42]  eta: 0:00:12  lr: 0.000219  loss: 0.4617 (0.4694)  time: 0.5093  data: 0.0001  max mem: 9341
[11:36:06.418350] Epoch: [651]  [40/42]  eta: 0:00:01  lr: 0.000218  loss: 0.4641 (0.4683)  time: 0.5076  data: 0.0001  max mem: 9341
[11:36:06.924430] Epoch: [651]  [41/42]  eta: 0:00:00  lr: 0.000218  loss: 0.4641 (0.4684)  time: 0.5077  data: 0.0001  max mem: 9341
[11:36:07.098966] Epoch: [651] Total time: 0:00:22 (0.5381 s / it)
[11:36:07.100982] Averaged stats: lr: 0.000218  loss: 0.4641 (0.4714)
[11:36:11.607351] {"train_lr": 0.00021957340485796403, "train_loss": 0.471376374541294, "epoch": 651}
[11:36:11.607716] [11:36:11.607812] Training epoch 651 for 0:00:27
[11:36:11.607862] [11:36:11.612241] log_dir: ./exp/debug/cifar100-LT/debug
[11:36:13.325504] Epoch: [652]  [ 0/42]  eta: 0:01:11  lr: 0.000218  loss: 0.4712 (0.4712)  time: 1.7123  data: 1.2177  max mem: 9341
[11:36:23.491404] Epoch: [652]  [20/42]  eta: 0:00:12  lr: 0.000217  loss: 0.4632 (0.4719)  time: 0.5082  data: 0.0001  max mem: 9341
[11:36:33.647050] Epoch: [652]  [40/42]  eta: 0:00:01  lr: 0.000215  loss: 0.4733 (0.4760)  time: 0.5077  data: 0.0001  max mem: 9341
[11:36:34.152225] Epoch: [652]  [41/42]  eta: 0:00:00  lr: 0.000215  loss: 0.4733 (0.4758)  time: 0.5077  data: 0.0001  max mem: 9341
[11:36:34.324204] Epoch: [652] Total time: 0:00:22 (0.5408 s / it)
[11:36:34.327382] Averaged stats: lr: 0.000215  loss: 0.4733 (0.4737)
[11:36:38.942359] {"train_lr": 0.00021672159260986582, "train_loss": 0.47372962286074954, "epoch": 652}
[11:36:38.942667] [11:36:38.942747] Training epoch 652 for 0:00:27
[11:36:38.942798] [11:36:38.947196] log_dir: ./exp/debug/cifar100-LT/debug
[11:36:40.592228] Epoch: [653]  [ 0/42]  eta: 0:01:09  lr: 0.000215  loss: 0.4469 (0.4469)  time: 1.6438  data: 1.1360  max mem: 9341
[11:36:50.758147] Epoch: [653]  [20/42]  eta: 0:00:12  lr: 0.000214  loss: 0.4711 (0.4785)  time: 0.5082  data: 0.0001  max mem: 9341
[11:37:00.907305] Epoch: [653]  [40/42]  eta: 0:00:01  lr: 0.000213  loss: 0.4685 (0.4767)  time: 0.5074  data: 0.0001  max mem: 9341
[11:37:01.412529] Epoch: [653]  [41/42]  eta: 0:00:00  lr: 0.000213  loss: 0.4711 (0.4774)  time: 0.5073  data: 0.0001  max mem: 9341
[11:37:01.576266] Epoch: [653] Total time: 0:00:22 (0.5388 s / it)
[11:37:01.576973] Averaged stats: lr: 0.000213  loss: 0.4711 (0.4698)
[11:37:06.207902] {"train_lr": 0.00021388658187679292, "train_loss": 0.4698351240229039, "epoch": 653}
[11:37:06.208299] [11:37:06.208381] Training epoch 653 for 0:00:27
[11:37:06.208433] [11:37:06.212660] log_dir: ./exp/debug/cifar100-LT/debug
[11:37:07.778879] Epoch: [654]  [ 0/42]  eta: 0:01:05  lr: 0.000212  loss: 0.4854 (0.4854)  time: 1.5648  data: 1.0697  max mem: 9341
[11:37:17.939239] Epoch: [654]  [20/42]  eta: 0:00:12  lr: 0.000211  loss: 0.4757 (0.4712)  time: 0.5080  data: 0.0001  max mem: 9341
[11:37:28.084656] Epoch: [654]  [40/42]  eta: 0:00:01  lr: 0.000210  loss: 0.4643 (0.4703)  time: 0.5072  data: 0.0001  max mem: 9341
[11:37:28.591822] Epoch: [654]  [41/42]  eta: 0:00:00  lr: 0.000210  loss: 0.4643 (0.4706)  time: 0.5073  data: 0.0001  max mem: 9341
[11:37:28.750908] Epoch: [654] Total time: 0:00:22 (0.5366 s / it)
[11:37:28.759857] Averaged stats: lr: 0.000210  loss: 0.4643 (0.4716)
[11:37:33.428041] {"train_lr": 0.00021106842110125594, "train_loss": 0.4716389031992072, "epoch": 654}
[11:37:33.428460] [11:37:33.428542] Training epoch 654 for 0:00:27
[11:37:33.428594] [11:37:33.432873] log_dir: ./exp/debug/cifar100-LT/debug
[11:37:35.067221] Epoch: [655]  [ 0/42]  eta: 0:01:08  lr: 0.000209  loss: 0.4598 (0.4598)  time: 1.6333  data: 1.1199  max mem: 9341
[11:37:45.262074] Epoch: [655]  [20/42]  eta: 0:00:12  lr: 0.000208  loss: 0.4723 (0.4769)  time: 0.5097  data: 0.0001  max mem: 9341
[11:37:55.433833] Epoch: [655]  [40/42]  eta: 0:00:01  lr: 0.000207  loss: 0.4603 (0.4694)  time: 0.5085  data: 0.0001  max mem: 9341
[11:37:55.940159] Epoch: [655]  [41/42]  eta: 0:00:00  lr: 0.000207  loss: 0.4603 (0.4698)  time: 0.5086  data: 0.0001  max mem: 9341
[11:37:56.112411] Epoch: [655] Total time: 0:00:22 (0.5400 s / it)
[11:37:56.113165] Averaged stats: lr: 0.000207  loss: 0.4603 (0.4710)
[11:38:00.814516] {"train_lr": 0.00020826715843784766, "train_loss": 0.47102339956022443, "epoch": 655}
[11:38:00.814905] [11:38:00.814990] Training epoch 655 for 0:00:27
[11:38:00.815040] [11:38:00.819427] log_dir: ./exp/debug/cifar100-LT/debug
[11:38:02.315407] Epoch: [656]  [ 0/42]  eta: 0:01:02  lr: 0.000206  loss: 0.4500 (0.4500)  time: 1.4948  data: 0.9971  max mem: 9341
[11:38:12.477911] Epoch: [656]  [20/42]  eta: 0:00:12  lr: 0.000205  loss: 0.4648 (0.4696)  time: 0.5081  data: 0.0001  max mem: 9341
[11:38:22.626873] Epoch: [656]  [40/42]  eta: 0:00:01  lr: 0.000204  loss: 0.4661 (0.4701)  time: 0.5074  data: 0.0001  max mem: 9341
[11:38:23.132527] Epoch: [656]  [41/42]  eta: 0:00:00  lr: 0.000204  loss: 0.4661 (0.4699)  time: 0.5073  data: 0.0001  max mem: 9341
[11:38:23.293911] Epoch: [656] Total time: 0:00:22 (0.5351 s / it)
[11:38:23.305594] Averaged stats: lr: 0.000204  loss: 0.4661 (0.4697)
[11:38:27.864957] {"train_lr": 0.00020548284175241757, "train_loss": 0.4697269745880649, "epoch": 656}
[11:38:27.865313] [11:38:27.865409] Training epoch 656 for 0:00:27
[11:38:27.865460] [11:38:27.869872] log_dir: ./exp/debug/cifar100-LT/debug
[11:38:29.547541] Epoch: [657]  [ 0/42]  eta: 0:01:10  lr: 0.000204  loss: 0.4745 (0.4745)  time: 1.6766  data: 1.1752  max mem: 9341
[11:38:39.709229] Epoch: [657]  [20/42]  eta: 0:00:12  lr: 0.000203  loss: 0.4678 (0.4708)  time: 0.5080  data: 0.0001  max mem: 9341
[11:38:49.857221] Epoch: [657]  [40/42]  eta: 0:00:01  lr: 0.000202  loss: 0.4596 (0.4680)  time: 0.5073  data: 0.0001  max mem: 9341
[11:38:50.363359] Epoch: [657]  [41/42]  eta: 0:00:00  lr: 0.000202  loss: 0.4596 (0.4671)  time: 0.5072  data: 0.0001  max mem: 9341
[11:38:50.535286] Epoch: [657] Total time: 0:00:22 (0.5397 s / it)
[11:38:50.537629] Averaged stats: lr: 0.000202  loss: 0.4596 (0.4687)
[11:38:55.012653] {"train_lr": 0.00020271551862125565, "train_loss": 0.4687442784862859, "epoch": 657}
[11:38:55.013047] [11:38:55.013127] Training epoch 657 for 0:00:27
[11:38:55.013179] [11:38:55.017445] log_dir: ./exp/debug/cifar100-LT/debug
[11:38:56.673816] Epoch: [658]  [ 0/42]  eta: 0:01:09  lr: 0.000201  loss: 0.4860 (0.4860)  time: 1.6552  data: 1.1410  max mem: 9341
[11:39:06.834394] Epoch: [658]  [20/42]  eta: 0:00:12  lr: 0.000200  loss: 0.4575 (0.4694)  time: 0.5080  data: 0.0001  max mem: 9341
[11:39:16.985519] Epoch: [658]  [40/42]  eta: 0:00:01  lr: 0.000199  loss: 0.4820 (0.4759)  time: 0.5075  data: 0.0001  max mem: 9341
[11:39:17.491788] Epoch: [658]  [41/42]  eta: 0:00:00  lr: 0.000199  loss: 0.4819 (0.4760)  time: 0.5076  data: 0.0001  max mem: 9341
[11:39:17.659783] Epoch: [658] Total time: 0:00:22 (0.5391 s / it)
[11:39:17.662707] Averaged stats: lr: 0.000199  loss: 0.4819 (0.4706)
[11:39:22.285847] {"train_lr": 0.00019996523633027883, "train_loss": 0.4706044825060027, "epoch": 658}
[11:39:22.286192] [11:39:22.286273] Training epoch 658 for 0:00:27
[11:39:22.286325] [11:39:22.290774] log_dir: ./exp/debug/cifar100-LT/debug
[11:39:23.970357] Epoch: [659]  [ 0/42]  eta: 0:01:10  lr: 0.000198  loss: 0.4683 (0.4683)  time: 1.6786  data: 1.1607  max mem: 9341
[11:39:34.137779] Epoch: [659]  [20/42]  eta: 0:00:12  lr: 0.000197  loss: 0.4683 (0.4733)  time: 0.5083  data: 0.0001  max mem: 9341
[11:39:44.290057] Epoch: [659]  [40/42]  eta: 0:00:01  lr: 0.000196  loss: 0.4708 (0.4713)  time: 0.5076  data: 0.0001  max mem: 9341
[11:39:44.797278] Epoch: [659]  [41/42]  eta: 0:00:00  lr: 0.000196  loss: 0.4708 (0.4709)  time: 0.5076  data: 0.0001  max mem: 9341
[11:39:44.966992] Epoch: [659] Total time: 0:00:22 (0.5399 s / it)
[11:39:44.988381] Averaged stats: lr: 0.000196  loss: 0.4708 (0.4686)
[11:39:49.457503] {"train_lr": 0.00019723204187422345, "train_loss": 0.46863010720837683, "epoch": 659}
[11:39:49.457854] [11:39:49.457936] Training epoch 659 for 0:00:27
[11:39:49.457986] [11:39:49.465368] log_dir: ./exp/debug/cifar100-LT/debug
[11:39:51.007108] Epoch: [660]  [ 0/42]  eta: 0:01:04  lr: 0.000195  loss: 0.4873 (0.4873)  time: 1.5402  data: 1.0437  max mem: 9341
[11:40:01.167914] Epoch: [660]  [20/42]  eta: 0:00:12  lr: 0.000194  loss: 0.4572 (0.4639)  time: 0.5080  data: 0.0001  max mem: 9341
[11:40:11.324025] Epoch: [660]  [40/42]  eta: 0:00:01  lr: 0.000193  loss: 0.4768 (0.4696)  time: 0.5078  data: 0.0001  max mem: 9341
[11:40:11.829330] Epoch: [660]  [41/42]  eta: 0:00:00  lr: 0.000193  loss: 0.4768 (0.4698)  time: 0.5077  data: 0.0001  max mem: 9341
[11:40:11.997082] Epoch: [660] Total time: 0:00:22 (0.5365 s / it)
[11:40:12.006718] Averaged stats: lr: 0.000193  loss: 0.4768 (0.4692)
[11:40:16.591471] {"train_lr": 0.00019451598195584168, "train_loss": 0.4691530764102936, "epoch": 660}
[11:40:16.591799] [11:40:16.591885] Training epoch 660 for 0:00:27
[11:40:16.592015] [11:40:16.596295] log_dir: ./exp/debug/cifar100-LT/debug
[11:40:18.050014] Epoch: [661]  [ 0/42]  eta: 0:01:01  lr: 0.000193  loss: 0.4855 (0.4855)  time: 1.4525  data: 0.9461  max mem: 9341
[11:40:28.216783] Epoch: [661]  [20/42]  eta: 0:00:12  lr: 0.000192  loss: 0.4666 (0.4704)  time: 0.5083  data: 0.0001  max mem: 9341
[11:40:38.361160] Epoch: [661]  [40/42]  eta: 0:00:01  lr: 0.000191  loss: 0.4745 (0.4713)  time: 0.5072  data: 0.0001  max mem: 9341
[11:40:38.867372] Epoch: [661]  [41/42]  eta: 0:00:00  lr: 0.000191  loss: 0.4745 (0.4711)  time: 0.5072  data: 0.0001  max mem: 9341
[11:40:39.037817] Epoch: [661] Total time: 0:00:22 (0.5343 s / it)
[11:40:39.038799] Averaged stats: lr: 0.000191  loss: 0.4745 (0.4707)
[11:40:43.595289] {"train_lr": 0.0001918171029851033, "train_loss": 0.47073923814154806, "epoch": 661}
[11:40:43.595644] [11:40:43.595723] Training epoch 661 for 0:00:27
[11:40:43.595774] [11:40:43.600122] log_dir: ./exp/debug/cifar100-LT/debug
[11:40:45.191206] Epoch: [662]  [ 0/42]  eta: 0:01:06  lr: 0.000190  loss: 0.4443 (0.4443)  time: 1.5898  data: 1.0763  max mem: 9341
[11:40:55.357916] Epoch: [662]  [20/42]  eta: 0:00:12  lr: 0.000189  loss: 0.4712 (0.4738)  time: 0.5083  data: 0.0001  max mem: 9341
[11:41:05.505171] Epoch: [662]  [40/42]  eta: 0:00:01  lr: 0.000188  loss: 0.4661 (0.4711)  time: 0.5073  data: 0.0001  max mem: 9341
[11:41:06.010891] Epoch: [662]  [41/42]  eta: 0:00:00  lr: 0.000188  loss: 0.4661 (0.4712)  time: 0.5073  data: 0.0001  max mem: 9341
[11:41:06.168814] Epoch: [662] Total time: 0:00:22 (0.5373 s / it)
[11:41:06.185543] Averaged stats: lr: 0.000188  loss: 0.4661 (0.4709)
[11:41:10.799394] {"train_lr": 0.0001891354510784038, "train_loss": 0.4709231789622988, "epoch": 662}
[11:41:10.799730] [11:41:10.799809] Training epoch 662 for 0:00:27
[11:41:10.799860] [11:41:10.804666] log_dir: ./exp/debug/cifar100-LT/debug
[11:41:12.435125] Epoch: [663]  [ 0/42]  eta: 0:01:08  lr: 0.000187  loss: 0.5143 (0.5143)  time: 1.6291  data: 1.1233  max mem: 9341
[11:41:22.599841] Epoch: [663]  [20/42]  eta: 0:00:12  lr: 0.000186  loss: 0.4754 (0.4799)  time: 0.5082  data: 0.0001  max mem: 9341
[11:41:32.750545] Epoch: [663]  [40/42]  eta: 0:00:01  lr: 0.000185  loss: 0.4634 (0.4734)  time: 0.5075  data: 0.0001  max mem: 9341
[11:41:33.256781] Epoch: [663]  [41/42]  eta: 0:00:00  lr: 0.000185  loss: 0.4634 (0.4727)  time: 0.5075  data: 0.0001  max mem: 9341
[11:41:33.435245] Epoch: [663] Total time: 0:00:22 (0.5388 s / it)
[11:41:33.438750] Averaged stats: lr: 0.000185  loss: 0.4634 (0.4712)
[11:41:38.094988] {"train_lr": 0.00018647107205777501, "train_loss": 0.4712281046169145, "epoch": 663}
[11:41:38.095306] [11:41:38.095389] Training epoch 663 for 0:00:27
[11:41:38.095439] [11:41:38.099885] log_dir: ./exp/debug/cifar100-LT/debug
[11:41:39.523449] Epoch: [664]  [ 0/42]  eta: 0:00:59  lr: 0.000185  loss: 0.4157 (0.4157)  time: 1.4223  data: 0.9190  max mem: 9341
[11:41:49.736322] Epoch: [664]  [20/42]  eta: 0:00:12  lr: 0.000184  loss: 0.4577 (0.4587)  time: 0.5106  data: 0.0001  max mem: 9341
[11:41:59.887791] Epoch: [664]  [40/42]  eta: 0:00:01  lr: 0.000183  loss: 0.4704 (0.4644)  time: 0.5075  data: 0.0001  max mem: 9341
[11:42:00.394923] Epoch: [664]  [41/42]  eta: 0:00:00  lr: 0.000183  loss: 0.4680 (0.4640)  time: 0.5076  data: 0.0001  max mem: 9341
[11:42:00.561532] Epoch: [664] Total time: 0:00:22 (0.5348 s / it)
[11:42:00.563197] Averaged stats: lr: 0.000183  loss: 0.4680 (0.4668)
[11:42:05.246906] {"train_lr": 0.00018382401145010358, "train_loss": 0.4668097985642297, "epoch": 664}
[11:42:05.247328] [11:42:05.247442] Training epoch 664 for 0:00:27
[11:42:05.247497] [11:42:05.252234] log_dir: ./exp/debug/cifar100-LT/debug
[11:42:06.724379] Epoch: [665]  [ 0/42]  eta: 0:01:01  lr: 0.000182  loss: 0.4271 (0.4271)  time: 1.4709  data: 0.9613  max mem: 9341
[11:42:16.892663] Epoch: [665]  [20/42]  eta: 0:00:12  lr: 0.000181  loss: 0.4670 (0.4614)  time: 0.5084  data: 0.0001  max mem: 9341
[11:42:27.041750] Epoch: [665]  [40/42]  eta: 0:00:01  lr: 0.000180  loss: 0.4739 (0.4695)  time: 0.5074  data: 0.0001  max mem: 9341
[11:42:27.548403] Epoch: [665]  [41/42]  eta: 0:00:00  lr: 0.000180  loss: 0.4782 (0.4702)  time: 0.5073  data: 0.0001  max mem: 9341
[11:42:27.715506] Epoch: [665] Total time: 0:00:22 (0.5348 s / it)
[11:42:27.725584] Averaged stats: lr: 0.000180  loss: 0.4782 (0.4703)
[11:42:32.244667] {"train_lr": 0.00018119431448635132, "train_loss": 0.47027244862346423, "epoch": 665}
[11:42:32.245006] [11:42:32.245085] Training epoch 665 for 0:00:26
[11:42:32.245135] [11:42:32.249425] log_dir: ./exp/debug/cifar100-LT/debug
[11:42:33.842101] Epoch: [666]  [ 0/42]  eta: 0:01:06  lr: 0.000179  loss: 0.4692 (0.4692)  time: 1.5914  data: 1.0817  max mem: 9341
[11:42:44.010650] Epoch: [666]  [20/42]  eta: 0:00:12  lr: 0.000178  loss: 0.4818 (0.4828)  time: 0.5084  data: 0.0002  max mem: 9341
[11:42:54.172733] Epoch: [666]  [40/42]  eta: 0:00:01  lr: 0.000177  loss: 0.4763 (0.4759)  time: 0.5081  data: 0.0001  max mem: 9341
[11:42:54.680509] Epoch: [666]  [41/42]  eta: 0:00:00  lr: 0.000177  loss: 0.4763 (0.4765)  time: 0.5081  data: 0.0001  max mem: 9341
[11:42:54.857859] Epoch: [666] Total time: 0:00:22 (0.5383 s / it)
[11:42:54.860433] Averaged stats: lr: 0.000177  loss: 0.4763 (0.4694)
[11:42:59.412168] {"train_lr": 0.00017858202610078412, "train_loss": 0.46940788857284044, "epoch": 666}
[11:42:59.412543] [11:42:59.412625] Training epoch 666 for 0:00:27
[11:42:59.412676] [11:42:59.416873] log_dir: ./exp/debug/cifar100-LT/debug
[11:43:00.887179] Epoch: [667]  [ 0/42]  eta: 0:01:01  lr: 0.000177  loss: 0.4797 (0.4797)  time: 1.4690  data: 0.9600  max mem: 9341
[11:43:11.049405] Epoch: [667]  [20/42]  eta: 0:00:12  lr: 0.000176  loss: 0.4723 (0.4742)  time: 0.5081  data: 0.0001  max mem: 9341
[11:43:21.204446] Epoch: [667]  [40/42]  eta: 0:00:01  lr: 0.000175  loss: 0.4698 (0.4706)  time: 0.5077  data: 0.0001  max mem: 9341
[11:43:21.710845] Epoch: [667]  [41/42]  eta: 0:00:00  lr: 0.000175  loss: 0.4684 (0.4696)  time: 0.5077  data: 0.0001  max mem: 9341
[11:43:21.873787] Epoch: [667] Total time: 0:00:22 (0.5347 s / it)
[11:43:21.884033] Averaged stats: lr: 0.000175  loss: 0.4684 (0.4700)
[11:43:26.424537] {"train_lr": 0.0001759871909302027, "train_loss": 0.4699652895686172, "epoch": 667}
[11:43:26.424939] [11:43:26.425020] Training epoch 667 for 0:00:27
[11:43:26.425071] [11:43:26.429456] log_dir: ./exp/debug/cifar100-LT/debug
[11:43:28.112789] Epoch: [668]  [ 0/42]  eta: 0:01:10  lr: 0.000174  loss: 0.4517 (0.4517)  time: 1.6821  data: 1.1800  max mem: 9341
[11:43:38.278984] Epoch: [668]  [20/42]  eta: 0:00:12  lr: 0.000173  loss: 0.4689 (0.4690)  time: 0.5083  data: 0.0001  max mem: 9341
[11:43:48.425212] Epoch: [668]  [40/42]  eta: 0:00:01  lr: 0.000172  loss: 0.4730 (0.4726)  time: 0.5073  data: 0.0001  max mem: 9341
[11:43:48.930595] Epoch: [668]  [41/42]  eta: 0:00:00  lr: 0.000172  loss: 0.4769 (0.4728)  time: 0.5072  data: 0.0001  max mem: 9341
[11:43:49.104734] Epoch: [668] Total time: 0:00:22 (0.5399 s / it)
[11:43:49.107439] Averaged stats: lr: 0.000172  loss: 0.4769 (0.4685)
[11:43:53.708698] {"train_lr": 0.00017340985331318088, "train_loss": 0.4685159737155551, "epoch": 668}
[11:43:53.708959] [11:43:53.709042] Training epoch 668 for 0:00:27
[11:43:53.709094] [11:43:53.713440] log_dir: ./exp/debug/cifar100-LT/debug
[11:43:55.259651] Epoch: [669]  [ 0/42]  eta: 0:01:04  lr: 0.000172  loss: 0.4470 (0.4470)  time: 1.5449  data: 1.0300  max mem: 9341
[11:44:05.427778] Epoch: [669]  [20/42]  eta: 0:00:12  lr: 0.000171  loss: 0.4717 (0.4709)  time: 0.5083  data: 0.0001  max mem: 9341
[11:44:15.578309] Epoch: [669]  [40/42]  eta: 0:00:01  lr: 0.000170  loss: 0.4637 (0.4683)  time: 0.5075  data: 0.0001  max mem: 9341
[11:44:16.084485] Epoch: [669]  [41/42]  eta: 0:00:00  lr: 0.000170  loss: 0.4637 (0.4673)  time: 0.5075  data: 0.0001  max mem: 9341
[11:44:16.271875] Epoch: [669] Total time: 0:00:22 (0.5371 s / it)
[11:44:16.272602] Averaged stats: lr: 0.000170  loss: 0.4637 (0.4702)
[11:44:20.819332] {"train_lr": 0.00017085005728930753, "train_loss": 0.4702335876368341, "epoch": 669}
[11:44:20.819677] [11:44:20.819758] Training epoch 669 for 0:00:27
[11:44:20.819809] [11:44:20.824354] log_dir: ./exp/debug/cifar100-LT/debug
[11:44:22.403198] Epoch: [670]  [ 0/42]  eta: 0:01:06  lr: 0.000169  loss: 0.4895 (0.4895)  time: 1.5777  data: 1.0658  max mem: 9341
[11:44:32.562165] Epoch: [670]  [20/42]  eta: 0:00:12  lr: 0.000168  loss: 0.4801 (0.4793)  time: 0.5079  data: 0.0001  max mem: 9341
[11:44:42.702969] Epoch: [670]  [40/42]  eta: 0:00:01  lr: 0.000167  loss: 0.4722 (0.4762)  time: 0.5070  data: 0.0001  max mem: 9341
[11:44:43.207971] Epoch: [670]  [41/42]  eta: 0:00:00  lr: 0.000167  loss: 0.4692 (0.4756)  time: 0.5070  data: 0.0001  max mem: 9341
[11:44:43.371068] Epoch: [670] Total time: 0:00:22 (0.5368 s / it)
[11:44:43.388361] Averaged stats: lr: 0.000167  loss: 0.4692 (0.4703)
[11:44:47.970152] {"train_lr": 0.00016830784659843345, "train_loss": 0.47026894046437173, "epoch": 670}
[11:44:47.970528] [11:44:47.970613] Training epoch 670 for 0:00:27
[11:44:47.970667] [11:44:47.975052] log_dir: ./exp/debug/cifar100-LT/debug
[11:44:49.684307] Epoch: [671]  [ 0/42]  eta: 0:01:11  lr: 0.000167  loss: 0.4850 (0.4850)  time: 1.7082  data: 1.2080  max mem: 9341
[11:44:59.841438] Epoch: [671]  [20/42]  eta: 0:00:12  lr: 0.000166  loss: 0.4677 (0.4744)  time: 0.5078  data: 0.0001  max mem: 9341
[11:45:09.978930] Epoch: [671]  [40/42]  eta: 0:00:01  lr: 0.000165  loss: 0.4656 (0.4724)  time: 0.5068  data: 0.0001  max mem: 9341
[11:45:10.485421] Epoch: [671]  [41/42]  eta: 0:00:00  lr: 0.000165  loss: 0.4700 (0.4724)  time: 0.5069  data: 0.0001  max mem: 9341
[11:45:10.648766] Epoch: [671] Total time: 0:00:22 (0.5398 s / it)
[11:45:10.650887] Averaged stats: lr: 0.000165  loss: 0.4700 (0.4688)
[11:45:15.207150] {"train_lr": 0.00016578326467992576, "train_loss": 0.4688321774204572, "epoch": 671}
[11:45:15.207507] [11:45:15.207588] Training epoch 671 for 0:00:27
[11:45:15.207639] [11:45:15.212189] log_dir: ./exp/debug/cifar100-LT/debug
[11:45:16.659612] Epoch: [672]  [ 0/42]  eta: 0:01:00  lr: 0.000164  loss: 0.4574 (0.4574)  time: 1.4461  data: 0.9404  max mem: 9341
[11:45:26.827181] Epoch: [672]  [20/42]  eta: 0:00:12  lr: 0.000163  loss: 0.4630 (0.4628)  time: 0.5083  data: 0.0001  max mem: 9341
[11:45:36.974541] Epoch: [672]  [40/42]  eta: 0:00:01  lr: 0.000162  loss: 0.4720 (0.4692)  time: 0.5073  data: 0.0001  max mem: 9341
[11:45:37.480148] Epoch: [672]  [41/42]  eta: 0:00:00  lr: 0.000162  loss: 0.4796 (0.4695)  time: 0.5073  data: 0.0001  max mem: 9341
[11:45:37.655606] Epoch: [672] Total time: 0:00:22 (0.5344 s / it)
[11:45:37.656316] Averaged stats: lr: 0.000162  loss: 0.4796 (0.4675)
[11:45:42.191544] {"train_lr": 0.00016327635467192344, "train_loss": 0.4675485931691669, "epoch": 672}
[11:45:42.191891] [11:45:42.191972] Training epoch 672 for 0:00:26
[11:45:42.192022] [11:45:42.196383] log_dir: ./exp/debug/cifar100-LT/debug
[11:45:43.850463] Epoch: [673]  [ 0/42]  eta: 0:01:09  lr: 0.000162  loss: 0.4810 (0.4810)  time: 1.6528  data: 1.1504  max mem: 9341
[11:45:54.019161] Epoch: [673]  [20/42]  eta: 0:00:12  lr: 0.000161  loss: 0.4675 (0.4709)  time: 0.5084  data: 0.0001  max mem: 9341
[11:46:04.170998] Epoch: [673]  [40/42]  eta: 0:00:01  lr: 0.000160  loss: 0.4656 (0.4697)  time: 0.5075  data: 0.0001  max mem: 9341
[11:46:04.675860] Epoch: [673]  [41/42]  eta: 0:00:00  lr: 0.000160  loss: 0.4654 (0.4694)  time: 0.5074  data: 0.0001  max mem: 9341
[11:46:04.837165] Epoch: [673] Total time: 0:00:22 (0.5391 s / it)
[11:46:04.846566] Averaged stats: lr: 0.000160  loss: 0.4654 (0.4694)
[11:46:09.371468] {"train_lr": 0.0001607871594106022, "train_loss": 0.4693707538147767, "epoch": 673}
[11:46:09.371820] [11:46:09.371903] Training epoch 673 for 0:00:27
[11:46:09.371971] [11:46:09.376347] log_dir: ./exp/debug/cifar100-LT/debug
[11:46:11.086002] Epoch: [674]  [ 0/42]  eta: 0:01:11  lr: 0.000159  loss: 0.4631 (0.4631)  time: 1.7075  data: 1.2063  max mem: 9341
[11:46:21.249376] Epoch: [674]  [20/42]  eta: 0:00:12  lr: 0.000158  loss: 0.4688 (0.4678)  time: 0.5081  data: 0.0001  max mem: 9341
[11:46:31.405071] Epoch: [674]  [40/42]  eta: 0:00:01  lr: 0.000157  loss: 0.4712 (0.4712)  time: 0.5077  data: 0.0001  max mem: 9341
[11:46:31.910266] Epoch: [674]  [41/42]  eta: 0:00:00  lr: 0.000157  loss: 0.4712 (0.4713)  time: 0.5078  data: 0.0001  max mem: 9341
[11:46:32.083520] Epoch: [674] Total time: 0:00:22 (0.5406 s / it)
[11:46:32.087598] Averaged stats: lr: 0.000157  loss: 0.4712 (0.4687)
[11:46:36.691502] {"train_lr": 0.00015831572142944056, "train_loss": 0.4686639571473712, "epoch": 674}
[11:46:36.691842] [11:46:36.691925] Training epoch 674 for 0:00:27
[11:46:36.691976] [11:46:36.696379] log_dir: ./exp/debug/cifar100-LT/debug
[11:46:38.249680] Epoch: [675]  [ 0/42]  eta: 0:01:05  lr: 0.000157  loss: 0.4717 (0.4717)  time: 1.5521  data: 1.0505  max mem: 9341
[11:46:48.441512] Epoch: [675]  [20/42]  eta: 0:00:12  lr: 0.000156  loss: 0.4741 (0.4653)  time: 0.5095  data: 0.0001  max mem: 9341
[11:46:58.618020] Epoch: [675]  [40/42]  eta: 0:00:01  lr: 0.000155  loss: 0.4639 (0.4644)  time: 0.5088  data: 0.0001  max mem: 9341
[11:46:59.124206] Epoch: [675]  [41/42]  eta: 0:00:00  lr: 0.000155  loss: 0.4671 (0.4650)  time: 0.5088  data: 0.0001  max mem: 9341
[11:46:59.297015] Epoch: [675] Total time: 0:00:22 (0.5381 s / it)
[11:46:59.297843] Averaged stats: lr: 0.000155  loss: 0.4671 (0.4686)
[11:47:03.954038] {"train_lr": 0.00015586208295849526, "train_loss": 0.46857941807025955, "epoch": 675}
[11:47:03.954414] [11:47:03.954494] Training epoch 675 for 0:00:27
[11:47:03.954545] [11:47:03.958927] log_dir: ./exp/debug/cifar100-LT/debug
[11:47:05.611392] Epoch: [676]  [ 0/42]  eta: 0:01:09  lr: 0.000154  loss: 0.4654 (0.4654)  time: 1.6514  data: 1.1437  max mem: 9341
[11:47:15.772204] Epoch: [676]  [20/42]  eta: 0:00:12  lr: 0.000153  loss: 0.4615 (0.4634)  time: 0.5080  data: 0.0001  max mem: 9341
[11:47:25.920805] Epoch: [676]  [40/42]  eta: 0:00:01  lr: 0.000152  loss: 0.4675 (0.4633)  time: 0.5074  data: 0.0001  max mem: 9341
[11:47:26.427554] Epoch: [676]  [41/42]  eta: 0:00:00  lr: 0.000152  loss: 0.4682 (0.4638)  time: 0.5073  data: 0.0001  max mem: 9341
[11:47:26.587705] Epoch: [676] Total time: 0:00:22 (0.5388 s / it)
[11:47:26.607775] Averaged stats: lr: 0.000152  loss: 0.4682 (0.4693)
[11:47:31.218425] {"train_lr": 0.00015342628592367763, "train_loss": 0.4692933089321568, "epoch": 676}
[11:47:31.218785] [11:47:31.218869] Training epoch 676 for 0:00:27
[11:47:31.218922] [11:47:31.223285] log_dir: ./exp/debug/cifar100-LT/debug
[11:47:32.746394] Epoch: [677]  [ 0/42]  eta: 0:01:03  lr: 0.000152  loss: 0.4995 (0.4995)  time: 1.5215  data: 1.0097  max mem: 9341
[11:47:42.908355] Epoch: [677]  [20/42]  eta: 0:00:12  lr: 0.000151  loss: 0.4635 (0.4680)  time: 0.5080  data: 0.0001  max mem: 9341
[11:47:53.044651] Epoch: [677]  [40/42]  eta: 0:00:01  lr: 0.000150  loss: 0.4692 (0.4712)  time: 0.5068  data: 0.0001  max mem: 9341
[11:47:53.550296] Epoch: [677]  [41/42]  eta: 0:00:00  lr: 0.000150  loss: 0.4694 (0.4713)  time: 0.5068  data: 0.0001  max mem: 9341
[11:47:53.712432] Epoch: [677] Total time: 0:00:22 (0.5355 s / it)
[11:47:53.715979] Averaged stats: lr: 0.000150  loss: 0.4694 (0.4693)
[11:47:58.243635] {"train_lr": 0.00015100837194603848, "train_loss": 0.4692898591359456, "epoch": 677}
[11:47:58.243983] [11:47:58.244063] Training epoch 677 for 0:00:27
[11:47:58.244162] [11:47:58.248628] log_dir: ./exp/debug/cifar100-LT/debug
[11:47:59.705458] Epoch: [678]  [ 0/42]  eta: 0:01:01  lr: 0.000149  loss: 0.4459 (0.4459)  time: 1.4554  data: 0.9557  max mem: 9341
[11:48:09.869768] Epoch: [678]  [20/42]  eta: 0:00:12  lr: 0.000148  loss: 0.4671 (0.4649)  time: 0.5082  data: 0.0001  max mem: 9341
[11:48:20.017072] Epoch: [678]  [40/42]  eta: 0:00:01  lr: 0.000148  loss: 0.4703 (0.4640)  time: 0.5073  data: 0.0001  max mem: 9341
[11:48:20.522058] Epoch: [678]  [41/42]  eta: 0:00:00  lr: 0.000148  loss: 0.4704 (0.4643)  time: 0.5072  data: 0.0001  max mem: 9341
[11:48:20.683448] Epoch: [678] Total time: 0:00:22 (0.5342 s / it)
[11:48:20.700372] Averaged stats: lr: 0.000148  loss: 0.4704 (0.4673)
[11:48:25.295963] {"train_lr": 0.0001486083823410561, "train_loss": 0.467314843443178, "epoch": 678}
[11:48:25.296334] [11:48:25.296461] Training epoch 678 for 0:00:27
[11:48:25.296533] [11:48:25.301462] log_dir: ./exp/debug/cifar100-LT/debug
[11:48:26.932267] Epoch: [679]  [ 0/42]  eta: 0:01:08  lr: 0.000147  loss: 0.4738 (0.4738)  time: 1.6299  data: 1.1135  max mem: 9341
[11:48:37.106152] Epoch: [679]  [20/42]  eta: 0:00:12  lr: 0.000146  loss: 0.4726 (0.4776)  time: 0.5087  data: 0.0001  max mem: 9341
[11:48:47.252028] Epoch: [679]  [40/42]  eta: 0:00:01  lr: 0.000145  loss: 0.4576 (0.4717)  time: 0.5072  data: 0.0001  max mem: 9341
[11:48:47.756356] Epoch: [679]  [41/42]  eta: 0:00:00  lr: 0.000145  loss: 0.4576 (0.4718)  time: 0.5071  data: 0.0001  max mem: 9341
[11:48:47.919345] Epoch: [679] Total time: 0:00:22 (0.5385 s / it)
[11:48:47.928815] Averaged stats: lr: 0.000145  loss: 0.4576 (0.4675)
[11:48:52.414931] {"train_lr": 0.00014622635811793193, "train_loss": 0.4674527875724293, "epoch": 679}
[11:48:52.415269] [11:48:52.415350] Training epoch 679 for 0:00:27
[11:48:52.415400] [11:48:52.419789] log_dir: ./exp/debug/cifar100-LT/debug
[11:48:53.945801] Epoch: [680]  [ 0/42]  eta: 0:01:04  lr: 0.000145  loss: 0.5047 (0.5047)  time: 1.5248  data: 1.0232  max mem: 9341
[11:49:04.115572] Epoch: [680]  [20/42]  eta: 0:00:12  lr: 0.000144  loss: 0.4601 (0.4678)  time: 0.5084  data: 0.0001  max mem: 9341
[11:49:14.269216] Epoch: [680]  [40/42]  eta: 0:00:01  lr: 0.000143  loss: 0.4681 (0.4694)  time: 0.5076  data: 0.0001  max mem: 9341
[11:49:14.776750] Epoch: [680]  [41/42]  eta: 0:00:00  lr: 0.000143  loss: 0.4713 (0.4696)  time: 0.5077  data: 0.0001  max mem: 9341
[11:49:14.944372] Epoch: [680] Total time: 0:00:22 (0.5363 s / it)
[11:49:14.945083] Averaged stats: lr: 0.000143  loss: 0.4713 (0.4689)
[11:49:19.561144] {"train_lr": 0.00014386233997888657, "train_loss": 0.46889403302754673, "epoch": 680}
[11:49:19.561508] [11:49:19.561592] Training epoch 680 for 0:00:27
[11:49:19.561643] [11:49:19.566061] log_dir: ./exp/debug/cifar100-LT/debug
[11:49:21.199567] Epoch: [681]  [ 0/42]  eta: 0:01:08  lr: 0.000142  loss: 0.4366 (0.4366)  time: 1.6322  data: 1.1133  max mem: 9341
[11:49:31.363861] Epoch: [681]  [20/42]  eta: 0:00:12  lr: 0.000141  loss: 0.4652 (0.4651)  time: 0.5082  data: 0.0001  max mem: 9341
[11:49:41.511154] Epoch: [681]  [40/42]  eta: 0:00:01  lr: 0.000140  loss: 0.4614 (0.4658)  time: 0.5073  data: 0.0001  max mem: 9341
[11:49:42.018598] Epoch: [681]  [41/42]  eta: 0:00:00  lr: 0.000140  loss: 0.4614 (0.4652)  time: 0.5074  data: 0.0001  max mem: 9341
[11:49:42.180089] Epoch: [681] Total time: 0:00:22 (0.5384 s / it)
[11:49:42.184293] Averaged stats: lr: 0.000140  loss: 0.4614 (0.4666)
[11:49:46.801738] {"train_lr": 0.00014151636831846842, "train_loss": 0.4665723377395244, "epoch": 681}
[11:49:46.802182] [11:49:46.802274] Training epoch 681 for 0:00:27
[11:49:46.802328] [11:49:46.807252] log_dir: ./exp/debug/cifar100-LT/debug
[11:49:48.280495] Epoch: [682]  [ 0/42]  eta: 0:01:01  lr: 0.000140  loss: 0.4669 (0.4669)  time: 1.4721  data: 0.9779  max mem: 9341
[11:49:58.454898] Epoch: [682]  [20/42]  eta: 0:00:12  lr: 0.000139  loss: 0.4692 (0.4662)  time: 0.5087  data: 0.0001  max mem: 9341
[11:50:08.606800] Epoch: [682]  [40/42]  eta: 0:00:01  lr: 0.000138  loss: 0.4660 (0.4683)  time: 0.5075  data: 0.0001  max mem: 9341
[11:50:09.112635] Epoch: [682]  [41/42]  eta: 0:00:00  lr: 0.000138  loss: 0.4664 (0.4688)  time: 0.5076  data: 0.0001  max mem: 9341
[11:50:09.275532] Epoch: [682] Total time: 0:00:22 (0.5350 s / it)
[11:50:09.282332] Averaged stats: lr: 0.000138  loss: 0.4664 (0.4665)
[11:50:13.788014] {"train_lr": 0.0001391884832228596, "train_loss": 0.4664935757006918, "epoch": 682}
[11:50:13.788493] [11:50:13.788580] Training epoch 682 for 0:00:26
[11:50:13.788632] [11:50:13.793071] log_dir: ./exp/debug/cifar100-LT/debug
[11:50:15.398387] Epoch: [683]  [ 0/42]  eta: 0:01:07  lr: 0.000138  loss: 0.4668 (0.4668)  time: 1.6042  data: 1.0993  max mem: 9341
[11:50:25.586398] Epoch: [683]  [20/42]  eta: 0:00:12  lr: 0.000137  loss: 0.4562 (0.4662)  time: 0.5094  data: 0.0001  max mem: 9341
[11:50:35.753688] Epoch: [683]  [40/42]  eta: 0:00:01  lr: 0.000136  loss: 0.4661 (0.4663)  time: 0.5083  data: 0.0001  max mem: 9341
[11:50:36.260806] Epoch: [683]  [41/42]  eta: 0:00:00  lr: 0.000136  loss: 0.4661 (0.4667)  time: 0.5084  data: 0.0001  max mem: 9341
[11:50:36.433670] Epoch: [683] Total time: 0:00:22 (0.5391 s / it)
[11:50:36.434380] Averaged stats: lr: 0.000136  loss: 0.4661 (0.4648)
[11:50:41.036583] {"train_lr": 0.00013687872446919438, "train_loss": 0.4647672057506584, "epoch": 683}
[11:50:41.036987] [11:50:41.037077] Training epoch 683 for 0:00:27
[11:50:41.037129] [11:50:41.041861] log_dir: ./exp/debug/cifar100-LT/debug
[11:50:42.662688] Epoch: [684]  [ 0/42]  eta: 0:01:08  lr: 0.000135  loss: 0.4666 (0.4666)  time: 1.6198  data: 1.1089  max mem: 9341
[11:50:52.832054] Epoch: [684]  [20/42]  eta: 0:00:12  lr: 0.000134  loss: 0.4581 (0.4624)  time: 0.5084  data: 0.0001  max mem: 9341
[11:51:02.983391] Epoch: [684]  [40/42]  eta: 0:00:01  lr: 0.000134  loss: 0.4676 (0.4643)  time: 0.5075  data: 0.0001  max mem: 9341
[11:51:03.489398] Epoch: [684]  [41/42]  eta: 0:00:00  lr: 0.000134  loss: 0.4676 (0.4655)  time: 0.5075  data: 0.0001  max mem: 9341
[11:51:03.661960] Epoch: [684] Total time: 0:00:22 (0.5386 s / it)
[11:51:03.662628] Averaged stats: lr: 0.000134  loss: 0.4676 (0.4694)
[11:51:08.277353] {"train_lr": 0.00013458713152487636, "train_loss": 0.4694197516710985, "epoch": 684}
[11:51:08.277683] [11:51:08.277763] Training epoch 684 for 0:00:27
[11:51:08.277813] [11:51:08.282198] log_dir: ./exp/debug/cifar100-LT/debug
[11:51:09.837856] Epoch: [685]  [ 0/42]  eta: 0:01:05  lr: 0.000133  loss: 0.4980 (0.4980)  time: 1.5544  data: 1.0453  max mem: 9341
[11:51:20.031661] Epoch: [685]  [20/42]  eta: 0:00:12  lr: 0.000132  loss: 0.4689 (0.4721)  time: 0.5096  data: 0.0001  max mem: 9341
[11:51:30.204594] Epoch: [685]  [40/42]  eta: 0:00:01  lr: 0.000131  loss: 0.4755 (0.4718)  time: 0.5086  data: 0.0001  max mem: 9341
[11:51:30.711022] Epoch: [685]  [41/42]  eta: 0:00:00  lr: 0.000131  loss: 0.4734 (0.4715)  time: 0.5086  data: 0.0001  max mem: 9341
[11:51:30.871485] Epoch: [685] Total time: 0:00:22 (0.5378 s / it)
[11:51:30.884047] Averaged stats: lr: 0.000131  loss: 0.4734 (0.4682)
[11:51:35.514837] {"train_lr": 0.00013231374354690648, "train_loss": 0.46824882960035685, "epoch": 685}
[11:51:35.515147] [11:51:35.515228] Training epoch 685 for 0:00:27
[11:51:35.515278] [11:51:35.519667] log_dir: ./exp/debug/cifar100-LT/debug
[11:51:37.101504] Epoch: [686]  [ 0/42]  eta: 0:01:06  lr: 0.000131  loss: 0.4945 (0.4945)  time: 1.5807  data: 1.0611  max mem: 9341
[11:51:47.274610] Epoch: [686]  [20/42]  eta: 0:00:12  lr: 0.000130  loss: 0.4738 (0.4772)  time: 0.5086  data: 0.0001  max mem: 9341
[11:51:57.422719] Epoch: [686]  [40/42]  eta: 0:00:01  lr: 0.000129  loss: 0.4665 (0.4721)  time: 0.5074  data: 0.0001  max mem: 9341
[11:51:57.928154] Epoch: [686]  [41/42]  eta: 0:00:00  lr: 0.000129  loss: 0.4636 (0.4709)  time: 0.5074  data: 0.0001  max mem: 9341
[11:51:58.105238] Epoch: [686] Total time: 0:00:22 (0.5377 s / it)
[11:51:58.108189] Averaged stats: lr: 0.000129  loss: 0.4636 (0.4659)
[11:52:02.732796] {"train_lr": 0.00013005859938121296, "train_loss": 0.4659258191074644, "epoch": 686}
[11:52:02.733129] [11:52:02.733211] Training epoch 686 for 0:00:27
[11:52:02.733262] [11:52:02.737622] log_dir: ./exp/debug/cifar100-LT/debug
[11:52:04.422987] Epoch: [687]  [ 0/42]  eta: 0:01:10  lr: 0.000129  loss: 0.4799 (0.4799)  time: 1.6843  data: 1.1767  max mem: 9341
[11:52:14.588906] Epoch: [687]  [20/42]  eta: 0:00:12  lr: 0.000128  loss: 0.4692 (0.4681)  time: 0.5082  data: 0.0001  max mem: 9341
[11:52:24.739502] Epoch: [687]  [40/42]  eta: 0:00:01  lr: 0.000127  loss: 0.4618 (0.4674)  time: 0.5075  data: 0.0001  max mem: 9341
[11:52:25.244673] Epoch: [687]  [41/42]  eta: 0:00:00  lr: 0.000127  loss: 0.4570 (0.4660)  time: 0.5075  data: 0.0001  max mem: 9341
[11:52:25.415555] Epoch: [687] Total time: 0:00:22 (0.5399 s / it)
[11:52:25.417480] Averaged stats: lr: 0.000127  loss: 0.4570 (0.4695)
[11:52:30.039380] {"train_lr": 0.00012782173756198693, "train_loss": 0.4694954080595857, "epoch": 687}
[11:52:30.039752] [11:52:30.039829] Training epoch 687 for 0:00:27
[11:52:30.039879] [11:52:30.044172] log_dir: ./exp/debug/cifar100-LT/debug
[11:52:31.614121] Epoch: [688]  [ 0/42]  eta: 0:01:05  lr: 0.000126  loss: 0.5463 (0.5463)  time: 1.5686  data: 1.0568  max mem: 9341
[11:52:41.784265] Epoch: [688]  [20/42]  eta: 0:00:12  lr: 0.000125  loss: 0.4618 (0.4713)  time: 0.5085  data: 0.0001  max mem: 9341
[11:52:51.938101] Epoch: [688]  [40/42]  eta: 0:00:01  lr: 0.000125  loss: 0.4595 (0.4653)  time: 0.5076  data: 0.0001  max mem: 9341
[11:52:52.443897] Epoch: [688]  [41/42]  eta: 0:00:00  lr: 0.000125  loss: 0.4595 (0.4652)  time: 0.5077  data: 0.0001  max mem: 9341
[11:52:52.620147] Epoch: [688] Total time: 0:00:22 (0.5375 s / it)
[11:52:52.633999] Averaged stats: lr: 0.000125  loss: 0.4595 (0.4652)
[11:52:57.194411] {"train_lr": 0.0001256031963110258, "train_loss": 0.4652103345308985, "epoch": 688}
[11:52:57.194760] [11:52:57.194848] Training epoch 688 for 0:00:27
[11:52:57.194899] [11:52:57.199754] log_dir: ./exp/debug/cifar100-LT/debug
[11:52:58.842882] Epoch: [689]  [ 0/42]  eta: 0:01:08  lr: 0.000124  loss: 0.4867 (0.4867)  time: 1.6418  data: 1.1363  max mem: 9341
[11:53:09.001282] Epoch: [689]  [20/42]  eta: 0:00:12  lr: 0.000123  loss: 0.4562 (0.4626)  time: 0.5079  data: 0.0001  max mem: 9341
[11:53:19.140181] Epoch: [689]  [40/42]  eta: 0:00:01  lr: 0.000122  loss: 0.4607 (0.4640)  time: 0.5069  data: 0.0001  max mem: 9341
[11:53:19.644535] Epoch: [689]  [41/42]  eta: 0:00:00  lr: 0.000122  loss: 0.4620 (0.4644)  time: 0.5069  data: 0.0001  max mem: 9341
[11:53:19.800446] Epoch: [689] Total time: 0:00:22 (0.5381 s / it)
[11:53:19.811754] Averaged stats: lr: 0.000122  loss: 0.4620 (0.4678)
[11:53:24.427510] {"train_lr": 0.00012340301353707813, "train_loss": 0.4678037484132108, "epoch": 689}
[11:53:24.427861] [11:53:24.427943] Training epoch 689 for 0:00:27
[11:53:24.427996] [11:53:24.432379] log_dir: ./exp/debug/cifar100-LT/debug
[11:53:25.981212] Epoch: [690]  [ 0/42]  eta: 0:01:04  lr: 0.000122  loss: 0.4956 (0.4956)  time: 1.5471  data: 1.0426  max mem: 9341
[11:53:36.161043] Epoch: [690]  [20/42]  eta: 0:00:12  lr: 0.000121  loss: 0.4560 (0.4608)  time: 0.5089  data: 0.0001  max mem: 9341
[11:53:46.329662] Epoch: [690]  [40/42]  eta: 0:00:01  lr: 0.000120  loss: 0.4614 (0.4627)  time: 0.5084  data: 0.0001  max mem: 9341
[11:53:46.835286] Epoch: [690]  [41/42]  eta: 0:00:00  lr: 0.000120  loss: 0.4629 (0.4630)  time: 0.5084  data: 0.0001  max mem: 9341
[11:53:47.003072] Epoch: [690] Total time: 0:00:22 (0.5374 s / it)
[11:53:47.004183] Averaged stats: lr: 0.000120  loss: 0.4629 (0.4652)
[11:53:51.606516] {"train_lr": 0.00012122122683519689, "train_loss": 0.4652228295093491, "epoch": 690}
[11:53:51.606778] [11:53:51.606858] Training epoch 690 for 0:00:27
[11:53:51.606908] [11:53:51.611280] log_dir: ./exp/debug/cifar100-LT/debug
[11:53:53.103150] Epoch: [691]  [ 0/42]  eta: 0:01:02  lr: 0.000120  loss: 0.4513 (0.4513)  time: 1.4900  data: 0.9769  max mem: 9341
[11:54:03.267610] Epoch: [691]  [20/42]  eta: 0:00:12  lr: 0.000119  loss: 0.4645 (0.4663)  time: 0.5082  data: 0.0002  max mem: 9341
[11:54:13.417169] Epoch: [691]  [40/42]  eta: 0:00:01  lr: 0.000118  loss: 0.4679 (0.4666)  time: 0.5074  data: 0.0001  max mem: 9341
[11:54:13.923304] Epoch: [691]  [41/42]  eta: 0:00:00  lr: 0.000118  loss: 0.4679 (0.4679)  time: 0.5075  data: 0.0001  max mem: 9341
[11:54:14.102263] Epoch: [691] Total time: 0:00:22 (0.5355 s / it)
[11:54:14.106994] Averaged stats: lr: 0.000118  loss: 0.4679 (0.4676)
[11:54:18.632928] {"train_lr": 0.00011905787348609707, "train_loss": 0.46759750161852154, "epoch": 691}
[11:54:18.633274] [11:54:18.633371] Training epoch 691 for 0:00:27
[11:54:18.633423] [11:54:18.637793] log_dir: ./exp/debug/cifar100-LT/debug
[11:54:20.203198] Epoch: [692]  [ 0/42]  eta: 0:01:05  lr: 0.000118  loss: 0.4726 (0.4726)  time: 1.5642  data: 1.0665  max mem: 9341
[11:54:30.374177] Epoch: [692]  [20/42]  eta: 0:00:12  lr: 0.000117  loss: 0.4676 (0.4712)  time: 0.5085  data: 0.0001  max mem: 9341
[11:54:40.522982] Epoch: [692]  [40/42]  eta: 0:00:01  lr: 0.000116  loss: 0.4757 (0.4677)  time: 0.5074  data: 0.0001  max mem: 9341
[11:54:41.030025] Epoch: [692]  [41/42]  eta: 0:00:00  lr: 0.000116  loss: 0.4562 (0.4672)  time: 0.5074  data: 0.0001  max mem: 9341
[11:54:41.205212] Epoch: [692] Total time: 0:00:22 (0.5373 s / it)
[11:54:41.213505] Averaged stats: lr: 0.000116  loss: 0.4562 (0.4688)
[11:54:45.825792] {"train_lr": 0.00011691299045551865, "train_loss": 0.4687792325303668, "epoch": 692}
[11:54:45.826158] [11:54:45.826265] Training epoch 692 for 0:00:27
[11:54:45.826319] [11:54:45.830684] log_dir: ./exp/debug/cifar100-LT/debug
[11:54:47.363713] Epoch: [693]  [ 0/42]  eta: 0:01:04  lr: 0.000115  loss: 0.4609 (0.4609)  time: 1.5318  data: 1.0198  max mem: 9341
[11:54:57.549310] Epoch: [693]  [20/42]  eta: 0:00:12  lr: 0.000115  loss: 0.4744 (0.4764)  time: 0.5092  data: 0.0001  max mem: 9341
[11:55:07.712879] Epoch: [693]  [40/42]  eta: 0:00:01  lr: 0.000114  loss: 0.4616 (0.4693)  time: 0.5081  data: 0.0001  max mem: 9341
[11:55:08.219236] Epoch: [693]  [41/42]  eta: 0:00:00  lr: 0.000114  loss: 0.4699 (0.4700)  time: 0.5082  data: 0.0001  max mem: 9341
[11:55:08.396907] Epoch: [693] Total time: 0:00:22 (0.5373 s / it)
[11:55:08.397794] Averaged stats: lr: 0.000114  loss: 0.4699 (0.4669)
[11:55:12.976831] {"train_lr": 0.0001147866143935944, "train_loss": 0.4668651565554596, "epoch": 693}
[11:55:12.977179] [11:55:12.977259] Training epoch 693 for 0:00:27
[11:55:12.977311] [11:55:12.981709] log_dir: ./exp/debug/cifar100-LT/debug
[11:55:14.725372] Epoch: [694]  [ 0/42]  eta: 0:01:13  lr: 0.000113  loss: 0.4422 (0.4422)  time: 1.7426  data: 1.2503  max mem: 9341
[11:55:24.896514] Epoch: [694]  [20/42]  eta: 0:00:12  lr: 0.000113  loss: 0.4664 (0.4649)  time: 0.5085  data: 0.0001  max mem: 9341
[11:55:35.043021] Epoch: [694]  [40/42]  eta: 0:00:01  lr: 0.000112  loss: 0.4667 (0.4692)  time: 0.5073  data: 0.0001  max mem: 9341
[11:55:35.549038] Epoch: [694]  [41/42]  eta: 0:00:00  lr: 0.000112  loss: 0.4738 (0.4697)  time: 0.5073  data: 0.0001  max mem: 9341
[11:55:35.722646] Epoch: [694] Total time: 0:00:22 (0.5414 s / it)
[11:55:35.728303] Averaged stats: lr: 0.000112  loss: 0.4738 (0.4673)
[11:55:40.254298] {"train_lr": 0.00011267878163422485, "train_loss": 0.4672672462960084, "epoch": 694}
[11:55:40.254665] [11:55:40.254748] Training epoch 694 for 0:00:27
[11:55:40.254829] [11:55:40.259020] log_dir: ./exp/debug/cifar100-LT/debug
[11:55:41.680363] Epoch: [695]  [ 0/42]  eta: 0:00:59  lr: 0.000111  loss: 0.4873 (0.4873)  time: 1.4200  data: 0.9083  max mem: 9341
[11:55:51.845737] Epoch: [695]  [20/42]  eta: 0:00:12  lr: 0.000110  loss: 0.4607 (0.4662)  time: 0.5082  data: 0.0001  max mem: 9341
[11:56:01.992520] Epoch: [695]  [40/42]  eta: 0:00:01  lr: 0.000110  loss: 0.4654 (0.4633)  time: 0.5073  data: 0.0001  max mem: 9341
[11:56:02.498456] Epoch: [695]  [41/42]  eta: 0:00:00  lr: 0.000110  loss: 0.4611 (0.4633)  time: 0.5073  data: 0.0001  max mem: 9341
[11:56:02.671816] Epoch: [695] Total time: 0:00:22 (0.5336 s / it)
[11:56:02.678585] Averaged stats: lr: 0.000110  loss: 0.4611 (0.4686)
[11:56:07.201757] {"train_lr": 0.00011058952819445585, "train_loss": 0.4686343084488596, "epoch": 695}
[11:56:07.202109] [11:56:07.202206] Training epoch 695 for 0:00:26
[11:56:07.202259] [11:56:07.206582] log_dir: ./exp/debug/cifar100-LT/debug
[11:56:08.814478] Epoch: [696]  [ 0/42]  eta: 0:01:07  lr: 0.000109  loss: 0.4526 (0.4526)  time: 1.6067  data: 1.1142  max mem: 9341
[11:56:19.004971] Epoch: [696]  [20/42]  eta: 0:00:12  lr: 0.000108  loss: 0.4620 (0.4609)  time: 0.5095  data: 0.0001  max mem: 9341
[11:56:29.168400] Epoch: [696]  [40/42]  eta: 0:00:01  lr: 0.000108  loss: 0.4729 (0.4642)  time: 0.5081  data: 0.0001  max mem: 9341
[11:56:29.674408] Epoch: [696]  [41/42]  eta: 0:00:00  lr: 0.000108  loss: 0.4647 (0.4638)  time: 0.5081  data: 0.0001  max mem: 9341
[11:56:29.836693] Epoch: [696] Total time: 0:00:22 (0.5388 s / it)
[11:56:29.842768] Averaged stats: lr: 0.000108  loss: 0.4647 (0.4694)
[11:56:34.454490] {"train_lr": 0.00010851888977386436, "train_loss": 0.46944840323357356, "epoch": 696}
[11:56:34.454808] [11:56:34.454889] Training epoch 696 for 0:00:27
[11:56:34.454939] [11:56:34.459238] log_dir: ./exp/debug/cifar100-LT/debug
[11:56:36.018740] Epoch: [697]  [ 0/42]  eta: 0:01:05  lr: 0.000107  loss: 0.4546 (0.4546)  time: 1.5580  data: 1.0515  max mem: 9341
[11:56:46.176978] Epoch: [697]  [20/42]  eta: 0:00:12  lr: 0.000106  loss: 0.4719 (0.4639)  time: 0.5078  data: 0.0001  max mem: 9341
[11:56:56.335503] Epoch: [697]  [40/42]  eta: 0:00:01  lr: 0.000106  loss: 0.4670 (0.4629)  time: 0.5079  data: 0.0001  max mem: 9341
[11:56:56.841409] Epoch: [697]  [41/42]  eta: 0:00:00  lr: 0.000106  loss: 0.4670 (0.4633)  time: 0.5079  data: 0.0001  max mem: 9341
[11:56:57.006302] Epoch: [697] Total time: 0:00:22 (0.5368 s / it)
[11:56:57.013306] Averaged stats: lr: 0.000106  loss: 0.4670 (0.4666)
[11:57:01.615211] {"train_lr": 0.00010646690175394733, "train_loss": 0.4665963976156144, "epoch": 697}
[11:57:01.615682] [11:57:01.615771] Training epoch 697 for 0:00:27
[11:57:01.615823] [11:57:01.620926] log_dir: ./exp/debug/cifar100-LT/debug
[11:57:03.331180] Epoch: [698]  [ 0/42]  eta: 0:01:11  lr: 0.000105  loss: 0.4207 (0.4207)  time: 1.7092  data: 1.2064  max mem: 9341
[11:57:13.492173] Epoch: [698]  [20/42]  eta: 0:00:12  lr: 0.000104  loss: 0.4714 (0.4729)  time: 0.5080  data: 0.0001  max mem: 9341
[11:57:23.638712] Epoch: [698]  [40/42]  eta: 0:00:01  lr: 0.000104  loss: 0.4672 (0.4706)  time: 0.5073  data: 0.0001  max mem: 9341
[11:57:24.146903] Epoch: [698]  [41/42]  eta: 0:00:00  lr: 0.000104  loss: 0.4672 (0.4705)  time: 0.5074  data: 0.0001  max mem: 9341
[11:57:24.311154] Epoch: [698] Total time: 0:00:22 (0.5402 s / it)
[11:57:24.317769] Averaged stats: lr: 0.000104  loss: 0.4672 (0.4648)
[11:57:28.905622] {"train_lr": 0.00010443359919751918, "train_loss": 0.46481955761001226, "epoch": 698}
[11:57:28.906009] [11:57:28.906097] Training epoch 698 for 0:00:27
[11:57:28.906149] [11:57:28.910550] log_dir: ./exp/debug/cifar100-LT/debug
[11:57:30.491637] Epoch: [699]  [ 0/42]  eta: 0:01:06  lr: 0.000103  loss: 0.4408 (0.4408)  time: 1.5798  data: 1.0625  max mem: 9341
[11:57:40.675578] Epoch: [699]  [20/42]  eta: 0:00:12  lr: 0.000102  loss: 0.4796 (0.4753)  time: 0.5091  data: 0.0001  max mem: 9341
[11:57:50.844006] Epoch: [699]  [40/42]  eta: 0:00:01  lr: 0.000102  loss: 0.4624 (0.4727)  time: 0.5084  data: 0.0001  max mem: 9341
[11:57:51.349430] Epoch: [699]  [41/42]  eta: 0:00:00  lr: 0.000102  loss: 0.4624 (0.4712)  time: 0.5084  data: 0.0001  max mem: 9341
[11:57:51.514150] Epoch: [699] Total time: 0:00:22 (0.5382 s / it)
[11:57:51.531515] Averaged stats: lr: 0.000102  loss: 0.4624 (0.4662)
[11:57:56.158628] {"train_lr": 0.00010241901684811036, "train_loss": 0.4662286795320965, "epoch": 699}
[11:57:56.158990] [11:57:56.159072] Training epoch 699 for 0:00:27
[11:57:56.159123] [11:57:56.163365] log_dir: ./exp/debug/cifar100-LT/debug
[11:57:57.705184] Epoch: [700]  [ 0/42]  eta: 0:01:04  lr: 0.000101  loss: 0.4494 (0.4494)  time: 1.5398  data: 1.0313  max mem: 9341
[11:58:07.866871] Epoch: [700]  [20/42]  eta: 0:00:12  lr: 0.000100  loss: 0.4687 (0.4669)  time: 0.5080  data: 0.0001  max mem: 9341
[11:58:18.014503] Epoch: [700]  [40/42]  eta: 0:00:01  lr: 0.000100  loss: 0.4717 (0.4679)  time: 0.5073  data: 0.0001  max mem: 9341
[11:58:18.520332] Epoch: [700]  [41/42]  eta: 0:00:00  lr: 0.000100  loss: 0.4556 (0.4673)  time: 0.5073  data: 0.0001  max mem: 9341
[11:58:18.680868] Epoch: [700] Total time: 0:00:22 (0.5361 s / it)
[11:58:18.697051] Averaged stats: lr: 0.000100  loss: 0.4556 (0.4651)
[11:58:23.271887] {"train_lr": 0.00010042318912937455, "train_loss": 0.46510767262606395, "epoch": 700}
[11:58:23.272299] [11:58:23.272386] Training epoch 700 for 0:00:27
[11:58:23.272474] [11:58:23.276901] log_dir: ./exp/debug/cifar100-LT/debug
[11:58:24.804865] Epoch: [701]  [ 0/42]  eta: 0:01:04  lr: 0.000099  loss: 0.4424 (0.4424)  time: 1.5267  data: 1.0113  max mem: 9341
[11:58:34.984577] Epoch: [701]  [20/42]  eta: 0:00:12  lr: 0.000098  loss: 0.4584 (0.4660)  time: 0.5089  data: 0.0001  max mem: 9341
[11:58:45.140509] Epoch: [701]  [40/42]  eta: 0:00:01  lr: 0.000098  loss: 0.4541 (0.4636)  time: 0.5077  data: 0.0001  max mem: 9341
[11:58:45.647505] Epoch: [701]  [41/42]  eta: 0:00:00  lr: 0.000098  loss: 0.4592 (0.4643)  time: 0.5078  data: 0.0001  max mem: 9341
[11:58:45.794280] Epoch: [701] Total time: 0:00:22 (0.5361 s / it)
[11:58:45.813832] Averaged stats: lr: 0.000098  loss: 0.4592 (0.4657)
[11:58:50.395169] {"train_lr": 9.844615014450067e-05, "train_loss": 0.4657314336370854, "epoch": 701}
[11:58:50.395520] [11:58:50.395599] Training epoch 701 for 0:00:27
[11:58:50.395649] [11:58:50.400045] log_dir: ./exp/debug/cifar100-LT/debug
[11:58:52.088721] Epoch: [702]  [ 0/42]  eta: 0:01:10  lr: 0.000097  loss: 0.4487 (0.4487)  time: 1.6876  data: 1.1812  max mem: 9341
[11:59:02.248830] Epoch: [702]  [20/42]  eta: 0:00:12  lr: 0.000096  loss: 0.4624 (0.4666)  time: 0.5079  data: 0.0001  max mem: 9341
[11:59:12.405024] Epoch: [702]  [40/42]  eta: 0:00:01  lr: 0.000096  loss: 0.4660 (0.4704)  time: 0.5078  data: 0.0001  max mem: 9341
[11:59:12.911451] Epoch: [702]  [41/42]  eta: 0:00:00  lr: 0.000096  loss: 0.4660 (0.4699)  time: 0.5078  data: 0.0001  max mem: 9341
[11:59:13.074233] Epoch: [702] Total time: 0:00:22 (0.5399 s / it)
[11:59:13.080047] Averaged stats: lr: 0.000096  loss: 0.4660 (0.4683)
[11:59:17.585625] {"train_lr": 9.648793367563067e-05, "train_loss": 0.4683135552775292, "epoch": 702}
[11:59:17.585988] [11:59:17.586070] Training epoch 702 for 0:00:27
[11:59:17.586121] [11:59:17.590498] log_dir: ./exp/debug/cifar100-LT/debug
[11:59:19.166766] Epoch: [703]  [ 0/42]  eta: 0:01:06  lr: 0.000095  loss: 0.5163 (0.5163)  time: 1.5751  data: 1.0719  max mem: 9341
[11:59:29.353709] Epoch: [703]  [20/42]  eta: 0:00:12  lr: 0.000094  loss: 0.4779 (0.4754)  time: 0.5093  data: 0.0001  max mem: 9341
[11:59:39.522673] Epoch: [703]  [40/42]  eta: 0:00:01  lr: 0.000094  loss: 0.4649 (0.4704)  time: 0.5084  data: 0.0001  max mem: 9341
[11:59:40.029204] Epoch: [703]  [41/42]  eta: 0:00:00  lr: 0.000094  loss: 0.4649 (0.4703)  time: 0.5085  data: 0.0001  max mem: 9341
[11:59:40.208242] Epoch: [703] Total time: 0:00:22 (0.5385 s / it)
[11:59:40.216541] Averaged stats: lr: 0.000094  loss: 0.4649 (0.4657)
[11:59:44.732365] {"train_lr": 9.454857318328071e-05, "train_loss": 0.46567147846023244, "epoch": 703}
[11:59:44.732783] [11:59:44.732869] Training epoch 703 for 0:00:27
[11:59:44.732920] [11:59:44.737303] log_dir: ./exp/debug/cifar100-LT/debug
[11:59:46.511286] Epoch: [704]  [ 0/42]  eta: 0:01:14  lr: 0.000093  loss: 0.4747 (0.4747)  time: 1.7730  data: 1.2679  max mem: 9341
[11:59:56.697175] Epoch: [704]  [20/42]  eta: 0:00:12  lr: 0.000093  loss: 0.4626 (0.4668)  time: 0.5092  data: 0.0001  max mem: 9341
[12:00:06.862853] Epoch: [704]  [40/42]  eta: 0:00:01  lr: 0.000092  loss: 0.4632 (0.4678)  time: 0.5082  data: 0.0001  max mem: 9341
[12:00:07.367879] Epoch: [704]  [41/42]  eta: 0:00:00  lr: 0.000092  loss: 0.4632 (0.4673)  time: 0.5082  data: 0.0001  max mem: 9341
[12:00:07.548884] Epoch: [704] Total time: 0:00:22 (0.5431 s / it)
[12:00:07.549580] Averaged stats: lr: 0.000092  loss: 0.4632 (0.4656)
[12:00:12.102602] {"train_lr": 9.262810180577054e-05, "train_loss": 0.4655777508659022, "epoch": 704}
[12:00:12.102932] [12:00:12.103011] Training epoch 704 for 0:00:27
[12:00:12.103061] [12:00:12.107415] log_dir: ./exp/debug/cifar100-LT/debug
[12:00:13.726623] Epoch: [705]  [ 0/42]  eta: 0:01:07  lr: 0.000091  loss: 0.4483 (0.4483)  time: 1.6179  data: 1.1059  max mem: 9341
[12:00:23.884565] Epoch: [705]  [20/42]  eta: 0:00:12  lr: 0.000091  loss: 0.4617 (0.4661)  time: 0.5078  data: 0.0001  max mem: 9341
[12:00:34.028893] Epoch: [705]  [40/42]  eta: 0:00:01  lr: 0.000090  loss: 0.4646 (0.4653)  time: 0.5072  data: 0.0001  max mem: 9341
[12:00:34.535449] Epoch: [705]  [41/42]  eta: 0:00:00  lr: 0.000090  loss: 0.4650 (0.4659)  time: 0.5072  data: 0.0001  max mem: 9341
[12:00:34.704931] Epoch: [705] Total time: 0:00:22 (0.5380 s / it)
[12:00:34.705617] Averaged stats: lr: 0.000090  loss: 0.4650 (0.4665)
[12:00:39.247520] {"train_lr": 9.072655235865773e-05, "train_loss": 0.46650505332010134, "epoch": 705}
[12:00:39.247865] [12:00:39.247949] Training epoch 705 for 0:00:27
[12:00:39.248001] [12:00:39.252680] log_dir: ./exp/debug/cifar100-LT/debug
[12:00:40.675336] Epoch: [706]  [ 0/42]  eta: 0:00:59  lr: 0.000089  loss: 0.5149 (0.5149)  time: 1.4214  data: 0.9035  max mem: 9341
[12:00:50.826178] Epoch: [706]  [20/42]  eta: 0:00:12  lr: 0.000089  loss: 0.4727 (0.4722)  time: 0.5075  data: 0.0001  max mem: 9341
[12:01:00.973414] Epoch: [706]  [40/42]  eta: 0:00:01  lr: 0.000088  loss: 0.4627 (0.4670)  time: 0.5073  data: 0.0001  max mem: 9341
[12:01:01.479759] Epoch: [706]  [41/42]  eta: 0:00:00  lr: 0.000088  loss: 0.4627 (0.4676)  time: 0.5074  data: 0.0001  max mem: 9341
[12:01:01.637429] Epoch: [706] Total time: 0:00:22 (0.5330 s / it)
[12:01:01.646752] Averaged stats: lr: 0.000088  loss: 0.4627 (0.4663)
[12:01:06.260172] {"train_lr": 8.884395733417557e-05, "train_loss": 0.46629848721481504, "epoch": 706}
[12:01:06.260531] [12:01:06.260610] Training epoch 706 for 0:00:27
[12:01:06.260662] [12:01:06.264973] log_dir: ./exp/debug/cifar100-LT/debug
[12:01:07.775757] Epoch: [707]  [ 0/42]  eta: 0:01:03  lr: 0.000088  loss: 0.4728 (0.4728)  time: 1.5086  data: 0.9951  max mem: 9341
[12:01:17.942990] Epoch: [707]  [20/42]  eta: 0:00:12  lr: 0.000087  loss: 0.4688 (0.4707)  time: 0.5083  data: 0.0001  max mem: 9341
[12:01:28.099931] Epoch: [707]  [40/42]  eta: 0:00:01  lr: 0.000086  loss: 0.4621 (0.4683)  time: 0.5078  data: 0.0001  max mem: 9341
[12:01:28.605420] Epoch: [707]  [41/42]  eta: 0:00:00  lr: 0.000086  loss: 0.4601 (0.4680)  time: 0.5078  data: 0.0001  max mem: 9341
[12:01:28.775341] Epoch: [707] Total time: 0:00:22 (0.5360 s / it)
[12:01:28.781465] Averaged stats: lr: 0.000086  loss: 0.4601 (0.4659)
[12:01:33.437455] {"train_lr": 8.698034890067881e-05, "train_loss": 0.46591180332359816, "epoch": 707}
[12:01:33.437770] [12:01:33.437852] Training epoch 707 for 0:00:27
[12:01:33.437903] [12:01:33.442209] log_dir: ./exp/debug/cifar100-LT/debug
[12:01:35.123901] Epoch: [708]  [ 0/42]  eta: 0:01:10  lr: 0.000086  loss: 0.5018 (0.5018)  time: 1.6806  data: 1.1721  max mem: 9341
[12:01:45.300160] Epoch: [708]  [20/42]  eta: 0:00:12  lr: 0.000085  loss: 0.4509 (0.4619)  time: 0.5088  data: 0.0001  max mem: 9341
[12:01:55.453976] Epoch: [708]  [40/42]  eta: 0:00:01  lr: 0.000084  loss: 0.4663 (0.4636)  time: 0.5076  data: 0.0001  max mem: 9341
[12:01:55.961062] Epoch: [708]  [41/42]  eta: 0:00:00  lr: 0.000084  loss: 0.4711 (0.4639)  time: 0.5077  data: 0.0001  max mem: 9341
[12:01:56.157859] Epoch: [708] Total time: 0:00:22 (0.5408 s / it)
[12:01:56.159124] Averaged stats: lr: 0.000084  loss: 0.4711 (0.4627)
[12:02:00.654483] {"train_lr": 8.513575890209328e-05, "train_loss": 0.4627097529314813, "epoch": 708}
[12:02:00.654836] [12:02:00.654924] Training epoch 708 for 0:00:27
[12:02:00.654976] [12:02:00.659805] log_dir: ./exp/debug/cifar100-LT/debug
[12:02:02.441187] Epoch: [709]  [ 0/42]  eta: 0:01:14  lr: 0.000084  loss: 0.4774 (0.4774)  time: 1.7803  data: 1.2749  max mem: 9341
[12:02:12.606169] Epoch: [709]  [20/42]  eta: 0:00:12  lr: 0.000083  loss: 0.4668 (0.4731)  time: 0.5082  data: 0.0001  max mem: 9341
[12:02:22.763675] Epoch: [709]  [40/42]  eta: 0:00:01  lr: 0.000083  loss: 0.4703 (0.4737)  time: 0.5078  data: 0.0001  max mem: 9341
[12:02:23.269667] Epoch: [709]  [41/42]  eta: 0:00:00  lr: 0.000083  loss: 0.4703 (0.4727)  time: 0.5078  data: 0.0001  max mem: 9341
[12:02:23.433953] Epoch: [709] Total time: 0:00:22 (0.5422 s / it)
[12:02:23.438215] Averaged stats: lr: 0.000083  loss: 0.4703 (0.4665)
[12:02:27.948473] {"train_lr": 8.331021885737333e-05, "train_loss": 0.46649067831181346, "epoch": 709}
[12:02:27.948863] [12:02:27.948947] Training epoch 709 for 0:00:27
[12:02:27.948999] [12:02:27.953475] log_dir: ./exp/debug/cifar100-LT/debug
[12:02:29.509802] Epoch: [710]  [ 0/42]  eta: 0:01:05  lr: 0.000082  loss: 0.4469 (0.4469)  time: 1.5552  data: 1.0488  max mem: 9341
[12:02:39.679199] Epoch: [710]  [20/42]  eta: 0:00:12  lr: 0.000081  loss: 0.4691 (0.4658)  time: 0.5084  data: 0.0001  max mem: 9341
[12:02:49.835321] Epoch: [710]  [40/42]  eta: 0:00:01  lr: 0.000081  loss: 0.4610 (0.4639)  time: 0.5078  data: 0.0001  max mem: 9341
[12:02:50.341971] Epoch: [710]  [41/42]  eta: 0:00:00  lr: 0.000081  loss: 0.4605 (0.4634)  time: 0.5078  data: 0.0001  max mem: 9341
[12:02:50.507586] Epoch: [710] Total time: 0:00:22 (0.5370 s / it)
[12:02:50.517621] Averaged stats: lr: 0.000081  loss: 0.4605 (0.4677)
[12:02:55.102342] {"train_lr": 8.15037599599612e-05, "train_loss": 0.4677112767738955, "epoch": 710}
[12:02:55.102735] [12:02:55.102822] Training epoch 710 for 0:00:27
[12:02:55.102875] [12:02:55.107368] log_dir: ./exp/debug/cifar100-LT/debug
[12:02:56.707338] Epoch: [711]  [ 0/42]  eta: 0:01:07  lr: 0.000080  loss: 0.4557 (0.4557)  time: 1.5987  data: 1.0949  max mem: 9341
[12:03:06.874299] Epoch: [711]  [20/42]  eta: 0:00:12  lr: 0.000080  loss: 0.4490 (0.4669)  time: 0.5083  data: 0.0001  max mem: 9341
[12:03:17.026387] Epoch: [711]  [40/42]  eta: 0:00:01  lr: 0.000079  loss: 0.4609 (0.4649)  time: 0.5075  data: 0.0001  max mem: 9341
[12:03:17.532701] Epoch: [711]  [41/42]  eta: 0:00:00  lr: 0.000079  loss: 0.4604 (0.4647)  time: 0.5076  data: 0.0001  max mem: 9341
[12:03:17.707656] Epoch: [711] Total time: 0:00:22 (0.5381 s / it)
[12:03:17.708353] Averaged stats: lr: 0.000079  loss: 0.4604 (0.4661)
[12:03:22.265073] {"train_lr": 7.971641307725522e-05, "train_loss": 0.466051337264833, "epoch": 711}
[12:03:22.265451] [12:03:22.265536] Training epoch 711 for 0:00:27
[12:03:22.265588] [12:03:22.270114] log_dir: ./exp/debug/cifar100-LT/debug
[12:03:23.762505] Epoch: [712]  [ 0/42]  eta: 0:01:02  lr: 0.000079  loss: 0.4481 (0.4481)  time: 1.4910  data: 0.9888  max mem: 9341
[12:03:33.926905] Epoch: [712]  [20/42]  eta: 0:00:12  lr: 0.000078  loss: 0.4682 (0.4644)  time: 0.5082  data: 0.0001  max mem: 9341
[12:03:44.072314] Epoch: [712]  [40/42]  eta: 0:00:01  lr: 0.000077  loss: 0.4607 (0.4645)  time: 0.5072  data: 0.0001  max mem: 9341
[12:03:44.576750] Epoch: [712]  [41/42]  eta: 0:00:00  lr: 0.000077  loss: 0.4588 (0.4643)  time: 0.5071  data: 0.0001  max mem: 9341
[12:03:44.748558] Epoch: [712] Total time: 0:00:22 (0.5352 s / it)
[12:03:44.756292] Averaged stats: lr: 0.000077  loss: 0.4588 (0.4660)
[12:03:49.264506] {"train_lr": 7.794820875008195e-05, "train_loss": 0.46600617024870145, "epoch": 712}
[12:03:49.264843] [12:03:49.264922] Training epoch 712 for 0:00:26
[12:03:49.264972] [12:03:49.269773] log_dir: ./exp/debug/cifar100-LT/debug
[12:03:50.986090] Epoch: [713]  [ 0/42]  eta: 0:01:12  lr: 0.000077  loss: 0.4542 (0.4542)  time: 1.7153  data: 1.2041  max mem: 9341
[12:04:01.147078] Epoch: [713]  [20/42]  eta: 0:00:12  lr: 0.000076  loss: 0.4636 (0.4671)  time: 0.5080  data: 0.0001  max mem: 9341
[12:04:11.299145] Epoch: [713]  [40/42]  eta: 0:00:01  lr: 0.000075  loss: 0.4594 (0.4649)  time: 0.5075  data: 0.0001  max mem: 9341
[12:04:11.804460] Epoch: [713]  [41/42]  eta: 0:00:00  lr: 0.000075  loss: 0.4594 (0.4649)  time: 0.5075  data: 0.0001  max mem: 9341
[12:04:11.975646] Epoch: [713] Total time: 0:00:22 (0.5406 s / it)
[12:04:11.976418] Averaged stats: lr: 0.000075  loss: 0.4594 (0.4678)
[12:04:16.475596] {"train_lr": 7.619917719217527e-05, "train_loss": 0.4678026219563825, "epoch": 713}
[12:04:16.475951] [12:04:16.476033] Training epoch 713 for 0:00:27
[12:04:16.476132] [12:04:16.480545] log_dir: ./exp/debug/cifar100-LT/debug
[12:04:18.068812] Epoch: [714]  [ 0/42]  eta: 0:01:06  lr: 0.000075  loss: 0.4990 (0.4990)  time: 1.5869  data: 1.0833  max mem: 9341
[12:04:28.233921] Epoch: [714]  [20/42]  eta: 0:00:12  lr: 0.000074  loss: 0.4712 (0.4701)  time: 0.5082  data: 0.0001  max mem: 9341
[12:04:38.389216] Epoch: [714]  [40/42]  eta: 0:00:01  lr: 0.000074  loss: 0.4624 (0.4678)  time: 0.5077  data: 0.0001  max mem: 9341
[12:04:38.895050] Epoch: [714]  [41/42]  eta: 0:00:00  lr: 0.000074  loss: 0.4624 (0.4671)  time: 0.5076  data: 0.0001  max mem: 9341
[12:04:39.052540] Epoch: [714] Total time: 0:00:22 (0.5374 s / it)
[12:04:39.069132] Averaged stats: lr: 0.000074  loss: 0.4624 (0.4680)
[12:04:43.553267] {"train_lr": 7.446934828965823e-05, "train_loss": 0.4679884451131026, "epoch": 714}
[12:04:43.553623] [12:04:43.553706] Training epoch 714 for 0:00:27
[12:04:43.553757] [12:04:43.558143] log_dir: ./exp/debug/cifar100-LT/debug
[12:04:45.222146] Epoch: [715]  [ 0/42]  eta: 0:01:09  lr: 0.000073  loss: 0.4874 (0.4874)  time: 1.6626  data: 1.1547  max mem: 9341
[12:04:55.393035] Epoch: [715]  [20/42]  eta: 0:00:12  lr: 0.000073  loss: 0.4621 (0.4601)  time: 0.5085  data: 0.0001  max mem: 9341
[12:05:05.545164] Epoch: [715]  [40/42]  eta: 0:00:01  lr: 0.000072  loss: 0.4644 (0.4656)  time: 0.5076  data: 0.0001  max mem: 9341
[12:05:06.052570] Epoch: [715]  [41/42]  eta: 0:00:00  lr: 0.000072  loss: 0.4665 (0.4658)  time: 0.5076  data: 0.0001  max mem: 9341
[12:05:06.219036] Epoch: [715] Total time: 0:00:22 (0.5395 s / it)
[12:05:06.226210] Averaged stats: lr: 0.000072  loss: 0.4665 (0.4670)
[12:05:10.722604] {"train_lr": 7.275875160053413e-05, "train_loss": 0.46701275113792645, "epoch": 715}
[12:05:10.722958] [12:05:10.723041] Training epoch 715 for 0:00:27
[12:05:10.723091] [12:05:10.727471] log_dir: ./exp/debug/cifar100-LT/debug
[12:05:12.253444] Epoch: [716]  [ 0/42]  eta: 0:01:03  lr: 0.000072  loss: 0.4730 (0.4730)  time: 1.5236  data: 0.9936  max mem: 9341
[12:05:22.412670] Epoch: [716]  [20/42]  eta: 0:00:12  lr: 0.000071  loss: 0.4674 (0.4566)  time: 0.5079  data: 0.0002  max mem: 9341
[12:05:32.547207] Epoch: [716]  [40/42]  eta: 0:00:01  lr: 0.000070  loss: 0.4674 (0.4617)  time: 0.5067  data: 0.0001  max mem: 9341
[12:05:33.053404] Epoch: [716]  [41/42]  eta: 0:00:00  lr: 0.000070  loss: 0.4648 (0.4604)  time: 0.5067  data: 0.0001  max mem: 9341
[12:05:33.225100] Epoch: [716] Total time: 0:00:22 (0.5357 s / it)
[12:05:33.230281] Averaged stats: lr: 0.000070  loss: 0.4648 (0.4669)
[12:05:37.846339] {"train_lr": 7.106741635418099e-05, "train_loss": 0.4669122041336128, "epoch": 716}
[12:05:37.846687] [12:05:37.846770] Training epoch 716 for 0:00:27
[12:05:37.846821] [12:05:37.851221] log_dir: ./exp/debug/cifar100-LT/debug
[12:05:39.542985] Epoch: [717]  [ 0/42]  eta: 0:01:11  lr: 0.000070  loss: 0.4582 (0.4582)  time: 1.6907  data: 1.1701  max mem: 9341
[12:05:49.711218] Epoch: [717]  [20/42]  eta: 0:00:12  lr: 0.000069  loss: 0.4665 (0.4655)  time: 0.5084  data: 0.0001  max mem: 9341
[12:05:59.862531] Epoch: [717]  [40/42]  eta: 0:00:01  lr: 0.000069  loss: 0.4587 (0.4644)  time: 0.5075  data: 0.0001  max mem: 9341
[12:06:00.369821] Epoch: [717]  [41/42]  eta: 0:00:00  lr: 0.000069  loss: 0.4630 (0.4647)  time: 0.5075  data: 0.0001  max mem: 9341
[12:06:00.527277] Epoch: [717] Total time: 0:00:22 (0.5399 s / it)
[12:06:00.536472] Averaged stats: lr: 0.000069  loss: 0.4630 (0.4660)
[12:06:05.053392] {"train_lr": 6.939537145085125e-05, "train_loss": 0.4659923469381673, "epoch": 717}
[12:06:05.053740] [12:06:05.053820] Training epoch 717 for 0:00:27
[12:06:05.053870] [12:06:05.058263] log_dir: ./exp/debug/cifar100-LT/debug
[12:06:06.602057] Epoch: [718]  [ 0/42]  eta: 0:01:04  lr: 0.000068  loss: 0.4780 (0.4780)  time: 1.5426  data: 1.0336  max mem: 9341
[12:06:16.757878] Epoch: [718]  [20/42]  eta: 0:00:12  lr: 0.000068  loss: 0.4623 (0.4629)  time: 0.5077  data: 0.0001  max mem: 9341
[12:06:26.906912] Epoch: [718]  [40/42]  eta: 0:00:01  lr: 0.000067  loss: 0.4609 (0.4650)  time: 0.5074  data: 0.0001  max mem: 9341
[12:06:27.411602] Epoch: [718]  [41/42]  eta: 0:00:00  lr: 0.000067  loss: 0.4615 (0.4651)  time: 0.5073  data: 0.0001  max mem: 9341
[12:06:27.584163] Epoch: [718] Total time: 0:00:22 (0.5363 s / it)
[12:06:27.588105] Averaged stats: lr: 0.000067  loss: 0.4615 (0.4652)
[12:06:32.230361] {"train_lr": 6.774264546117926e-05, "train_loss": 0.46515646381747155, "epoch": 718}
[12:06:32.230674] [12:06:32.230756] Training epoch 718 for 0:00:27
[12:06:32.230807] [12:06:32.235091] log_dir: ./exp/debug/cifar100-LT/debug
[12:06:33.724982] Epoch: [719]  [ 0/42]  eta: 0:01:02  lr: 0.000067  loss: 0.4877 (0.4877)  time: 1.4886  data: 0.9876  max mem: 9341
[12:06:43.895045] Epoch: [719]  [20/42]  eta: 0:00:12  lr: 0.000066  loss: 0.4670 (0.4721)  time: 0.5084  data: 0.0002  max mem: 9341
[12:06:54.050764] Epoch: [719]  [40/42]  eta: 0:00:01  lr: 0.000065  loss: 0.4533 (0.4660)  time: 0.5077  data: 0.0001  max mem: 9341
[12:06:54.557559] Epoch: [719]  [41/42]  eta: 0:00:00  lr: 0.000065  loss: 0.4533 (0.4661)  time: 0.5078  data: 0.0001  max mem: 9341
[12:06:54.735749] Epoch: [719] Total time: 0:00:22 (0.5357 s / it)
[12:06:54.736703] Averaged stats: lr: 0.000065  loss: 0.4533 (0.4678)
[12:06:59.290120] {"train_lr": 6.61092666256917e-05, "train_loss": 0.4677784544016634, "epoch": 719}
[12:06:59.290435] [12:06:59.290534] Training epoch 719 for 0:00:27
[12:06:59.290650] [12:06:59.295013] log_dir: ./exp/debug/cifar100-LT/debug
[12:07:00.948053] Epoch: [720]  [ 0/42]  eta: 0:01:09  lr: 0.000065  loss: 0.4670 (0.4670)  time: 1.6520  data: 1.1290  max mem: 9341
[12:07:11.106251] Epoch: [720]  [20/42]  eta: 0:00:12  lr: 0.000064  loss: 0.4588 (0.4589)  time: 0.5079  data: 0.0001  max mem: 9341
[12:07:21.247300] Epoch: [720]  [40/42]  eta: 0:00:01  lr: 0.000064  loss: 0.4722 (0.4645)  time: 0.5070  data: 0.0001  max mem: 9341
[12:07:21.751880] Epoch: [720]  [41/42]  eta: 0:00:00  lr: 0.000064  loss: 0.4722 (0.4657)  time: 0.5069  data: 0.0001  max mem: 9341
[12:07:21.916448] Epoch: [720] Total time: 0:00:22 (0.5386 s / it)
[12:07:21.929326] Averaged stats: lr: 0.000064  loss: 0.4722 (0.4662)
[12:07:26.533205] {"train_lr": 6.449526285432689e-05, "train_loss": 0.46619195792646634, "epoch": 720}
[12:07:26.533604] [12:07:26.533691] Training epoch 720 for 0:00:27
[12:07:26.533744] [12:07:26.538244] log_dir: ./exp/debug/cifar100-LT/debug
[12:07:28.189203] Epoch: [721]  [ 0/42]  eta: 0:01:09  lr: 0.000063  loss: 0.4834 (0.4834)  time: 1.6499  data: 1.1368  max mem: 9341
[12:07:38.355199] Epoch: [721]  [20/42]  eta: 0:00:12  lr: 0.000063  loss: 0.4593 (0.4710)  time: 0.5082  data: 0.0001  max mem: 9341
[12:07:48.504640] Epoch: [721]  [40/42]  eta: 0:00:01  lr: 0.000062  loss: 0.4603 (0.4713)  time: 0.5074  data: 0.0001  max mem: 9341
[12:07:49.009391] Epoch: [721]  [41/42]  eta: 0:00:00  lr: 0.000062  loss: 0.4603 (0.4708)  time: 0.5074  data: 0.0001  max mem: 9341
[12:07:49.172303] Epoch: [721] Total time: 0:00:22 (0.5389 s / it)
[12:07:49.175091] Averaged stats: lr: 0.000062  loss: 0.4603 (0.4642)
[12:07:53.752408] {"train_lr": 6.290066172595569e-05, "train_loss": 0.4641922983740057, "epoch": 721}
[12:07:53.752781] [12:07:53.752883] Training epoch 721 for 0:00:27
[12:07:53.752934] [12:07:53.757218] log_dir: ./exp/debug/cifar100-LT/debug
[12:07:55.474841] Epoch: [722]  [ 0/42]  eta: 0:01:12  lr: 0.000062  loss: 0.4920 (0.4920)  time: 1.7166  data: 1.2169  max mem: 9341
[12:08:05.675780] Epoch: [722]  [20/42]  eta: 0:00:12  lr: 0.000061  loss: 0.4549 (0.4629)  time: 0.5100  data: 0.0001  max mem: 9341
[12:08:15.824448] Epoch: [722]  [40/42]  eta: 0:00:01  lr: 0.000061  loss: 0.4724 (0.4668)  time: 0.5074  data: 0.0001  max mem: 9341
[12:08:16.331121] Epoch: [722]  [41/42]  eta: 0:00:00  lr: 0.000061  loss: 0.4761 (0.4672)  time: 0.5075  data: 0.0001  max mem: 9341
[12:08:16.501336] Epoch: [722] Total time: 0:00:22 (0.5415 s / it)
[12:08:16.502307] Averaged stats: lr: 0.000061  loss: 0.4761 (0.4652)
[12:08:21.112781] {"train_lr": 6.132549048791181e-05, "train_loss": 0.46519106661989573, "epoch": 722}
[12:08:21.113144] [12:08:21.113228] Training epoch 722 for 0:00:27
[12:08:21.113278] [12:08:21.117572] log_dir: ./exp/debug/cifar100-LT/debug
[12:08:22.792677] Epoch: [723]  [ 0/42]  eta: 0:01:10  lr: 0.000060  loss: 0.4351 (0.4351)  time: 1.6739  data: 1.1563  max mem: 9341
[12:08:32.986839] Epoch: [723]  [20/42]  eta: 0:00:12  lr: 0.000060  loss: 0.4748 (0.4710)  time: 0.5097  data: 0.0001  max mem: 9341
[12:08:43.161726] Epoch: [723]  [40/42]  eta: 0:00:01  lr: 0.000059  loss: 0.4534 (0.4669)  time: 0.5087  data: 0.0001  max mem: 9341
[12:08:43.667277] Epoch: [723]  [41/42]  eta: 0:00:00  lr: 0.000059  loss: 0.4534 (0.4668)  time: 0.5086  data: 0.0001  max mem: 9341
[12:08:43.834925] Epoch: [723] Total time: 0:00:22 (0.5409 s / it)
[12:08:43.841812] Averaged stats: lr: 0.000059  loss: 0.4534 (0.4678)
[12:08:48.400424] {"train_lr": 5.976977605552542e-05, "train_loss": 0.4678103932667346, "epoch": 723}
[12:08:48.400777] [12:08:48.400856] Training epoch 723 for 0:00:27
[12:08:48.400905] [12:08:48.405197] log_dir: ./exp/debug/cifar100-LT/debug
[12:08:50.054895] Epoch: [724]  [ 0/42]  eta: 0:01:09  lr: 0.000059  loss: 0.4888 (0.4888)  time: 1.6485  data: 1.1513  max mem: 9341
[12:09:00.219742] Epoch: [724]  [20/42]  eta: 0:00:12  lr: 0.000058  loss: 0.4666 (0.4676)  time: 0.5082  data: 0.0001  max mem: 9341
[12:09:10.369895] Epoch: [724]  [40/42]  eta: 0:00:01  lr: 0.000058  loss: 0.4577 (0.4642)  time: 0.5075  data: 0.0001  max mem: 9341
[12:09:10.875476] Epoch: [724]  [41/42]  eta: 0:00:00  lr: 0.000058  loss: 0.4578 (0.4655)  time: 0.5075  data: 0.0001  max mem: 9341
[12:09:11.046122] Epoch: [724] Total time: 0:00:22 (0.5391 s / it)
[12:09:11.049468] Averaged stats: lr: 0.000058  loss: 0.4578 (0.4663)
[12:09:15.520766] {"train_lr": 5.823354501166424e-05, "train_loss": 0.4663348572239989, "epoch": 724}
[12:09:15.521139] [12:09:15.521223] Training epoch 724 for 0:00:27
[12:09:15.521274] [12:09:15.525479] log_dir: ./exp/debug/cifar100-LT/debug
[12:09:17.194015] Epoch: [725]  [ 0/42]  eta: 0:01:10  lr: 0.000057  loss: 0.4790 (0.4790)  time: 1.6674  data: 1.1706  max mem: 9341
[12:09:27.349972] Epoch: [725]  [20/42]  eta: 0:00:12  lr: 0.000057  loss: 0.4703 (0.4671)  time: 0.5077  data: 0.0001  max mem: 9341
[12:09:37.502671] Epoch: [725]  [40/42]  eta: 0:00:01  lr: 0.000056  loss: 0.4509 (0.4635)  time: 0.5076  data: 0.0001  max mem: 9341
[12:09:38.009240] Epoch: [725]  [41/42]  eta: 0:00:00  lr: 0.000056  loss: 0.4533 (0.4635)  time: 0.5076  data: 0.0001  max mem: 9341
[12:09:38.165634] Epoch: [725] Total time: 0:00:22 (0.5390 s / it)
[12:09:38.172873] Averaged stats: lr: 0.000056  loss: 0.4533 (0.4635)
[12:09:42.657525] {"train_lr": 5.671682360627791e-05, "train_loss": 0.4635409129162629, "epoch": 725}
[12:09:42.657854] [12:09:42.657934] Training epoch 725 for 0:00:27
[12:09:42.657984] [12:09:42.662394] log_dir: ./exp/debug/cifar100-LT/debug
[12:09:44.179978] Epoch: [726]  [ 0/42]  eta: 0:01:03  lr: 0.000056  loss: 0.4672 (0.4672)  time: 1.5163  data: 1.0002  max mem: 9341
[12:09:54.346181] Epoch: [726]  [20/42]  eta: 0:00:12  lr: 0.000055  loss: 0.4575 (0.4564)  time: 0.5083  data: 0.0001  max mem: 9341
[12:10:04.495469] Epoch: [726]  [40/42]  eta: 0:00:01  lr: 0.000055  loss: 0.4611 (0.4591)  time: 0.5074  data: 0.0001  max mem: 9341
[12:10:05.002497] Epoch: [726]  [41/42]  eta: 0:00:00  lr: 0.000055  loss: 0.4624 (0.4595)  time: 0.5074  data: 0.0001  max mem: 9341
[12:10:05.162005] Epoch: [726] Total time: 0:00:22 (0.5357 s / it)
[12:10:05.171360] Averaged stats: lr: 0.000055  loss: 0.4624 (0.4625)
[12:10:09.701558] {"train_lr": 5.5219637755950454e-05, "train_loss": 0.46250598611576216, "epoch": 726}
[12:10:09.701911] [12:10:09.701995] Training epoch 726 for 0:00:27
[12:10:09.702047] [12:10:09.706441] log_dir: ./exp/debug/cifar100-LT/debug
[12:10:11.382324] Epoch: [727]  [ 0/42]  eta: 0:01:10  lr: 0.000054  loss: 0.4842 (0.4842)  time: 1.6748  data: 1.1607  max mem: 9341
[12:10:21.551215] Epoch: [727]  [20/42]  eta: 0:00:12  lr: 0.000054  loss: 0.4565 (0.4645)  time: 0.5084  data: 0.0001  max mem: 9341
[12:10:31.705474] Epoch: [727]  [40/42]  eta: 0:00:01  lr: 0.000053  loss: 0.4563 (0.4640)  time: 0.5077  data: 0.0001  max mem: 9341
[12:10:32.212214] Epoch: [727]  [41/42]  eta: 0:00:00  lr: 0.000053  loss: 0.4549 (0.4633)  time: 0.5077  data: 0.0001  max mem: 9341
[12:10:32.364049] Epoch: [727] Total time: 0:00:22 (0.5395 s / it)
[12:10:32.372982] Averaged stats: lr: 0.000053  loss: 0.4549 (0.4686)
[12:10:37.005042] {"train_lr": 5.374201304345736e-05, "train_loss": 0.46860823194895473, "epoch": 727}
[12:10:37.005394] [12:10:37.005474] Training epoch 727 for 0:00:27
[12:10:37.005524] [12:10:37.009826] log_dir: ./exp/debug/cifar100-LT/debug
[12:10:38.489477] Epoch: [728]  [ 0/42]  eta: 0:01:02  lr: 0.000053  loss: 0.4382 (0.4382)  time: 1.4781  data: 0.9643  max mem: 9341
[12:10:48.655699] Epoch: [728]  [20/42]  eta: 0:00:12  lr: 0.000052  loss: 0.4712 (0.4659)  time: 0.5083  data: 0.0001  max mem: 9341
[12:10:58.809597] Epoch: [728]  [40/42]  eta: 0:00:01  lr: 0.000052  loss: 0.4714 (0.4692)  time: 0.5076  data: 0.0001  max mem: 9341
[12:10:59.315256] Epoch: [728]  [41/42]  eta: 0:00:00  lr: 0.000052  loss: 0.4714 (0.4694)  time: 0.5076  data: 0.0001  max mem: 9341
[12:10:59.474049] Epoch: [728] Total time: 0:00:22 (0.5349 s / it)
[12:10:59.484306] Averaged stats: lr: 0.000052  loss: 0.4714 (0.4661)
[12:11:04.078945] {"train_lr": 5.228397471732811e-05, "train_loss": 0.46608921761314076, "epoch": 728}
[12:11:04.079205] [12:11:04.079285] Training epoch 728 for 0:00:27
[12:11:04.079335] [12:11:04.083678] log_dir: ./exp/debug/cifar100-LT/debug
[12:11:05.712873] Epoch: [729]  [ 0/42]  eta: 0:01:08  lr: 0.000051  loss: 0.4442 (0.4442)  time: 1.6284  data: 1.1191  max mem: 9341
[12:11:15.932179] Epoch: [729]  [20/42]  eta: 0:00:12  lr: 0.000051  loss: 0.4576 (0.4614)  time: 0.5109  data: 0.0001  max mem: 9341
[12:11:26.114037] Epoch: [729]  [40/42]  eta: 0:00:01  lr: 0.000050  loss: 0.4755 (0.4648)  time: 0.5090  data: 0.0001  max mem: 9341
[12:11:26.620822] Epoch: [729]  [41/42]  eta: 0:00:00  lr: 0.000050  loss: 0.4755 (0.4646)  time: 0.5091  data: 0.0001  max mem: 9341
[12:11:26.791750] Epoch: [729] Total time: 0:00:22 (0.5407 s / it)
[12:11:26.794690] Averaged stats: lr: 0.000050  loss: 0.4755 (0.4623)
[12:11:31.406543] {"train_lr": 5.084554769141463e-05, "train_loss": 0.46227236340443295, "epoch": 729}
[12:11:31.406928] [12:11:31.407029] Training epoch 729 for 0:00:27
[12:11:31.407083] [12:11:31.411575] log_dir: ./exp/debug/cifar100-LT/debug
[12:11:32.926546] Epoch: [730]  [ 0/42]  eta: 0:01:03  lr: 0.000050  loss: 0.4487 (0.4487)  time: 1.5137  data: 0.9955  max mem: 9341
[12:11:43.104601] Epoch: [730]  [20/42]  eta: 0:00:12  lr: 0.000049  loss: 0.4619 (0.4632)  time: 0.5088  data: 0.0001  max mem: 9341
[12:11:53.255062] Epoch: [730]  [40/42]  eta: 0:00:01  lr: 0.000049  loss: 0.4581 (0.4628)  time: 0.5075  data: 0.0001  max mem: 9341
[12:11:53.761889] Epoch: [730]  [41/42]  eta: 0:00:00  lr: 0.000049  loss: 0.4711 (0.4630)  time: 0.5075  data: 0.0001  max mem: 9341
[12:11:53.925915] Epoch: [730] Total time: 0:00:22 (0.5361 s / it)
[12:11:53.936422] Averaged stats: lr: 0.000049  loss: 0.4711 (0.4655)
[12:11:58.560797] {"train_lr": 4.942675654446595e-05, "train_loss": 0.46547601584877285, "epoch": 730}
[12:11:58.561140] [12:11:58.561224] Training epoch 730 for 0:00:27
[12:11:58.561275] [12:11:58.565688] log_dir: ./exp/debug/cifar100-LT/debug
[12:12:00.095746] Epoch: [731]  [ 0/42]  eta: 0:01:04  lr: 0.000048  loss: 0.4405 (0.4405)  time: 1.5289  data: 1.0035  max mem: 9341
[12:12:10.263731] Epoch: [731]  [20/42]  eta: 0:00:12  lr: 0.000048  loss: 0.4602 (0.4665)  time: 0.5083  data: 0.0001  max mem: 9341
[12:12:20.414137] Epoch: [731]  [40/42]  eta: 0:00:01  lr: 0.000047  loss: 0.4559 (0.4628)  time: 0.5075  data: 0.0001  max mem: 9341
[12:12:20.919335] Epoch: [731]  [41/42]  eta: 0:00:00  lr: 0.000047  loss: 0.4559 (0.4629)  time: 0.5074  data: 0.0001  max mem: 9341
[12:12:21.094693] Epoch: [731] Total time: 0:00:22 (0.5364 s / it)
[12:12:21.098536] Averaged stats: lr: 0.000047  loss: 0.4559 (0.4664)
[12:12:25.677480] {"train_lr": 4.802762551970853e-05, "train_loss": 0.4663988518572989, "epoch": 731}
[12:12:25.677820] [12:12:25.677903] Training epoch 731 for 0:00:27
[12:12:25.677954] [12:12:25.682158] log_dir: ./exp/debug/cifar100-LT/debug
[12:12:27.245676] Epoch: [732]  [ 0/42]  eta: 0:01:05  lr: 0.000047  loss: 0.4782 (0.4782)  time: 1.5623  data: 1.0495  max mem: 9341
[12:12:37.404668] Epoch: [732]  [20/42]  eta: 0:00:12  lr: 0.000047  loss: 0.4673 (0.4671)  time: 0.5079  data: 0.0001  max mem: 9341
[12:12:47.555722] Epoch: [732]  [40/42]  eta: 0:00:01  lr: 0.000046  loss: 0.4662 (0.4673)  time: 0.5075  data: 0.0001  max mem: 9341
[12:12:48.061873] Epoch: [732]  [41/42]  eta: 0:00:00  lr: 0.000046  loss: 0.4662 (0.4681)  time: 0.5075  data: 0.0001  max mem: 9341
[12:12:48.231994] Epoch: [732] Total time: 0:00:22 (0.5369 s / it)
[12:12:48.232703] Averaged stats: lr: 0.000046  loss: 0.4662 (0.4673)
[12:12:52.857526] {"train_lr": 4.664817852443106e-05, "train_loss": 0.4672898685648328, "epoch": 732}
[12:12:52.857876] [12:12:52.857961] Training epoch 732 for 0:00:27
[12:12:52.858012] [12:12:52.862443] log_dir: ./exp/debug/cifar100-LT/debug
[12:12:54.445811] Epoch: [733]  [ 0/42]  eta: 0:01:06  lr: 0.000046  loss: 0.4557 (0.4557)  time: 1.5820  data: 1.0775  max mem: 9341
[12:13:04.610678] Epoch: [733]  [20/42]  eta: 0:00:12  lr: 0.000045  loss: 0.4760 (0.4717)  time: 0.5082  data: 0.0001  max mem: 9341
[12:13:14.769586] Epoch: [733]  [40/42]  eta: 0:00:01  lr: 0.000045  loss: 0.4702 (0.4678)  time: 0.5079  data: 0.0001  max mem: 9341
[12:13:15.274451] Epoch: [733]  [41/42]  eta: 0:00:00  lr: 0.000045  loss: 0.4702 (0.4680)  time: 0.5078  data: 0.0001  max mem: 9341
[12:13:15.443414] Epoch: [733] Total time: 0:00:22 (0.5376 s / it)
[12:13:15.446357] Averaged stats: lr: 0.000045  loss: 0.4702 (0.4667)
[12:13:20.049906] {"train_lr": 4.5288439129576625e-05, "train_loss": 0.4666729234158993, "epoch": 733}
[12:13:20.050277] [12:13:20.050356] Training epoch 733 for 0:00:27
[12:13:20.050407] [12:13:20.054796] log_dir: ./exp/debug/cifar100-LT/debug
[12:13:21.647288] Epoch: [734]  [ 0/42]  eta: 0:01:06  lr: 0.000044  loss: 0.4784 (0.4784)  time: 1.5914  data: 1.0852  max mem: 9341
[12:13:31.808630] Epoch: [734]  [20/42]  eta: 0:00:12  lr: 0.000044  loss: 0.4697 (0.4703)  time: 0.5080  data: 0.0001  max mem: 9341
[12:13:41.946991] Epoch: [734]  [40/42]  eta: 0:00:01  lr: 0.000043  loss: 0.4634 (0.4658)  time: 0.5069  data: 0.0001  max mem: 9341
[12:13:42.453236] Epoch: [734]  [41/42]  eta: 0:00:00  lr: 0.000043  loss: 0.4648 (0.4658)  time: 0.5069  data: 0.0001  max mem: 9341
[12:13:42.623097] Epoch: [734] Total time: 0:00:22 (0.5373 s / it)
[12:13:42.625192] Averaged stats: lr: 0.000043  loss: 0.4648 (0.4675)
[12:13:47.083013] {"train_lr": 4.394843056933974e-05, "train_loss": 0.4675324722414925, "epoch": 734}
[12:13:47.083396] [12:13:47.083482] Training epoch 734 for 0:00:27
[12:13:47.083534] [12:13:47.088059] log_dir: ./exp/debug/cifar100-LT/debug
[12:13:48.539353] Epoch: [735]  [ 0/42]  eta: 0:01:00  lr: 0.000043  loss: 0.4544 (0.4544)  time: 1.4497  data: 0.9370  max mem: 9341
[12:13:58.722373] Epoch: [735]  [20/42]  eta: 0:00:12  lr: 0.000043  loss: 0.4610 (0.4605)  time: 0.5091  data: 0.0001  max mem: 9341
[12:14:08.890264] Epoch: [735]  [40/42]  eta: 0:00:01  lr: 0.000042  loss: 0.4522 (0.4595)  time: 0.5083  data: 0.0001  max mem: 9341
[12:14:09.397428] Epoch: [735]  [41/42]  eta: 0:00:00  lr: 0.000042  loss: 0.4528 (0.4596)  time: 0.5085  data: 0.0001  max mem: 9341
[12:14:09.559035] Epoch: [735] Total time: 0:00:22 (0.5350 s / it)
[12:14:09.560721] Averaged stats: lr: 0.000042  loss: 0.4528 (0.4647)
[12:14:14.104004] {"train_lr": 4.262817574076982e-05, "train_loss": 0.464718534832909, "epoch": 735}
[12:14:14.104414] [12:14:14.104514] Training epoch 735 for 0:00:27
[12:14:14.104585] [12:14:14.108850] log_dir: ./exp/debug/cifar100-LT/debug
[12:14:15.583441] Epoch: [736]  [ 0/42]  eta: 0:01:01  lr: 0.000042  loss: 0.4563 (0.4563)  time: 1.4731  data: 0.9664  max mem: 9341
[12:14:25.744848] Epoch: [736]  [20/42]  eta: 0:00:12  lr: 0.000041  loss: 0.4527 (0.4662)  time: 0.5080  data: 0.0001  max mem: 9341
[12:14:35.887555] Epoch: [736]  [40/42]  eta: 0:00:01  lr: 0.000041  loss: 0.4673 (0.4652)  time: 0.5071  data: 0.0001  max mem: 9341
[12:14:36.392523] Epoch: [736]  [41/42]  eta: 0:00:00  lr: 0.000041  loss: 0.4630 (0.4635)  time: 0.5070  data: 0.0001  max mem: 9341
[12:14:36.568084] Epoch: [736] Total time: 0:00:22 (0.5347 s / it)
[12:14:36.568890] Averaged stats: lr: 0.000041  loss: 0.4630 (0.4639)
[12:14:41.117993] {"train_lr": 4.1327697203379034e-05, "train_loss": 0.46391709414975985, "epoch": 736}
[12:14:41.118382] [12:14:41.118486] Training epoch 736 for 0:00:27
[12:14:41.118540] [12:14:41.123270] log_dir: ./exp/debug/cifar100-LT/debug
[12:14:42.593328] Epoch: [737]  [ 0/42]  eta: 0:01:01  lr: 0.000040  loss: 0.4411 (0.4411)  time: 1.4680  data: 0.9692  max mem: 9341
[12:14:52.761996] Epoch: [737]  [20/42]  eta: 0:00:12  lr: 0.000040  loss: 0.4645 (0.4625)  time: 0.5083  data: 0.0001  max mem: 9341
[12:15:02.911239] Epoch: [737]  [40/42]  eta: 0:00:01  lr: 0.000039  loss: 0.4653 (0.4661)  time: 0.5074  data: 0.0001  max mem: 9341
[12:15:03.417277] Epoch: [737]  [41/42]  eta: 0:00:00  lr: 0.000039  loss: 0.4653 (0.4661)  time: 0.5075  data: 0.0001  max mem: 9341
[12:15:03.594935] Epoch: [737] Total time: 0:00:22 (0.5350 s / it)
[12:15:03.604372] Averaged stats: lr: 0.000039  loss: 0.4653 (0.4648)
[12:15:08.263747] {"train_lr": 4.004701717875723e-05, "train_loss": 0.4647966351892267, "epoch": 737}
[12:15:08.264167] [12:15:08.264253] Training epoch 737 for 0:00:27
[12:15:08.264307] [12:15:08.268792] log_dir: ./exp/debug/cifar100-LT/debug
[12:15:09.857530] Epoch: [738]  [ 0/42]  eta: 0:01:06  lr: 0.000039  loss: 0.4327 (0.4327)  time: 1.5875  data: 1.0726  max mem: 9341
[12:15:20.026102] Epoch: [738]  [20/42]  eta: 0:00:12  lr: 0.000039  loss: 0.4696 (0.4700)  time: 0.5084  data: 0.0001  max mem: 9341
[12:15:30.183068] Epoch: [738]  [40/42]  eta: 0:00:01  lr: 0.000038  loss: 0.4622 (0.4659)  time: 0.5078  data: 0.0001  max mem: 9341
[12:15:30.690460] Epoch: [738]  [41/42]  eta: 0:00:00  lr: 0.000038  loss: 0.4622 (0.4659)  time: 0.5078  data: 0.0001  max mem: 9341
[12:15:30.856440] Epoch: [738] Total time: 0:00:22 (0.5378 s / it)
[12:15:30.861678] Averaged stats: lr: 0.000038  loss: 0.4622 (0.4682)
[12:15:35.471525] {"train_lr": 3.8786157550192786e-05, "train_loss": 0.4682074073879492, "epoch": 738}
[12:15:35.471886] [12:15:35.471975] Training epoch 738 for 0:00:27
[12:15:35.472028] [12:15:35.476825] log_dir: ./exp/debug/cifar100-LT/debug
[12:15:37.104486] Epoch: [739]  [ 0/42]  eta: 0:01:08  lr: 0.000038  loss: 0.4758 (0.4758)  time: 1.6266  data: 1.1222  max mem: 9341
[12:15:47.270149] Epoch: [739]  [20/42]  eta: 0:00:12  lr: 0.000037  loss: 0.4801 (0.4712)  time: 0.5082  data: 0.0001  max mem: 9341
[12:15:57.418468] Epoch: [739]  [40/42]  eta: 0:00:01  lr: 0.000037  loss: 0.4681 (0.4696)  time: 0.5074  data: 0.0001  max mem: 9341
[12:15:57.925483] Epoch: [739]  [41/42]  eta: 0:00:00  lr: 0.000037  loss: 0.4630 (0.4695)  time: 0.5074  data: 0.0001  max mem: 9341
[12:15:58.094519] Epoch: [739] Total time: 0:00:22 (0.5385 s / it)
[12:15:58.097846] Averaged stats: lr: 0.000037  loss: 0.4630 (0.4663)
[12:16:02.850322] {"train_lr": 3.7545139862297823e-05, "train_loss": 0.46634457260370255, "epoch": 739}
[12:16:02.850702] [12:16:02.850790] Training epoch 739 for 0:00:27
[12:16:02.850840] [12:16:02.855406] log_dir: ./exp/debug/cifar100-LT/debug
[12:16:04.530597] Epoch: [740]  [ 0/42]  eta: 0:01:10  lr: 0.000037  loss: 0.4443 (0.4443)  time: 1.6740  data: 1.1572  max mem: 9341
[12:16:14.722224] Epoch: [740]  [20/42]  eta: 0:00:12  lr: 0.000036  loss: 0.4652 (0.4642)  time: 0.5095  data: 0.0001  max mem: 9341
[12:16:24.895819] Epoch: [740]  [40/42]  eta: 0:00:01  lr: 0.000036  loss: 0.4559 (0.4640)  time: 0.5086  data: 0.0001  max mem: 9341
[12:16:25.401117] Epoch: [740]  [41/42]  eta: 0:00:00  lr: 0.000036  loss: 0.4559 (0.4646)  time: 0.5085  data: 0.0001  max mem: 9341
[12:16:25.574117] Epoch: [740] Total time: 0:00:22 (0.5409 s / it)
[12:16:25.574859] Averaged stats: lr: 0.000036  loss: 0.4559 (0.4657)
[12:16:30.196414] {"train_lr": 3.632398532064056e-05, "train_loss": 0.4657082071616536, "epoch": 740}
[12:16:30.196750] [12:16:30.196833] Training epoch 740 for 0:00:27
[12:16:30.196884] [12:16:30.201172] log_dir: ./exp/debug/cifar100-LT/debug
[12:16:31.808343] Epoch: [741]  [ 0/42]  eta: 0:01:07  lr: 0.000036  loss: 0.4797 (0.4797)  time: 1.6061  data: 1.0975  max mem: 9341
[12:16:41.975486] Epoch: [741]  [20/42]  eta: 0:00:12  lr: 0.000035  loss: 0.4649 (0.4697)  time: 0.5083  data: 0.0001  max mem: 9341
[12:16:52.122319] Epoch: [741]  [40/42]  eta: 0:00:01  lr: 0.000035  loss: 0.4620 (0.4639)  time: 0.5073  data: 0.0001  max mem: 9341
[12:16:52.628470] Epoch: [741]  [41/42]  eta: 0:00:00  lr: 0.000035  loss: 0.4620 (0.4640)  time: 0.5073  data: 0.0001  max mem: 9341
[12:16:52.791681] Epoch: [741] Total time: 0:00:22 (0.5379 s / it)
[12:16:52.807790] Averaged stats: lr: 0.000035  loss: 0.4620 (0.4656)
[12:16:57.297958] {"train_lr": 3.512271479138268e-05, "train_loss": 0.46562741235608146, "epoch": 741}
[12:16:57.298318] [12:16:57.298399] Training epoch 741 for 0:00:27
[12:16:57.298450] [12:16:57.302932] log_dir: ./exp/debug/cifar100-LT/debug
[12:16:58.942637] Epoch: [742]  [ 0/42]  eta: 0:01:08  lr: 0.000034  loss: 0.4664 (0.4664)  time: 1.6384  data: 1.1450  max mem: 9341
[12:17:09.109099] Epoch: [742]  [20/42]  eta: 0:00:12  lr: 0.000034  loss: 0.4478 (0.4569)  time: 0.5083  data: 0.0001  max mem: 9341
[12:17:19.259462] Epoch: [742]  [40/42]  eta: 0:00:01  lr: 0.000033  loss: 0.4630 (0.4595)  time: 0.5075  data: 0.0001  max mem: 9341
[12:17:19.765003] Epoch: [742]  [41/42]  eta: 0:00:00  lr: 0.000033  loss: 0.4630 (0.4598)  time: 0.5075  data: 0.0001  max mem: 9341
[12:17:19.923141] Epoch: [742] Total time: 0:00:22 (0.5386 s / it)
[12:17:19.930659] Averaged stats: lr: 0.000033  loss: 0.4630 (0.4634)
[12:17:24.520867] {"train_lr": 3.3941348800923405e-05, "train_loss": 0.4633894158261163, "epoch": 742}
[12:17:24.521202] [12:17:24.521289] Training epoch 742 for 0:00:27
[12:17:24.521340] [12:17:24.526147] log_dir: ./exp/debug/cifar100-LT/debug
[12:17:26.151464] Epoch: [743]  [ 0/42]  eta: 0:01:08  lr: 0.000033  loss: 0.4861 (0.4861)  time: 1.6241  data: 1.1058  max mem: 9341
[12:17:36.318496] Epoch: [743]  [20/42]  eta: 0:00:12  lr: 0.000033  loss: 0.4592 (0.4625)  time: 0.5083  data: 0.0001  max mem: 9341
[12:17:46.466452] Epoch: [743]  [40/42]  eta: 0:00:01  lr: 0.000032  loss: 0.4638 (0.4642)  time: 0.5072  data: 0.0001  max mem: 9341
[12:17:46.969470] Epoch: [743]  [41/42]  eta: 0:00:00  lr: 0.000032  loss: 0.4638 (0.4635)  time: 0.5071  data: 0.0001  max mem: 9341
[12:17:47.130824] Epoch: [743] Total time: 0:00:22 (0.5382 s / it)
[12:17:47.138071] Averaged stats: lr: 0.000032  loss: 0.4638 (0.4684)
[12:17:51.685707] {"train_lr": 3.2779907535547884e-05, "train_loss": 0.46844000103218214, "epoch": 743}
[12:17:51.686067] [12:17:51.686147] Training epoch 743 for 0:00:27
[12:17:51.686196] [12:17:51.690472] log_dir: ./exp/debug/cifar100-LT/debug
[12:17:53.371043] Epoch: [744]  [ 0/42]  eta: 0:01:10  lr: 0.000032  loss: 0.4535 (0.4535)  time: 1.6792  data: 1.1665  max mem: 9341
[12:18:03.538731] Epoch: [744]  [20/42]  eta: 0:00:12  lr: 0.000032  loss: 0.4615 (0.4638)  time: 0.5083  data: 0.0001  max mem: 9341
[12:18:13.702136] Epoch: [744]  [40/42]  eta: 0:00:01  lr: 0.000031  loss: 0.4707 (0.4669)  time: 0.5081  data: 0.0001  max mem: 9341
[12:18:14.208531] Epoch: [744]  [41/42]  eta: 0:00:00  lr: 0.000031  loss: 0.4707 (0.4667)  time: 0.5081  data: 0.0001  max mem: 9341
[12:18:14.375095] Epoch: [744] Total time: 0:00:22 (0.5401 s / it)
[12:18:14.375998] Averaged stats: lr: 0.000031  loss: 0.4707 (0.4662)
[12:18:18.913949] {"train_lr": 3.163841084108277e-05, "train_loss": 0.4661583286665735, "epoch": 744}
[12:18:18.914301] [12:18:18.914382] Training epoch 744 for 0:00:27
[12:18:18.914434] [12:18:18.918882] log_dir: ./exp/debug/cifar100-LT/debug
[12:18:20.425190] Epoch: [745]  [ 0/42]  eta: 0:01:03  lr: 0.000031  loss: 0.4770 (0.4770)  time: 1.5050  data: 0.9873  max mem: 9341
[12:18:30.595897] Epoch: [745]  [20/42]  eta: 0:00:12  lr: 0.000030  loss: 0.4539 (0.4641)  time: 0.5085  data: 0.0001  max mem: 9341
[12:18:40.734521] Epoch: [745]  [40/42]  eta: 0:00:01  lr: 0.000030  loss: 0.4556 (0.4609)  time: 0.5069  data: 0.0001  max mem: 9341
[12:18:41.240481] Epoch: [745]  [41/42]  eta: 0:00:00  lr: 0.000030  loss: 0.4556 (0.4624)  time: 0.5069  data: 0.0001  max mem: 9341
[12:18:41.404839] Epoch: [745] Total time: 0:00:22 (0.5354 s / it)
[12:18:41.405765] Averaged stats: lr: 0.000030  loss: 0.4556 (0.4640)
[12:18:45.917987] {"train_lr": 3.0516878222556917e-05, "train_loss": 0.463958454983575, "epoch": 745}
[12:18:45.918379] [12:18:45.918459] Training epoch 745 for 0:00:27
[12:18:45.918510] [12:18:45.922912] log_dir: ./exp/debug/cifar100-LT/debug
[12:18:47.385226] Epoch: [746]  [ 0/42]  eta: 0:01:01  lr: 0.000030  loss: 0.4821 (0.4821)  time: 1.4611  data: 0.9465  max mem: 9341
[12:18:57.546445] Epoch: [746]  [20/42]  eta: 0:00:12  lr: 0.000029  loss: 0.4787 (0.4798)  time: 0.5080  data: 0.0001  max mem: 9341
[12:19:07.693818] Epoch: [746]  [40/42]  eta: 0:00:01  lr: 0.000029  loss: 0.4773 (0.4757)  time: 0.5073  data: 0.0001  max mem: 9341
[12:19:08.200019] Epoch: [746]  [41/42]  eta: 0:00:00  lr: 0.000029  loss: 0.4773 (0.4756)  time: 0.5074  data: 0.0001  max mem: 9341
[12:19:08.370293] Epoch: [746] Total time: 0:00:22 (0.5345 s / it)
[12:19:08.370987] Averaged stats: lr: 0.000029  loss: 0.4773 (0.4688)
[12:19:12.940128] {"train_lr": 2.9415328843868706e-05, "train_loss": 0.4688405978182952, "epoch": 746}
[12:19:12.940494] [12:19:12.940576] Training epoch 746 for 0:00:27
[12:19:12.940627] [12:19:12.945027] log_dir: ./exp/debug/cifar100-LT/debug
[12:19:14.579414] Epoch: [747]  [ 0/42]  eta: 0:01:08  lr: 0.000029  loss: 0.4772 (0.4772)  time: 1.6332  data: 1.1239  max mem: 9341
[12:19:24.747097] Epoch: [747]  [20/42]  eta: 0:00:12  lr: 0.000028  loss: 0.4527 (0.4594)  time: 0.5083  data: 0.0001  max mem: 9341
[12:19:34.890798] Epoch: [747]  [40/42]  eta: 0:00:01  lr: 0.000028  loss: 0.4626 (0.4620)  time: 0.5071  data: 0.0001  max mem: 9341
[12:19:35.396711] Epoch: [747]  [41/42]  eta: 0:00:00  lr: 0.000028  loss: 0.4643 (0.4623)  time: 0.5071  data: 0.0001  max mem: 9341
[12:19:35.563202] Epoch: [747] Total time: 0:00:22 (0.5385 s / it)
[12:19:35.569195] Averaged stats: lr: 0.000028  loss: 0.4643 (0.4646)
[12:19:40.121025] {"train_lr": 2.8333781527457527e-05, "train_loss": 0.46463138698821976, "epoch": 747}
[12:19:40.121369] [12:19:40.121457] Training epoch 747 for 0:00:27
[12:19:40.121507] [12:19:40.126241] log_dir: ./exp/debug/cifar100-LT/debug
[12:19:41.742127] Epoch: [748]  [ 0/42]  eta: 0:01:07  lr: 0.000028  loss: 0.4268 (0.4268)  time: 1.6149  data: 1.1086  max mem: 9341
[12:19:51.904195] Epoch: [748]  [20/42]  eta: 0:00:12  lr: 0.000027  loss: 0.4600 (0.4622)  time: 0.5081  data: 0.0001  max mem: 9341
[12:20:02.049303] Epoch: [748]  [40/42]  eta: 0:00:01  lr: 0.000027  loss: 0.4687 (0.4678)  time: 0.5072  data: 0.0001  max mem: 9341
[12:20:02.556252] Epoch: [748]  [41/42]  eta: 0:00:00  lr: 0.000027  loss: 0.4691 (0.4691)  time: 0.5073  data: 0.0001  max mem: 9341
[12:20:02.729871] Epoch: [748] Total time: 0:00:22 (0.5382 s / it)
[12:20:02.738073] Averaged stats: lr: 0.000027  loss: 0.4691 (0.4665)
[12:20:07.176998] {"train_lr": 2.7272254753982902e-05, "train_loss": 0.46650770164671396, "epoch": 748}
[12:20:07.177338] [12:20:07.177419] Training epoch 748 for 0:00:27
[12:20:07.177469] [12:20:07.181904] log_dir: ./exp/debug/cifar100-LT/debug
[12:20:08.760825] Epoch: [749]  [ 0/42]  eta: 0:01:06  lr: 0.000027  loss: 0.4558 (0.4558)  time: 1.5778  data: 1.0630  max mem: 9341
[12:20:18.926576] Epoch: [749]  [20/42]  eta: 0:00:12  lr: 0.000026  loss: 0.4612 (0.4617)  time: 0.5082  data: 0.0001  max mem: 9341
[12:20:29.073430] Epoch: [749]  [40/42]  eta: 0:00:01  lr: 0.000026  loss: 0.4492 (0.4568)  time: 0.5073  data: 0.0001  max mem: 9341
[12:20:29.579006] Epoch: [749]  [41/42]  eta: 0:00:00  lr: 0.000026  loss: 0.4492 (0.4573)  time: 0.5073  data: 0.0001  max mem: 9341
[12:20:29.743197] Epoch: [749] Total time: 0:00:22 (0.5372 s / it)
[12:20:29.745388] Averaged stats: lr: 0.000026  loss: 0.4492 (0.4643)
[12:20:34.259208] {"train_lr": 2.623076666200857e-05, "train_loss": 0.4643243631081922, "epoch": 749}
[12:20:34.259574] [12:20:34.259657] Training epoch 749 for 0:00:27
[12:20:34.259709] [12:20:34.264101] log_dir: ./exp/debug/cifar100-LT/debug
[12:20:35.928432] Epoch: [750]  [ 0/42]  eta: 0:01:09  lr: 0.000026  loss: 0.4659 (0.4659)  time: 1.6628  data: 1.1478  max mem: 9341
[12:20:46.094508] Epoch: [750]  [20/42]  eta: 0:00:12  lr: 0.000025  loss: 0.4648 (0.4692)  time: 0.5082  data: 0.0001  max mem: 9341
[12:20:56.247601] Epoch: [750]  [40/42]  eta: 0:00:01  lr: 0.000025  loss: 0.4705 (0.4693)  time: 0.5076  data: 0.0001  max mem: 9341
[12:20:56.752149] Epoch: [750]  [41/42]  eta: 0:00:00  lr: 0.000025  loss: 0.4705 (0.4681)  time: 0.5076  data: 0.0001  max mem: 9341
[12:20:56.920948] Epoch: [750] Total time: 0:00:22 (0.5394 s / it)
[12:20:56.923113] Averaged stats: lr: 0.000025  loss: 0.4705 (0.4665)
[12:21:01.452663] {"train_lr": 2.5209335047692683e-05, "train_loss": 0.4664610883309728, "epoch": 750}
[12:21:01.452988] [12:21:01.453069] Training epoch 750 for 0:00:27
[12:21:01.453121] [12:21:01.457516] log_dir: ./exp/debug/cifar100-LT/debug
[12:21:03.132900] Epoch: [751]  [ 0/42]  eta: 0:01:10  lr: 0.000025  loss: 0.4723 (0.4723)  time: 1.6742  data: 1.1669  max mem: 9341
[12:21:13.295316] Epoch: [751]  [20/42]  eta: 0:00:12  lr: 0.000024  loss: 0.4568 (0.4628)  time: 0.5081  data: 0.0001  max mem: 9341
[12:21:23.439404] Epoch: [751]  [40/42]  eta: 0:00:01  lr: 0.000024  loss: 0.4573 (0.4606)  time: 0.5072  data: 0.0001  max mem: 9341
[12:21:23.945067] Epoch: [751]  [41/42]  eta: 0:00:00  lr: 0.000024  loss: 0.4639 (0.4610)  time: 0.5071  data: 0.0001  max mem: 9341
[12:21:24.101072] Epoch: [751] Total time: 0:00:22 (0.5391 s / it)
[12:21:24.106567] Averaged stats: lr: 0.000024  loss: 0.4639 (0.4622)
[12:21:28.720145] {"train_lr": 2.420797736448313e-05, "train_loss": 0.46224875730418025, "epoch": 751}
[12:21:28.720484] [12:21:28.720566] Training epoch 751 for 0:00:27
[12:21:28.720635] [12:21:28.724880] log_dir: ./exp/debug/cifar100-LT/debug
[12:21:30.430028] Epoch: [752]  [ 0/42]  eta: 0:01:11  lr: 0.000024  loss: 0.4005 (0.4005)  time: 1.7041  data: 1.2016  max mem: 9341
[12:21:40.593761] Epoch: [752]  [20/42]  eta: 0:00:12  lr: 0.000023  loss: 0.4550 (0.4553)  time: 0.5081  data: 0.0001  max mem: 9341
[12:21:50.747652] Epoch: [752]  [40/42]  eta: 0:00:01  lr: 0.000023  loss: 0.4718 (0.4620)  time: 0.5076  data: 0.0001  max mem: 9341
[12:21:51.252710] Epoch: [752]  [41/42]  eta: 0:00:00  lr: 0.000023  loss: 0.4718 (0.4618)  time: 0.5076  data: 0.0001  max mem: 9341
[12:21:51.416383] Epoch: [752] Total time: 0:00:22 (0.5403 s / it)
[12:21:51.422499] Averaged stats: lr: 0.000023  loss: 0.4718 (0.4656)
[12:21:55.985474] {"train_lr": 2.3226710722819638e-05, "train_loss": 0.46556592892323223, "epoch": 752}
[12:21:55.985816] [12:21:55.985898] Training epoch 752 for 0:00:27
[12:21:55.985960] [12:21:55.990172] log_dir: ./exp/debug/cifar100-LT/debug
[12:21:57.701930] Epoch: [753]  [ 0/42]  eta: 0:01:11  lr: 0.000023  loss: 0.4547 (0.4547)  time: 1.7106  data: 1.2058  max mem: 9341
[12:22:07.909563] Epoch: [753]  [20/42]  eta: 0:00:12  lr: 0.000022  loss: 0.4736 (0.4680)  time: 0.5103  data: 0.0001  max mem: 9341
[12:22:18.055509] Epoch: [753]  [40/42]  eta: 0:00:01  lr: 0.000022  loss: 0.4612 (0.4644)  time: 0.5072  data: 0.0001  max mem: 9341
[12:22:18.560350] Epoch: [753]  [41/42]  eta: 0:00:00  lr: 0.000022  loss: 0.4612 (0.4646)  time: 0.5072  data: 0.0001  max mem: 9341
[12:22:18.729898] Epoch: [753] Total time: 0:00:22 (0.5414 s / it)
[12:22:18.737073] Averaged stats: lr: 0.000022  loss: 0.4612 (0.4639)
[12:22:23.292190] {"train_lr": 2.226555188984207e-05, "train_loss": 0.4638966027469862, "epoch": 753}
[12:22:23.292538] [12:22:23.292618] Training epoch 753 for 0:00:27
[12:22:23.292727] [12:22:23.297017] log_dir: ./exp/debug/cifar100-LT/debug
[12:22:24.863025] Epoch: [754]  [ 0/42]  eta: 0:01:05  lr: 0.000022  loss: 0.4713 (0.4713)  time: 1.5647  data: 1.0485  max mem: 9341
[12:22:35.027036] Epoch: [754]  [20/42]  eta: 0:00:12  lr: 0.000021  loss: 0.4811 (0.4774)  time: 0.5081  data: 0.0001  max mem: 9341
[12:22:45.178713] Epoch: [754]  [40/42]  eta: 0:00:01  lr: 0.000021  loss: 0.4551 (0.4686)  time: 0.5075  data: 0.0001  max mem: 9341
[12:22:45.683939] Epoch: [754]  [41/42]  eta: 0:00:00  lr: 0.000021  loss: 0.4551 (0.4691)  time: 0.5075  data: 0.0001  max mem: 9341
[12:22:45.852139] Epoch: [754] Total time: 0:00:22 (0.5370 s / it)
[12:22:45.860220] Averaged stats: lr: 0.000021  loss: 0.4551 (0.4686)
[12:22:50.368318] {"train_lr": 2.132451728910287e-05, "train_loss": 0.46861625037022997, "epoch": 754}
[12:22:50.368674] [12:22:50.368757] Training epoch 754 for 0:00:27
[12:22:50.368809] [12:22:50.373198] log_dir: ./exp/debug/cifar100-LT/debug
[12:22:51.898891] Epoch: [755]  [ 0/42]  eta: 0:01:04  lr: 0.000021  loss: 0.5100 (0.5100)  time: 1.5248  data: 1.0251  max mem: 9341
[12:23:02.069118] Epoch: [755]  [20/42]  eta: 0:00:12  lr: 0.000020  loss: 0.4605 (0.4666)  time: 0.5085  data: 0.0001  max mem: 9341
[12:23:12.223456] Epoch: [755]  [40/42]  eta: 0:00:01  lr: 0.000020  loss: 0.4611 (0.4624)  time: 0.5077  data: 0.0001  max mem: 9341
[12:23:12.728977] Epoch: [755]  [41/42]  eta: 0:00:00  lr: 0.000020  loss: 0.4611 (0.4609)  time: 0.5076  data: 0.0001  max mem: 9341
[12:23:12.899145] Epoch: [755] Total time: 0:00:22 (0.5363 s / it)
[12:23:12.904761] Averaged stats: lr: 0.000020  loss: 0.4611 (0.4659)
[12:23:17.468736] {"train_lr": 2.0403623000286767e-05, "train_loss": 0.465902999575649, "epoch": 755}
[12:23:17.469055] [12:23:17.469135] Training epoch 755 for 0:00:27
[12:23:17.469226] [12:23:17.473506] log_dir: ./exp/debug/cifar100-LT/debug
[12:23:19.182411] Epoch: [756]  [ 0/42]  eta: 0:01:11  lr: 0.000020  loss: 0.4829 (0.4829)  time: 1.7079  data: 1.1960  max mem: 9341
[12:23:29.350629] Epoch: [756]  [20/42]  eta: 0:00:12  lr: 0.000019  loss: 0.4660 (0.4674)  time: 0.5084  data: 0.0001  max mem: 9341
[12:23:39.500230] Epoch: [756]  [40/42]  eta: 0:00:01  lr: 0.000019  loss: 0.4477 (0.4626)  time: 0.5074  data: 0.0001  max mem: 9341
[12:23:40.005192] Epoch: [756]  [41/42]  eta: 0:00:00  lr: 0.000019  loss: 0.4477 (0.4624)  time: 0.5074  data: 0.0001  max mem: 9341
[12:23:40.167218] Epoch: [756] Total time: 0:00:22 (0.5403 s / it)
[12:23:40.176826] Averaged stats: lr: 0.000019  loss: 0.4477 (0.4662)
[12:23:44.711398] {"train_lr": 1.9502884758936522e-05, "train_loss": 0.4662206300667354, "epoch": 756}
[12:23:44.711738] [12:23:44.711820] Training epoch 756 for 0:00:27
[12:23:44.711870] [12:23:44.716689] log_dir: ./exp/debug/cifar100-LT/debug
[12:23:46.193575] Epoch: [757]  [ 0/42]  eta: 0:01:01  lr: 0.000019  loss: 0.4292 (0.4292)  time: 1.4753  data: 0.9529  max mem: 9341
[12:23:56.361293] Epoch: [757]  [20/42]  eta: 0:00:12  lr: 0.000019  loss: 0.4597 (0.4671)  time: 0.5083  data: 0.0001  max mem: 9341
[12:24:06.511956] Epoch: [757]  [40/42]  eta: 0:00:01  lr: 0.000018  loss: 0.4619 (0.4653)  time: 0.5075  data: 0.0001  max mem: 9341
[12:24:07.018960] Epoch: [757]  [41/42]  eta: 0:00:00  lr: 0.000018  loss: 0.4619 (0.4650)  time: 0.5076  data: 0.0001  max mem: 9341
[12:24:07.187626] Epoch: [757] Total time: 0:00:22 (0.5350 s / it)
[12:24:07.191874] Averaged stats: lr: 0.000018  loss: 0.4619 (0.4625)
[12:24:11.701457] {"train_lr": 1.862231795618348e-05, "train_loss": 0.4624584162873881, "epoch": 757}
[12:24:11.701816] [12:24:11.701895] Training epoch 757 for 0:00:26
[12:24:11.701945] [12:24:11.706216] log_dir: ./exp/debug/cifar100-LT/debug
[12:24:13.297720] Epoch: [758]  [ 0/42]  eta: 0:01:06  lr: 0.000018  loss: 0.4567 (0.4567)  time: 1.5903  data: 1.0809  max mem: 9341
[12:24:23.470573] Epoch: [758]  [20/42]  eta: 0:00:12  lr: 0.000018  loss: 0.4526 (0.4627)  time: 0.5086  data: 0.0001  max mem: 9341
[12:24:33.620205] Epoch: [758]  [40/42]  eta: 0:00:01  lr: 0.000017  loss: 0.4623 (0.4639)  time: 0.5074  data: 0.0001  max mem: 9341
[12:24:34.126004] Epoch: [758]  [41/42]  eta: 0:00:00  lr: 0.000017  loss: 0.4623 (0.4640)  time: 0.5075  data: 0.0001  max mem: 9341
[12:24:34.291509] Epoch: [758] Total time: 0:00:22 (0.5377 s / it)
[12:24:34.292227] Averaged stats: lr: 0.000017  loss: 0.4623 (0.4638)
[12:24:38.821382] {"train_lr": 1.776193763848461e-05, "train_loss": 0.46383276989772204, "epoch": 758}
[12:24:38.821723] [12:24:38.821804] Training epoch 758 for 0:00:27
[12:24:38.821856] [12:24:38.826190] log_dir: ./exp/debug/cifar100-LT/debug
[12:24:40.480039] Epoch: [759]  [ 0/42]  eta: 0:01:09  lr: 0.000017  loss: 0.4790 (0.4790)  time: 1.6528  data: 1.1500  max mem: 9341
[12:24:50.644467] Epoch: [759]  [20/42]  eta: 0:00:12  lr: 0.000017  loss: 0.4442 (0.4544)  time: 0.5082  data: 0.0001  max mem: 9341
[12:25:00.795885] Epoch: [759]  [40/42]  eta: 0:00:01  lr: 0.000017  loss: 0.4592 (0.4566)  time: 0.5075  data: 0.0001  max mem: 9341
[12:25:01.301520] Epoch: [759]  [41/42]  eta: 0:00:00  lr: 0.000017  loss: 0.4592 (0.4571)  time: 0.5075  data: 0.0001  max mem: 9341
[12:25:01.473116] Epoch: [759] Total time: 0:00:22 (0.5392 s / it)
[12:25:01.482277] Averaged stats: lr: 0.000017  loss: 0.4592 (0.4627)
[12:25:06.049090] {"train_lr": 1.692175850736559e-05, "train_loss": 0.46271263453222455, "epoch": 759}
[12:25:06.049367] [12:25:06.049464] Training epoch 759 for 0:00:27
[12:25:06.049514] [12:25:06.053798] log_dir: ./exp/debug/cifar100-LT/debug
[12:25:07.542943] Epoch: [760]  [ 0/42]  eta: 0:01:02  lr: 0.000016  loss: 0.4326 (0.4326)  time: 1.4876  data: 0.9865  max mem: 9341
[12:25:17.708905] Epoch: [760]  [20/42]  eta: 0:00:12  lr: 0.000016  loss: 0.4586 (0.4626)  time: 0.5082  data: 0.0001  max mem: 9341
[12:25:27.863322] Epoch: [760]  [40/42]  eta: 0:00:01  lr: 0.000016  loss: 0.4546 (0.4629)  time: 0.5077  data: 0.0001  max mem: 9341
[12:25:28.369456] Epoch: [760]  [41/42]  eta: 0:00:00  lr: 0.000016  loss: 0.4546 (0.4625)  time: 0.5077  data: 0.0001  max mem: 9341
[12:25:28.529247] Epoch: [760] Total time: 0:00:22 (0.5351 s / it)
[12:25:28.544365] Averaged stats: lr: 0.000016  loss: 0.4546 (0.4642)
[12:25:33.082566] {"train_lr": 1.6101794919169925e-05, "train_loss": 0.4641750334274201, "epoch": 760}
[12:25:33.082998] [12:25:33.083085] Training epoch 760 for 0:00:27
[12:25:33.083137] [12:25:33.088253] log_dir: ./exp/debug/cifar100-LT/debug
[12:25:34.816893] Epoch: [761]  [ 0/42]  eta: 0:01:12  lr: 0.000016  loss: 0.4448 (0.4448)  time: 1.7275  data: 1.2217  max mem: 9341
[12:25:44.986295] Epoch: [761]  [20/42]  eta: 0:00:12  lr: 0.000015  loss: 0.4604 (0.4633)  time: 0.5084  data: 0.0001  max mem: 9341
[12:25:55.132511] Epoch: [761]  [40/42]  eta: 0:00:01  lr: 0.000015  loss: 0.4635 (0.4643)  time: 0.5073  data: 0.0001  max mem: 9341
[12:25:55.639926] Epoch: [761]  [41/42]  eta: 0:00:00  lr: 0.000015  loss: 0.4642 (0.4646)  time: 0.5073  data: 0.0001  max mem: 9341
[12:25:55.797831] Epoch: [761] Total time: 0:00:22 (0.5407 s / it)
[12:25:55.809233] Averaged stats: lr: 0.000015  loss: 0.4642 (0.4652)
[12:26:00.381962] {"train_lr": 1.5302060884812813e-05, "train_loss": 0.46520829147526194, "epoch": 761}
[12:26:00.382360] [12:26:00.382451] Training epoch 761 for 0:00:27
[12:26:00.382508] [12:26:00.386919] log_dir: ./exp/debug/cifar100-LT/debug
[12:26:01.887766] Epoch: [762]  [ 0/42]  eta: 0:01:02  lr: 0.000015  loss: 0.4482 (0.4482)  time: 1.4995  data: 0.9959  max mem: 9341
[12:26:12.070829] Epoch: [762]  [20/42]  eta: 0:00:12  lr: 0.000014  loss: 0.4551 (0.4595)  time: 0.5091  data: 0.0002  max mem: 9341
[12:26:22.239670] Epoch: [762]  [40/42]  eta: 0:00:01  lr: 0.000014  loss: 0.4638 (0.4621)  time: 0.5084  data: 0.0001  max mem: 9341
[12:26:22.746765] Epoch: [762]  [41/42]  eta: 0:00:00  lr: 0.000014  loss: 0.4594 (0.4619)  time: 0.5084  data: 0.0001  max mem: 9341
[12:26:22.920669] Epoch: [762] Total time: 0:00:22 (0.5365 s / it)
[12:26:22.924620] Averaged stats: lr: 0.000014  loss: 0.4594 (0.4636)
[12:26:27.699718] {"train_lr": 1.452257006954236e-05, "train_loss": 0.4636465995794251, "epoch": 762}
[12:26:27.700095] [12:26:27.700207] Training epoch 762 for 0:00:27
[12:26:27.700275] [12:26:27.704538] log_dir: ./exp/debug/cifar100-LT/debug
[12:26:29.328680] Epoch: [763]  [ 0/42]  eta: 0:01:08  lr: 0.000014  loss: 0.4867 (0.4867)  time: 1.6229  data: 1.1111  max mem: 9341
[12:26:39.498918] Epoch: [763]  [20/42]  eta: 0:00:12  lr: 0.000014  loss: 0.4667 (0.4648)  time: 0.5084  data: 0.0001  max mem: 9341
[12:26:49.657729] Epoch: [763]  [40/42]  eta: 0:00:01  lr: 0.000013  loss: 0.4651 (0.4667)  time: 0.5079  data: 0.0001  max mem: 9341
[12:26:50.164124] Epoch: [763]  [41/42]  eta: 0:00:00  lr: 0.000013  loss: 0.4651 (0.4673)  time: 0.5078  data: 0.0001  max mem: 9341
[12:26:50.326827] Epoch: [763] Total time: 0:00:22 (0.5386 s / it)
[12:26:50.330803] Averaged stats: lr: 0.000013  loss: 0.4651 (0.4648)
[12:26:54.874194] {"train_lr": 1.3763335792705826e-05, "train_loss": 0.4647689800532091, "epoch": 763}
[12:26:54.874602] [12:26:54.874710] Training epoch 763 for 0:00:27
[12:26:54.874786] [12:26:54.879386] log_dir: ./exp/debug/cifar100-LT/debug
[12:26:56.438700] Epoch: [764]  [ 0/42]  eta: 0:01:05  lr: 0.000013  loss: 0.4385 (0.4385)  time: 1.5577  data: 1.0505  max mem: 9341
[12:27:06.600306] Epoch: [764]  [20/42]  eta: 0:00:12  lr: 0.000013  loss: 0.4596 (0.4625)  time: 0.5080  data: 0.0001  max mem: 9341
[12:27:16.753642] Epoch: [764]  [40/42]  eta: 0:00:01  lr: 0.000013  loss: 0.4637 (0.4629)  time: 0.5076  data: 0.0001  max mem: 9341
[12:27:17.261485] Epoch: [764]  [41/42]  eta: 0:00:00  lr: 0.000013  loss: 0.4656 (0.4637)  time: 0.5077  data: 0.0001  max mem: 9341
[12:27:17.440604] Epoch: [764] Total time: 0:00:22 (0.5372 s / it)
[12:27:17.441510] Averaged stats: lr: 0.000013  loss: 0.4656 (0.4633)
[12:27:22.068507] {"train_lr": 1.3024371027522299e-05, "train_loss": 0.4632542741795381, "epoch": 764}
[12:27:22.068905] [12:27:22.068994] Training epoch 764 for 0:00:27
[12:27:22.069052] [12:27:22.073393] log_dir: ./exp/debug/cifar100-LT/debug
[12:27:23.645074] Epoch: [765]  [ 0/42]  eta: 0:01:05  lr: 0.000013  loss: 0.4227 (0.4227)  time: 1.5702  data: 1.0706  max mem: 9341
[12:27:33.815517] Epoch: [765]  [20/42]  eta: 0:00:12  lr: 0.000012  loss: 0.4638 (0.4669)  time: 0.5085  data: 0.0001  max mem: 9341
[12:27:43.969846] Epoch: [765]  [40/42]  eta: 0:00:01  lr: 0.000012  loss: 0.4679 (0.4674)  time: 0.5077  data: 0.0001  max mem: 9341
[12:27:44.476697] Epoch: [765]  [41/42]  eta: 0:00:00  lr: 0.000012  loss: 0.4679 (0.4675)  time: 0.5077  data: 0.0001  max mem: 9341
[12:27:44.639464] Epoch: [765] Total time: 0:00:22 (0.5373 s / it)
[12:27:44.649649] Averaged stats: lr: 0.000012  loss: 0.4679 (0.4634)
[12:27:49.181004] {"train_lr": 1.2305688400860862e-05, "train_loss": 0.46344621302116484, "epoch": 765}
[12:27:49.181387] [12:27:49.181476] Training epoch 765 for 0:00:27
[12:27:49.181535] [12:27:49.185966] log_dir: ./exp/debug/cifar100-LT/debug
[12:27:50.784116] Epoch: [766]  [ 0/42]  eta: 0:01:07  lr: 0.000012  loss: 0.4743 (0.4743)  time: 1.5971  data: 1.0942  max mem: 9341
[12:28:00.950852] Epoch: [766]  [20/42]  eta: 0:00:12  lr: 0.000012  loss: 0.4582 (0.4640)  time: 0.5083  data: 0.0002  max mem: 9341
[12:28:11.104867] Epoch: [766]  [40/42]  eta: 0:00:01  lr: 0.000011  loss: 0.4648 (0.4666)  time: 0.5076  data: 0.0001  max mem: 9341
[12:28:11.613257] Epoch: [766]  [41/42]  eta: 0:00:00  lr: 0.000011  loss: 0.4648 (0.4664)  time: 0.5077  data: 0.0001  max mem: 9341
[12:28:11.786001] Epoch: [766] Total time: 0:00:22 (0.5381 s / it)
[12:28:11.790145] Averaged stats: lr: 0.000011  loss: 0.4648 (0.4644)
[12:28:16.453385] {"train_lr": 1.1607300193024507e-05, "train_loss": 0.4644053640464942, "epoch": 766}
[12:28:16.453781] [12:28:16.453888] Training epoch 766 for 0:00:27
[12:28:16.453964] [12:28:16.458314] log_dir: ./exp/debug/cifar100-LT/debug
[12:28:18.243886] Epoch: [767]  [ 0/42]  eta: 0:01:14  lr: 0.000011  loss: 0.4112 (0.4112)  time: 1.7844  data: 1.2744  max mem: 9341
[12:28:28.417435] Epoch: [767]  [20/42]  eta: 0:00:12  lr: 0.000011  loss: 0.4726 (0.4623)  time: 0.5086  data: 0.0001  max mem: 9341
[12:28:38.572869] Epoch: [767]  [40/42]  eta: 0:00:01  lr: 0.000011  loss: 0.4640 (0.4653)  time: 0.5077  data: 0.0001  max mem: 9341
[12:28:39.078824] Epoch: [767]  [41/42]  eta: 0:00:00  lr: 0.000011  loss: 0.4640 (0.4654)  time: 0.5077  data: 0.0001  max mem: 9341
[12:28:39.250733] Epoch: [767] Total time: 0:00:22 (0.5427 s / it)
[12:28:39.251520] Averaged stats: lr: 0.000011  loss: 0.4640 (0.4642)
[12:28:43.718532] {"train_lr": 1.0929218337540772e-05, "train_loss": 0.4642423496005081, "epoch": 767}
[12:28:43.718980] [12:28:43.719094] Training epoch 767 for 0:00:27
[12:28:43.719170] [12:28:43.723510] log_dir: ./exp/debug/cifar100-LT/debug
[12:28:45.312204] Epoch: [768]  [ 0/42]  eta: 0:01:06  lr: 0.000010  loss: 0.4555 (0.4555)  time: 1.5876  data: 1.0845  max mem: 9341
[12:28:55.481872] Epoch: [768]  [20/42]  eta: 0:00:12  lr: 0.000010  loss: 0.4530 (0.4528)  time: 0.5084  data: 0.0001  max mem: 9341
[12:29:05.634772] Epoch: [768]  [40/42]  eta: 0:00:01  lr: 0.000010  loss: 0.4658 (0.4593)  time: 0.5076  data: 0.0001  max mem: 9341
[12:29:06.141696] Epoch: [768]  [41/42]  eta: 0:00:00  lr: 0.000010  loss: 0.4658 (0.4582)  time: 0.5075  data: 0.0001  max mem: 9341
[12:29:06.297520] Epoch: [768] Total time: 0:00:22 (0.5375 s / it)
[12:29:06.301709] Averaged stats: lr: 0.000010  loss: 0.4658 (0.4604)
[12:29:10.916132] {"train_lr": 1.0271454420957877e-05, "train_loss": 0.4604296038548152, "epoch": 768}
[12:29:10.916473] [12:29:10.916561] Training epoch 768 for 0:00:27
[12:29:10.916650] [12:29:10.920979] log_dir: ./exp/debug/cifar100-LT/debug
[12:29:12.608004] Epoch: [769]  [ 0/42]  eta: 0:01:10  lr: 0.000010  loss: 0.4277 (0.4277)  time: 1.6860  data: 1.1787  max mem: 9341
[12:29:22.802519] Epoch: [769]  [20/42]  eta: 0:00:12  lr: 0.000010  loss: 0.4704 (0.4649)  time: 0.5097  data: 0.0001  max mem: 9341
[12:29:32.971633] Epoch: [769]  [40/42]  eta: 0:00:01  lr: 0.000009  loss: 0.4656 (0.4648)  time: 0.5084  data: 0.0001  max mem: 9341
[12:29:33.478027] Epoch: [769]  [41/42]  eta: 0:00:00  lr: 0.000009  loss: 0.4656 (0.4645)  time: 0.5085  data: 0.0001  max mem: 9341
[12:29:33.642461] Epoch: [769] Total time: 0:00:22 (0.5410 s / it)
[12:29:33.657445] Averaged stats: lr: 0.000009  loss: 0.4656 (0.4637)
[12:29:38.181787] {"train_lr": 9.63401968264618e-06, "train_loss": 0.46370989243899075, "epoch": 769}
[12:29:38.182178] [12:29:38.182268] Training epoch 769 for 0:00:27
[12:29:38.182324] [12:29:38.186634] log_dir: ./exp/debug/cifar100-LT/debug
[12:29:39.874679] Epoch: [770]  [ 0/42]  eta: 0:01:10  lr: 0.000009  loss: 0.4857 (0.4857)  time: 1.6869  data: 1.1688  max mem: 9341
[12:29:50.048902] Epoch: [770]  [20/42]  eta: 0:00:12  lr: 0.000009  loss: 0.4628 (0.4552)  time: 0.5086  data: 0.0001  max mem: 9341
[12:30:00.216336] Epoch: [770]  [40/42]  eta: 0:00:01  lr: 0.000009  loss: 0.4465 (0.4571)  time: 0.5083  data: 0.0001  max mem: 9341
[12:30:00.723682] Epoch: [770]  [41/42]  eta: 0:00:00  lr: 0.000009  loss: 0.4531 (0.4570)  time: 0.5082  data: 0.0001  max mem: 9341
[12:30:00.887982] Epoch: [770] Total time: 0:00:22 (0.5405 s / it)
[12:30:00.896214] Averaged stats: lr: 0.000009  loss: 0.4531 (0.4623)
[12:30:05.411541] {"train_lr": 9.016925014606516e-06, "train_loss": 0.46225032228089513, "epoch": 770}
[12:30:05.412038] [12:30:05.412146] Training epoch 770 for 0:00:27
[12:30:05.412205] [12:30:05.417341] log_dir: ./exp/debug/cifar100-LT/debug
[12:30:07.119556] Epoch: [771]  [ 0/42]  eta: 0:01:11  lr: 0.000009  loss: 0.4608 (0.4608)  time: 1.7012  data: 1.2067  max mem: 9341
[12:30:17.287436] Epoch: [771]  [20/42]  eta: 0:00:12  lr: 0.000008  loss: 0.4545 (0.4602)  time: 0.5083  data: 0.0001  max mem: 9341
[12:30:27.427750] Epoch: [771]  [40/42]  eta: 0:00:01  lr: 0.000008  loss: 0.4529 (0.4550)  time: 0.5070  data: 0.0001  max mem: 9341
[12:30:27.934157] Epoch: [771]  [41/42]  eta: 0:00:00  lr: 0.000008  loss: 0.4529 (0.4539)  time: 0.5070  data: 0.0001  max mem: 9341
[12:30:28.104883] Epoch: [771] Total time: 0:00:22 (0.5402 s / it)
[12:30:28.113199] Averaged stats: lr: 0.000008  loss: 0.4529 (0.4630)
[12:30:32.685572] {"train_lr": 8.420180961284361e-06, "train_loss": 0.4629550226742313, "epoch": 771}
[12:30:32.685975] [12:30:32.686068] Training epoch 771 for 0:00:27
[12:30:32.686127] [12:30:32.690907] log_dir: ./exp/debug/cifar100-LT/debug
[12:30:34.181428] Epoch: [772]  [ 0/42]  eta: 0:01:02  lr: 0.000008  loss: 0.5193 (0.5193)  time: 1.4892  data: 0.9699  max mem: 9341
[12:30:44.350250] Epoch: [772]  [20/42]  eta: 0:00:12  lr: 0.000008  loss: 0.4504 (0.4641)  time: 0.5084  data: 0.0001  max mem: 9341
[12:30:54.498375] Epoch: [772]  [40/42]  eta: 0:00:01  lr: 0.000008  loss: 0.4416 (0.4576)  time: 0.5074  data: 0.0001  max mem: 9341
[12:30:55.005502] Epoch: [772]  [41/42]  eta: 0:00:00  lr: 0.000008  loss: 0.4456 (0.4573)  time: 0.5074  data: 0.0001  max mem: 9341
[12:30:55.165174] Epoch: [772] Total time: 0:00:22 (0.5351 s / it)
[12:30:55.166129] Averaged stats: lr: 0.000008  loss: 0.4456 (0.4646)
[12:30:59.804469] {"train_lr": 7.843797719389012e-06, "train_loss": 0.46456045515480493, "epoch": 772}
[12:30:59.804918] [12:30:59.805019] Training epoch 772 for 0:00:27
[12:30:59.805078] [12:30:59.809994] log_dir: ./exp/debug/cifar100-LT/debug
[12:31:01.490080] Epoch: [773]  [ 0/42]  eta: 0:01:10  lr: 0.000007  loss: 0.4656 (0.4656)  time: 1.6789  data: 1.1679  max mem: 9341
[12:31:11.658823] Epoch: [773]  [20/42]  eta: 0:00:12  lr: 0.000007  loss: 0.4703 (0.4670)  time: 0.5084  data: 0.0001  max mem: 9341
[12:31:21.811893] Epoch: [773]  [40/42]  eta: 0:00:01  lr: 0.000007  loss: 0.4652 (0.4673)  time: 0.5076  data: 0.0001  max mem: 9341
[12:31:22.317962] Epoch: [773]  [41/42]  eta: 0:00:00  lr: 0.000007  loss: 0.4652 (0.4682)  time: 0.5077  data: 0.0001  max mem: 9341
[12:31:22.487740] Epoch: [773] Total time: 0:00:22 (0.5399 s / it)
[12:31:22.490300] Averaged stats: lr: 0.000007  loss: 0.4652 (0.4616)
[12:31:27.136793] {"train_lr": 7.2877851377197736e-06, "train_loss": 0.4615990365190165, "epoch": 773}
[12:31:27.137271] [12:31:27.137406] Training epoch 773 for 0:00:27
[12:31:27.137464] [12:31:27.142167] log_dir: ./exp/debug/cifar100-LT/debug
[12:31:28.898874] Epoch: [774]  [ 0/42]  eta: 0:01:13  lr: 0.000007  loss: 0.4901 (0.4901)  time: 1.7555  data: 1.2582  max mem: 9341
[12:31:39.069503] Epoch: [774]  [20/42]  eta: 0:00:12  lr: 0.000007  loss: 0.4655 (0.4743)  time: 0.5085  data: 0.0001  max mem: 9341
[12:31:49.231781] Epoch: [774]  [40/42]  eta: 0:00:01  lr: 0.000007  loss: 0.4650 (0.4722)  time: 0.5081  data: 0.0001  max mem: 9341
[12:31:49.738770] Epoch: [774]  [41/42]  eta: 0:00:00  lr: 0.000007  loss: 0.4650 (0.4724)  time: 0.5082  data: 0.0001  max mem: 9341
[12:31:49.907246] Epoch: [774] Total time: 0:00:22 (0.5420 s / it)
[12:31:49.913688] Averaged stats: lr: 0.000007  loss: 0.4650 (0.4661)
[12:31:54.446545] {"train_lr": 6.752152716997531e-06, "train_loss": 0.4660898182718527, "epoch": 774}
[12:31:54.446940] [12:31:54.447083] Training epoch 774 for 0:00:27
[12:31:54.447157] [12:31:54.451601] log_dir: ./exp/debug/cifar100-LT/debug
[12:31:56.213629] Epoch: [775]  [ 0/42]  eta: 0:01:13  lr: 0.000006  loss: 0.4447 (0.4447)  time: 1.7610  data: 1.2636  max mem: 9341
[12:32:06.398834] Epoch: [775]  [20/42]  eta: 0:00:12  lr: 0.000006  loss: 0.4613 (0.4620)  time: 0.5092  data: 0.0001  max mem: 9341
[12:32:16.556208] Epoch: [775]  [40/42]  eta: 0:00:01  lr: 0.000006  loss: 0.4550 (0.4592)  time: 0.5078  data: 0.0001  max mem: 9341
[12:32:17.062255] Epoch: [775]  [41/42]  eta: 0:00:00  lr: 0.000006  loss: 0.4550 (0.4590)  time: 0.5077  data: 0.0001  max mem: 9341
[12:32:17.234415] Epoch: [775] Total time: 0:00:22 (0.5424 s / it)
[12:32:17.235185] Averaged stats: lr: 0.000006  loss: 0.4550 (0.4654)
[12:32:21.824630] {"train_lr": 6.236909609702581e-06, "train_loss": 0.4654040648823693, "epoch": 775}
[12:32:21.825024] [12:32:21.825115] Training epoch 775 for 0:00:27
[12:32:21.825174] [12:32:21.829518] log_dir: ./exp/debug/cifar100-LT/debug
[12:32:23.429019] Epoch: [776]  [ 0/42]  eta: 0:01:07  lr: 0.000006  loss: 0.4685 (0.4685)  time: 1.5983  data: 1.0916  max mem: 9341
[12:32:33.620559] Epoch: [776]  [20/42]  eta: 0:00:12  lr: 0.000006  loss: 0.4664 (0.4664)  time: 0.5095  data: 0.0001  max mem: 9341
[12:32:43.765352] Epoch: [776]  [40/42]  eta: 0:00:01  lr: 0.000006  loss: 0.4655 (0.4686)  time: 0.5072  data: 0.0001  max mem: 9341
[12:32:44.271700] Epoch: [776]  [41/42]  eta: 0:00:00  lr: 0.000006  loss: 0.4655 (0.4687)  time: 0.5073  data: 0.0001  max mem: 9341
[12:32:44.443673] Epoch: [776] Total time: 0:00:22 (0.5384 s / it)
[12:32:44.445573] Averaged stats: lr: 0.000006  loss: 0.4655 (0.4664)
[12:32:49.029696] {"train_lr": 5.742064619917986e-06, "train_loss": 0.4663972904284795, "epoch": 776}
[12:32:49.030090] [12:32:49.030196] Training epoch 776 for 0:00:27
[12:32:49.030273] [12:32:49.034689] log_dir: ./exp/debug/cifar100-LT/debug
[12:32:50.662393] Epoch: [777]  [ 0/42]  eta: 0:01:08  lr: 0.000005  loss: 0.4691 (0.4691)  time: 1.6267  data: 1.1126  max mem: 9341
[12:33:00.835812] Epoch: [777]  [20/42]  eta: 0:00:12  lr: 0.000005  loss: 0.4560 (0.4608)  time: 0.5086  data: 0.0001  max mem: 9341
[12:33:10.988380] Epoch: [777]  [40/42]  eta: 0:00:01  lr: 0.000005  loss: 0.4586 (0.4615)  time: 0.5076  data: 0.0001  max mem: 9341
[12:33:11.494748] Epoch: [777]  [41/42]  eta: 0:00:00  lr: 0.000005  loss: 0.4586 (0.4616)  time: 0.5076  data: 0.0001  max mem: 9341
[12:33:11.657466] Epoch: [777] Total time: 0:00:22 (0.5386 s / it)
[12:33:11.658247] Averaged stats: lr: 0.000005  loss: 0.4586 (0.4651)
[12:33:16.265788] {"train_lr": 5.267626203179298e-06, "train_loss": 0.4651252432238488, "epoch": 777}
[12:33:16.266290] [12:33:16.266409] Training epoch 777 for 0:00:27
[12:33:16.266502] [12:33:16.271677] log_dir: ./exp/debug/cifar100-LT/debug
[12:33:17.852248] Epoch: [778]  [ 0/42]  eta: 0:01:06  lr: 0.000005  loss: 0.4872 (0.4872)  time: 1.5791  data: 1.0667  max mem: 9341
[12:33:28.021355] Epoch: [778]  [20/42]  eta: 0:00:12  lr: 0.000005  loss: 0.4699 (0.4687)  time: 0.5084  data: 0.0001  max mem: 9341
[12:33:38.174998] Epoch: [778]  [40/42]  eta: 0:00:01  lr: 0.000005  loss: 0.4650 (0.4675)  time: 0.5076  data: 0.0001  max mem: 9341
[12:33:38.683318] Epoch: [778]  [41/42]  eta: 0:00:00  lr: 0.000005  loss: 0.4664 (0.4676)  time: 0.5078  data: 0.0001  max mem: 9341
[12:33:38.844340] Epoch: [778] Total time: 0:00:22 (0.5374 s / it)
[12:33:38.855861] Averaged stats: lr: 0.000005  loss: 0.4664 (0.4669)
[12:33:43.474292] {"train_lr": 4.813602466330003e-06, "train_loss": 0.46686718258119764, "epoch": 778}
[12:33:43.474689] [12:33:43.474782] Training epoch 778 for 0:00:27
[12:33:43.474839] [12:33:43.479241] log_dir: ./exp/debug/cifar100-LT/debug
[12:33:45.080225] Epoch: [779]  [ 0/42]  eta: 0:01:07  lr: 0.000005  loss: 0.5237 (0.5237)  time: 1.5998  data: 1.0816  max mem: 9341
[12:33:55.248194] Epoch: [779]  [20/42]  eta: 0:00:12  lr: 0.000004  loss: 0.4788 (0.4809)  time: 0.5083  data: 0.0001  max mem: 9341
[12:34:05.403434] Epoch: [779]  [40/42]  eta: 0:00:01  lr: 0.000004  loss: 0.4598 (0.4700)  time: 0.5077  data: 0.0001  max mem: 9341
[12:34:05.907007] Epoch: [779]  [41/42]  eta: 0:00:00  lr: 0.000004  loss: 0.4598 (0.4699)  time: 0.5075  data: 0.0001  max mem: 9341
[12:34:06.061647] Epoch: [779] Total time: 0:00:22 (0.5377 s / it)
[12:34:06.072670] Averaged stats: lr: 0.000004  loss: 0.4598 (0.4643)
[12:34:10.709112] {"train_lr": 4.380001167383168e-06, "train_loss": 0.464291186559768, "epoch": 779}
[12:34:10.709554] [12:34:10.709693] Training epoch 779 for 0:00:27
[12:34:10.709787] [12:34:10.714667] log_dir: ./exp/debug/cifar100-LT/debug
[12:34:12.368884] Epoch: [780]  [ 0/42]  eta: 0:01:09  lr: 0.000004  loss: 0.4702 (0.4702)  time: 1.6530  data: 1.1586  max mem: 9341
[12:34:22.545017] Epoch: [780]  [20/42]  eta: 0:00:12  lr: 0.000004  loss: 0.4700 (0.4694)  time: 0.5087  data: 0.0001  max mem: 9341
[12:34:32.706923] Epoch: [780]  [40/42]  eta: 0:00:01  lr: 0.000004  loss: 0.4521 (0.4622)  time: 0.5080  data: 0.0001  max mem: 9341
[12:34:33.214558] Epoch: [780]  [41/42]  eta: 0:00:00  lr: 0.000004  loss: 0.4579 (0.4626)  time: 0.5081  data: 0.0001  max mem: 9341
[12:34:33.388061] Epoch: [780] Total time: 0:00:22 (0.5398 s / it)
[12:34:33.388904] Averaged stats: lr: 0.000004  loss: 0.4579 (0.4603)
[12:34:37.996944] {"train_lr": 3.966829715388543e-06, "train_loss": 0.4603082748750846, "epoch": 780}
[12:34:37.997348] [12:34:37.997471] Training epoch 780 for 0:00:27
[12:34:37.997544] [12:34:38.001966] log_dir: ./exp/debug/cifar100-LT/debug
[12:34:39.649830] Epoch: [781]  [ 0/42]  eta: 0:01:09  lr: 0.000004  loss: 0.5114 (0.5114)  time: 1.6466  data: 1.1382  max mem: 9341
[12:34:49.819319] Epoch: [781]  [20/42]  eta: 0:00:12  lr: 0.000004  loss: 0.4464 (0.4551)  time: 0.5084  data: 0.0001  max mem: 9341
[12:34:59.974081] Epoch: [781]  [40/42]  eta: 0:00:01  lr: 0.000003  loss: 0.4578 (0.4556)  time: 0.5077  data: 0.0001  max mem: 9341
[12:35:00.481145] Epoch: [781]  [41/42]  eta: 0:00:00  lr: 0.000003  loss: 0.4598 (0.4565)  time: 0.5077  data: 0.0001  max mem: 9341
[12:35:00.645551] Epoch: [781] Total time: 0:00:22 (0.5391 s / it)
[12:35:00.653585] Averaged stats: lr: 0.000003  loss: 0.4598 (0.4630)
[12:35:05.300026] {"train_lr": 3.5740951703063132e-06, "train_loss": 0.4629979348253636, "epoch": 781}
[12:35:05.300390] [12:35:05.300481] Training epoch 781 for 0:00:27
[12:35:05.300606] [12:35:05.305034] log_dir: ./exp/debug/cifar100-LT/debug
[12:35:06.790783] Epoch: [782]  [ 0/42]  eta: 0:01:02  lr: 0.000003  loss: 0.4489 (0.4489)  time: 1.4845  data: 0.9793  max mem: 9341
[12:35:16.963924] Epoch: [782]  [20/42]  eta: 0:00:12  lr: 0.000003  loss: 0.4617 (0.4659)  time: 0.5086  data: 0.0001  max mem: 9341
[12:35:27.123284] Epoch: [782]  [40/42]  eta: 0:00:01  lr: 0.000003  loss: 0.4570 (0.4623)  time: 0.5079  data: 0.0001  max mem: 9341
[12:35:27.629086] Epoch: [782]  [41/42]  eta: 0:00:00  lr: 0.000003  loss: 0.4570 (0.4623)  time: 0.5078  data: 0.0001  max mem: 9341
[12:35:27.762893] Epoch: [782] Total time: 0:00:22 (0.5347 s / it)
[12:35:27.813768] Averaged stats: lr: 0.000003  loss: 0.4570 (0.4671)
[12:35:32.426555] {"train_lr": 3.2018042428862166e-06, "train_loss": 0.4671257519651027, "epoch": 782}
[12:35:32.426952] [12:35:32.427078] Training epoch 782 for 0:00:27
[12:35:32.427134] [12:35:32.431618] log_dir: ./exp/debug/cifar100-LT/debug
[12:35:33.946791] Epoch: [783]  [ 0/42]  eta: 0:01:03  lr: 0.000003  loss: 0.4662 (0.4662)  time: 1.5139  data: 1.0071  max mem: 9341
[12:35:44.136259] Epoch: [783]  [20/42]  eta: 0:00:12  lr: 0.000003  loss: 0.4617 (0.4640)  time: 0.5094  data: 0.0001  max mem: 9341
[12:35:54.317836] Epoch: [783]  [40/42]  eta: 0:00:01  lr: 0.000003  loss: 0.4474 (0.4607)  time: 0.5090  data: 0.0001  max mem: 9341
[12:35:54.824716] Epoch: [783]  [41/42]  eta: 0:00:00  lr: 0.000003  loss: 0.4502 (0.4610)  time: 0.5090  data: 0.0001  max mem: 9341
[12:35:54.985782] Epoch: [783] Total time: 0:00:22 (0.5370 s / it)
[12:35:54.988654] Averaged stats: lr: 0.000003  loss: 0.4502 (0.4621)
[12:35:59.567004] {"train_lr": 2.8499632945530776e-06, "train_loss": 0.4621077525828566, "epoch": 783}
[12:35:59.567362] [12:35:59.567449] Training epoch 783 for 0:00:27
[12:35:59.567508] [12:35:59.571979] log_dir: ./exp/debug/cifar100-LT/debug
[12:36:01.007990] Epoch: [784]  [ 0/42]  eta: 0:01:00  lr: 0.000003  loss: 0.4710 (0.4710)  time: 1.4348  data: 0.9167  max mem: 9341
[12:36:11.174120] Epoch: [784]  [20/42]  eta: 0:00:12  lr: 0.000003  loss: 0.4510 (0.4495)  time: 0.5082  data: 0.0001  max mem: 9341
[12:36:21.324484] Epoch: [784]  [40/42]  eta: 0:00:01  lr: 0.000002  loss: 0.4528 (0.4542)  time: 0.5075  data: 0.0001  max mem: 9341
[12:36:21.831478] Epoch: [784]  [41/42]  eta: 0:00:00  lr: 0.000002  loss: 0.4541 (0.4555)  time: 0.5075  data: 0.0001  max mem: 9341
[12:36:21.996267] Epoch: [784] Total time: 0:00:22 (0.5339 s / it)
[12:36:22.002367] Averaged stats: lr: 0.000002  loss: 0.4541 (0.4599)
[12:36:26.734269] {"train_lr": 2.518578337297953e-06, "train_loss": 0.459945362948236, "epoch": 784}
[12:36:26.734624] [12:36:26.734716] Training epoch 784 for 0:00:27
[12:36:26.734787] [12:36:26.739244] log_dir: ./exp/debug/cifar100-LT/debug
[12:36:28.275437] Epoch: [785]  [ 0/42]  eta: 0:01:04  lr: 0.000002  loss: 0.4425 (0.4425)  time: 1.5347  data: 1.0310  max mem: 9341
[12:36:38.489619] Epoch: [785]  [20/42]  eta: 0:00:12  lr: 0.000002  loss: 0.4655 (0.4605)  time: 0.5107  data: 0.0001  max mem: 9341
[12:36:48.663590] Epoch: [785]  [40/42]  eta: 0:00:01  lr: 0.000002  loss: 0.4594 (0.4629)  time: 0.5086  data: 0.0001  max mem: 9341
[12:36:49.169532] Epoch: [785]  [41/42]  eta: 0:00:00  lr: 0.000002  loss: 0.4594 (0.4630)  time: 0.5087  data: 0.0001  max mem: 9341
[12:36:49.332167] Epoch: [785] Total time: 0:00:22 (0.5379 s / it)
[12:36:49.351134] Averaged stats: lr: 0.000002  loss: 0.4594 (0.4616)
[12:36:53.891489] {"train_lr": 2.2076550335753373e-06, "train_loss": 0.4616489942584719, "epoch": 785}
[12:36:53.891863] [12:36:53.891953] Training epoch 785 for 0:00:27
[12:36:53.892013] [12:36:53.896340] log_dir: ./exp/debug/cifar100-LT/debug
[12:36:55.327230] Epoch: [786]  [ 0/42]  eta: 0:01:00  lr: 0.000002  loss: 0.4534 (0.4534)  time: 1.4296  data: 0.9300  max mem: 9341
[12:37:05.499171] Epoch: [786]  [20/42]  eta: 0:00:12  lr: 0.000002  loss: 0.4517 (0.4492)  time: 0.5085  data: 0.0001  max mem: 9341
[12:37:15.654072] Epoch: [786]  [40/42]  eta: 0:00:01  lr: 0.000002  loss: 0.4704 (0.4584)  time: 0.5077  data: 0.0001  max mem: 9341
[12:37:16.161107] Epoch: [786]  [41/42]  eta: 0:00:00  lr: 0.000002  loss: 0.4713 (0.4594)  time: 0.5076  data: 0.0001  max mem: 9341
[12:37:16.319444] Epoch: [786] Total time: 0:00:22 (0.5339 s / it)
[12:37:16.329600] Averaged stats: lr: 0.000002  loss: 0.4713 (0.4608)
[12:37:21.019483] {"train_lr": 1.9171986962067688e-06, "train_loss": 0.4607535994478634, "epoch": 786}
[12:37:21.019876] [12:37:21.019970] Training epoch 786 for 0:00:27
[12:37:21.020026] [12:37:21.024612] log_dir: ./exp/debug/cifar100-LT/debug
[12:37:22.633521] Epoch: [787]  [ 0/42]  eta: 0:01:07  lr: 0.000002  loss: 0.4550 (0.4550)  time: 1.6076  data: 1.0937  max mem: 9341
[12:37:32.807398] Epoch: [787]  [20/42]  eta: 0:00:12  lr: 0.000002  loss: 0.4538 (0.4573)  time: 0.5086  data: 0.0001  max mem: 9341
[12:37:42.977241] Epoch: [787]  [40/42]  eta: 0:00:01  lr: 0.000002  loss: 0.4623 (0.4610)  time: 0.5084  data: 0.0001  max mem: 9341
[12:37:43.484570] Epoch: [787]  [41/42]  eta: 0:00:00  lr: 0.000002  loss: 0.4649 (0.4611)  time: 0.5084  data: 0.0001  max mem: 9341
[12:37:43.646114] Epoch: [787] Total time: 0:00:22 (0.5386 s / it)
[12:37:43.670997] Averaged stats: lr: 0.000002  loss: 0.4649 (0.4643)
[12:37:48.263175] {"train_lr": 1.647214288289536e-06, "train_loss": 0.4642836130445912, "epoch": 787}
[12:37:48.263615] [12:37:48.263728] Training epoch 787 for 0:00:27
[12:37:48.263804] [12:37:48.268380] log_dir: ./exp/debug/cifar100-LT/debug
[12:37:49.787257] Epoch: [788]  [ 0/42]  eta: 0:01:03  lr: 0.000001  loss: 0.4573 (0.4573)  time: 1.5172  data: 1.0003  max mem: 9341
[12:37:59.958704] Epoch: [788]  [20/42]  eta: 0:00:12  lr: 0.000001  loss: 0.4645 (0.4657)  time: 0.5085  data: 0.0001  max mem: 9341
[12:38:10.117061] Epoch: [788]  [40/42]  eta: 0:00:01  lr: 0.000001  loss: 0.4589 (0.4644)  time: 0.5079  data: 0.0001  max mem: 9341
[12:38:10.622503] Epoch: [788]  [41/42]  eta: 0:00:00  lr: 0.000001  loss: 0.4615 (0.4648)  time: 0.5078  data: 0.0001  max mem: 9341
[12:38:10.781190] Epoch: [788] Total time: 0:00:22 (0.5360 s / it)
[12:38:10.786064] Averaged stats: lr: 0.000001  loss: 0.4615 (0.4617)
[12:38:15.458097] {"train_lr": 1.3977064231122516e-06, "train_loss": 0.46172465721056577, "epoch": 788}
[12:38:15.458612] [12:38:15.458729] Training epoch 788 for 0:00:27
[12:38:15.458786] [12:38:15.463804] log_dir: ./exp/debug/cifar100-LT/debug
[12:38:16.950964] Epoch: [789]  [ 0/42]  eta: 0:01:02  lr: 0.000001  loss: 0.4327 (0.4327)  time: 1.4859  data: 0.9775  max mem: 9341
[12:38:27.119136] Epoch: [789]  [20/42]  eta: 0:00:12  lr: 0.000001  loss: 0.4624 (0.4625)  time: 0.5083  data: 0.0001  max mem: 9341
[12:38:37.268171] Epoch: [789]  [40/42]  eta: 0:00:01  lr: 0.000001  loss: 0.4591 (0.4635)  time: 0.5074  data: 0.0001  max mem: 9341
[12:38:37.775513] Epoch: [789]  [41/42]  eta: 0:00:00  lr: 0.000001  loss: 0.4591 (0.4628)  time: 0.5074  data: 0.0001  max mem: 9341
[12:38:37.953621] Epoch: [789] Total time: 0:00:22 (0.5355 s / it)
[12:38:37.954490] Averaged stats: lr: 0.000001  loss: 0.4591 (0.4629)
[12:38:42.541309] {"train_lr": 1.1686793640760363e-06, "train_loss": 0.4628509848955132, "epoch": 789}
[12:38:42.541852] [12:38:42.541986] Training epoch 789 for 0:00:27
[12:38:42.542049] [12:38:42.547155] log_dir: ./exp/debug/cifar100-LT/debug
[12:38:44.047120] Epoch: [790]  [ 0/42]  eta: 0:01:02  lr: 0.000001  loss: 0.4718 (0.4718)  time: 1.4986  data: 0.9966  max mem: 9341
[12:38:54.213528] Epoch: [790]  [20/42]  eta: 0:00:12  lr: 0.000001  loss: 0.4678 (0.4671)  time: 0.5083  data: 0.0001  max mem: 9341
[12:39:04.357701] Epoch: [790]  [40/42]  eta: 0:00:01  lr: 0.000001  loss: 0.4536 (0.4614)  time: 0.5072  data: 0.0001  max mem: 9341
[12:39:04.865434] Epoch: [790]  [41/42]  eta: 0:00:00  lr: 0.000001  loss: 0.4587 (0.4622)  time: 0.5072  data: 0.0001  max mem: 9341
[12:39:05.029945] Epoch: [790] Total time: 0:00:22 (0.5353 s / it)
[12:39:05.037666] Averaged stats: lr: 0.000001  loss: 0.4587 (0.4621)
[12:39:09.627120] {"train_lr": 9.601370246213397e-07, "train_loss": 0.46212902948969886, "epoch": 790}
[12:39:09.627521] [12:39:09.627612] Training epoch 790 for 0:00:27
[12:39:09.627670] [12:39:09.632015] log_dir: ./exp/debug/cifar100-LT/debug
[12:39:11.283060] Epoch: [791]  [ 0/42]  eta: 0:01:09  lr: 0.000001  loss: 0.4633 (0.4633)  time: 1.6499  data: 1.1285  max mem: 9341
[12:39:21.460619] Epoch: [791]  [20/42]  eta: 0:00:12  lr: 0.000001  loss: 0.4669 (0.4677)  time: 0.5088  data: 0.0002  max mem: 9341
[12:39:31.618996] Epoch: [791]  [40/42]  eta: 0:00:01  lr: 0.000001  loss: 0.4477 (0.4625)  time: 0.5079  data: 0.0001  max mem: 9341
[12:39:32.127562] Epoch: [791]  [41/42]  eta: 0:00:00  lr: 0.000001  loss: 0.4477 (0.4625)  time: 0.5080  data: 0.0001  max mem: 9341
[12:39:32.286857] Epoch: [791] Total time: 0:00:22 (0.5394 s / it)
[12:39:32.290899] Averaged stats: lr: 0.000001  loss: 0.4477 (0.4625)
[12:39:36.867842] {"train_lr": 7.720829681613331e-07, "train_loss": 0.46249639544458615, "epoch": 791}
[12:39:36.868290] [12:39:36.868381] Training epoch 791 for 0:00:27
[12:39:36.868453] [12:39:36.872861] log_dir: ./exp/debug/cifar100-LT/debug
[12:39:38.667103] Epoch: [792]  [ 0/42]  eta: 0:01:15  lr: 0.000001  loss: 0.4624 (0.4624)  time: 1.7932  data: 1.2903  max mem: 9341
[12:39:48.894740] Epoch: [792]  [20/42]  eta: 0:00:12  lr: 0.000001  loss: 0.4658 (0.4670)  time: 0.5113  data: 0.0001  max mem: 9341
[12:39:59.043366] Epoch: [792]  [40/42]  eta: 0:00:01  lr: 0.000001  loss: 0.4782 (0.4697)  time: 0.5074  data: 0.0001  max mem: 9341
[12:39:59.550880] Epoch: [792]  [41/42]  eta: 0:00:00  lr: 0.000001  loss: 0.4745 (0.4697)  time: 0.5075  data: 0.0001  max mem: 9341
[12:39:59.741030] Epoch: [792] Total time: 0:00:22 (0.5445 s / it)
[12:39:59.746978] Averaged stats: lr: 0.000001  loss: 0.4745 (0.4636)
[12:40:04.345751] {"train_lr": 6.045204080209982e-07, "train_loss": 0.4636357556141558, "epoch": 792}
[12:40:04.346154] [12:40:04.346247] Training epoch 792 for 0:00:27
[12:40:04.346304] [12:40:04.350716] log_dir: ./exp/debug/cifar100-LT/debug
[12:40:05.917730] Epoch: [793]  [ 0/42]  eta: 0:01:05  lr: 0.000001  loss: 0.4439 (0.4439)  time: 1.5658  data: 1.0628  max mem: 9341
[12:40:16.079407] Epoch: [793]  [20/42]  eta: 0:00:12  lr: 0.000000  loss: 0.4553 (0.4630)  time: 0.5080  data: 0.0001  max mem: 9341
[12:40:26.237268] Epoch: [793]  [40/42]  eta: 0:00:01  lr: 0.000000  loss: 0.4645 (0.4634)  time: 0.5078  data: 0.0001  max mem: 9341
[12:40:26.743483] Epoch: [793]  [41/42]  eta: 0:00:00  lr: 0.000000  loss: 0.4645 (0.4643)  time: 0.5079  data: 0.0001  max mem: 9341
[12:40:26.917001] Epoch: [793] Total time: 0:00:22 (0.5373 s / it)
[12:40:26.926584] Averaged stats: lr: 0.000000  loss: 0.4645 (0.4659)
[12:40:31.560690] {"train_lr": 4.5745220738224413e-07, "train_loss": 0.46594246352712315, "epoch": 793}
[12:40:31.561063] [12:40:31.561149] Training epoch 793 for 0:00:27
[12:40:31.561207] [12:40:31.565666] log_dir: ./exp/debug/cifar100-LT/debug
[12:40:33.217178] Epoch: [794]  [ 0/42]  eta: 0:01:09  lr: 0.000000  loss: 0.4695 (0.4695)  time: 1.6503  data: 1.1465  max mem: 9341
[12:40:43.389390] Epoch: [794]  [20/42]  eta: 0:00:12  lr: 0.000000  loss: 0.4551 (0.4543)  time: 0.5086  data: 0.0001  max mem: 9341
[12:40:53.548294] Epoch: [794]  [40/42]  eta: 0:00:01  lr: 0.000000  loss: 0.4594 (0.4586)  time: 0.5079  data: 0.0001  max mem: 9341
[12:40:54.054213] Epoch: [794]  [41/42]  eta: 0:00:00  lr: 0.000000  loss: 0.4594 (0.4587)  time: 0.5078  data: 0.0001  max mem: 9341
[12:40:54.221463] Epoch: [794] Total time: 0:00:22 (0.5394 s / it)
[12:40:54.230444] Averaged stats: lr: 0.000000  loss: 0.4594 (0.4611)
[12:40:58.853804] {"train_lr": 3.308808792347659e-07, "train_loss": 0.4610717048247655, "epoch": 794}
[12:40:58.854201] [12:40:58.854292] Training epoch 794 for 0:00:27
[12:40:58.854350] [12:40:58.858870] log_dir: ./exp/debug/cifar100-LT/debug
[12:41:00.353850] Epoch: [795]  [ 0/42]  eta: 0:01:02  lr: 0.000000  loss: 0.4797 (0.4797)  time: 1.4937  data: 0.9758  max mem: 9341
[12:41:10.526288] Epoch: [795]  [20/42]  eta: 0:00:12  lr: 0.000000  loss: 0.4674 (0.4687)  time: 0.5086  data: 0.0001  max mem: 9341
[12:41:20.692130] Epoch: [795]  [40/42]  eta: 0:00:01  lr: 0.000000  loss: 0.4651 (0.4700)  time: 0.5082  data: 0.0001  max mem: 9341
[12:41:21.198548] Epoch: [795]  [41/42]  eta: 0:00:00  lr: 0.000000  loss: 0.4704 (0.4708)  time: 0.5082  data: 0.0001  max mem: 9341
[12:41:21.367933] Epoch: [795] Total time: 0:00:22 (0.5359 s / it)
[12:41:21.371642] Averaged stats: lr: 0.000000  loss: 0.4704 (0.4634)
[12:41:26.076275] {"train_lr": 2.2480858633331402e-07, "train_loss": 0.4633539732368219, "epoch": 795}
[12:41:26.076690] [12:41:26.076799] Training epoch 795 for 0:00:27
[12:41:26.076874] [12:41:26.081256] log_dir: ./exp/debug/cifar100-LT/debug
[12:41:27.739311] Epoch: [796]  [ 0/42]  eta: 0:01:09  lr: 0.000000  loss: 0.4277 (0.4277)  time: 1.6568  data: 1.1593  max mem: 9341
[12:41:37.912534] Epoch: [796]  [20/42]  eta: 0:00:12  lr: 0.000000  loss: 0.4632 (0.4569)  time: 0.5086  data: 0.0001  max mem: 9341
[12:41:48.065624] Epoch: [796]  [40/42]  eta: 0:00:01  lr: 0.000000  loss: 0.4698 (0.4618)  time: 0.5076  data: 0.0001  max mem: 9341
[12:41:48.571554] Epoch: [796]  [41/42]  eta: 0:00:00  lr: 0.000000  loss: 0.4663 (0.4619)  time: 0.5076  data: 0.0001  max mem: 9341
[12:41:48.750873] Epoch: [796] Total time: 0:00:22 (0.5398 s / it)
[12:41:48.751615] Averaged stats: lr: 0.000000  loss: 0.4663 (0.4628)
[12:41:53.418935] {"train_lr": 1.3923714116076243e-07, "train_loss": 0.4628257403771083, "epoch": 796}
[12:41:53.419314] [12:41:53.419418] Training epoch 796 for 0:00:27
[12:41:53.419494] [12:41:53.423806] log_dir: ./exp/debug/cifar100-LT/debug
[12:41:54.907387] Epoch: [797]  [ 0/42]  eta: 0:01:02  lr: 0.000000  loss: 0.4416 (0.4416)  time: 1.4823  data: 0.9661  max mem: 9341
[12:42:05.077307] Epoch: [797]  [20/42]  eta: 0:00:12  lr: 0.000000  loss: 0.4717 (0.4651)  time: 0.5084  data: 0.0001  max mem: 9341
[12:42:15.229413] Epoch: [797]  [40/42]  eta: 0:00:01  lr: 0.000000  loss: 0.4589 (0.4630)  time: 0.5076  data: 0.0001  max mem: 9341
[12:42:15.735477] Epoch: [797]  [41/42]  eta: 0:00:00  lr: 0.000000  loss: 0.4624 (0.4632)  time: 0.5075  data: 0.0001  max mem: 9341
[12:42:15.888383] Epoch: [797] Total time: 0:00:22 (0.5349 s / it)
[12:42:15.906495] Averaged stats: lr: 0.000000  loss: 0.4624 (0.4643)
[12:42:20.499289] {"train_lr": 7.416800589707004e-08, "train_loss": 0.46426590585282873, "epoch": 797}
[12:42:20.499663] [12:42:20.499765] Training epoch 797 for 0:00:27
[12:42:20.499826] [12:42:20.504722] log_dir: ./exp/debug/cifar100-LT/debug
[12:42:22.201727] Epoch: [798]  [ 0/42]  eta: 0:01:11  lr: 0.000000  loss: 0.3928 (0.3928)  time: 1.6960  data: 1.1961  max mem: 9341
[12:42:32.398172] Epoch: [798]  [20/42]  eta: 0:00:12  lr: 0.000000  loss: 0.4701 (0.4701)  time: 0.5098  data: 0.0001  max mem: 9341
[12:42:42.575305] Epoch: [798]  [40/42]  eta: 0:00:01  lr: 0.000000  loss: 0.4592 (0.4665)  time: 0.5088  data: 0.0001  max mem: 9341
[12:42:43.083056] Epoch: [798]  [41/42]  eta: 0:00:00  lr: 0.000000  loss: 0.4586 (0.4661)  time: 0.5088  data: 0.0001  max mem: 9341
[12:42:43.237077] Epoch: [798] Total time: 0:00:22 (0.5412 s / it)
[12:42:43.244811] Averaged stats: lr: 0.000000  loss: 0.4586 (0.4618)
[12:42:47.966051] {"train_lr": 2.9602292394142238e-08, "train_loss": 0.46184987894126345, "epoch": 798}
[12:42:47.966535] [12:42:47.966638] Training epoch 798 for 0:00:27
[12:42:47.966729] [12:42:47.971940] log_dir: ./exp/debug/cifar100-LT/debug
[12:42:49.444216] Epoch: [799]  [ 0/42]  eta: 0:01:01  lr: 0.000000  loss: 0.4887 (0.4887)  time: 1.4712  data: 0.9658  max mem: 9341
[12:42:59.606286] Epoch: [799]  [20/42]  eta: 0:00:12  lr: 0.000000  loss: 0.4613 (0.4584)  time: 0.5080  data: 0.0001  max mem: 9341
[12:43:09.767675] Epoch: [799]  [40/42]  eta: 0:00:01  lr: 0.000000  loss: 0.4570 (0.4605)  time: 0.5080  data: 0.0001  max mem: 9341
[12:43:10.273490] Epoch: [799]  [41/42]  eta: 0:00:00  lr: 0.000000  loss: 0.4570 (0.4598)  time: 0.5080  data: 0.0001  max mem: 9341
[12:43:10.445535] Epoch: [799] Total time: 0:00:22 (0.5351 s / it)
[12:43:10.446251] Averaged stats: lr: 0.000000  loss: 0.4570 (0.4631)
[12:43:14.718431] {"train_lr": 5.5407621570350115e-09, "train_loss": 0.46311174599187716, "epoch": 799}
[12:43:14.719066] [12:43:14.719230] Training epoch 799 for 0:00:26
[12:43:14.719333] [12:43:14.719403] Training time 6:02:13
[12:43:14.719471] /home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
[12:43:53.953175] job dir: /home/vision/wonjun/LiVT-main
[12:43:53.953265] [12:43:53.953477] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=8,
adamW2=0.999,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt/debug/cifar100-LT/vit_base_patch16/debug',
clip_grad=None,
color_jitter=None,
cutmix=0.0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=100,
eval=False,
finetune='./ckpt/debug/cifar100-LT/debug/checkpoint.pth',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.65,
local_rank=0,
log_dir='./exp/debug/cifar100-LT/vit_base_patch16/debug',
loss='CE',
lr=None,
min_lr=1e-06,
mixup=0.0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=1,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=4)
[12:43:53.953549] [12:43:54.262437] Files already downloaded and verified
[12:43:55.003417] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[12:43:55.330630] Files already downloaded and verified
[12:43:55.690717] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[12:43:55.690909] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f8234d21210>
[12:43:55.691007] [12:43:55.692678] Train on 10847 Image w.r.t. 100 classes
[12:43:55.692750] [12:43:58.399715] Load pre-trained checkpoint from: ./ckpt/debug/cifar100-LT/debug/checkpoint.pth
[12:43:58.399865] [12:43:58.535594] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.qkv.scaling_factor', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.attn.proj.scaling_factor', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc1.scaling_factor', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.0.mlp.fc2.scaling_factor', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.qkv.scaling_factor', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.attn.proj.scaling_factor', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc1.scaling_factor', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.1.mlp.fc2.scaling_factor', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.qkv.scaling_factor', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.attn.proj.scaling_factor', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc1.scaling_factor', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.2.mlp.fc2.scaling_factor', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.qkv.scaling_factor', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.attn.proj.scaling_factor', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc1.scaling_factor', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.3.mlp.fc2.scaling_factor', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.qkv.scaling_factor', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.attn.proj.scaling_factor', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc1.scaling_factor', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.4.mlp.fc2.scaling_factor', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.qkv.scaling_factor', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.attn.proj.scaling_factor', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc1.scaling_factor', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.5.mlp.fc2.scaling_factor', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.qkv.scaling_factor', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.attn.proj.scaling_factor', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc1.scaling_factor', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.6.mlp.fc2.scaling_factor', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.qkv.scaling_factor', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.attn.proj.scaling_factor', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc1.scaling_factor', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_blocks.7.mlp.fc2.scaling_factor', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[12:43:58.535825] [12:43:58.659357] Model = vit_base_patch16
[12:43:58.659511] [12:43:58.659568] number of params (M): 85.96
[12:43:58.659617] [12:43:58.659661] base lr: 1.00e-03
[12:43:58.659707] [12:43:58.659742] actual lr: 8.00e-03
[12:43:58.659784] [12:43:58.659817] accumulate grad iterations: 8
[12:43:58.659858] [12:43:58.659891] effective batch size: 2048
[12:43:58.659932] [12:43:58.673252] criterion = CrossEntropyLoss()
[12:43:58.673331] [12:43:58.673594] Save config to: ./exp/debug/cifar100-LT/vit_base_patch16/debug/args.txt
[12:43:58.673629] Start training for 100 epochs
[12:43:58.673698] [12:43:58.674735] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:44:03.458891] Epoch: [0]  [ 0/42]  eta: 0:03:20  lr: 0.000000  loss: 4.6055 (4.6055)  time: 4.7833  data: 1.8095  max mem: 9052
[12:44:23.342956] Epoch: [0]  [41/42]  eta: 0:00:00  lr: 0.001524  loss: 4.4191 (4.5128)  time: 0.4847  data: 0.0001  max mem: 10039
[12:44:23.531661] Epoch: [0] Total time: 0:00:24 (0.5918 s / it)
[12:44:23.532427] Averaged stats: lr: 0.001524  loss: 4.4191 (4.5161)
[12:44:25.242065] Test:  [ 0/40]  eta: 0:01:08  loss: 4.6780 (4.6780)  acc1: 6.2500 (6.2500)  acc5: 10.9375 (10.9375)  time: 1.7050  data: 1.5501  max mem: 10039
[12:44:31.218918] Test:  [39/40]  eta: 0:00:00  loss: 4.6350 (4.6488)  acc1: 3.1250 (3.8000)  acc5: 12.5000 (10.4800)  time: 0.1522  data: 0.0001  max mem: 10039
[12:44:31.355575] Test: Total time: 0:00:07 (0.1955 s / it)
[12:44:31.620055] * Acc@1 3.390 Acc@5 10.170 loss 4.644
[12:44:31.620270] Accuracy of the network on the 10000 test images: 3.4%
[12:44:31.620500] [12:44:35.034766] Max accuracy: 3.39%
[12:44:35.035024] [12:44:35.036007] {"train_lr": 0.0006530612244897961, "train_loss": 4.516089666457403, "test_loss": 4.643952369689941, "test_acc1": 3.39, "test_acc5": 10.17, "epoch": 0, "n_parameters": 85958500}
[12:44:35.036110] [12:44:35.036173] Training epoch 0 for 0:00:36
[12:44:35.036226] [12:44:35.038914] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:44:36.960906] Epoch: [1]  [ 0/42]  eta: 0:01:20  lr: 0.001600  loss: 4.2203 (4.2203)  time: 1.9209  data: 1.4422  max mem: 10040
[12:44:56.885996] Epoch: [1]  [41/42]  eta: 0:00:00  lr: 0.003124  loss: 4.0202 (4.0761)  time: 0.4865  data: 0.0001  max mem: 10046
[12:44:57.111695] Epoch: [1] Total time: 0:00:22 (0.5255 s / it)
[12:44:57.115176] Averaged stats: lr: 0.003124  loss: 4.0202 (4.0737)
[12:44:59.216631] Test:  [ 0/40]  eta: 0:01:23  loss: 6.3192 (6.3192)  acc1: 3.1250 (3.1250)  acc5: 10.9375 (10.9375)  time: 2.0981  data: 1.9205  max mem: 10046
[12:45:05.131295] Test:  [39/40]  eta: 0:00:00  loss: 5.9762 (6.1301)  acc1: 3.1250 (2.9200)  acc5: 9.3750 (9.0400)  time: 0.1488  data: 0.0001  max mem: 10046
[12:45:05.269352] Test: Total time: 0:00:08 (0.2038 s / it)
[12:45:05.270682] * Acc@1 2.600 Acc@5 8.790 loss 6.096
[12:45:05.270829] Accuracy of the network on the 10000 test images: 2.6%
[12:45:05.271034] [12:45:05.271110] Max accuracy: 3.39%
[12:45:05.271165] [12:45:05.271949] {"train_lr": 0.0022530612244897968, "train_loss": 4.0736541748046875, "test_loss": 6.096186828613281, "test_acc1": 2.6, "test_acc5": 8.79, "epoch": 1, "n_parameters": 85958500}
[12:45:05.272020] [12:45:05.272085] Training epoch 1 for 0:00:30
[12:45:05.272144] [12:45:05.274997] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:45:07.786891] Epoch: [2]  [ 0/42]  eta: 0:01:45  lr: 0.003200  loss: 4.1544 (4.1544)  time: 2.5105  data: 1.7960  max mem: 10046
[12:45:27.843729] Epoch: [2]  [41/42]  eta: 0:00:00  lr: 0.004724  loss: 3.8504 (3.9591)  time: 0.4896  data: 0.0001  max mem: 10046
[12:45:28.057172] Epoch: [2] Total time: 0:00:22 (0.5424 s / it)
[12:45:28.057967] Averaged stats: lr: 0.004724  loss: 3.8504 (3.9834)
[12:45:30.070495] Test:  [ 0/40]  eta: 0:01:20  loss: 5.5790 (5.5790)  acc1: 4.6875 (4.6875)  acc5: 14.0625 (14.0625)  time: 2.0091  data: 1.8335  max mem: 10046
[12:45:35.994738] Test:  [39/40]  eta: 0:00:00  loss: 5.2977 (5.3600)  acc1: 4.6875 (4.2400)  acc5: 14.0625 (13.5200)  time: 0.1488  data: 0.0001  max mem: 10046
[12:45:36.123751] Test: Total time: 0:00:08 (0.2016 s / it)
[12:45:36.125259] * Acc@1 3.950 Acc@5 12.780 loss 5.346
[12:45:36.125425] Accuracy of the network on the 10000 test images: 4.0%
[12:45:36.125636] [12:45:39.721807] Max accuracy: 3.95%
[12:45:39.722108] [12:45:39.723007] {"train_lr": 0.003853061224489796, "train_loss": 3.983441489083426, "test_loss": 5.345899629592895, "test_acc1": 3.95, "test_acc5": 12.78, "epoch": 2, "n_parameters": 85958500}
[12:45:39.723102] [12:45:39.723199] Training epoch 2 for 0:00:34
[12:45:39.723269] [12:45:39.726221] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:45:41.834607] Epoch: [3]  [ 0/42]  eta: 0:01:28  lr: 0.004800  loss: 3.8989 (3.8989)  time: 2.1075  data: 1.6354  max mem: 10046
[12:46:01.872483] Epoch: [3]  [41/42]  eta: 0:00:00  lr: 0.006324  loss: 3.8364 (3.8780)  time: 0.4896  data: 0.0001  max mem: 10046
[12:46:02.101101] Epoch: [3] Total time: 0:00:22 (0.5327 s / it)
[12:46:02.101978] Averaged stats: lr: 0.006324  loss: 3.8364 (3.8643)
[12:46:04.000321] Test:  [ 0/40]  eta: 0:01:15  loss: 5.3024 (5.3024)  acc1: 7.8125 (7.8125)  acc5: 10.9375 (10.9375)  time: 1.8948  data: 1.7173  max mem: 10046
[12:46:09.942173] Test:  [39/40]  eta: 0:00:00  loss: 5.1031 (5.1639)  acc1: 4.6875 (4.6000)  acc5: 10.9375 (11.5600)  time: 0.1488  data: 0.0001  max mem: 10046
[12:46:10.059010] Test: Total time: 0:00:07 (0.1989 s / it)
[12:46:10.060301] * Acc@1 3.710 Acc@5 11.780 loss 5.160
[12:46:10.060454] Accuracy of the network on the 10000 test images: 3.7%
[12:46:10.060655] [12:46:10.060731] Max accuracy: 3.95%
[12:46:10.060792] [12:46:10.061567] {"train_lr": 0.005453061224489799, "train_loss": 3.8643051783243814, "test_loss": 5.160404539108276, "test_acc1": 3.71, "test_acc5": 11.78, "epoch": 3, "n_parameters": 85958500}
[12:46:10.061640] [12:46:10.061705] Training epoch 3 for 0:00:30
[12:46:10.061762] [12:46:10.065339] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:46:12.282330] Epoch: [4]  [ 0/42]  eta: 0:01:33  lr: 0.006400  loss: 3.7715 (3.7715)  time: 2.2158  data: 1.7062  max mem: 10046
[12:46:32.325592] Epoch: [4]  [41/42]  eta: 0:00:00  lr: 0.007924  loss: 3.8104 (3.8182)  time: 0.4897  data: 0.0001  max mem: 10046
[12:46:32.584640] Epoch: [4] Total time: 0:00:22 (0.5362 s / it)
[12:46:32.585502] Averaged stats: lr: 0.007924  loss: 3.8104 (3.8130)
[12:46:34.543174] Test:  [ 0/40]  eta: 0:01:18  loss: 5.0265 (5.0265)  acc1: 6.2500 (6.2500)  acc5: 14.0625 (14.0625)  time: 1.9538  data: 1.7828  max mem: 10046
[12:46:40.482587] Test:  [39/40]  eta: 0:00:00  loss: 4.7991 (4.8938)  acc1: 4.6875 (5.2000)  acc5: 15.6250 (14.9200)  time: 0.1490  data: 0.0001  max mem: 10046
[12:46:40.630411] Test: Total time: 0:00:08 (0.2011 s / it)
[12:46:40.631637] * Acc@1 5.140 Acc@5 15.010 loss 4.887
[12:46:40.631778] Accuracy of the network on the 10000 test images: 5.1%
[12:46:40.631976] [12:46:44.399515] Max accuracy: 5.14%
[12:46:44.399796] [12:46:44.400574] {"train_lr": 0.007053061224489795, "train_loss": 3.812950168337141, "test_loss": 4.887410402297974, "test_acc1": 5.14, "test_acc5": 15.01, "epoch": 4, "n_parameters": 85958500}
[12:46:44.400659] [12:46:44.400726] Training epoch 4 for 0:00:34
[12:46:44.400795] [12:46:44.403497] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:46:46.616675] Epoch: [5]  [ 0/42]  eta: 0:01:32  lr: 0.008000  loss: 3.8885 (3.8885)  time: 2.2123  data: 1.7260  max mem: 10046
[12:47:06.687081] Epoch: [5]  [41/42]  eta: 0:00:00  lr: 0.007998  loss: 3.7507 (3.7382)  time: 0.4902  data: 0.0001  max mem: 10046
[12:47:06.919465] Epoch: [5] Total time: 0:00:22 (0.5361 s / it)
[12:47:06.920347] Averaged stats: lr: 0.007998  loss: 3.7507 (3.7567)
[12:47:08.623952] Test:  [ 0/40]  eta: 0:01:07  loss: 5.2302 (5.2302)  acc1: 4.6875 (4.6875)  acc5: 15.6250 (15.6250)  time: 1.6992  data: 1.5393  max mem: 10046
[12:47:14.576222] Test:  [39/40]  eta: 0:00:00  loss: 4.9372 (5.0215)  acc1: 6.2500 (5.8400)  acc5: 15.6250 (15.6400)  time: 0.1493  data: 0.0001  max mem: 10046
[12:47:14.737690] Test: Total time: 0:00:07 (0.1954 s / it)
[12:47:14.936923] * Acc@1 5.360 Acc@5 15.540 loss 5.021
[12:47:14.937109] Accuracy of the network on the 10000 test images: 5.4%
[12:47:14.937306] [12:47:17.426348] Max accuracy: 5.36%
[12:47:17.426630] [12:47:17.427457] {"train_lr": 0.007999452179690645, "train_loss": 3.756738537833804, "test_loss": 5.020705151557922, "test_acc1": 5.36, "test_acc5": 15.54, "epoch": 5, "n_parameters": 85958500}
[12:47:17.427530] [12:47:17.427601] Training epoch 5 for 0:00:33
[12:47:17.427661] [12:47:17.430552] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:47:19.577612] Epoch: [6]  [ 0/42]  eta: 0:01:30  lr: 0.007998  loss: 3.5122 (3.5122)  time: 2.1463  data: 1.6765  max mem: 10046
[12:47:39.660285] Epoch: [6]  [41/42]  eta: 0:00:00  lr: 0.007992  loss: 3.6107 (3.6886)  time: 0.4907  data: 0.0001  max mem: 10046
[12:47:39.880331] Epoch: [6] Total time: 0:00:22 (0.5345 s / it)
[12:47:39.882046] Averaged stats: lr: 0.007992  loss: 3.6107 (3.7088)
[12:47:41.761762] Test:  [ 0/40]  eta: 0:01:14  loss: 5.1039 (5.1039)  acc1: 7.8125 (7.8125)  acc5: 17.1875 (17.1875)  time: 1.8744  data: 1.6897  max mem: 10046
[12:47:47.717124] Test:  [39/40]  eta: 0:00:00  loss: 4.7278 (4.8272)  acc1: 7.8125 (6.8000)  acc5: 20.3125 (18.0400)  time: 0.1490  data: 0.0001  max mem: 10046
[12:47:47.866303] Test: Total time: 0:00:07 (0.1995 s / it)
[12:47:47.921455] * Acc@1 6.200 Acc@5 18.130 loss 4.825
[12:47:47.921644] Accuracy of the network on the 10000 test images: 6.2%
[12:47:47.921853] [12:47:51.464018] Max accuracy: 6.20%
[12:47:51.464310] [12:47:51.465163] {"train_lr": 0.007995481023328083, "train_loss": 3.7088248162042525, "test_loss": 4.825264358520508, "test_acc1": 6.2, "test_acc5": 18.13, "epoch": 6, "n_parameters": 85958500}
[12:47:51.465233] [12:47:51.465303] Training epoch 6 for 0:00:34
[12:47:51.465356] [12:47:51.468166] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:47:53.488356] Epoch: [7]  [ 0/42]  eta: 0:01:24  lr: 0.007991  loss: 3.7138 (3.7138)  time: 2.0193  data: 1.5373  max mem: 10046
[12:48:13.590965] Epoch: [7]  [41/42]  eta: 0:00:00  lr: 0.007981  loss: 3.6532 (3.6598)  time: 0.4915  data: 0.0001  max mem: 10046
[12:48:13.817005] Epoch: [7] Total time: 0:00:22 (0.5321 s / it)
[12:48:13.818604] Averaged stats: lr: 0.007981  loss: 3.6532 (3.6612)
[12:48:15.708950] Test:  [ 0/40]  eta: 0:01:15  loss: 5.0813 (5.0813)  acc1: 9.3750 (9.3750)  acc5: 10.9375 (10.9375)  time: 1.8851  data: 1.7075  max mem: 10046
[12:48:21.668210] Test:  [39/40]  eta: 0:00:00  loss: 4.7513 (4.8433)  acc1: 6.2500 (7.0000)  acc5: 18.7500 (17.8400)  time: 0.1497  data: 0.0001  max mem: 10046
[12:48:21.810129] Test: Total time: 0:00:07 (0.1997 s / it)
[12:48:21.905588] * Acc@1 6.660 Acc@5 18.000 loss 4.846
[12:48:21.905810] Accuracy of the network on the 10000 test images: 6.7%
[12:48:21.906045] [12:48:25.535278] Max accuracy: 6.66%
[12:48:25.535648] [12:48:25.536925] {"train_lr": 0.007987141413870226, "train_loss": 3.661219108672369, "test_loss": 4.8461819887161255, "test_acc1": 6.66, "test_acc5": 18.0, "epoch": 7, "n_parameters": 85958500}
[12:48:25.537012] [12:48:25.537083] Training epoch 7 for 0:00:34
[12:48:25.537138] [12:48:25.540398] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:48:27.706102] Epoch: [8]  [ 0/42]  eta: 0:01:30  lr: 0.007980  loss: 3.6351 (3.6351)  time: 2.1646  data: 1.6761  max mem: 10046
[12:48:47.815365] Epoch: [8]  [41/42]  eta: 0:00:00  lr: 0.007966  loss: 3.5509 (3.6073)  time: 0.4933  data: 0.0001  max mem: 10046
[12:48:48.046583] Epoch: [8] Total time: 0:00:22 (0.5359 s / it)
[12:48:48.047484] Averaged stats: lr: 0.007966  loss: 3.5509 (3.6250)
[12:48:50.082535] Test:  [ 0/40]  eta: 0:01:21  loss: 4.9548 (4.9548)  acc1: 9.3750 (9.3750)  acc5: 12.5000 (12.5000)  time: 2.0318  data: 1.8582  max mem: 10046
[12:48:56.013120] Test:  [39/40]  eta: 0:00:00  loss: 4.6092 (4.6996)  acc1: 7.8125 (7.5600)  acc5: 20.3125 (18.9600)  time: 0.1491  data: 0.0001  max mem: 10046
[12:48:56.127508] Test: Total time: 0:00:08 (0.2019 s / it)
[12:48:56.128724] * Acc@1 7.310 Acc@5 19.340 loss 4.710
[12:48:56.128853] Accuracy of the network on the 10000 test images: 7.3%
[12:48:56.129037] [12:48:59.667159] Max accuracy: 7.31%
[12:48:59.667431] [12:48:59.668252] {"train_lr": 0.007974442470557574, "train_loss": 3.6250383059183755, "test_loss": 4.710041785240174, "test_acc1": 7.31, "test_acc5": 19.34, "epoch": 8, "n_parameters": 85958500}
[12:48:59.668325] [12:48:59.668391] Training epoch 8 for 0:00:34
[12:48:59.668443] [12:48:59.671229] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:49:01.866736] Epoch: [9]  [ 0/42]  eta: 0:01:32  lr: 0.007965  loss: 3.4599 (3.4599)  time: 2.1946  data: 1.7259  max mem: 10046
[12:49:21.949582] Epoch: [9]  [41/42]  eta: 0:00:00  lr: 0.007946  loss: 3.5386 (3.5971)  time: 0.4910  data: 0.0001  max mem: 10046
[12:49:22.191136] Epoch: [9] Total time: 0:00:22 (0.5362 s / it)
[12:49:22.191988] Averaged stats: lr: 0.007946  loss: 3.5386 (3.5623)
[12:49:24.259015] Test:  [ 0/40]  eta: 0:01:22  loss: 4.8397 (4.8397)  acc1: 9.3750 (9.3750)  acc5: 17.1875 (17.1875)  time: 2.0637  data: 1.8864  max mem: 10046
[12:49:30.199306] Test:  [39/40]  eta: 0:00:00  loss: 4.5160 (4.6343)  acc1: 9.3750 (8.5200)  acc5: 21.8750 (20.9600)  time: 0.1492  data: 0.0001  max mem: 10046
[12:49:30.340797] Test: Total time: 0:00:08 (0.2037 s / it)
[12:49:30.342099] * Acc@1 8.310 Acc@5 20.980 loss 4.644
[12:49:30.342241] Accuracy of the network on the 10000 test images: 8.3%
[12:49:30.342432] [12:49:32.596270] Max accuracy: 8.31%
[12:49:32.596507] [12:49:32.597302] {"train_lr": 0.007957398079498206, "train_loss": 3.5622780947458175, "test_loss": 4.644123089313507, "test_acc1": 8.31, "test_acc5": 20.98, "epoch": 9, "n_parameters": 85958500}
[12:49:32.597371] [12:49:32.597430] Training epoch 9 for 0:00:32
[12:49:32.597480] [12:49:32.600311] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:49:34.709034] Epoch: [10]  [ 0/42]  eta: 0:01:28  lr: 0.007945  loss: 3.2952 (3.2952)  time: 2.1076  data: 1.6193  max mem: 10046
[12:49:54.884803] Epoch: [10]  [41/42]  eta: 0:00:00  lr: 0.007923  loss: 3.5266 (3.5159)  time: 0.4936  data: 0.0001  max mem: 10046
[12:49:55.097219] Epoch: [10] Total time: 0:00:22 (0.5356 s / it)
[12:49:55.098019] Averaged stats: lr: 0.007923  loss: 3.5266 (3.5384)
[12:49:57.031695] Test:  [ 0/40]  eta: 0:01:17  loss: 4.8973 (4.8973)  acc1: 9.3750 (9.3750)  acc5: 17.1875 (17.1875)  time: 1.9302  data: 1.7518  max mem: 10046
[12:50:02.989507] Test:  [39/40]  eta: 0:00:00  loss: 4.5696 (4.6781)  acc1: 9.3750 (8.5600)  acc5: 25.0000 (23.3600)  time: 0.1491  data: 0.0001  max mem: 10046
[12:50:03.138143] Test: Total time: 0:00:08 (0.2009 s / it)
[12:50:03.192482] * Acc@1 7.950 Acc@5 22.540 loss 4.678
[12:50:03.192679] Accuracy of the network on the 10000 test images: 8.0%
[12:50:03.192890] [12:50:03.192963] Max accuracy: 8.31%
[12:50:03.193020] [12:50:03.193824] {"train_lr": 0.007936026878483507, "train_loss": 3.53838803086962, "test_loss": 4.678390252590179, "test_acc1": 7.95, "test_acc5": 22.54, "epoch": 10, "n_parameters": 85958500}
[12:50:03.193893] [12:50:03.193946] Training epoch 10 for 0:00:30
[12:50:03.193997] [12:50:03.196833] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:50:05.708126] Epoch: [11]  [ 0/42]  eta: 0:01:45  lr: 0.007922  loss: 3.5316 (3.5316)  time: 2.5101  data: 1.8544  max mem: 10046
[12:50:25.837510] Epoch: [11]  [41/42]  eta: 0:00:00  lr: 0.007895  loss: 3.6149 (3.4941)  time: 0.4920  data: 0.0001  max mem: 10046
[12:50:26.049159] Epoch: [11] Total time: 0:00:22 (0.5441 s / it)
[12:50:26.057729] Averaged stats: lr: 0.007895  loss: 3.6149 (3.5103)
[12:50:28.088197] Test:  [ 0/40]  eta: 0:01:21  loss: 4.7509 (4.7509)  acc1: 9.3750 (9.3750)  acc5: 18.7500 (18.7500)  time: 2.0272  data: 1.8348  max mem: 10046
[12:50:34.052978] Test:  [39/40]  eta: 0:00:00  loss: 4.4097 (4.5342)  acc1: 9.3750 (8.8000)  acc5: 25.0000 (24.6800)  time: 0.1493  data: 0.0001  max mem: 10046
[12:50:34.199317] Test: Total time: 0:00:08 (0.2035 s / it)
[12:50:34.200619] * Acc@1 8.700 Acc@5 23.920 loss 4.542
[12:50:34.200762] Accuracy of the network on the 10000 test images: 8.7%
[12:50:34.200974] [12:50:37.647082] Max accuracy: 8.70%
[12:50:37.647372] [12:50:37.648388] {"train_lr": 0.007910352236608017, "train_loss": 3.5102875516528176, "test_loss": 4.542023229598999, "test_acc1": 8.7, "test_acc5": 23.92, "epoch": 11, "n_parameters": 85958500}
[12:50:37.648467] [12:50:37.648531] Training epoch 11 for 0:00:34
[12:50:37.648585] [12:50:37.651515] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:50:39.559149] Epoch: [12]  [ 0/42]  eta: 0:01:20  lr: 0.007893  loss: 3.5143 (3.5143)  time: 1.9066  data: 1.4333  max mem: 10046
[12:50:59.658418] Epoch: [12]  [41/42]  eta: 0:00:00  lr: 0.007862  loss: 3.4804 (3.5093)  time: 0.4912  data: 0.0001  max mem: 10046
[12:50:59.918621] Epoch: [12] Total time: 0:00:22 (0.5302 s / it)
[12:50:59.922702] Averaged stats: lr: 0.007862  loss: 3.4804 (3.4894)
[12:51:01.829976] Test:  [ 0/40]  eta: 0:01:16  loss: 4.6162 (4.6162)  acc1: 14.0625 (14.0625)  acc5: 18.7500 (18.7500)  time: 1.9039  data: 1.7298  max mem: 10046
[12:51:07.789283] Test:  [39/40]  eta: 0:00:00  loss: 4.3987 (4.4900)  acc1: 9.3750 (8.8000)  acc5: 26.5625 (24.8400)  time: 0.1499  data: 0.0001  max mem: 10046
[12:51:07.963752] Test: Total time: 0:00:08 (0.2010 s / it)
[12:51:07.964946] * Acc@1 9.120 Acc@5 24.610 loss 4.496
[12:51:07.965097] Accuracy of the network on the 10000 test images: 9.1%
[12:51:07.965291] [12:51:11.450625] Max accuracy: 9.12%
[12:51:11.451056] [12:51:11.452478] {"train_lr": 0.007880402228715652, "train_loss": 3.489356307756333, "test_loss": 4.496245169639588, "test_acc1": 9.12, "test_acc5": 24.61, "epoch": 12, "n_parameters": 85958500}
[12:51:11.452631] [12:51:11.452741] Training epoch 12 for 0:00:33
[12:51:11.452819] [12:51:11.457824] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:51:13.530636] Epoch: [13]  [ 0/42]  eta: 0:01:27  lr: 0.007861  loss: 3.1220 (3.1220)  time: 2.0719  data: 1.5912  max mem: 10046
[12:51:33.646796] Epoch: [13]  [41/42]  eta: 0:00:00  lr: 0.007826  loss: 3.4296 (3.4424)  time: 0.4914  data: 0.0001  max mem: 10046
[12:51:33.884632] Epoch: [13] Total time: 0:00:22 (0.5340 s / it)
[12:51:33.885527] Averaged stats: lr: 0.007826  loss: 3.4296 (3.4622)
[12:51:35.752671] Test:  [ 0/40]  eta: 0:01:14  loss: 4.6998 (4.6998)  acc1: 10.9375 (10.9375)  acc5: 23.4375 (23.4375)  time: 1.8633  data: 1.6854  max mem: 10046
[12:51:41.701511] Test:  [39/40]  eta: 0:00:00  loss: 4.4486 (4.5461)  acc1: 10.9375 (10.4000)  acc5: 26.5625 (25.5200)  time: 0.1490  data: 0.0001  max mem: 10046
[12:51:41.818728] Test: Total time: 0:00:07 (0.1983 s / it)
[12:51:42.041804] * Acc@1 9.890 Acc@5 25.180 loss 4.552
[12:51:42.042034] Accuracy of the network on the 10000 test images: 9.9%
[12:51:42.042233] [12:51:45.600627] Max accuracy: 9.89%
[12:51:45.600875] [12:51:45.601755] {"train_lr": 0.00784620960470034, "train_loss": 3.4622289737065635, "test_loss": 4.55244619846344, "test_acc1": 9.89, "test_acc5": 25.18, "epoch": 13, "n_parameters": 85958500}
[12:51:45.601878] [12:51:45.601944] Training epoch 13 for 0:00:34
[12:51:45.601998] [12:51:45.604907] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:51:47.552472] Epoch: [14]  [ 0/42]  eta: 0:01:21  lr: 0.007824  loss: 3.0675 (3.0675)  time: 1.9459  data: 1.4619  max mem: 10046
[12:52:07.655736] Epoch: [14]  [41/42]  eta: 0:00:00  lr: 0.007785  loss: 3.3805 (3.3815)  time: 0.4916  data: 0.0001  max mem: 10046
[12:52:07.883435] Epoch: [14] Total time: 0:00:22 (0.5304 s / it)
[12:52:07.884131] Averaged stats: lr: 0.007785  loss: 3.3805 (3.4144)
[12:52:09.631008] Test:  [ 0/40]  eta: 0:01:09  loss: 4.5139 (4.5139)  acc1: 14.0625 (14.0625)  acc5: 28.1250 (28.1250)  time: 1.7426  data: 1.5648  max mem: 10046
[12:52:15.579922] Test:  [39/40]  eta: 0:00:00  loss: 4.2487 (4.3995)  acc1: 9.3750 (10.3200)  acc5: 28.1250 (27.2800)  time: 0.1490  data: 0.0001  max mem: 10046
[12:52:15.725517] Test: Total time: 0:00:07 (0.1960 s / it)
[12:52:15.950847] * Acc@1 10.550 Acc@5 27.130 loss 4.395
[12:52:15.951052] Accuracy of the network on the 10000 test images: 10.6%
[12:52:15.951259] [12:52:18.479296] Max accuracy: 10.55%
[12:52:18.479577] [12:52:18.480408] {"train_lr": 0.007807811753694423, "train_loss": 3.4143955352760496, "test_loss": 4.394903752207756, "test_acc1": 10.55, "test_acc5": 27.13, "epoch": 14, "n_parameters": 85958500}
[12:52:18.480485] [12:52:18.480547] Training epoch 14 for 0:00:32
[12:52:18.480600] [12:52:18.483525] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:52:20.645712] Epoch: [15]  [ 0/42]  eta: 0:01:30  lr: 0.007783  loss: 3.4745 (3.4745)  time: 2.1609  data: 1.6862  max mem: 10046
[12:52:40.763053] Epoch: [15]  [41/42]  eta: 0:00:00  lr: 0.007741  loss: 3.3728 (3.3737)  time: 0.4916  data: 0.0001  max mem: 10046
[12:52:41.006909] Epoch: [15] Total time: 0:00:22 (0.5363 s / it)
[12:52:41.007643] Averaged stats: lr: 0.007741  loss: 3.3728 (3.3867)
[12:52:43.026662] Test:  [ 0/40]  eta: 0:01:20  loss: 4.6567 (4.6567)  acc1: 10.9375 (10.9375)  acc5: 26.5625 (26.5625)  time: 2.0152  data: 1.8378  max mem: 10046
[12:52:48.994552] Test:  [39/40]  eta: 0:00:00  loss: 4.3144 (4.4735)  acc1: 12.5000 (10.7200)  acc5: 28.1250 (27.0400)  time: 0.1497  data: 0.0001  max mem: 10046
[12:52:49.131385] Test: Total time: 0:00:08 (0.2030 s / it)
[12:52:49.132894] * Acc@1 10.750 Acc@5 26.610 loss 4.471
[12:52:49.133060] Accuracy of the network on the 10000 test images: 10.8%
[12:52:49.133272] [12:52:52.873652] Max accuracy: 10.75%
[12:52:52.873914] [12:52:52.874795] {"train_lr": 0.007765250663184244, "train_loss": 3.3867243017469133, "test_loss": 4.471339103579521, "test_acc1": 10.75, "test_acc5": 26.61, "epoch": 15, "n_parameters": 85958500}
[12:52:52.874914] [12:52:52.874978] Training epoch 15 for 0:00:34
[12:52:52.875031] [12:52:52.879862] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:52:55.028724] Epoch: [16]  [ 0/42]  eta: 0:01:30  lr: 0.007738  loss: 3.4423 (3.4423)  time: 2.1481  data: 1.6649  max mem: 10046
[12:53:15.117996] Epoch: [16]  [41/42]  eta: 0:00:00  lr: 0.007692  loss: 3.2932 (3.3754)  time: 0.4912  data: 0.0001  max mem: 10046
[12:53:15.344631] Epoch: [16] Total time: 0:00:22 (0.5349 s / it)
[12:53:15.345356] Averaged stats: lr: 0.007692  loss: 3.2932 (3.3789)
[12:53:17.138101] Test:  [ 0/40]  eta: 0:01:11  loss: 4.5487 (4.5487)  acc1: 12.5000 (12.5000)  acc5: 29.6875 (29.6875)  time: 1.7892  data: 1.6107  max mem: 10046
[12:53:23.147887] Test:  [39/40]  eta: 0:00:00  loss: 4.2226 (4.3691)  acc1: 10.9375 (11.6400)  acc5: 29.6875 (28.6000)  time: 0.1490  data: 0.0001  max mem: 10046
[12:53:23.304233] Test: Total time: 0:00:07 (0.1989 s / it)
[12:53:23.584385] * Acc@1 11.480 Acc@5 28.380 loss 4.375
[12:53:23.584594] Accuracy of the network on the 10000 test images: 11.5%
[12:53:23.584824] [12:53:27.155791] Max accuracy: 11.48%
[12:53:27.156152] [12:53:27.157371] {"train_lr": 0.0077185728730973764, "train_loss": 3.378899171238854, "test_loss": 4.374619251489639, "test_acc1": 11.48, "test_acc5": 28.38, "epoch": 16, "n_parameters": 85958500}
[12:53:27.157489] [12:53:27.157594] Training epoch 16 for 0:00:34
[12:53:27.157694] [12:53:27.163141] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:53:29.161162] Epoch: [17]  [ 0/42]  eta: 0:01:23  lr: 0.007689  loss: 3.4363 (3.4363)  time: 1.9969  data: 1.5272  max mem: 10046
[12:53:49.250603] Epoch: [17]  [41/42]  eta: 0:00:00  lr: 0.007639  loss: 3.2878 (3.3420)  time: 0.4916  data: 0.0001  max mem: 10046
[12:53:49.468687] Epoch: [17] Total time: 0:00:22 (0.5311 s / it)
[12:53:49.469410] Averaged stats: lr: 0.007639  loss: 3.2878 (3.3222)
[12:53:51.352233] Test:  [ 0/40]  eta: 0:01:15  loss: 4.7369 (4.7369)  acc1: 10.9375 (10.9375)  acc5: 21.8750 (21.8750)  time: 1.8786  data: 1.7004  max mem: 10046
[12:53:57.303198] Test:  [39/40]  eta: 0:00:00  loss: 4.3168 (4.4332)  acc1: 12.5000 (12.6000)  acc5: 31.2500 (28.4800)  time: 0.1494  data: 0.0001  max mem: 10046
[12:53:57.443220] Test: Total time: 0:00:07 (0.1993 s / it)
[12:53:57.444541] * Acc@1 11.840 Acc@5 28.370 loss 4.433
[12:53:57.444719] Accuracy of the network on the 10000 test images: 11.8%
[12:53:57.444946] [12:54:01.077351] Max accuracy: 11.84%
[12:54:01.077623] [12:54:01.078496] {"train_lr": 0.007667829424911969, "train_loss": 3.3222336726529256, "test_loss": 4.433404856920243, "test_acc1": 11.84, "test_acc5": 28.37, "epoch": 17, "n_parameters": 85958500}
[12:54:01.078569] [12:54:01.078630] Training epoch 17 for 0:00:33
[12:54:01.078682] [12:54:01.081595] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:54:03.178982] Epoch: [18]  [ 0/42]  eta: 0:01:28  lr: 0.007636  loss: 3.4721 (3.4721)  time: 2.0964  data: 1.6237  max mem: 10046
[12:54:23.260779] Epoch: [18]  [41/42]  eta: 0:00:00  lr: 0.007582  loss: 3.2973 (3.3226)  time: 0.4903  data: 0.0001  max mem: 10046
[12:54:23.478753] Epoch: [18] Total time: 0:00:22 (0.5333 s / it)
[12:54:23.483952] Averaged stats: lr: 0.007582  loss: 3.2973 (3.3244)
[12:54:25.237409] Test:  [ 0/40]  eta: 0:01:09  loss: 4.5086 (4.5086)  acc1: 14.0625 (14.0625)  acc5: 20.3125 (20.3125)  time: 1.7496  data: 1.5722  max mem: 10046
[12:54:31.368169] Test:  [39/40]  eta: 0:00:00  loss: 4.1858 (4.2907)  acc1: 10.9375 (11.5200)  acc5: 32.8125 (29.8400)  time: 0.1488  data: 0.0001  max mem: 10046
[12:54:31.501181] Test: Total time: 0:00:08 (0.2004 s / it)
[12:54:31.646535] * Acc@1 11.940 Acc@5 30.000 loss 4.287
[12:54:31.646705] Accuracy of the network on the 10000 test images: 11.9%
[12:54:31.646908] [12:54:35.053846] Max accuracy: 11.94%
[12:54:35.054142] [12:54:35.054999] {"train_lr": 0.007613075805843609, "train_loss": 3.3244169382821944, "test_loss": 4.2866378873586655, "test_acc1": 11.94, "test_acc5": 30.0, "epoch": 18, "n_parameters": 85958500}
[12:54:35.055074] [12:54:35.055141] Training epoch 18 for 0:00:33
[12:54:35.055198] [12:54:35.059092] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:54:37.278054] Epoch: [19]  [ 0/42]  eta: 0:01:33  lr: 0.007579  loss: 3.2923 (3.2923)  time: 2.2181  data: 1.7414  max mem: 10046
[12:54:57.356940] Epoch: [19]  [41/42]  eta: 0:00:00  lr: 0.007521  loss: 3.4066 (3.3231)  time: 0.4916  data: 0.0001  max mem: 10046
[12:54:57.581763] Epoch: [19] Total time: 0:00:22 (0.5363 s / it)
[12:54:57.590387] Averaged stats: lr: 0.007521  loss: 3.4066 (3.2987)
[12:54:59.310706] Test:  [ 0/40]  eta: 0:01:08  loss: 4.5036 (4.5036)  acc1: 12.5000 (12.5000)  acc5: 25.0000 (25.0000)  time: 1.7158  data: 1.5545  max mem: 10046
[12:55:05.247870] Test:  [39/40]  eta: 0:00:00  loss: 4.1998 (4.2899)  acc1: 12.5000 (12.1600)  acc5: 31.2500 (30.4000)  time: 0.1488  data: 0.0001  max mem: 10046
[12:55:05.387069] Test: Total time: 0:00:07 (0.1949 s / it)
[12:55:05.678651] * Acc@1 12.490 Acc@5 30.490 loss 4.306
[12:55:05.678843] Accuracy of the network on the 10000 test images: 12.5%
[12:55:05.679042] [12:55:09.206083] Max accuracy: 12.49%
[12:55:09.206359] [12:55:09.207293] {"train_lr": 0.007554371888170873, "train_loss": 3.2986757045700434, "test_loss": 4.30612365603447, "test_acc1": 12.49, "test_acc5": 30.49, "epoch": 19, "n_parameters": 85958500}
[12:55:09.207373] [12:55:09.207438] Training epoch 19 for 0:00:34
[12:55:09.207507] [12:55:09.210504] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:55:11.249466] Epoch: [20]  [ 0/42]  eta: 0:01:25  lr: 0.007518  loss: 3.1095 (3.1095)  time: 2.0381  data: 1.5670  max mem: 10046
[12:55:31.391473] Epoch: [20]  [41/42]  eta: 0:00:00  lr: 0.007456  loss: 3.3172 (3.3300)  time: 0.4907  data: 0.0001  max mem: 10046
[12:55:31.618572] Epoch: [20] Total time: 0:00:22 (0.5335 s / it)
[12:55:31.619979] Averaged stats: lr: 0.007456  loss: 3.3172 (3.2951)
[12:55:33.466823] Test:  [ 0/40]  eta: 0:01:13  loss: 4.5332 (4.5332)  acc1: 14.0625 (14.0625)  acc5: 29.6875 (29.6875)  time: 1.8429  data: 1.6643  max mem: 10046
[12:55:39.410673] Test:  [39/40]  eta: 0:00:00  loss: 4.1749 (4.2969)  acc1: 12.5000 (13.3200)  acc5: 32.8125 (30.9200)  time: 0.1491  data: 0.0001  max mem: 10046
[12:55:39.593509] Test: Total time: 0:00:07 (0.1993 s / it)
[12:55:39.601112] * Acc@1 12.850 Acc@5 30.930 loss 4.311
[12:55:39.601265] Accuracy of the network on the 10000 test images: 12.8%
[12:55:39.601500] [12:55:43.164188] Max accuracy: 12.85%
[12:55:43.164552] [12:55:43.165689] {"train_lr": 0.007491781863765832, "train_loss": 3.29514256971223, "test_loss": 4.310691863298416, "test_acc1": 12.85, "test_acc5": 30.93, "epoch": 20, "n_parameters": 85958500}
[12:55:43.165769] [12:55:43.165842] Training epoch 20 for 0:00:33
[12:55:43.165898] [12:55:43.168964] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:55:45.280284] Epoch: [21]  [ 0/42]  eta: 0:01:28  lr: 0.007453  loss: 3.1652 (3.1652)  time: 2.1102  data: 1.6368  max mem: 10046
[12:56:05.390719] Epoch: [21]  [41/42]  eta: 0:00:00  lr: 0.007388  loss: 3.3089 (3.2917)  time: 0.4921  data: 0.0001  max mem: 10046
[12:56:05.619358] Epoch: [21] Total time: 0:00:22 (0.5345 s / it)
[12:56:05.620068] Averaged stats: lr: 0.007388  loss: 3.3089 (3.2790)
[12:56:07.468272] Test:  [ 0/40]  eta: 0:01:13  loss: 4.4500 (4.4500)  acc1: 10.9375 (10.9375)  acc5: 28.1250 (28.1250)  time: 1.8446  data: 1.6669  max mem: 10046
[12:56:13.406592] Test:  [39/40]  eta: 0:00:00  loss: 4.2046 (4.2652)  acc1: 14.0625 (13.7200)  acc5: 29.6875 (30.6400)  time: 0.1489  data: 0.0001  max mem: 10046
[12:56:13.550429] Test: Total time: 0:00:07 (0.1982 s / it)
[12:56:13.617975] * Acc@1 13.000 Acc@5 30.890 loss 4.274
[12:56:13.618171] Accuracy of the network on the 10000 test images: 13.0%
[12:56:13.618377] [12:56:16.033948] Max accuracy: 13.00%
[12:56:16.034247] [12:56:16.035337] {"train_lr": 0.007425374173901249, "train_loss": 3.2790348657539914, "test_loss": 4.274169251322746, "test_acc1": 13.0, "test_acc5": 30.89, "epoch": 21, "n_parameters": 85958500}
[12:56:16.035411] [12:56:16.035488] Training epoch 21 for 0:00:32
[12:56:16.035557] [12:56:16.038432] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:56:17.960529] Epoch: [22]  [ 0/42]  eta: 0:01:20  lr: 0.007384  loss: 3.3439 (3.3439)  time: 1.9213  data: 1.4525  max mem: 10046
[12:56:38.117329] Epoch: [22]  [41/42]  eta: 0:00:00  lr: 0.007316  loss: 3.2428 (3.2675)  time: 0.4932  data: 0.0001  max mem: 10046
[12:56:38.336646] Epoch: [22] Total time: 0:00:22 (0.5309 s / it)
[12:56:38.337447] Averaged stats: lr: 0.007316  loss: 3.2428 (3.2666)
[12:56:40.098256] Test:  [ 0/40]  eta: 0:01:10  loss: 4.4384 (4.4384)  acc1: 9.3750 (9.3750)  acc5: 31.2500 (31.2500)  time: 1.7566  data: 1.5837  max mem: 10046
[12:56:46.074359] Test:  [39/40]  eta: 0:00:00  loss: 4.1746 (4.2862)  acc1: 12.5000 (12.8000)  acc5: 34.3750 (30.9600)  time: 0.1493  data: 0.0001  max mem: 10046
[12:56:46.275492] Test: Total time: 0:00:07 (0.1984 s / it)
[12:56:46.360297] * Acc@1 12.850 Acc@5 31.650 loss 4.293
[12:56:46.360431] Accuracy of the network on the 10000 test images: 12.8%
[12:56:46.360609] [12:56:46.360697] Max accuracy: 13.00%
[12:56:46.360753] [12:56:46.361564] {"train_lr": 0.007355221434411005, "train_loss": 3.2665698528289795, "test_loss": 4.292781183123589, "test_acc1": 12.85, "test_acc5": 31.65, "epoch": 22, "n_parameters": 85958500}
[12:56:46.361637] [12:56:46.361696] Training epoch 22 for 0:00:30
[12:56:46.361749] [12:56:46.364470] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:56:48.831300] Epoch: [23]  [ 0/42]  eta: 0:01:43  lr: 0.007312  loss: 3.4378 (3.4378)  time: 2.4657  data: 1.9381  max mem: 10046
[12:57:09.084584] Epoch: [23]  [41/42]  eta: 0:00:00  lr: 0.007240  loss: 3.2554 (3.2632)  time: 0.4937  data: 0.0001  max mem: 10046
[12:57:09.323722] Epoch: [23] Total time: 0:00:22 (0.5466 s / it)
[12:57:09.324591] Averaged stats: lr: 0.007240  loss: 3.2554 (3.2523)
[12:57:10.981015] Test:  [ 0/40]  eta: 0:01:06  loss: 4.3855 (4.3855)  acc1: 12.5000 (12.5000)  acc5: 29.6875 (29.6875)  time: 1.6508  data: 1.4944  max mem: 10046
[12:57:17.024372] Test:  [39/40]  eta: 0:00:00  loss: 4.1390 (4.2373)  acc1: 14.0625 (13.1600)  acc5: 32.8125 (30.5600)  time: 0.1492  data: 0.0001  max mem: 10046
[12:57:17.168346] Test: Total time: 0:00:07 (0.1960 s / it)
[12:57:17.305469] * Acc@1 12.960 Acc@5 31.500 loss 4.238
[12:57:17.305668] Accuracy of the network on the 10000 test images: 13.0%
[12:57:17.305883] [12:57:17.305963] Max accuracy: 13.00%
[12:57:17.306021] [12:57:17.306808] {"train_lr": 0.007281400356285755, "train_loss": 3.252342928023565, "test_loss": 4.2379644334316255, "test_acc1": 12.96, "test_acc5": 31.5, "epoch": 23, "n_parameters": 85958500}
[12:57:17.306877] [12:57:17.306934] Training epoch 23 for 0:00:30
[12:57:17.306986] [12:57:17.309759] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:57:19.723096] Epoch: [24]  [ 0/42]  eta: 0:01:41  lr: 0.007236  loss: 3.1575 (3.1575)  time: 2.4123  data: 1.9299  max mem: 10046
[12:57:39.888850] Epoch: [24]  [41/42]  eta: 0:00:00  lr: 0.007161  loss: 3.2687 (3.2238)  time: 0.4930  data: 0.0001  max mem: 10046
[12:57:40.111464] Epoch: [24] Total time: 0:00:22 (0.5429 s / it)
[12:57:40.112475] Averaged stats: lr: 0.007161  loss: 3.2687 (3.2329)
[12:57:42.055846] Test:  [ 0/40]  eta: 0:01:17  loss: 4.6091 (4.6091)  acc1: 14.0625 (14.0625)  acc5: 26.5625 (26.5625)  time: 1.9382  data: 1.7647  max mem: 10046
[12:57:48.022033] Test:  [39/40]  eta: 0:00:00  loss: 4.2348 (4.3326)  acc1: 12.5000 (12.6000)  acc5: 34.3750 (32.6000)  time: 0.1499  data: 0.0001  max mem: 10046
[12:57:48.160551] Test: Total time: 0:00:08 (0.2011 s / it)
[12:57:48.224215] * Acc@1 12.360 Acc@5 31.840 loss 4.362
[12:57:48.224392] Accuracy of the network on the 10000 test images: 12.4%
[12:57:48.224617] [12:57:48.224700] Max accuracy: 13.00%
[12:57:48.224757] [12:57:48.225571] {"train_lr": 0.0072039916617906, "train_loss": 3.2328659920465377, "test_loss": 4.361896610260009, "test_acc1": 12.36, "test_acc5": 31.84, "epoch": 24, "n_parameters": 85958500}
[12:57:48.225641] [12:57:48.225699] Training epoch 24 for 0:00:30
[12:57:48.225752] [12:57:48.228565] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:57:50.480794] Epoch: [25]  [ 0/42]  eta: 0:01:34  lr: 0.007157  loss: 3.2312 (3.2312)  time: 2.2514  data: 1.7743  max mem: 10046
[12:58:10.655758] Epoch: [25]  [41/42]  eta: 0:00:00  lr: 0.007078  loss: 3.1987 (3.1971)  time: 0.4934  data: 0.0001  max mem: 10046
[12:58:10.887577] Epoch: [25] Total time: 0:00:22 (0.5395 s / it)
[12:58:10.888476] Averaged stats: lr: 0.007078  loss: 3.1987 (3.1904)
[12:58:12.776750] Test:  [ 0/40]  eta: 0:01:15  loss: 4.4437 (4.4437)  acc1: 12.5000 (12.5000)  acc5: 29.6875 (29.6875)  time: 1.8849  data: 1.7072  max mem: 10046
[12:58:18.748954] Test:  [39/40]  eta: 0:00:00  loss: 3.9963 (4.1516)  acc1: 15.6250 (14.7600)  acc5: 34.3750 (33.7200)  time: 0.1495  data: 0.0001  max mem: 10046
[12:58:18.892456] Test: Total time: 0:00:08 (0.2000 s / it)
[12:58:18.990145] * Acc@1 14.350 Acc@5 33.710 loss 4.167
[12:58:18.990354] Accuracy of the network on the 10000 test images: 14.3%
[12:58:18.990587] [12:58:22.379066] Max accuracy: 14.35%
[12:58:22.379324] [12:58:22.380366] {"train_lr": 0.007123079996196465, "train_loss": 3.190433566059385, "test_loss": 4.166859711706638, "test_acc1": 14.35, "test_acc5": 33.71, "epoch": 25, "n_parameters": 85958500}
[12:58:22.380440] [12:58:22.380504] Training epoch 25 for 0:00:34
[12:58:22.380557] [12:58:22.383463] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:58:24.429398] Epoch: [26]  [ 0/42]  eta: 0:01:25  lr: 0.007074  loss: 3.0843 (3.0843)  time: 2.0450  data: 1.5702  max mem: 10046
[12:58:44.547984] Epoch: [26]  [41/42]  eta: 0:00:00  lr: 0.006992  loss: 3.1549 (3.1927)  time: 0.4914  data: 0.0001  max mem: 10046
[12:58:44.784488] Epoch: [26] Total time: 0:00:22 (0.5334 s / it)
[12:58:44.785291] Averaged stats: lr: 0.006992  loss: 3.1549 (3.1747)
[12:58:46.929719] Test:  [ 0/40]  eta: 0:01:25  loss: 4.4515 (4.4515)  acc1: 10.9375 (10.9375)  acc5: 31.2500 (31.2500)  time: 2.1407  data: 1.9666  max mem: 10046
[12:58:52.880268] Test:  [39/40]  eta: 0:00:00  loss: 4.1257 (4.2071)  acc1: 15.6250 (14.3600)  acc5: 34.3750 (33.6800)  time: 0.1497  data: 0.0001  max mem: 10046
[12:58:53.027423] Test: Total time: 0:00:08 (0.2060 s / it)
[12:58:53.028758] * Acc@1 13.900 Acc@5 33.720 loss 4.220
[12:58:53.028932] Accuracy of the network on the 10000 test images: 13.9%
[12:58:53.029124] [12:58:53.029195] Max accuracy: 14.35%
[12:58:53.029250] [12:58:53.030022] {"train_lr": 0.0070387538352217486, "train_loss": 3.174693960519064, "test_loss": 4.220108649134636, "test_acc1": 13.9, "test_acc5": 33.72, "epoch": 26, "n_parameters": 85958500}
[12:58:53.030091] [12:58:53.030149] Training epoch 26 for 0:00:30
[12:58:53.030200] [12:58:53.033007] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:58:55.474540] Epoch: [27]  [ 0/42]  eta: 0:01:42  lr: 0.006987  loss: 3.3996 (3.3996)  time: 2.4405  data: 1.9621  max mem: 10046
[12:59:15.726227] Epoch: [27]  [41/42]  eta: 0:00:00  lr: 0.006902  loss: 3.1021 (3.1733)  time: 0.4931  data: 0.0001  max mem: 10046
[12:59:15.955008] Epoch: [27] Total time: 0:00:22 (0.5458 s / it)
[12:59:15.955806] Averaged stats: lr: 0.006902  loss: 3.1021 (3.1705)
[12:59:17.818605] Test:  [ 0/40]  eta: 0:01:14  loss: 4.5290 (4.5290)  acc1: 14.0625 (14.0625)  acc5: 29.6875 (29.6875)  time: 1.8582  data: 1.6821  max mem: 10046
[12:59:23.795335] Test:  [39/40]  eta: 0:00:00  loss: 4.0731 (4.1746)  acc1: 14.0625 (13.8800)  acc5: 35.9375 (34.0400)  time: 0.1497  data: 0.0001  max mem: 10046
[12:59:23.960846] Test: Total time: 0:00:08 (0.2001 s / it)
[12:59:23.987141] * Acc@1 14.240 Acc@5 33.620 loss 4.192
[12:59:23.987331] Accuracy of the network on the 10000 test images: 14.2%
[12:59:23.987565] [12:59:23.987644] Max accuracy: 14.35%
[12:59:23.987700] [12:59:23.988512] {"train_lr": 0.006951105388285409, "train_loss": 3.1705272041615986, "test_loss": 4.191914382576942, "test_acc1": 14.24, "test_acc5": 33.62, "epoch": 27, "n_parameters": 85958500}
[12:59:23.988585] [12:59:23.988642] Training epoch 27 for 0:00:30
[12:59:23.988693] [12:59:23.991520] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[12:59:26.523815] Epoch: [28]  [ 0/42]  eta: 0:01:46  lr: 0.006898  loss: 3.4799 (3.4799)  time: 2.5312  data: 1.9910  max mem: 10046
[12:59:46.802216] Epoch: [28]  [41/42]  eta: 0:00:00  lr: 0.006810  loss: 3.1707 (3.1856)  time: 0.4924  data: 0.0001  max mem: 10046
[12:59:47.040164] Epoch: [28] Total time: 0:00:23 (0.5488 s / it)
[12:59:47.040898] Averaged stats: lr: 0.006810  loss: 3.1707 (3.2013)
[12:59:48.774027] Test:  [ 0/40]  eta: 0:01:09  loss: 4.4735 (4.4735)  acc1: 14.0625 (14.0625)  acc5: 28.1250 (28.1250)  time: 1.7286  data: 1.5502  max mem: 10046
[12:59:54.755284] Test:  [39/40]  eta: 0:00:00  loss: 3.9602 (4.1375)  acc1: 15.6250 (15.4000)  acc5: 35.9375 (34.9600)  time: 0.1500  data: 0.0001  max mem: 10046
[12:59:54.869806] Test: Total time: 0:00:07 (0.1957 s / it)
[12:59:55.205277] * Acc@1 14.600 Acc@5 34.670 loss 4.150
[12:59:55.205491] Accuracy of the network on the 10000 test images: 14.6%
[12:59:55.205707] [12:59:58.839979] Max accuracy: 14.60%
[12:59:58.840245] [12:59:58.841141] {"train_lr": 0.006860230497677332, "train_loss": 3.2013239846343087, "test_loss": 4.150147987902164, "test_acc1": 14.6, "test_acc5": 34.67, "epoch": 28, "n_parameters": 85958500}
[12:59:58.841265] [12:59:58.841330] Training epoch 28 for 0:00:34
[12:59:58.841385] [12:59:58.844233] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:00:00.850295] Epoch: [29]  [ 0/42]  eta: 0:01:24  lr: 0.006805  loss: 3.2939 (3.2939)  time: 2.0048  data: 1.5236  max mem: 10046
[13:00:21.044102] Epoch: [29]  [41/42]  eta: 0:00:00  lr: 0.006714  loss: 3.2006 (3.2157)  time: 0.4938  data: 0.0001  max mem: 10046
[13:00:21.285074] Epoch: [29] Total time: 0:00:22 (0.5343 s / it)
[13:00:21.290383] Averaged stats: lr: 0.006714  loss: 3.2006 (3.1773)
[13:00:23.284236] Test:  [ 0/40]  eta: 0:01:19  loss: 4.3171 (4.3171)  acc1: 15.6250 (15.6250)  acc5: 29.6875 (29.6875)  time: 1.9901  data: 1.8192  max mem: 10046
[13:00:29.225090] Test:  [39/40]  eta: 0:00:00  loss: 4.0471 (4.1902)  acc1: 14.0625 (14.8000)  acc5: 35.9375 (34.7200)  time: 0.1491  data: 0.0001  max mem: 10046
[13:00:29.357001] Test: Total time: 0:00:08 (0.2016 s / it)
[13:00:29.358338] * Acc@1 14.610 Acc@5 34.340 loss 4.203
[13:00:29.358479] Accuracy of the network on the 10000 test images: 14.6%
[13:00:29.358665] [13:00:32.042516] Max accuracy: 14.61%
[13:00:32.042828] [13:00:32.043668] {"train_lr": 0.0067662285337561886, "train_loss": 3.17731987010865, "test_loss": 4.20313533693552, "test_acc1": 14.61, "test_acc5": 34.34, "epoch": 29, "n_parameters": 85958500}
[13:00:32.043766] [13:00:32.043853] Training epoch 29 for 0:00:33
[13:00:32.043931] [13:00:32.046736] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:00:33.981358] Epoch: [30]  [ 0/42]  eta: 0:01:21  lr: 0.006709  loss: 3.2393 (3.2393)  time: 1.9336  data: 1.4538  max mem: 10046
[13:00:54.082200] Epoch: [30]  [41/42]  eta: 0:00:00  lr: 0.006615  loss: 3.1127 (3.1203)  time: 0.4921  data: 0.0001  max mem: 10046
[13:00:54.297528] Epoch: [30] Total time: 0:00:22 (0.5298 s / it)
[13:00:54.300152] Averaged stats: lr: 0.006615  loss: 3.1127 (3.1462)
[13:00:56.064361] Test:  [ 0/40]  eta: 0:01:10  loss: 4.3073 (4.3073)  acc1: 14.0625 (14.0625)  acc5: 28.1250 (28.1250)  time: 1.7597  data: 1.5779  max mem: 10046
[13:01:02.029293] Test:  [39/40]  eta: 0:00:00  loss: 3.9515 (4.0968)  acc1: 17.1875 (16.1200)  acc5: 37.5000 (35.2800)  time: 0.1495  data: 0.0001  max mem: 10046
[13:01:02.212655] Test: Total time: 0:00:07 (0.1978 s / it)
[13:01:02.294265] * Acc@1 15.370 Acc@5 35.730 loss 4.094
[13:01:02.294426] Accuracy of the network on the 10000 test images: 15.4%
[13:01:02.294604] [13:01:05.373548] Max accuracy: 15.37%
[13:01:05.373816] [13:01:05.374667] {"train_lr": 0.006669202286289392, "train_loss": 3.1462341206414357, "test_loss": 4.09420462846756, "test_acc1": 15.37, "test_acc5": 35.73, "epoch": 30, "n_parameters": 85958500}
[13:01:05.374746] [13:01:05.374809] Training epoch 30 for 0:00:33
[13:01:05.374861] [13:01:05.377838] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:01:07.288177] Epoch: [31]  [ 0/42]  eta: 0:01:20  lr: 0.006611  loss: 3.0520 (3.0520)  time: 1.9095  data: 1.4394  max mem: 10046
[13:01:27.425525] Epoch: [31]  [41/42]  eta: 0:00:00  lr: 0.006514  loss: 3.1056 (3.1129)  time: 0.4927  data: 0.0001  max mem: 10046
[13:01:27.641479] Epoch: [31] Total time: 0:00:22 (0.5301 s / it)
[13:01:27.642201] Averaged stats: lr: 0.006514  loss: 3.1056 (3.0977)
[13:01:29.735507] Test:  [ 0/40]  eta: 0:01:23  loss: 4.2873 (4.2873)  acc1: 14.0625 (14.0625)  acc5: 32.8125 (32.8125)  time: 2.0898  data: 1.9196  max mem: 10046
[13:01:35.680029] Test:  [39/40]  eta: 0:00:00  loss: 3.9880 (4.1368)  acc1: 15.6250 (15.1200)  acc5: 39.0625 (36.9200)  time: 0.1495  data: 0.0001  max mem: 10046
[13:01:35.803149] Test: Total time: 0:00:08 (0.2040 s / it)
[13:01:35.804478] * Acc@1 14.990 Acc@5 36.150 loss 4.143
[13:01:35.804630] Accuracy of the network on the 10000 test images: 15.0%
[13:01:35.804809] [13:01:35.804882] Max accuracy: 15.37%
[13:01:35.804938] [13:01:35.805688] {"train_lr": 0.0065692578520540335, "train_loss": 3.0977075000603995, "test_loss": 4.14336941987276, "test_acc1": 14.99, "test_acc5": 36.15, "epoch": 31, "n_parameters": 85958500}
[13:01:35.805769] [13:01:35.805831] Training epoch 31 for 0:00:30
[13:01:35.805884] [13:01:35.808687] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:01:38.119468] Epoch: [32]  [ 0/42]  eta: 0:01:36  lr: 0.006509  loss: 3.1770 (3.1770)  time: 2.3092  data: 1.8401  max mem: 10046
[13:01:58.361908] Epoch: [32]  [41/42]  eta: 0:00:00  lr: 0.006410  loss: 3.0003 (3.1300)  time: 0.4939  data: 0.0001  max mem: 10046
[13:01:58.619689] Epoch: [32] Total time: 0:00:22 (0.5431 s / it)
[13:01:58.620885] Averaged stats: lr: 0.006410  loss: 3.0003 (3.0977)
[13:02:00.621007] Test:  [ 0/40]  eta: 0:01:19  loss: 4.3506 (4.3506)  acc1: 12.5000 (12.5000)  acc5: 31.2500 (31.2500)  time: 1.9966  data: 1.8189  max mem: 10046
[13:02:06.603384] Test:  [39/40]  eta: 0:00:00  loss: 3.9744 (4.1481)  acc1: 15.6250 (14.5600)  acc5: 35.9375 (35.1200)  time: 0.1505  data: 0.0001  max mem: 10046
[13:02:06.736850] Test: Total time: 0:00:08 (0.2028 s / it)
[13:02:06.738187] * Acc@1 14.850 Acc@5 34.970 loss 4.170
[13:02:06.738333] Accuracy of the network on the 10000 test images: 14.8%
[13:02:06.738583] [13:02:06.738662] Max accuracy: 15.37%
[13:02:06.738720] [13:02:06.739492] {"train_lr": 0.006466504518821576, "train_loss": 3.097663557245618, "test_loss": 4.170318229496479, "test_acc1": 14.85, "test_acc5": 34.97, "epoch": 32, "n_parameters": 85958500}
[13:02:06.739563] [13:02:06.739622] Training epoch 32 for 0:00:30
[13:02:06.739675] [13:02:06.742580] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:02:09.256923] Epoch: [33]  [ 0/42]  eta: 0:01:45  lr: 0.006405  loss: 3.3169 (3.3169)  time: 2.5135  data: 1.9183  max mem: 10046
[13:02:29.440768] Epoch: [33]  [41/42]  eta: 0:00:00  lr: 0.006303  loss: 3.1207 (3.1240)  time: 0.4939  data: 0.0001  max mem: 10046
[13:02:29.663683] Epoch: [33] Total time: 0:00:22 (0.5457 s / it)
[13:02:29.668005] Averaged stats: lr: 0.006303  loss: 3.1207 (3.1110)
[13:02:31.507406] Test:  [ 0/40]  eta: 0:01:13  loss: 4.1921 (4.1921)  acc1: 15.6250 (15.6250)  acc5: 32.8125 (32.8125)  time: 1.8361  data: 1.6556  max mem: 10046
[13:02:37.473666] Test:  [39/40]  eta: 0:00:00  loss: 3.9241 (4.0830)  acc1: 17.1875 (16.5200)  acc5: 37.5000 (36.4000)  time: 0.1497  data: 0.0001  max mem: 10046
[13:02:37.606836] Test: Total time: 0:00:07 (0.1984 s / it)
[13:02:37.729337] * Acc@1 15.630 Acc@5 36.570 loss 4.093
[13:02:37.729534] Accuracy of the network on the 10000 test images: 15.6%
[13:02:37.729750] [13:02:41.159841] Max accuracy: 15.63%
[13:02:41.160103] [13:02:41.160963] {"train_lr": 0.0063610546458533045, "train_loss": 3.1109801686945415, "test_loss": 4.093255941569805, "test_acc1": 15.63, "test_acc5": 36.57, "epoch": 33, "n_parameters": 85958500}
[13:02:41.161091] [13:02:41.161159] Training epoch 33 for 0:00:34
[13:02:41.161213] [13:02:41.164245] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:02:43.155363] Epoch: [34]  [ 0/42]  eta: 0:01:23  lr: 0.006298  loss: 3.2030 (3.2030)  time: 1.9900  data: 1.5185  max mem: 10046
[13:03:03.278783] Epoch: [34]  [41/42]  eta: 0:00:00  lr: 0.006193  loss: 3.0842 (3.0934)  time: 0.4923  data: 0.0001  max mem: 10046
[13:03:03.508456] Epoch: [34] Total time: 0:00:22 (0.5320 s / it)
[13:03:03.509165] Averaged stats: lr: 0.006193  loss: 3.0842 (3.0919)
[13:03:05.189165] Test:  [ 0/40]  eta: 0:01:07  loss: 4.3072 (4.3072)  acc1: 14.0625 (14.0625)  acc5: 34.3750 (34.3750)  time: 1.6757  data: 1.5187  max mem: 10046
[13:03:11.296640] Test:  [39/40]  eta: 0:00:00  loss: 3.9990 (4.1128)  acc1: 18.7500 (16.1600)  acc5: 35.9375 (35.6400)  time: 0.1493  data: 0.0001  max mem: 10046
[13:03:11.440115] Test: Total time: 0:00:07 (0.1982 s / it)
[13:03:11.566206] * Acc@1 15.930 Acc@5 36.290 loss 4.108
[13:03:11.566377] Accuracy of the network on the 10000 test images: 15.9%
[13:03:11.566587] [13:03:15.036595] Max accuracy: 15.93%
[13:03:15.036836] [13:03:15.037698] {"train_lr": 0.006253023541037099, "train_loss": 3.0918817619482675, "test_loss": 4.108129698038101, "test_acc1": 15.93, "test_acc5": 36.29, "epoch": 34, "n_parameters": 85958500}
[13:03:15.037823] [13:03:15.037892] Training epoch 34 for 0:00:33
[13:03:15.037947] [13:03:15.040773] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:03:17.203953] Epoch: [35]  [ 0/42]  eta: 0:01:30  lr: 0.006188  loss: 2.8825 (2.8825)  time: 2.1623  data: 1.6932  max mem: 10046
[13:03:37.324595] Epoch: [35]  [41/42]  eta: 0:00:00  lr: 0.006081  loss: 3.1037 (3.0700)  time: 0.4915  data: 0.0001  max mem: 10046
[13:03:37.560474] Epoch: [35] Total time: 0:00:22 (0.5362 s / it)
[13:03:37.561228] Averaged stats: lr: 0.006081  loss: 3.1037 (3.0651)
[13:03:39.233078] Test:  [ 0/40]  eta: 0:01:06  loss: 4.1045 (4.1045)  acc1: 14.0625 (14.0625)  acc5: 34.3750 (34.3750)  time: 1.6669  data: 1.5061  max mem: 10046
[13:03:45.341119] Test:  [39/40]  eta: 0:00:00  loss: 3.9029 (4.0270)  acc1: 18.7500 (17.3200)  acc5: 39.0625 (37.9600)  time: 0.1492  data: 0.0001  max mem: 10046
[13:03:45.463953] Test: Total time: 0:00:07 (0.1975 s / it)
[13:03:45.672976] * Acc@1 16.950 Acc@5 37.860 loss 4.022
[13:03:45.673190] Accuracy of the network on the 10000 test images: 16.9%
[13:03:45.673426] [13:03:48.418807] Max accuracy: 16.95%
[13:03:48.419085] [13:03:48.419900] {"train_lr": 0.006142529334799996, "train_loss": 3.0651167517616633, "test_loss": 4.022475038468838, "test_acc1": 16.95, "test_acc5": 37.86, "epoch": 35, "n_parameters": 85958500}
[13:03:48.419971] [13:03:48.420033] Training epoch 35 for 0:00:33
[13:03:48.420094] [13:03:48.422892] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:03:50.505729] Epoch: [36]  [ 0/42]  eta: 0:01:27  lr: 0.006076  loss: 3.1519 (3.1519)  time: 2.0819  data: 1.5956  max mem: 10046
[13:04:10.603614] Epoch: [36]  [41/42]  eta: 0:00:00  lr: 0.005967  loss: 3.0876 (3.0729)  time: 0.4912  data: 0.0001  max mem: 10046
[13:04:10.852776] Epoch: [36] Total time: 0:00:22 (0.5340 s / it)
[13:04:10.853673] Averaged stats: lr: 0.005967  loss: 3.0876 (3.0595)
[13:04:12.684561] Test:  [ 0/40]  eta: 0:01:13  loss: 4.2721 (4.2721)  acc1: 15.6250 (15.6250)  acc5: 31.2500 (31.2500)  time: 1.8272  data: 1.6519  max mem: 10046
[13:04:18.640478] Test:  [39/40]  eta: 0:00:00  loss: 3.8747 (4.0211)  acc1: 17.1875 (17.5200)  acc5: 39.0625 (38.1200)  time: 0.1497  data: 0.0001  max mem: 10046
[13:04:18.818192] Test: Total time: 0:00:07 (0.1991 s / it)
[13:04:18.921948] * Acc@1 17.250 Acc@5 37.520 loss 4.032
[13:04:18.922146] Accuracy of the network on the 10000 test images: 17.2%
[13:04:18.922360] [13:04:22.308700] Max accuracy: 17.25%
[13:04:22.308983] [13:04:22.309988] {"train_lr": 0.0060296928509342455, "train_loss": 3.059501056160246, "test_loss": 4.032091169059276, "test_acc1": 17.25, "test_acc5": 37.52, "epoch": 36, "n_parameters": 85958500}
[13:04:22.310065] [13:04:22.310126] Training epoch 36 for 0:00:33
[13:04:22.310179] [13:04:22.313284] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:04:24.455757] Epoch: [37]  [ 0/42]  eta: 0:01:29  lr: 0.005962  loss: 3.1913 (3.1913)  time: 2.1416  data: 1.6702  max mem: 10046
[13:04:44.627368] Epoch: [37]  [41/42]  eta: 0:00:00  lr: 0.005851  loss: 3.0554 (3.0801)  time: 0.4936  data: 0.0001  max mem: 10046
[13:04:44.856887] Epoch: [37] Total time: 0:00:22 (0.5367 s / it)
[13:04:44.870309] Averaged stats: lr: 0.005851  loss: 3.0554 (3.0284)
[13:04:46.789693] Test:  [ 0/40]  eta: 0:01:16  loss: 4.3239 (4.3239)  acc1: 10.9375 (10.9375)  acc5: 31.2500 (31.2500)  time: 1.9160  data: 1.7270  max mem: 10046
[13:04:52.746015] Test:  [39/40]  eta: 0:00:00  loss: 3.9781 (4.1263)  acc1: 15.6250 (16.3200)  acc5: 37.5000 (36.5200)  time: 0.1491  data: 0.0001  max mem: 10046
[13:04:52.898849] Test: Total time: 0:00:08 (0.2007 s / it)
[13:04:52.944884] * Acc@1 16.200 Acc@5 36.500 loss 4.134
[13:04:52.945061] Accuracy of the network on the 10000 test images: 16.2%
[13:04:52.945259] [13:04:52.945332] Max accuracy: 17.25%
[13:04:52.945388] [13:04:52.946214] {"train_lr": 0.0059146374744783025, "train_loss": 3.028422866548811, "test_loss": 4.133646090328694, "test_acc1": 16.2, "test_acc5": 36.5, "epoch": 37, "n_parameters": 85958500}
[13:04:52.946284] [13:04:52.946341] Training epoch 37 for 0:00:30
[13:04:52.946392] [13:04:52.949266] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:04:55.355773] Epoch: [38]  [ 0/42]  eta: 0:01:41  lr: 0.005846  loss: 2.9603 (2.9603)  time: 2.4053  data: 1.7393  max mem: 10046
[13:05:15.589640] Epoch: [38]  [41/42]  eta: 0:00:00  lr: 0.005733  loss: 2.9661 (2.9726)  time: 0.4949  data: 0.0001  max mem: 10046
[13:05:15.813818] Epoch: [38] Total time: 0:00:22 (0.5444 s / it)
[13:05:15.815908] Averaged stats: lr: 0.005733  loss: 2.9661 (3.0304)
[13:05:17.804454] Test:  [ 0/40]  eta: 0:01:19  loss: 4.2585 (4.2585)  acc1: 14.0625 (14.0625)  acc5: 37.5000 (37.5000)  time: 1.9849  data: 1.8006  max mem: 10046
[13:05:23.778063] Test:  [39/40]  eta: 0:00:00  loss: 3.9229 (4.0552)  acc1: 17.1875 (16.6800)  acc5: 39.0625 (37.8800)  time: 0.1499  data: 0.0001  max mem: 10046
[13:05:23.923585] Test: Total time: 0:00:08 (0.2026 s / it)
[13:05:23.924976] * Acc@1 16.520 Acc@5 37.880 loss 4.062
[13:05:23.925143] Accuracy of the network on the 10000 test images: 16.5%
[13:05:23.925373] [13:05:23.925449] Max accuracy: 17.25%
[13:05:23.925506] [13:05:23.926272] {"train_lr": 0.005797489016797103, "train_loss": 3.030442286105383, "test_loss": 4.062003126740455, "test_acc1": 16.52, "test_acc5": 37.88, "epoch": 38, "n_parameters": 85958500}
[13:05:23.926341] [13:05:23.926396] Training epoch 38 for 0:00:30
[13:05:23.926447] [13:05:23.929400] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:05:26.402866] Epoch: [39]  [ 0/42]  eta: 0:01:43  lr: 0.005727  loss: 3.1293 (3.1293)  time: 2.4724  data: 1.6757  max mem: 10046
[13:05:46.669823] Epoch: [39]  [41/42]  eta: 0:00:00  lr: 0.005613  loss: 3.0759 (3.0612)  time: 0.4938  data: 0.0001  max mem: 10046
[13:05:46.881235] Epoch: [39] Total time: 0:00:22 (0.5465 s / it)
[13:05:46.887966] Averaged stats: lr: 0.005613  loss: 3.0759 (3.0226)
[13:05:48.519037] Test:  [ 0/40]  eta: 0:01:05  loss: 4.3594 (4.3594)  acc1: 14.0625 (14.0625)  acc5: 32.8125 (32.8125)  time: 1.6268  data: 1.4680  max mem: 10046
[13:05:54.825538] Test:  [39/40]  eta: 0:00:00  loss: 3.9405 (4.1128)  acc1: 17.1875 (16.8000)  acc5: 39.0625 (37.4000)  time: 0.1508  data: 0.0001  max mem: 10046
[13:05:54.993842] Test: Total time: 0:00:08 (0.2026 s / it)
[13:05:55.118172] * Acc@1 16.570 Acc@5 36.970 loss 4.119
[13:05:55.118354] Accuracy of the network on the 10000 test images: 16.6%
[13:05:55.118584] [13:05:55.118666] Max accuracy: 17.25%
[13:05:55.118723] [13:05:55.119529] {"train_lr": 0.00567837557800916, "train_loss": 3.0226260466235026, "test_loss": 4.119480946660042, "test_acc1": 16.57, "test_acc5": 36.97, "epoch": 39, "n_parameters": 85958500}
[13:05:55.119601] [13:05:55.119658] Training epoch 39 for 0:00:31
[13:05:55.119709] [13:05:55.122642] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:05:57.466285] Epoch: [40]  [ 0/42]  eta: 0:01:38  lr: 0.005607  loss: 3.0472 (3.0472)  time: 2.3426  data: 1.6943  max mem: 10046
[13:06:17.650162] Epoch: [40]  [41/42]  eta: 0:00:00  lr: 0.005491  loss: 3.0841 (3.0169)  time: 0.4933  data: 0.0001  max mem: 10046
[13:06:17.869651] Epoch: [40] Total time: 0:00:22 (0.5416 s / it)
[13:06:17.870642] Averaged stats: lr: 0.005491  loss: 3.0841 (3.0108)
[13:06:19.662654] Test:  [ 0/40]  eta: 0:01:11  loss: 4.1457 (4.1457)  acc1: 14.0625 (14.0625)  acc5: 34.3750 (34.3750)  time: 1.7882  data: 1.6176  max mem: 10046
[13:06:25.652958] Test:  [39/40]  eta: 0:00:00  loss: 3.8761 (4.0100)  acc1: 15.6250 (16.5200)  acc5: 37.5000 (37.5600)  time: 0.1503  data: 0.0001  max mem: 10046
[13:06:25.797313] Test: Total time: 0:00:07 (0.1981 s / it)
[13:06:26.030553] * Acc@1 16.220 Acc@5 37.850 loss 4.019
[13:06:26.030790] Accuracy of the network on the 10000 test images: 16.2%
[13:06:26.031049] [13:06:26.031133] Max accuracy: 17.25%
[13:06:26.031191] [13:06:26.032044] {"train_lr": 0.005557427406910991, "train_loss": 3.0108160504273007, "test_loss": 4.019104580581188, "test_acc1": 16.22, "test_acc5": 37.85, "epoch": 40, "n_parameters": 85958500}
[13:06:26.032126] [13:06:26.032192] Training epoch 40 for 0:00:30
[13:06:26.032245] [13:06:26.035000] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:06:28.449593] Epoch: [41]  [ 0/42]  eta: 0:01:41  lr: 0.005485  loss: 2.8310 (2.8310)  time: 2.4136  data: 1.7915  max mem: 10046
[13:06:48.793524] Epoch: [41]  [41/42]  eta: 0:00:00  lr: 0.005367  loss: 2.9371 (2.9811)  time: 0.4940  data: 0.0001  max mem: 10046
[13:06:49.027855] Epoch: [41] Total time: 0:00:22 (0.5474 s / it)
[13:06:49.031231] Averaged stats: lr: 0.005367  loss: 2.9371 (2.9926)
[13:06:51.062619] Test:  [ 0/40]  eta: 0:01:21  loss: 4.1806 (4.1806)  acc1: 15.6250 (15.6250)  acc5: 35.9375 (35.9375)  time: 2.0280  data: 1.8502  max mem: 10046
[13:06:57.054176] Test:  [39/40]  eta: 0:00:00  loss: 3.8617 (4.0163)  acc1: 18.7500 (17.1200)  acc5: 40.6250 (39.6000)  time: 0.1505  data: 0.0001  max mem: 10046
[13:06:57.186218] Test: Total time: 0:00:08 (0.2038 s / it)
[13:06:57.187518] * Acc@1 17.180 Acc@5 39.130 loss 4.015
[13:06:57.187655] Accuracy of the network on the 10000 test images: 17.2%
[13:06:57.187882] [13:06:57.187958] Max accuracy: 17.25%
[13:06:57.188015] [13:06:57.188817] {"train_lr": 0.005434776758551934, "train_loss": 2.9925735791524253, "test_loss": 4.015190455317497, "test_acc1": 17.18, "test_acc5": 39.13, "epoch": 41, "n_parameters": 85958500}
[13:06:57.188888] [13:06:57.188949] Training epoch 41 for 0:00:31
[13:06:57.189000] [13:06:57.191827] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:06:59.495815] Epoch: [42]  [ 0/42]  eta: 0:01:36  lr: 0.005362  loss: 2.9295 (2.9295)  time: 2.3029  data: 1.6819  max mem: 10046
[13:07:19.703851] Epoch: [42]  [41/42]  eta: 0:00:00  lr: 0.005242  loss: 2.9711 (2.9326)  time: 0.4943  data: 0.0001  max mem: 10046
[13:07:19.932452] Epoch: [42] Total time: 0:00:22 (0.5414 s / it)
[13:07:19.948666] Averaged stats: lr: 0.005242  loss: 2.9711 (2.9819)
[13:07:21.969733] Test:  [ 0/40]  eta: 0:01:20  loss: 4.2231 (4.2231)  acc1: 15.6250 (15.6250)  acc5: 37.5000 (37.5000)  time: 2.0166  data: 1.8462  max mem: 10046
[13:07:27.969297] Test:  [39/40]  eta: 0:00:00  loss: 3.8568 (3.9840)  acc1: 20.3125 (18.0400)  acc5: 40.6250 (39.8000)  time: 0.1507  data: 0.0001  max mem: 10046
[13:07:28.133742] Test: Total time: 0:00:08 (0.2045 s / it)
[13:07:28.135036] * Acc@1 17.230 Acc@5 38.530 loss 4.024
[13:07:28.135199] Accuracy of the network on the 10000 test images: 17.2%
[13:07:28.135405] [13:07:28.135486] Max accuracy: 17.25%
[13:07:28.135542] [13:07:28.136316] {"train_lr": 0.005310557749615243, "train_loss": 2.9818938743500483, "test_loss": 4.02422781586647, "test_acc1": 17.23, "test_acc5": 38.53, "epoch": 42, "n_parameters": 85958500}
[13:07:28.136388] [13:07:28.136449] Training epoch 42 for 0:00:30
[13:07:28.136501] [13:07:28.139320] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:07:30.687607] Epoch: [43]  [ 0/42]  eta: 0:01:46  lr: 0.005236  loss: 3.1620 (3.1620)  time: 2.5472  data: 1.9395  max mem: 10046
[13:07:50.877092] Epoch: [43]  [41/42]  eta: 0:00:00  lr: 0.005116  loss: 3.0198 (3.0615)  time: 0.4941  data: 0.0001  max mem: 10046
[13:07:51.109153] Epoch: [43] Total time: 0:00:22 (0.5469 s / it)
[13:07:51.109867] Averaged stats: lr: 0.005116  loss: 3.0198 (3.0353)
[13:07:52.867367] Test:  [ 0/40]  eta: 0:01:10  loss: 4.2448 (4.2448)  acc1: 12.5000 (12.5000)  acc5: 29.6875 (29.6875)  time: 1.7532  data: 1.5877  max mem: 10046
[13:07:58.875451] Test:  [39/40]  eta: 0:00:00  loss: 3.8679 (4.0359)  acc1: 18.7500 (16.4400)  acc5: 37.5000 (37.1600)  time: 0.1505  data: 0.0001  max mem: 10046
[13:07:59.005415] Test: Total time: 0:00:07 (0.1973 s / it)
[13:07:59.246022] * Acc@1 16.180 Acc@5 36.880 loss 4.059
[13:07:59.246232] Accuracy of the network on the 10000 test images: 16.2%
[13:07:59.246453] [13:07:59.246535] Max accuracy: 17.25%
[13:07:59.246594] [13:07:59.247398] {"train_lr": 0.005184906211763425, "train_loss": 3.035338112286159, "test_loss": 4.059468951821327, "test_acc1": 16.18, "test_acc5": 36.88, "epoch": 43, "n_parameters": 85958500}
[13:07:59.247469] [13:07:59.247527] Training epoch 43 for 0:00:31
[13:07:59.247579] [13:07:59.250342] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:08:01.590691] Epoch: [44]  [ 0/42]  eta: 0:01:38  lr: 0.005110  loss: 2.8661 (2.8661)  time: 2.3390  data: 1.7933  max mem: 10046
[13:08:21.792313] Epoch: [44]  [41/42]  eta: 0:00:00  lr: 0.004988  loss: 2.9406 (2.9911)  time: 0.4945  data: 0.0001  max mem: 10046
[13:08:22.018128] Epoch: [44] Total time: 0:00:22 (0.5421 s / it)
[13:08:22.030487] Averaged stats: lr: 0.004988  loss: 2.9406 (2.9796)
[13:08:23.830802] Test:  [ 0/40]  eta: 0:01:11  loss: 4.2856 (4.2856)  acc1: 14.0625 (14.0625)  acc5: 35.9375 (35.9375)  time: 1.7963  data: 1.6200  max mem: 10046
[13:08:29.873185] Test:  [39/40]  eta: 0:00:00  loss: 3.8729 (4.0224)  acc1: 17.1875 (18.1200)  acc5: 42.1875 (39.5200)  time: 0.1512  data: 0.0001  max mem: 10046
[13:08:30.009522] Test: Total time: 0:00:07 (0.1994 s / it)
[13:08:30.152470] * Acc@1 17.860 Acc@5 39.250 loss 4.028
[13:08:30.152635] Accuracy of the network on the 10000 test images: 17.9%
[13:08:30.152804] [13:08:33.755414] Max accuracy: 17.86%
[13:08:33.755657] [13:08:33.756502] {"train_lr": 0.005057959543108319, "train_loss": 2.979551328080041, "test_loss": 4.027928857505321, "test_acc1": 17.86, "test_acc5": 39.25, "epoch": 44, "n_parameters": 85958500}
[13:08:33.756628] [13:08:33.756696] Training epoch 44 for 0:00:34
[13:08:33.756759] [13:08:33.759743] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:08:35.827099] Epoch: [45]  [ 0/42]  eta: 0:01:26  lr: 0.004982  loss: 2.7304 (2.7304)  time: 2.0665  data: 1.5931  max mem: 10046
[13:08:55.954761] Epoch: [45]  [41/42]  eta: 0:00:00  lr: 0.004860  loss: 2.9483 (2.9186)  time: 0.4923  data: 0.0001  max mem: 10046
[13:08:56.190476] Epoch: [45] Total time: 0:00:22 (0.5341 s / it)
[13:08:56.191243] Averaged stats: lr: 0.004860  loss: 2.9483 (2.9447)
[13:08:58.155844] Test:  [ 0/40]  eta: 0:01:18  loss: 4.3103 (4.3103)  acc1: 17.1875 (17.1875)  acc5: 32.8125 (32.8125)  time: 1.9611  data: 1.7844  max mem: 10046
[13:09:04.162506] Test:  [39/40]  eta: 0:00:00  loss: 3.8788 (4.0023)  acc1: 18.7500 (17.8400)  acc5: 37.5000 (38.8800)  time: 0.1499  data: 0.0001  max mem: 10046
[13:09:04.287003] Test: Total time: 0:00:08 (0.2023 s / it)
[13:09:04.434111] * Acc@1 17.100 Acc@5 38.400 loss 4.031
[13:09:04.434281] Accuracy of the network on the 10000 test images: 17.1%
[13:09:04.434480] [13:09:04.434558] Max accuracy: 17.86%
[13:09:04.434615] [13:09:04.435367] {"train_lr": 0.004929856557968283, "train_loss": 2.9446655951795124, "test_loss": 4.030946767330169, "test_acc1": 17.1, "test_acc5": 38.4, "epoch": 45, "n_parameters": 85958500}
[13:09:04.435437] [13:09:04.435499] Training epoch 45 for 0:00:30
[13:09:04.435551] [13:09:04.438286] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:09:06.980524] Epoch: [46]  [ 0/42]  eta: 0:01:46  lr: 0.004854  loss: 2.5563 (2.5563)  time: 2.5412  data: 1.6632  max mem: 10046
[13:09:27.181956] Epoch: [46]  [41/42]  eta: 0:00:00  lr: 0.004730  loss: 2.9249 (2.8973)  time: 0.4937  data: 0.0001  max mem: 10046
[13:09:27.406453] Epoch: [46] Total time: 0:00:22 (0.5469 s / it)
[13:09:27.407193] Averaged stats: lr: 0.004730  loss: 2.9249 (2.9470)
[13:09:29.124785] Test:  [ 0/40]  eta: 0:01:08  loss: 4.2111 (4.2111)  acc1: 20.3125 (20.3125)  acc5: 34.3750 (34.3750)  time: 1.7132  data: 1.5396  max mem: 10046
[13:09:35.233257] Test:  [39/40]  eta: 0:00:00  loss: 3.9235 (4.0193)  acc1: 18.7500 (18.6800)  acc5: 40.6250 (40.2400)  time: 0.1508  data: 0.0001  max mem: 10046
[13:09:35.383982] Test: Total time: 0:00:07 (0.1994 s / it)
[13:09:35.630733] * Acc@1 18.000 Acc@5 39.780 loss 4.036
[13:09:35.630949] Accuracy of the network on the 10000 test images: 18.0%
[13:09:35.631166] [13:09:39.115123] Max accuracy: 18.00%
[13:09:39.115412] [13:09:39.116297] {"train_lr": 0.004800737335076797, "train_loss": 2.947007853360403, "test_loss": 4.035863862931729, "test_acc1": 18.0, "test_acc5": 39.78, "epoch": 46, "n_parameters": 85958500}
[13:09:39.116371] [13:09:39.116432] Training epoch 46 for 0:00:34
[13:09:39.116484] [13:09:39.119474] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:09:41.354804] Epoch: [47]  [ 0/42]  eta: 0:01:33  lr: 0.004724  loss: 3.0453 (3.0453)  time: 2.2345  data: 1.7639  max mem: 10046
[13:10:01.518409] Epoch: [47]  [41/42]  eta: 0:00:00  lr: 0.004600  loss: 2.9509 (2.9361)  time: 0.4935  data: 0.0001  max mem: 10046
[13:10:01.764036] Epoch: [47] Total time: 0:00:22 (0.5392 s / it)
[13:10:01.764793] Averaged stats: lr: 0.004600  loss: 2.9509 (2.9440)
[13:10:03.537754] Test:  [ 0/40]  eta: 0:01:10  loss: 4.1356 (4.1356)  acc1: 14.0625 (14.0625)  acc5: 35.9375 (35.9375)  time: 1.7695  data: 1.5904  max mem: 10046
[13:10:09.539579] Test:  [39/40]  eta: 0:00:00  loss: 3.8668 (3.9939)  acc1: 18.7500 (18.0400)  acc5: 40.6250 (40.1200)  time: 0.1498  data: 0.0001  max mem: 10046
[13:10:09.659441] Test: Total time: 0:00:07 (0.1973 s / it)
[13:10:09.791862] * Acc@1 17.760 Acc@5 39.330 loss 4.014
[13:10:09.792036] Accuracy of the network on the 10000 test images: 17.8%
[13:10:09.792227] [13:10:09.792310] Max accuracy: 18.00%
[13:10:09.792371] [13:10:09.793178] {"train_lr": 0.0046707430644083595, "train_loss": 2.9440463284651437, "test_loss": 4.01441756784916, "test_acc1": 17.76, "test_acc5": 39.33, "epoch": 47, "n_parameters": 85958500}
[13:10:09.793260] [13:10:09.793328] Training epoch 47 for 0:00:30
[13:10:09.793385] [13:10:09.797109] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:10:11.994126] Epoch: [48]  [ 0/42]  eta: 0:01:32  lr: 0.004593  loss: 3.0809 (3.0809)  time: 2.1957  data: 1.7054  max mem: 10046
[13:10:32.240677] Epoch: [48]  [41/42]  eta: 0:00:00  lr: 0.004469  loss: 2.9902 (2.9564)  time: 0.4943  data: 0.0001  max mem: 10046
[13:10:32.456608] Epoch: [48] Total time: 0:00:22 (0.5395 s / it)
[13:10:32.457388] Averaged stats: lr: 0.004469  loss: 2.9902 (2.9404)
[13:10:34.314971] Test:  [ 0/40]  eta: 0:01:14  loss: 4.3187 (4.3187)  acc1: 17.1875 (17.1875)  acc5: 37.5000 (37.5000)  time: 1.8542  data: 1.6725  max mem: 10046
[13:10:40.319014] Test:  [39/40]  eta: 0:00:00  loss: 3.8749 (4.0161)  acc1: 18.7500 (18.0000)  acc5: 42.1875 (40.5200)  time: 0.1506  data: 0.0001  max mem: 10046
[13:10:40.495416] Test: Total time: 0:00:08 (0.2009 s / it)
[13:10:40.543134] * Acc@1 18.300 Acc@5 40.200 loss 4.041
[13:10:40.543336] Accuracy of the network on the 10000 test images: 18.3%
[13:10:40.543557] [13:10:43.962775] Max accuracy: 18.30%
[13:10:43.963041] [13:10:43.963881] {"train_lr": 0.004540015892789405, "train_loss": 2.9403764491989497, "test_loss": 4.0405175045132635, "test_acc1": 18.3, "test_acc5": 40.2, "epoch": 48, "n_parameters": 85958500}
[13:10:43.964004] [13:10:43.964072] Training epoch 48 for 0:00:34
[13:10:43.964136] [13:10:43.966874] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:10:46.141621] Epoch: [49]  [ 0/42]  eta: 0:01:31  lr: 0.004462  loss: 2.9887 (2.9887)  time: 2.1740  data: 1.6934  max mem: 10046
[13:11:06.341888] Epoch: [49]  [41/42]  eta: 0:00:00  lr: 0.004337  loss: 2.9493 (2.9362)  time: 0.4936  data: 0.0001  max mem: 10046
[13:11:06.572962] Epoch: [49] Total time: 0:00:22 (0.5382 s / it)
[13:11:06.573686] Averaged stats: lr: 0.004337  loss: 2.9493 (2.9286)
[13:11:08.470435] Test:  [ 0/40]  eta: 0:01:15  loss: 4.1203 (4.1203)  acc1: 18.7500 (18.7500)  acc5: 35.9375 (35.9375)  time: 1.8934  data: 1.7183  max mem: 10046
[13:11:14.441818] Test:  [39/40]  eta: 0:00:00  loss: 3.9129 (4.0164)  acc1: 18.7500 (18.8800)  acc5: 40.6250 (39.6800)  time: 0.1499  data: 0.0001  max mem: 10046
[13:11:14.592560] Test: Total time: 0:00:08 (0.2004 s / it)
[13:11:14.603282] * Acc@1 17.980 Acc@5 39.040 loss 4.035
[13:11:14.603461] Accuracy of the network on the 10000 test images: 18.0%
[13:11:14.603666] [13:11:14.603742] Max accuracy: 18.30%
[13:11:14.603800] [13:11:14.604584] {"train_lr": 0.004408698768462777, "train_loss": 2.9286128225780668, "test_loss": 4.035112191736698, "test_acc1": 17.98, "test_acc5": 39.04, "epoch": 49, "n_parameters": 85958500}
[13:11:14.604658] [13:11:14.604724] Training epoch 49 for 0:00:30
[13:11:14.604778] [13:11:14.607516] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:11:16.916992] Epoch: [50]  [ 0/42]  eta: 0:01:36  lr: 0.004331  loss: 2.9148 (2.9148)  time: 2.3085  data: 1.6196  max mem: 10046
[13:11:37.137562] Epoch: [50]  [41/42]  eta: 0:00:00  lr: 0.004205  loss: 2.9729 (2.9106)  time: 0.4941  data: 0.0001  max mem: 10046
[13:11:37.369378] Epoch: [50] Total time: 0:00:22 (0.5419 s / it)
[13:11:37.370189] Averaged stats: lr: 0.004205  loss: 2.9729 (2.9055)
[13:11:39.079097] Test:  [ 0/40]  eta: 0:01:08  loss: 4.1801 (4.1801)  acc1: 14.0625 (14.0625)  acc5: 34.3750 (34.3750)  time: 1.7045  data: 1.5206  max mem: 10046
[13:11:45.081250] Test:  [39/40]  eta: 0:00:00  loss: 3.9054 (3.9678)  acc1: 20.3125 (19.0400)  acc5: 40.6250 (40.6800)  time: 0.1503  data: 0.0001  max mem: 10046
[13:11:45.221234] Test: Total time: 0:00:07 (0.1962 s / it)
[13:11:45.599952] * Acc@1 18.340 Acc@5 40.320 loss 3.983
[13:11:45.600150] Accuracy of the network on the 10000 test images: 18.3%
[13:11:45.600369] [13:11:49.217038] Max accuracy: 18.34%
[13:11:49.217388] [13:11:49.218698] {"train_lr": 0.00427693528477595, "train_loss": 2.9054822496005466, "test_loss": 3.983396273851395, "test_acc1": 18.34, "test_acc5": 40.32, "epoch": 50, "n_parameters": 85958500}
[13:11:49.218814] [13:11:49.218911] Training epoch 50 for 0:00:34
[13:11:49.218986] [13:11:49.223109] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:11:51.399158] Epoch: [51]  [ 0/42]  eta: 0:01:31  lr: 0.004199  loss: 3.2555 (3.2555)  time: 2.1752  data: 1.6917  max mem: 10046
[13:12:11.518906] Epoch: [51]  [41/42]  eta: 0:00:00  lr: 0.004073  loss: 2.9090 (2.9080)  time: 0.4923  data: 0.0001  max mem: 10046
[13:12:11.762326] Epoch: [51] Total time: 0:00:22 (0.5366 s / it)
[13:12:11.763089] Averaged stats: lr: 0.004073  loss: 2.9090 (2.9147)
[13:12:13.892383] Test:  [ 0/40]  eta: 0:01:25  loss: 4.2852 (4.2852)  acc1: 17.1875 (17.1875)  acc5: 32.8125 (32.8125)  time: 2.1259  data: 1.9486  max mem: 10046
[13:12:19.866231] Test:  [39/40]  eta: 0:00:00  loss: 3.9812 (4.0544)  acc1: 17.1875 (17.9200)  acc5: 39.0625 (39.0800)  time: 0.1502  data: 0.0001  max mem: 10046
[13:12:20.002681] Test: Total time: 0:00:08 (0.2059 s / it)
[13:12:20.003920] * Acc@1 18.350 Acc@5 39.370 loss 4.057
[13:12:20.004054] Accuracy of the network on the 10000 test images: 18.4%
[13:12:20.004235] [13:12:23.394699] Max accuracy: 18.35%
[13:12:23.395007] [13:12:23.395868] {"train_lr": 0.004144869523163801, "train_loss": 2.9146851102511087, "test_loss": 4.056910954415798, "test_acc1": 18.35, "test_acc5": 39.37, "epoch": 51, "n_parameters": 85958500}
[13:12:23.395951] [13:12:23.396017] Training epoch 51 for 0:00:34
[13:12:23.396069] [13:12:23.398841] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:12:25.462271] Epoch: [52]  [ 0/42]  eta: 0:01:26  lr: 0.004067  loss: 2.5811 (2.5811)  time: 2.0626  data: 1.5788  max mem: 10046
[13:12:45.573404] Epoch: [52]  [41/42]  eta: 0:00:00  lr: 0.003941  loss: 2.8614 (2.9399)  time: 0.4923  data: 0.0001  max mem: 10046
[13:12:45.800668] Epoch: [52] Total time: 0:00:22 (0.5334 s / it)
[13:12:45.818959] Averaged stats: lr: 0.003941  loss: 2.8614 (2.9051)
[13:12:47.923411] Test:  [ 0/40]  eta: 0:01:24  loss: 4.0859 (4.0859)  acc1: 17.1875 (17.1875)  acc5: 35.9375 (35.9375)  time: 2.1010  data: 1.9236  max mem: 10046
[13:12:53.899339] Test:  [39/40]  eta: 0:00:00  loss: 3.7806 (3.9532)  acc1: 18.7500 (20.0000)  acc5: 43.7500 (41.3600)  time: 0.1503  data: 0.0001  max mem: 10046
[13:12:54.036869] Test: Total time: 0:00:08 (0.2054 s / it)
[13:12:54.038310] * Acc@1 19.150 Acc@5 40.260 loss 3.969
[13:12:54.038469] Accuracy of the network on the 10000 test images: 19.1%
[13:12:54.038694] [13:12:57.461443] Max accuracy: 19.15%
[13:12:57.461693] [13:12:57.462591] {"train_lr": 0.004012645895597684, "train_loss": 2.9051205217838287, "test_loss": 3.9694772213697433, "test_acc1": 19.15, "test_acc5": 40.26, "epoch": 52, "n_parameters": 85958500}
[13:12:57.462721] [13:12:57.462786] Training epoch 52 for 0:00:34
[13:12:57.462841] [13:12:57.465674] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:12:59.548533] Epoch: [53]  [ 0/42]  eta: 0:01:27  lr: 0.003934  loss: 2.7932 (2.7932)  time: 2.0820  data: 1.6034  max mem: 10046
[13:13:19.690566] Epoch: [53]  [41/42]  eta: 0:00:00  lr: 0.003808  loss: 2.8714 (2.8625)  time: 0.4931  data: 0.0001  max mem: 10046
[13:13:19.917596] Epoch: [53] Total time: 0:00:22 (0.5346 s / it)
[13:13:19.923526] Averaged stats: lr: 0.003808  loss: 2.8714 (2.8941)
[13:13:21.706093] Test:  [ 0/40]  eta: 0:01:11  loss: 4.1175 (4.1175)  acc1: 20.3125 (20.3125)  acc5: 37.5000 (37.5000)  time: 1.7789  data: 1.6085  max mem: 10046
[13:13:27.676502] Test:  [39/40]  eta: 0:00:00  loss: 3.7988 (3.9611)  acc1: 20.3125 (19.7600)  acc5: 43.7500 (41.6400)  time: 0.1500  data: 0.0001  max mem: 10046
[13:13:27.829460] Test: Total time: 0:00:07 (0.1976 s / it)
[13:13:28.061488] * Acc@1 19.430 Acc@5 41.270 loss 3.962
[13:13:28.061740] Accuracy of the network on the 10000 test images: 19.4%
[13:13:28.061995] [13:13:31.743330] Max accuracy: 19.43%
[13:13:31.743661] [13:13:31.744760] {"train_lr": 0.003880408986673056, "train_loss": 2.8941066705045246, "test_loss": 3.961990489065647, "test_acc1": 19.43, "test_acc5": 41.27, "epoch": 53, "n_parameters": 85958500}
[13:13:31.744838] [13:13:31.744906] Training epoch 53 for 0:00:34
[13:13:31.744959] [13:13:31.748038] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:13:33.881418] Epoch: [54]  [ 0/42]  eta: 0:01:29  lr: 0.003802  loss: 2.5863 (2.5863)  time: 2.1327  data: 1.6646  max mem: 10046
[13:13:54.027890] Epoch: [54]  [41/42]  eta: 0:00:00  lr: 0.003677  loss: 2.8467 (2.8933)  time: 0.4925  data: 0.0001  max mem: 10046
[13:13:54.249169] Epoch: [54] Total time: 0:00:22 (0.5357 s / it)
[13:13:54.253646] Averaged stats: lr: 0.003677  loss: 2.8467 (2.8757)
[13:13:56.197946] Test:  [ 0/40]  eta: 0:01:17  loss: 4.2563 (4.2563)  acc1: 20.3125 (20.3125)  acc5: 34.3750 (34.3750)  time: 1.9411  data: 1.7637  max mem: 10046
[13:14:02.168336] Test:  [39/40]  eta: 0:00:00  loss: 3.9825 (4.0650)  acc1: 18.7500 (19.2400)  acc5: 40.6250 (39.9200)  time: 0.1496  data: 0.0001  max mem: 10046
[13:14:02.315044] Test: Total time: 0:00:08 (0.2015 s / it)
[13:14:02.316510] * Acc@1 18.800 Acc@5 40.320 loss 4.054
[13:14:02.316674] Accuracy of the network on the 10000 test images: 18.8%
[13:14:02.316841] [13:14:02.316911] Max accuracy: 19.43%
[13:14:02.316966] [13:14:02.317709] {"train_lr": 0.0037483033955083437, "train_loss": 2.875696145352863, "test_loss": 4.054348394274712, "test_acc1": 18.8, "test_acc5": 40.32, "epoch": 54, "n_parameters": 85958500}
[13:14:02.317779] [13:14:02.317839] Training epoch 54 for 0:00:30
[13:14:02.317890] [13:14:02.320694] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:14:04.786977] Epoch: [55]  [ 0/42]  eta: 0:01:43  lr: 0.003670  loss: 3.1047 (3.1047)  time: 2.4650  data: 1.7319  max mem: 10046
[13:14:24.972742] Epoch: [55]  [41/42]  eta: 0:00:00  lr: 0.003545  loss: 2.8919 (2.8903)  time: 0.4939  data: 0.0001  max mem: 10046
[13:14:25.193943] Epoch: [55] Total time: 0:00:22 (0.5446 s / it)
[13:14:25.194641] Averaged stats: lr: 0.003545  loss: 2.8919 (2.8567)
[13:14:26.942859] Test:  [ 0/40]  eta: 0:01:09  loss: 4.1377 (4.1377)  acc1: 15.6250 (15.6250)  acc5: 34.3750 (34.3750)  time: 1.7438  data: 1.5668  max mem: 10046
[13:14:32.967526] Test:  [39/40]  eta: 0:00:00  loss: 3.8034 (3.8776)  acc1: 20.3125 (19.5200)  acc5: 43.7500 (42.4800)  time: 0.1501  data: 0.0001  max mem: 10046
[13:14:33.089880] Test: Total time: 0:00:07 (0.1973 s / it)
[13:14:33.274211] * Acc@1 19.930 Acc@5 42.730 loss 3.872
[13:14:33.274376] Accuracy of the network on the 10000 test images: 19.9%
[13:14:33.274576] [13:14:36.879852] Max accuracy: 19.93%
[13:14:36.880156] [13:14:36.881073] {"train_lr": 0.0036164735776279306, "train_loss": 2.8566975721291135, "test_loss": 3.871625064313412, "test_acc1": 19.93, "test_acc5": 42.73, "epoch": 55, "n_parameters": 85958500}
[13:14:36.881155] [13:14:36.881224] Training epoch 55 for 0:00:34
[13:14:36.881281] [13:14:36.885264] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:14:38.768852] Epoch: [56]  [ 0/42]  eta: 0:01:19  lr: 0.003539  loss: 2.8056 (2.8056)  time: 1.8825  data: 1.4137  max mem: 10046
[13:14:58.905706] Epoch: [56]  [41/42]  eta: 0:00:00  lr: 0.003414  loss: 2.9194 (2.9105)  time: 0.4926  data: 0.0001  max mem: 10046
[13:14:59.122367] Epoch: [56] Total time: 0:00:22 (0.5295 s / it)
[13:14:59.133288] Averaged stats: lr: 0.003414  loss: 2.9194 (2.8516)
[13:15:00.925367] Test:  [ 0/40]  eta: 0:01:11  loss: 4.0830 (4.0830)  acc1: 15.6250 (15.6250)  acc5: 34.3750 (34.3750)  time: 1.7885  data: 1.6115  max mem: 10046
[13:15:06.892505] Test:  [39/40]  eta: 0:00:00  loss: 3.7965 (3.9532)  acc1: 20.3125 (19.8400)  acc5: 42.1875 (41.8800)  time: 0.1496  data: 0.0001  max mem: 10046
[13:15:07.049082] Test: Total time: 0:00:07 (0.1978 s / it)
[13:15:07.260665] * Acc@1 19.420 Acc@5 41.330 loss 3.968
[13:15:07.260856] Accuracy of the network on the 10000 test images: 19.4%
[13:15:07.261063] [13:15:07.261146] Max accuracy: 19.93%
[13:15:07.261208] [13:15:07.262012] {"train_lr": 0.003485063687002159, "train_loss": 2.85156805458523, "test_loss": 3.9683179557323456, "test_acc1": 19.42, "test_acc5": 41.33, "epoch": 56, "n_parameters": 85958500}
[13:15:07.262084] [13:15:07.262147] Training epoch 56 for 0:00:30
[13:15:07.262202] [13:15:07.264943] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:15:09.670760] Epoch: [57]  [ 0/42]  eta: 0:01:41  lr: 0.003408  loss: 2.9204 (2.9204)  time: 2.4048  data: 1.9167  max mem: 10046
[13:15:29.886329] Epoch: [57]  [41/42]  eta: 0:00:00  lr: 0.003283  loss: 2.7608 (2.8436)  time: 0.4948  data: 0.0001  max mem: 10046
[13:15:30.099715] Epoch: [57] Total time: 0:00:22 (0.5437 s / it)
[13:15:30.141000] Averaged stats: lr: 0.003283  loss: 2.7608 (2.8509)
[13:15:31.956407] Test:  [ 0/40]  eta: 0:01:12  loss: 4.1344 (4.1344)  acc1: 17.1875 (17.1875)  acc5: 35.9375 (35.9375)  time: 1.8101  data: 1.6333  max mem: 10046
[13:15:37.985939] Test:  [39/40]  eta: 0:00:00  loss: 3.8737 (3.9631)  acc1: 20.3125 (20.2400)  acc5: 42.1875 (42.0400)  time: 0.1501  data: 0.0001  max mem: 10046
[13:15:38.128512] Test: Total time: 0:00:07 (0.1996 s / it)
[13:15:38.213677] * Acc@1 19.960 Acc@5 41.530 loss 3.954
[13:15:38.213871] Accuracy of the network on the 10000 test images: 20.0%
[13:15:38.214045] [13:15:41.798776] Max accuracy: 19.96%
[13:15:41.799069] [13:15:41.800099] {"train_lr": 0.003354217418417092, "train_loss": 2.8508964549927485, "test_loss": 3.9537954464554788, "test_acc1": 19.96, "test_acc5": 41.53, "epoch": 57, "n_parameters": 85958500}
[13:15:41.800176] [13:15:41.800238] Training epoch 57 for 0:00:34
[13:15:41.800290] [13:15:41.803085] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:15:43.875550] Epoch: [58]  [ 0/42]  eta: 0:01:27  lr: 0.003277  loss: 2.8346 (2.8346)  time: 2.0716  data: 1.5997  max mem: 10046
[13:16:04.082602] Epoch: [58]  [41/42]  eta: 0:00:00  lr: 0.003154  loss: 2.7844 (2.8328)  time: 0.4946  data: 0.0001  max mem: 10046
[13:16:04.321447] Epoch: [58] Total time: 0:00:22 (0.5361 s / it)
[13:16:04.327207] Averaged stats: lr: 0.003154  loss: 2.7844 (2.8422)
[13:16:06.253864] Test:  [ 0/40]  eta: 0:01:16  loss: 4.0566 (4.0566)  acc1: 17.1875 (17.1875)  acc5: 35.9375 (35.9375)  time: 1.9232  data: 1.7457  max mem: 10046
[13:16:12.221685] Test:  [39/40]  eta: 0:00:00  loss: 3.9237 (4.0173)  acc1: 18.7500 (19.2800)  acc5: 42.1875 (41.3200)  time: 0.1497  data: 0.0001  max mem: 10046
[13:16:12.364374] Test: Total time: 0:00:08 (0.2009 s / it)
[13:16:12.452517] * Acc@1 19.270 Acc@5 41.400 loss 4.016
[13:16:12.452698] Accuracy of the network on the 10000 test images: 19.3%
[13:16:12.452889] [13:16:12.452965] Max accuracy: 19.96%
[13:16:12.453022] [13:16:12.453865] {"train_lr": 0.003224077850346362, "train_loss": 2.8422196237813857, "test_loss": 4.016032120585441, "test_acc1": 19.27, "test_acc5": 41.4, "epoch": 58, "n_parameters": 85958500}
[13:16:12.453946] [13:16:12.454010] Training epoch 58 for 0:00:30
[13:16:12.454064] [13:16:12.456991] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:16:14.895728] Epoch: [59]  [ 0/42]  eta: 0:01:42  lr: 0.003147  loss: 2.9903 (2.9903)  time: 2.4379  data: 1.9630  max mem: 10046
[13:16:35.094215] Epoch: [59]  [41/42]  eta: 0:00:00  lr: 0.003025  loss: 2.8252 (2.8137)  time: 0.4940  data: 0.0001  max mem: 10046
[13:16:35.311981] Epoch: [59] Total time: 0:00:22 (0.5442 s / it)
[13:16:35.317195] Averaged stats: lr: 0.003025  loss: 2.8252 (2.8159)
[13:16:37.023575] Test:  [ 0/40]  eta: 0:01:08  loss: 3.9822 (3.9822)  acc1: 18.7500 (18.7500)  acc5: 35.9375 (35.9375)  time: 1.7014  data: 1.5294  max mem: 10046
[13:16:43.091895] Test:  [39/40]  eta: 0:00:00  loss: 3.8723 (3.9944)  acc1: 20.3125 (19.3200)  acc5: 43.7500 (42.4400)  time: 0.1506  data: 0.0001  max mem: 10046
[13:16:43.226366] Test: Total time: 0:00:07 (0.1977 s / it)
[13:16:43.374769] * Acc@1 19.370 Acc@5 42.230 loss 3.981
[13:16:43.374960] Accuracy of the network on the 10000 test images: 19.4%
[13:16:43.375179] [13:16:43.375259] Max accuracy: 19.96%
[13:16:43.375316] [13:16:43.376124] {"train_lr": 0.00309478728849696, "train_loss": 2.81591021163123, "test_loss": 3.981024894118309, "test_acc1": 19.37, "test_acc5": 42.23, "epoch": 59, "n_parameters": 85958500}
[13:16:43.376199] [13:16:43.376259] Training epoch 59 for 0:00:30
[13:16:43.376312] [13:16:43.379165] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:16:45.920903] Epoch: [60]  [ 0/42]  eta: 0:01:46  lr: 0.003019  loss: 2.9419 (2.9419)  time: 2.5408  data: 1.6582  max mem: 10046
[13:17:06.122221] Epoch: [60]  [41/42]  eta: 0:00:00  lr: 0.002897  loss: 2.8125 (2.8097)  time: 0.4935  data: 0.0001  max mem: 10046
[13:17:06.348032] Epoch: [60] Total time: 0:00:22 (0.5469 s / it)
[13:17:06.351535] Averaged stats: lr: 0.002897  loss: 2.8125 (2.8191)
[13:17:08.244908] Test:  [ 0/40]  eta: 0:01:15  loss: 4.0857 (4.0857)  acc1: 17.1875 (17.1875)  acc5: 37.5000 (37.5000)  time: 1.8899  data: 1.7154  max mem: 10046
[13:17:14.265988] Test:  [39/40]  eta: 0:00:00  loss: 3.7542 (3.9538)  acc1: 18.7500 (20.4000)  acc5: 43.7500 (43.2800)  time: 0.1516  data: 0.0001  max mem: 10046
[13:17:14.446516] Test: Total time: 0:00:08 (0.2023 s / it)
[13:17:14.448377] * Acc@1 20.440 Acc@5 43.100 loss 3.930
[13:17:14.448545] Accuracy of the network on the 10000 test images: 20.4%
[13:17:14.448766] [13:17:17.904760] Max accuracy: 20.44%
[13:17:17.905020] [13:17:17.905930] {"train_lr": 0.0029664871102000495, "train_loss": 2.8191231205349876, "test_loss": 3.9300454020500184, "test_acc1": 20.44, "test_acc5": 43.1, "epoch": 60, "n_parameters": 85958500}
[13:17:17.906062] [13:17:17.906128] Training epoch 60 for 0:00:34
[13:17:17.906182] [13:17:17.909245] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:17:19.895252] Epoch: [61]  [ 0/42]  eta: 0:01:23  lr: 0.002891  loss: 2.6816 (2.6816)  time: 1.9850  data: 1.5008  max mem: 10046
[13:17:40.079026] Epoch: [61]  [41/42]  eta: 0:00:00  lr: 0.002771  loss: 2.7894 (2.8017)  time: 0.4939  data: 0.0001  max mem: 10046
[13:17:40.309164] Epoch: [61] Total time: 0:00:22 (0.5333 s / it)
[13:17:40.319114] Averaged stats: lr: 0.002771  loss: 2.7894 (2.8124)
[13:17:42.374594] Test:  [ 0/40]  eta: 0:01:22  loss: 4.2391 (4.2391)  acc1: 15.6250 (15.6250)  acc5: 32.8125 (32.8125)  time: 2.0520  data: 1.8747  max mem: 10046
[13:17:48.353026] Test:  [39/40]  eta: 0:00:00  loss: 3.9836 (4.0625)  acc1: 21.8750 (20.0800)  acc5: 42.1875 (41.4000)  time: 0.1504  data: 0.0001  max mem: 10046
[13:17:48.498246] Test: Total time: 0:00:08 (0.2044 s / it)
[13:17:48.499574] * Acc@1 19.920 Acc@5 41.670 loss 4.041
[13:17:48.499736] Accuracy of the network on the 10000 test images: 19.9%
[13:17:48.499937] [13:17:48.500011] Max accuracy: 20.44%
[13:17:48.500067] [13:17:48.500848] {"train_lr": 0.0028393176098169106, "train_loss": 2.812352939730599, "test_loss": 4.040700808167458, "test_acc1": 19.92, "test_acc5": 41.67, "epoch": 61, "n_parameters": 85958500}
[13:17:48.500917] [13:17:48.500977] Training epoch 61 for 0:00:30
[13:17:48.501028] [13:17:48.503808] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:17:50.863072] Epoch: [62]  [ 0/42]  eta: 0:01:39  lr: 0.002765  loss: 2.7307 (2.7307)  time: 2.3583  data: 1.7577  max mem: 10046
[13:18:11.048476] Epoch: [62]  [41/42]  eta: 0:00:00  lr: 0.002645  loss: 2.8000 (2.8779)  time: 0.4935  data: 0.0001  max mem: 10046
[13:18:11.266440] Epoch: [62] Total time: 0:00:22 (0.5420 s / it)
[13:18:11.267146] Averaged stats: lr: 0.002645  loss: 2.8000 (2.7999)
[13:18:13.398313] Test:  [ 0/40]  eta: 0:01:25  loss: 3.9487 (3.9487)  acc1: 15.6250 (15.6250)  acc5: 39.0625 (39.0625)  time: 2.1279  data: 1.9506  max mem: 10046
[13:18:19.387575] Test:  [39/40]  eta: 0:00:00  loss: 3.8025 (3.9375)  acc1: 23.4375 (21.0000)  acc5: 43.7500 (43.3200)  time: 0.1503  data: 0.0001  max mem: 10046
[13:18:19.523564] Test: Total time: 0:00:08 (0.2064 s / it)
[13:18:19.525329] * Acc@1 20.360 Acc@5 43.450 loss 3.938
[13:18:19.525477] Accuracy of the network on the 10000 test images: 20.4%
[13:18:19.525679] [13:18:19.525755] Max accuracy: 20.44%
[13:18:19.525812] [13:18:19.526588] {"train_lr": 0.002713417845329128, "train_loss": 2.7999103707926616, "test_loss": 3.9379240706562997, "test_acc1": 20.36, "test_acc5": 43.45, "epoch": 62, "n_parameters": 85958500}
[13:18:19.526662] [13:18:19.526719] Training epoch 62 for 0:00:31
[13:18:19.526770] [13:18:19.529570] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:18:22.018113] Epoch: [63]  [ 0/42]  eta: 0:01:44  lr: 0.002639  loss: 3.0440 (3.0440)  time: 2.4878  data: 1.6203  max mem: 10046
[13:18:42.214314] Epoch: [63]  [41/42]  eta: 0:00:00  lr: 0.002522  loss: 2.9011 (2.8780)  time: 0.4936  data: 0.0001  max mem: 10046
[13:18:42.453943] Epoch: [63] Total time: 0:00:22 (0.5458 s / it)
[13:18:42.454650] Averaged stats: lr: 0.002522  loss: 2.9011 (2.8051)
[13:18:44.421636] Test:  [ 0/40]  eta: 0:01:18  loss: 4.1495 (4.1495)  acc1: 20.3125 (20.3125)  acc5: 35.9375 (35.9375)  time: 1.9635  data: 1.7861  max mem: 10046
[13:18:50.425530] Test:  [39/40]  eta: 0:00:00  loss: 3.7875 (3.9577)  acc1: 21.8750 (20.3600)  acc5: 43.7500 (43.7600)  time: 0.1506  data: 0.0001  max mem: 10046
[13:18:50.586965] Test: Total time: 0:00:08 (0.2032 s / it)
[13:18:50.588193] * Acc@1 20.430 Acc@5 43.440 loss 3.953
[13:18:50.588350] Accuracy of the network on the 10000 test images: 20.4%
[13:18:50.588583] [13:18:50.588659] Max accuracy: 20.44%
[13:18:50.588716] [13:18:50.589521] {"train_lr": 0.00258892548628073, "train_loss": 2.8051395487217676, "test_loss": 3.9532201647758485, "test_acc1": 20.43, "test_acc5": 43.44, "epoch": 63, "n_parameters": 85958500}
[13:18:50.589599] [13:18:50.589671] Training epoch 63 for 0:00:31
[13:18:50.589726] [13:18:50.592657] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:18:52.797884] Epoch: [64]  [ 0/42]  eta: 0:01:32  lr: 0.002516  loss: 2.6802 (2.6802)  time: 2.2041  data: 1.6986  max mem: 10046
[13:19:13.022201] Epoch: [64]  [41/42]  eta: 0:00:00  lr: 0.002400  loss: 2.7992 (2.7560)  time: 0.4951  data: 0.0001  max mem: 10046
[13:19:13.239539] Epoch: [64] Total time: 0:00:22 (0.5392 s / it)
[13:19:13.240273] Averaged stats: lr: 0.002400  loss: 2.7992 (2.7656)
[13:19:15.021980] Test:  [ 0/40]  eta: 0:01:11  loss: 4.1326 (4.1326)  acc1: 18.7500 (18.7500)  acc5: 37.5000 (37.5000)  time: 1.7775  data: 1.6210  max mem: 10046
[13:19:21.026007] Test:  [39/40]  eta: 0:00:00  loss: 3.8070 (3.9524)  acc1: 23.4375 (21.8800)  acc5: 43.7500 (43.9200)  time: 0.1510  data: 0.0001  max mem: 10046
[13:19:21.180937] Test: Total time: 0:00:07 (0.1985 s / it)
[13:19:21.410252] * Acc@1 20.880 Acc@5 43.660 loss 3.948
[13:19:21.410466] Accuracy of the network on the 10000 test images: 20.9%
[13:19:21.410695] [13:19:25.006985] Max accuracy: 20.88%
[13:19:25.007302] [13:19:25.008328] {"train_lr": 0.0024659766632385543, "train_loss": 2.765563483749117, "test_loss": 3.9481912180781364, "test_acc1": 20.88, "test_acc5": 43.66, "epoch": 64, "n_parameters": 85958500}
[13:19:25.008410] [13:19:25.008477] Training epoch 64 for 0:00:34
[13:19:25.008545] [13:19:25.011278] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:19:27.115814] Epoch: [65]  [ 0/42]  eta: 0:01:28  lr: 0.002394  loss: 2.8868 (2.8868)  time: 2.1038  data: 1.6255  max mem: 10046
[13:19:47.298123] Epoch: [65]  [41/42]  eta: 0:00:00  lr: 0.002279  loss: 2.7681 (2.8159)  time: 0.4944  data: 0.0001  max mem: 10046
[13:19:47.530250] Epoch: [65] Total time: 0:00:22 (0.5362 s / it)
[13:19:47.531578] Averaged stats: lr: 0.002279  loss: 2.7681 (2.7821)
[13:19:49.230496] Test:  [ 0/40]  eta: 0:01:07  loss: 4.0792 (4.0792)  acc1: 17.1875 (17.1875)  acc5: 34.3750 (34.3750)  time: 1.6945  data: 1.5381  max mem: 10046
[13:19:55.207060] Test:  [39/40]  eta: 0:00:00  loss: 3.8400 (3.9501)  acc1: 20.3125 (20.0400)  acc5: 42.1875 (42.9600)  time: 0.1502  data: 0.0001  max mem: 10046
[13:19:55.362479] Test: Total time: 0:00:07 (0.1957 s / it)
[13:19:55.494207] * Acc@1 20.050 Acc@5 42.870 loss 3.949
[13:19:55.494382] Accuracy of the network on the 10000 test images: 20.1%
[13:19:55.494563] [13:19:55.494635] Max accuracy: 20.88%
[13:19:55.494690] [13:19:55.495458] {"train_lr": 0.0023447058189354796, "train_loss": 2.7821369682039534, "test_loss": 3.949243289232254, "test_acc1": 20.05, "test_acc5": 42.87, "epoch": 65, "n_parameters": 85958500}
[13:19:55.495528] [13:19:55.495589] Training epoch 65 for 0:00:30
[13:19:55.495640] [13:19:55.498467] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:19:57.976858] Epoch: [66]  [ 0/42]  eta: 0:01:44  lr: 0.002274  loss: 2.6725 (2.6725)  time: 2.4774  data: 1.8258  max mem: 10046
[13:20:18.177080] Epoch: [66]  [41/42]  eta: 0:00:00  lr: 0.002161  loss: 2.8407 (2.7783)  time: 0.4939  data: 0.0001  max mem: 10046
[13:20:18.390007] Epoch: [66] Total time: 0:00:22 (0.5450 s / it)
[13:20:18.398958] Averaged stats: lr: 0.002161  loss: 2.8407 (2.7784)
[13:20:20.342073] Test:  [ 0/40]  eta: 0:01:17  loss: 4.0835 (4.0835)  acc1: 15.6250 (15.6250)  acc5: 39.0625 (39.0625)  time: 1.9398  data: 1.7624  max mem: 10046
[13:20:26.344814] Test:  [39/40]  eta: 0:00:00  loss: 3.9207 (4.0511)  acc1: 20.3125 (20.0400)  acc5: 42.1875 (42.6800)  time: 0.1506  data: 0.0001  max mem: 10046
[13:20:26.524787] Test: Total time: 0:00:08 (0.2031 s / it)
[13:20:26.526148] * Acc@1 20.330 Acc@5 42.610 loss 4.055
[13:20:26.526297] Accuracy of the network on the 10000 test images: 20.3%
[13:20:26.526533] [13:20:26.526609] Max accuracy: 20.88%
[13:20:26.526664] [13:20:26.527482] {"train_lr": 0.00222524556125928, "train_loss": 2.7784450309617177, "test_loss": 4.0552137911319734, "test_acc1": 20.33, "test_acc5": 42.61, "epoch": 66, "n_parameters": 85958500}
[13:20:26.527560] [13:20:26.527620] Training epoch 66 for 0:00:31
[13:20:26.527672] [13:20:26.530492] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:20:29.060312] Epoch: [67]  [ 0/42]  eta: 0:01:46  lr: 0.002155  loss: 2.7874 (2.7874)  time: 2.5285  data: 1.6725  max mem: 10046
[13:20:49.273026] Epoch: [67]  [41/42]  eta: 0:00:00  lr: 0.002045  loss: 2.7263 (2.7552)  time: 0.4945  data: 0.0001  max mem: 10046
[13:20:49.492932] Epoch: [67] Total time: 0:00:22 (0.5467 s / it)
[13:20:49.503480] Averaged stats: lr: 0.002045  loss: 2.7263 (2.7524)
[13:20:51.346430] Test:  [ 0/40]  eta: 0:01:13  loss: 4.1550 (4.1550)  acc1: 17.1875 (17.1875)  acc5: 32.8125 (32.8125)  time: 1.8378  data: 1.6673  max mem: 10046
[13:20:57.337716] Test:  [39/40]  eta: 0:00:00  loss: 3.8389 (3.9425)  acc1: 18.7500 (20.4800)  acc5: 45.3125 (43.2000)  time: 0.1503  data: 0.0001  max mem: 10046
[13:20:57.487281] Test: Total time: 0:00:07 (0.1995 s / it)
[13:20:57.589306] * Acc@1 20.820 Acc@5 43.090 loss 3.931
[13:20:57.589538] Accuracy of the network on the 10000 test images: 20.8%
[13:20:57.589773] [13:20:57.589856] Max accuracy: 20.88%
[13:20:57.589914] [13:20:57.590768] {"train_lr": 0.002107726518247815, "train_loss": 2.752416265862329, "test_loss": 3.9308640852570536, "test_acc1": 20.82, "test_acc5": 43.09, "epoch": 67, "n_parameters": 85958500}
[13:20:57.590839] [13:20:57.590901] Training epoch 67 for 0:00:31
[13:20:57.590954] [13:20:57.593810] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:20:59.766384] Epoch: [68]  [ 0/42]  eta: 0:01:31  lr: 0.002039  loss: 2.7044 (2.7044)  time: 2.1705  data: 1.6662  max mem: 10046
[13:21:20.015894] Epoch: [68]  [41/42]  eta: 0:00:00  lr: 0.001930  loss: 2.6594 (2.7317)  time: 0.4955  data: 0.0001  max mem: 10046
[13:21:20.251241] Epoch: [68] Total time: 0:00:22 (0.5395 s / it)
[13:21:20.254304] Averaged stats: lr: 0.001930  loss: 2.6594 (2.7359)
[13:21:22.076107] Test:  [ 0/40]  eta: 0:01:12  loss: 4.3577 (4.3577)  acc1: 14.0625 (14.0625)  acc5: 35.9375 (35.9375)  time: 1.8179  data: 1.6398  max mem: 10046
[13:21:28.092322] Test:  [39/40]  eta: 0:00:00  loss: 3.9545 (4.0663)  acc1: 23.4375 (22.1600)  acc5: 45.3125 (43.8400)  time: 0.1511  data: 0.0001  max mem: 10046
[13:21:28.238235] Test: Total time: 0:00:07 (0.1995 s / it)
[13:21:28.405935] * Acc@1 21.540 Acc@5 43.740 loss 4.048
[13:21:28.406161] Accuracy of the network on the 10000 test images: 21.5%
[13:21:28.406429] [13:21:31.839378] Max accuracy: 21.54%
[13:21:31.839657] [13:21:31.840668] {"train_lr": 0.0019922771952492306, "train_loss": 2.7359222826503573, "test_loss": 4.048114129900933, "test_acc1": 21.54, "test_acc5": 43.74, "epoch": 68, "n_parameters": 85958500}
[13:21:31.840743] [13:21:31.840807] Training epoch 68 for 0:00:34
[13:21:31.840876] [13:21:31.843646] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:21:34.076575] Epoch: [69]  [ 0/42]  eta: 0:01:33  lr: 0.001925  loss: 2.7042 (2.7042)  time: 2.2322  data: 1.7505  max mem: 10046
[13:21:54.210380] Epoch: [69]  [41/42]  eta: 0:00:00  lr: 0.001818  loss: 2.8177 (2.7472)  time: 0.4919  data: 0.0001  max mem: 10046
[13:21:54.437447] Epoch: [69] Total time: 0:00:22 (0.5379 s / it)
[13:21:54.439686] Averaged stats: lr: 0.001818  loss: 2.8177 (2.7524)
[13:21:56.408624] Test:  [ 0/40]  eta: 0:01:18  loss: 4.1296 (4.1296)  acc1: 15.6250 (15.6250)  acc5: 35.9375 (35.9375)  time: 1.9644  data: 1.7872  max mem: 10046
[13:22:02.376530] Test:  [39/40]  eta: 0:00:00  loss: 3.8785 (4.0234)  acc1: 18.7500 (19.5600)  acc5: 42.1875 (42.2800)  time: 0.1501  data: 0.0001  max mem: 10046
[13:22:02.514858] Test: Total time: 0:00:08 (0.2018 s / it)
[13:22:02.706119] * Acc@1 20.050 Acc@5 42.650 loss 3.992
[13:22:02.706318] Accuracy of the network on the 10000 test images: 20.1%
[13:22:02.706546] [13:22:02.706637] Max accuracy: 21.54%
[13:22:02.706696] [13:22:02.707481] {"train_lr": 0.0018790238344032094, "train_loss": 2.752351773636682, "test_loss": 3.992303931713104, "test_acc1": 20.05, "test_acc5": 42.65, "epoch": 69, "n_parameters": 85958500}
[13:22:02.707551] [13:22:02.707606] Training epoch 69 for 0:00:30
[13:22:02.707658] [13:22:02.710597] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:22:05.323307] Epoch: [70]  [ 0/42]  eta: 0:01:49  lr: 0.001813  loss: 2.5981 (2.5981)  time: 2.6119  data: 1.7656  max mem: 10046
[13:22:25.504488] Epoch: [70]  [41/42]  eta: 0:00:00  lr: 0.001709  loss: 2.6524 (2.7058)  time: 0.4938  data: 0.0001  max mem: 10046
[13:22:25.717213] Epoch: [70] Total time: 0:00:23 (0.5478 s / it)
[13:22:25.736585] Averaged stats: lr: 0.001709  loss: 2.6524 (2.7269)
[13:22:27.634704] Test:  [ 0/40]  eta: 0:01:15  loss: 4.0375 (4.0375)  acc1: 18.7500 (18.7500)  acc5: 39.0625 (39.0625)  time: 1.8946  data: 1.7158  max mem: 10046
[13:22:33.618686] Test:  [39/40]  eta: 0:00:00  loss: 3.8193 (3.9412)  acc1: 21.8750 (21.1600)  acc5: 45.3125 (45.3600)  time: 0.1505  data: 0.0001  max mem: 10046
[13:22:33.775334] Test: Total time: 0:00:08 (0.2009 s / it)
[13:22:33.776838] * Acc@1 21.420 Acc@5 44.570 loss 3.936
[13:22:33.776999] Accuracy of the network on the 10000 test images: 21.4%
[13:22:33.777217] [13:22:33.777293] Max accuracy: 21.54%
[13:22:33.777352] [13:22:33.778137] {"train_lr": 0.0017680902765970672, "train_loss": 2.726936351685297, "test_loss": 3.9359985083341598, "test_acc1": 21.42, "test_acc5": 44.57, "epoch": 70, "n_parameters": 85958500}
[13:22:33.778208] [13:22:33.778268] Training epoch 70 for 0:00:31
[13:22:33.778319] [13:22:33.781117] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:22:36.032005] Epoch: [71]  [ 0/42]  eta: 0:01:34  lr: 0.001703  loss: 2.7986 (2.7986)  time: 2.2498  data: 1.7667  max mem: 10046
[13:22:56.375809] Epoch: [71]  [41/42]  eta: 0:00:00  lr: 0.001602  loss: 2.6837 (2.6932)  time: 0.4968  data: 0.0001  max mem: 10046
[13:22:56.608037] Epoch: [71] Total time: 0:00:22 (0.5435 s / it)
[13:22:56.608793] Averaged stats: lr: 0.001602  loss: 2.6837 (2.7146)
[13:22:58.581432] Test:  [ 0/40]  eta: 0:01:18  loss: 4.1270 (4.1270)  acc1: 15.6250 (15.6250)  acc5: 37.5000 (37.5000)  time: 1.9694  data: 1.7888  max mem: 10046
[13:23:04.586637] Test:  [39/40]  eta: 0:00:00  loss: 3.9305 (4.0465)  acc1: 20.3125 (20.3600)  acc5: 42.1875 (43.6400)  time: 0.1508  data: 0.0001  max mem: 10046
[13:23:04.725231] Test: Total time: 0:00:08 (0.2029 s / it)
[13:23:04.726540] * Acc@1 20.370 Acc@5 43.180 loss 4.036
[13:23:04.726684] Accuracy of the network on the 10000 test images: 20.4%
[13:23:04.726882] [13:23:04.726957] Max accuracy: 21.54%
[13:23:04.727014] [13:23:04.727774] {"train_lr": 0.0016595978260475585, "train_loss": 2.7146135611193523, "test_loss": 4.036196584999561, "test_acc1": 20.37, "test_acc5": 43.18, "epoch": 71, "n_parameters": 85958500}
[13:23:04.727844] [13:23:04.727905] Training epoch 71 for 0:00:30
[13:23:04.727957] [13:23:04.730837] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:23:07.216169] Epoch: [72]  [ 0/42]  eta: 0:01:44  lr: 0.001596  loss: 2.4345 (2.4345)  time: 2.4846  data: 2.0015  max mem: 10046
[13:23:27.406985] Epoch: [72]  [41/42]  eta: 0:00:00  lr: 0.001497  loss: 2.7765 (2.7350)  time: 0.4935  data: 0.0001  max mem: 10046
[13:23:27.629557] Epoch: [72] Total time: 0:00:22 (0.5452 s / it)
[13:23:27.630386] Averaged stats: lr: 0.001497  loss: 2.7765 (2.7061)
[13:23:29.596011] Test:  [ 0/40]  eta: 0:01:18  loss: 4.2826 (4.2826)  acc1: 18.7500 (18.7500)  acc5: 39.0625 (39.0625)  time: 1.9623  data: 1.7872  max mem: 10046
[13:23:35.586248] Test:  [39/40]  eta: 0:00:00  loss: 3.9788 (4.1074)  acc1: 21.8750 (21.0800)  acc5: 43.7500 (44.8800)  time: 0.1504  data: 0.0001  max mem: 10046
[13:23:35.699966] Test: Total time: 0:00:08 (0.2017 s / it)
[13:23:35.701258] * Acc@1 21.330 Acc@5 44.060 loss 4.099
[13:23:35.701435] Accuracy of the network on the 10000 test images: 21.3%
[13:23:35.701649] [13:23:35.701721] Max accuracy: 21.54%
[13:23:35.701790] [13:23:35.702633] {"train_lr": 0.0015536651176564875, "train_loss": 2.7061303612731753, "test_loss": 4.098927074670792, "test_acc1": 21.33, "test_acc5": 44.06, "epoch": 72, "n_parameters": 85958500}
[13:23:35.702711] [13:23:35.702773] Training epoch 72 for 0:00:30
[13:23:35.702826] [13:23:35.705770] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:23:38.136624] Epoch: [73]  [ 0/42]  eta: 0:01:42  lr: 0.001492  loss: 2.2954 (2.2954)  time: 2.4297  data: 1.9550  max mem: 10046
[13:23:58.335097] Epoch: [73]  [41/42]  eta: 0:00:00  lr: 0.001395  loss: 2.7116 (2.7248)  time: 0.4931  data: 0.0001  max mem: 10046
[13:23:58.550400] Epoch: [73] Total time: 0:00:22 (0.5439 s / it)
[13:23:58.555247] Averaged stats: lr: 0.001395  loss: 2.7116 (2.7084)
[13:24:00.239103] Test:  [ 0/40]  eta: 0:01:07  loss: 4.0298 (4.0298)  acc1: 14.0625 (14.0625)  acc5: 40.6250 (40.6250)  time: 1.6779  data: 1.5178  max mem: 10046
[13:24:06.453811] Test:  [39/40]  eta: 0:00:00  loss: 3.8926 (4.0076)  acc1: 21.8750 (20.7600)  acc5: 43.7500 (44.1200)  time: 0.1510  data: 0.0001  max mem: 10046
[13:24:06.593976] Test: Total time: 0:00:08 (0.2009 s / it)
[13:24:06.595359] * Acc@1 21.200 Acc@5 43.940 loss 3.997
[13:24:06.595508] Accuracy of the network on the 10000 test images: 21.2%
[13:24:06.595725] [13:24:06.595807] Max accuracy: 21.54%
[13:24:06.595867] [13:24:06.596682] {"train_lr": 0.0014504079872851914, "train_loss": 2.7084071778115772, "test_loss": 3.997253839671612, "test_acc1": 21.2, "test_acc5": 43.94, "epoch": 73, "n_parameters": 85958500}
[13:24:06.596756] [13:24:06.596818] Training epoch 73 for 0:00:30
[13:24:06.596874] [13:24:06.600424] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:24:08.735035] Epoch: [74]  [ 0/42]  eta: 0:01:29  lr: 0.001390  loss: 2.6906 (2.6906)  time: 2.1336  data: 1.6486  max mem: 10046
[13:24:28.988028] Epoch: [74]  [41/42]  eta: 0:00:00  lr: 0.001296  loss: 2.6585 (2.7196)  time: 0.4944  data: 0.0001  max mem: 10046
[13:24:29.210781] Epoch: [74] Total time: 0:00:22 (0.5383 s / it)
[13:24:29.217413] Averaged stats: lr: 0.001296  loss: 2.6585 (2.6946)
[13:24:31.153696] Test:  [ 0/40]  eta: 0:01:17  loss: 4.1348 (4.1348)  acc1: 14.0625 (14.0625)  acc5: 37.5000 (37.5000)  time: 1.9329  data: 1.7554  max mem: 10046
[13:24:37.168231] Test:  [39/40]  eta: 0:00:00  loss: 3.8739 (4.0128)  acc1: 20.3125 (20.6400)  acc5: 43.7500 (44.1600)  time: 0.1506  data: 0.0001  max mem: 10046
[13:24:37.291585] Test: Total time: 0:00:08 (0.2018 s / it)
[13:24:37.455911] * Acc@1 21.240 Acc@5 44.050 loss 4.000
[13:24:37.456106] Accuracy of the network on the 10000 test images: 21.2%
[13:24:37.456350] [13:24:37.456456] Max accuracy: 21.54%
[13:24:37.456514] [13:24:37.457344] {"train_lr": 0.0013499393450897242, "train_loss": 2.6946488476934887, "test_loss": 3.9997957915067675, "test_acc1": 21.24, "test_acc5": 44.05, "epoch": 74, "n_parameters": 85958500}
[13:24:37.457414] [13:24:37.457474] Training epoch 74 for 0:00:30
[13:24:37.457527] [13:24:37.460352] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:24:39.784185] Epoch: [75]  [ 0/42]  eta: 0:01:37  lr: 0.001292  loss: 2.5291 (2.5291)  time: 2.3230  data: 1.6779  max mem: 10046
[13:25:00.032458] Epoch: [75]  [41/42]  eta: 0:00:00  lr: 0.001200  loss: 2.6913 (2.6509)  time: 0.4942  data: 0.0001  max mem: 10046
[13:25:00.264384] Epoch: [75] Total time: 0:00:22 (0.5430 s / it)
[13:25:00.265118] Averaged stats: lr: 0.001200  loss: 2.6913 (2.6782)
[13:25:02.361362] Test:  [ 0/40]  eta: 0:01:23  loss: 4.2180 (4.2180)  acc1: 20.3125 (20.3125)  acc5: 39.0625 (39.0625)  time: 2.0929  data: 1.9107  max mem: 10046
[13:25:08.355619] Test:  [39/40]  eta: 0:00:00  loss: 3.8650 (4.0724)  acc1: 21.8750 (22.1200)  acc5: 45.3125 (44.8000)  time: 0.1507  data: 0.0001  max mem: 10046
[13:25:08.490325] Test: Total time: 0:00:08 (0.2056 s / it)
[13:25:08.491628] * Acc@1 21.980 Acc@5 44.830 loss 4.050
[13:25:08.491767] Accuracy of the network on the 10000 test images: 22.0%
[13:25:08.491976] [13:25:12.080827] Max accuracy: 21.98%
[13:25:12.081169] [13:25:12.082204] {"train_lr": 0.001252369052055264, "train_loss": 2.67819021713166, "test_loss": 4.0501430332660675, "test_acc1": 21.98, "test_acc5": 44.83, "epoch": 75, "n_parameters": 85958500}
[13:25:12.082297] [13:25:12.082361] Training epoch 75 for 0:00:34
[13:25:12.082413] [13:25:12.085305] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:25:14.227371] Epoch: [76]  [ 0/42]  eta: 0:01:29  lr: 0.001196  loss: 2.2298 (2.2298)  time: 2.1413  data: 1.6558  max mem: 10046
[13:25:34.341635] Epoch: [76]  [41/42]  eta: 0:00:00  lr: 0.001108  loss: 2.6059 (2.6822)  time: 0.4922  data: 0.0001  max mem: 10046
[13:25:34.578881] Epoch: [76] Total time: 0:00:22 (0.5356 s / it)
[13:25:34.579732] Averaged stats: lr: 0.001108  loss: 2.6059 (2.6651)
[13:25:36.370959] Test:  [ 0/40]  eta: 0:01:11  loss: 4.1394 (4.1394)  acc1: 17.1875 (17.1875)  acc5: 39.0625 (39.0625)  time: 1.7861  data: 1.6080  max mem: 10046
[13:25:42.351355] Test:  [39/40]  eta: 0:00:00  loss: 3.8864 (4.0240)  acc1: 23.4375 (22.0400)  acc5: 45.3125 (45.0000)  time: 0.1504  data: 0.0001  max mem: 10046
[13:25:42.499553] Test: Total time: 0:00:07 (0.1979 s / it)
[13:25:42.701659] * Acc@1 21.570 Acc@5 44.190 loss 4.013
[13:25:42.701870] Accuracy of the network on the 10000 test images: 21.6%
[13:25:42.702102] [13:25:42.702187] Max accuracy: 21.98%
[13:25:42.702247] [13:25:42.703098] {"train_lr": 0.001157803799864728, "train_loss": 2.6651093108313426, "test_loss": 4.013342098891735, "test_acc1": 21.57, "test_acc5": 44.19, "epoch": 76, "n_parameters": 85958500}
[13:25:42.703173] [13:25:42.703238] Training epoch 76 for 0:00:30
[13:25:42.703293] [13:25:42.706999] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:25:45.117185] Epoch: [77]  [ 0/42]  eta: 0:01:41  lr: 0.001103  loss: 2.6810 (2.6810)  time: 2.4094  data: 1.9309  max mem: 10046
[13:26:05.335223] Epoch: [77]  [41/42]  eta: 0:00:00  lr: 0.001018  loss: 2.6868 (2.6444)  time: 0.4941  data: 0.0001  max mem: 10046
[13:26:05.562744] Epoch: [77] Total time: 0:00:22 (0.5442 s / it)
[13:26:05.563524] Averaged stats: lr: 0.001018  loss: 2.6868 (2.6635)
[13:26:07.312433] Test:  [ 0/40]  eta: 0:01:09  loss: 4.1367 (4.1367)  acc1: 17.1875 (17.1875)  acc5: 40.6250 (40.6250)  time: 1.7446  data: 1.5835  max mem: 10046
[13:26:13.312803] Test:  [39/40]  eta: 0:00:00  loss: 3.8464 (4.0287)  acc1: 23.4375 (21.6800)  acc5: 46.8750 (45.5200)  time: 0.1504  data: 0.0001  max mem: 10046
[13:26:13.452955] Test: Total time: 0:00:07 (0.1972 s / it)
[13:26:13.527679] * Acc@1 21.760 Acc@5 45.110 loss 4.001
[13:26:13.527897] Accuracy of the network on the 10000 test images: 21.8%
[13:26:13.528138] [13:26:13.528215] Max accuracy: 21.98%
[13:26:13.528272] [13:26:13.529084] {"train_lr": 0.0010663469942329945, "train_loss": 2.6635337358429316, "test_loss": 4.000954516232014, "test_acc1": 21.76, "test_acc5": 45.11, "epoch": 77, "n_parameters": 85958500}
[13:26:13.529156] [13:26:13.529230] Training epoch 77 for 0:00:30
[13:26:13.529284] [13:26:13.532072] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:26:15.970604] Epoch: [78]  [ 0/42]  eta: 0:01:42  lr: 0.001014  loss: 2.6042 (2.6042)  time: 2.4372  data: 1.9541  max mem: 10046
[13:26:36.223153] Epoch: [78]  [41/42]  eta: 0:00:00  lr: 0.000931  loss: 2.6876 (2.7093)  time: 0.4956  data: 0.0001  max mem: 10046
[13:26:36.443277] Epoch: [78] Total time: 0:00:22 (0.5455 s / it)
[13:26:36.448188] Averaged stats: lr: 0.000931  loss: 2.6876 (2.6910)
[13:26:38.239044] Test:  [ 0/40]  eta: 0:01:11  loss: 4.2268 (4.2268)  acc1: 18.7500 (18.7500)  acc5: 37.5000 (37.5000)  time: 1.7869  data: 1.6053  max mem: 10046
[13:26:44.261705] Test:  [39/40]  eta: 0:00:00  loss: 3.8816 (4.0434)  acc1: 21.8750 (21.5200)  acc5: 43.7500 (44.5600)  time: 0.1509  data: 0.0001  max mem: 10046
[13:26:44.445851] Test: Total time: 0:00:07 (0.1999 s / it)
[13:26:44.447113] * Acc@1 21.510 Acc@5 44.060 loss 4.033
[13:26:44.447258] Accuracy of the network on the 10000 test images: 21.5%
[13:26:44.447456] [13:26:44.447533] Max accuracy: 21.98%
[13:26:44.447590] [13:26:44.448373] {"train_lr": 0.0009780986418342695, "train_loss": 2.6910490961301896, "test_loss": 4.033137609064579, "test_acc1": 21.51, "test_acc5": 44.06, "epoch": 78, "n_parameters": 85958500}
[13:26:44.448458] [13:26:44.448517] Training epoch 78 for 0:00:30
[13:26:44.448569] [13:26:44.451372] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:26:46.965888] Epoch: [79]  [ 0/42]  eta: 0:01:45  lr: 0.000927  loss: 2.6710 (2.6710)  time: 2.5137  data: 2.0337  max mem: 10046
[13:27:07.172065] Epoch: [79]  [41/42]  eta: 0:00:00  lr: 0.000848  loss: 2.5398 (2.6171)  time: 0.4942  data: 0.0001  max mem: 10046
[13:27:07.390254] Epoch: [79] Total time: 0:00:22 (0.5462 s / it)
[13:27:07.390984] Averaged stats: lr: 0.000848  loss: 2.5398 (2.6671)
[13:27:09.317348] Test:  [ 0/40]  eta: 0:01:16  loss: 4.1465 (4.1465)  acc1: 18.7500 (18.7500)  acc5: 39.0625 (39.0625)  time: 1.9228  data: 1.7455  max mem: 10046
[13:27:15.310513] Test:  [39/40]  eta: 0:00:00  loss: 3.9266 (4.0257)  acc1: 21.8750 (21.6400)  acc5: 45.3125 (45.0000)  time: 0.1505  data: 0.0001  max mem: 10046
[13:27:15.457068] Test: Total time: 0:00:08 (0.2016 s / it)
[13:27:15.565590] * Acc@1 21.600 Acc@5 44.340 loss 4.022
[13:27:15.565786] Accuracy of the network on the 10000 test images: 21.6%
[13:27:15.565985] [13:27:15.566061] Max accuracy: 21.98%
[13:27:15.566116] [13:27:15.566884] {"train_lr": 0.0008931552409462468, "train_loss": 2.6670623152028945, "test_loss": 4.0221390798687935, "test_acc1": 21.6, "test_acc5": 44.34, "epoch": 79, "n_parameters": 85958500}
[13:27:15.566953] [13:27:15.567012] Training epoch 79 for 0:00:31
[13:27:15.567064] [13:27:15.569931] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:27:17.878078] Epoch: [80]  [ 0/42]  eta: 0:01:36  lr: 0.000844  loss: 2.9026 (2.9026)  time: 2.3066  data: 1.8185  max mem: 10046
[13:27:38.143748] Epoch: [80]  [41/42]  eta: 0:00:00  lr: 0.000769  loss: 2.6061 (2.6315)  time: 0.4961  data: 0.0001  max mem: 10046
[13:27:38.361626] Epoch: [80] Total time: 0:00:22 (0.5427 s / it)
[13:27:38.362327] Averaged stats: lr: 0.000769  loss: 2.6061 (2.6409)
[13:27:40.279770] Test:  [ 0/40]  eta: 0:01:16  loss: 4.0742 (4.0742)  acc1: 18.7500 (18.7500)  acc5: 40.6250 (40.6250)  time: 1.9141  data: 1.7359  max mem: 10046
[13:27:46.307671] Test:  [39/40]  eta: 0:00:00  loss: 3.8252 (3.9917)  acc1: 21.8750 (21.6400)  acc5: 46.8750 (45.5200)  time: 0.1515  data: 0.0001  max mem: 10046
[13:27:46.443671] Test: Total time: 0:00:08 (0.2020 s / it)
[13:27:46.445017] * Acc@1 22.040 Acc@5 44.910 loss 3.987
[13:27:46.445192] Accuracy of the network on the 10000 test images: 22.0%
[13:27:46.445381] [13:27:50.165689] Max accuracy: 22.04%
[13:27:50.165957] [13:27:50.166842] {"train_lr": 0.0008116096759306745, "train_loss": 2.6409289283411845, "test_loss": 3.986625204980373, "test_acc1": 22.04, "test_acc5": 44.91, "epoch": 80, "n_parameters": 85958500}
[13:27:50.166922] [13:27:50.166988] Training epoch 80 for 0:00:34
[13:27:50.167041] [13:27:50.170066] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:27:52.106679] Epoch: [81]  [ 0/42]  eta: 0:01:21  lr: 0.000765  loss: 2.8349 (2.8349)  time: 1.9356  data: 1.4531  max mem: 10046
[13:28:12.259452] Epoch: [81]  [41/42]  eta: 0:00:00  lr: 0.000692  loss: 2.6584 (2.5998)  time: 0.4933  data: 0.0001  max mem: 10046
[13:28:12.500788] Epoch: [81] Total time: 0:00:22 (0.5317 s / it)
[13:28:12.501680] Averaged stats: lr: 0.000692  loss: 2.6584 (2.6513)
[13:28:14.540347] Test:  [ 0/40]  eta: 0:01:21  loss: 4.0789 (4.0789)  acc1: 15.6250 (15.6250)  acc5: 43.7500 (43.7500)  time: 2.0354  data: 1.8578  max mem: 10046
[13:28:20.530236] Test:  [39/40]  eta: 0:00:00  loss: 3.9547 (4.0737)  acc1: 25.0000 (21.8800)  acc5: 46.8750 (45.6400)  time: 0.1506  data: 0.0001  max mem: 10046
[13:28:20.664605] Test: Total time: 0:00:08 (0.2040 s / it)
[13:28:20.666000] * Acc@1 21.740 Acc@5 44.530 loss 4.064
[13:28:20.666161] Accuracy of the network on the 10000 test images: 21.7%
[13:28:20.666340] [13:28:20.666425] Max accuracy: 22.04%
[13:28:20.666494] [13:28:20.667252] {"train_lr": 0.000733551115665655, "train_loss": 2.651292702981404, "test_loss": 4.063977153599263, "test_acc1": 21.74, "test_acc5": 44.53, "epoch": 81, "n_parameters": 85958500}
[13:28:20.667343] [13:28:20.667408] Training epoch 81 for 0:00:30
[13:28:20.667469] [13:28:20.670911] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:28:23.240752] Epoch: [82]  [ 0/42]  eta: 0:01:47  lr: 0.000689  loss: 2.7303 (2.7303)  time: 2.5686  data: 1.9022  max mem: 10046
[13:28:43.472880] Epoch: [82]  [41/42]  eta: 0:00:00  lr: 0.000620  loss: 2.6177 (2.6522)  time: 0.4948  data: 0.0001  max mem: 10046
[13:28:43.693820] Epoch: [82] Total time: 0:00:23 (0.5482 s / it)
[13:28:43.704372] Averaged stats: lr: 0.000620  loss: 2.6177 (2.6572)
[13:28:45.557670] Test:  [ 0/40]  eta: 0:01:13  loss: 4.1020 (4.1020)  acc1: 18.7500 (18.7500)  acc5: 37.5000 (37.5000)  time: 1.8498  data: 1.6717  max mem: 10046
[13:28:51.549880] Test:  [39/40]  eta: 0:00:00  loss: 3.7891 (4.0004)  acc1: 21.8750 (21.7200)  acc5: 46.8750 (45.0000)  time: 0.1501  data: 0.0001  max mem: 10046
[13:28:51.698426] Test: Total time: 0:00:07 (0.1998 s / it)
[13:28:51.699684] * Acc@1 21.800 Acc@5 44.260 loss 3.988
[13:28:51.699819] Accuracy of the network on the 10000 test images: 21.8%
[13:28:51.699999] [13:28:51.700074] Max accuracy: 22.04%
[13:28:51.700138] [13:28:51.700943] {"train_lr": 0.0006590649160407968, "train_loss": 2.6572386764344715, "test_loss": 3.9882764026522635, "test_acc1": 21.8, "test_acc5": 44.26, "epoch": 82, "n_parameters": 85958500}
[13:28:51.701022] [13:28:51.701082] Training epoch 82 for 0:00:31
[13:28:51.701135] [13:28:51.703907] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:28:54.201572] Epoch: [83]  [ 0/42]  eta: 0:01:44  lr: 0.000617  loss: 2.5705 (2.5705)  time: 2.4966  data: 2.0146  max mem: 10046
[13:29:14.480160] Epoch: [83]  [41/42]  eta: 0:00:00  lr: 0.000551  loss: 2.6507 (2.5989)  time: 0.4958  data: 0.0001  max mem: 10046
[13:29:14.688843] Epoch: [83] Total time: 0:00:22 (0.5473 s / it)
[13:29:14.689555] Averaged stats: lr: 0.000551  loss: 2.6507 (2.6613)
[13:29:16.609042] Test:  [ 0/40]  eta: 0:01:16  loss: 4.1100 (4.1100)  acc1: 18.7500 (18.7500)  acc5: 37.5000 (37.5000)  time: 1.9161  data: 1.7384  max mem: 10046
[13:29:22.599873] Test:  [39/40]  eta: 0:00:00  loss: 3.7968 (4.0038)  acc1: 21.8750 (21.9200)  acc5: 45.3125 (44.5600)  time: 0.1501  data: 0.0001  max mem: 10046
[13:29:22.724633] Test: Total time: 0:00:08 (0.2008 s / it)
[13:29:22.852286] * Acc@1 21.970 Acc@5 44.180 loss 3.999
[13:29:22.852482] Accuracy of the network on the 10000 test images: 22.0%
[13:29:22.852672] [13:29:22.852749] Max accuracy: 22.04%
[13:29:22.852805] [13:29:22.853584] {"train_lr": 0.0005882325266217914, "train_loss": 2.661273392893019, "test_loss": 3.9993355736136436, "test_acc1": 21.97, "test_acc5": 44.18, "epoch": 83, "n_parameters": 85958500}
[13:29:22.853653] [13:29:22.853710] Training epoch 83 for 0:00:31
[13:29:22.853761] [13:29:22.856583] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:29:25.102186] Epoch: [84]  [ 0/42]  eta: 0:01:34  lr: 0.000548  loss: 2.1581 (2.1581)  time: 2.2446  data: 1.6376  max mem: 10046
[13:29:45.359566] Epoch: [84]  [41/42]  eta: 0:00:00  lr: 0.000486  loss: 2.7381 (2.6934)  time: 0.4955  data: 0.0001  max mem: 10046
[13:29:45.580504] Epoch: [84] Total time: 0:00:22 (0.5410 s / it)
[13:29:45.581326] Averaged stats: lr: 0.000486  loss: 2.7381 (2.6550)
[13:29:47.371310] Test:  [ 0/40]  eta: 0:01:11  loss: 4.0448 (4.0448)  acc1: 20.3125 (20.3125)  acc5: 40.6250 (40.6250)  time: 1.7864  data: 1.6138  max mem: 10046
[13:29:53.435894] Test:  [39/40]  eta: 0:00:00  loss: 3.7769 (3.9991)  acc1: 21.8750 (22.4800)  acc5: 45.3125 (45.1600)  time: 0.1505  data: 0.0001  max mem: 10046
[13:29:53.568638] Test: Total time: 0:00:07 (0.1996 s / it)
[13:29:53.709774] * Acc@1 22.400 Acc@5 45.450 loss 3.998
[13:29:53.710013] Accuracy of the network on the 10000 test images: 22.4%
[13:29:53.710252] [13:29:57.343257] Max accuracy: 22.40%
[13:29:57.343541] [13:29:57.344354] {"train_lr": 0.0005211314015865183, "train_loss": 2.6550466929163252, "test_loss": 3.997651232779026, "test_acc1": 22.4, "test_acc5": 45.45, "epoch": 84, "n_parameters": 85958500}
[13:29:57.344445] [13:29:57.344507] Training epoch 84 for 0:00:34
[13:29:57.344558] [13:29:57.347474] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:29:59.332939] Epoch: [85]  [ 0/42]  eta: 0:01:23  lr: 0.000483  loss: 2.5192 (2.5192)  time: 1.9845  data: 1.5095  max mem: 10046
[13:30:19.490804] Epoch: [85]  [41/42]  eta: 0:00:00  lr: 0.000425  loss: 2.6510 (2.6844)  time: 0.4929  data: 0.0001  max mem: 10046
[13:30:19.730090] Epoch: [85] Total time: 0:00:22 (0.5329 s / it)
[13:30:19.730804] Averaged stats: lr: 0.000425  loss: 2.6510 (2.6346)
[13:30:21.832045] Test:  [ 0/40]  eta: 0:01:23  loss: 4.0685 (4.0685)  acc1: 15.6250 (15.6250)  acc5: 40.6250 (40.6250)  time: 2.0976  data: 1.9210  max mem: 10046
[13:30:27.803822] Test:  [39/40]  eta: 0:00:00  loss: 3.7978 (4.0143)  acc1: 21.8750 (21.4000)  acc5: 46.8750 (45.0000)  time: 0.1498  data: 0.0001  max mem: 10046
[13:30:27.952176] Test: Total time: 0:00:08 (0.2055 s / it)
[13:30:27.953509] * Acc@1 21.960 Acc@5 44.840 loss 4.007
[13:30:27.953672] Accuracy of the network on the 10000 test images: 22.0%
[13:30:27.953869] [13:30:27.953945] Max accuracy: 22.40%
[13:30:27.954002] [13:30:27.954837] {"train_lr": 0.00045783491503003623, "train_loss": 2.6345677077770233, "test_loss": 4.006521892547608, "test_acc1": 21.96, "test_acc5": 44.84, "epoch": 85, "n_parameters": 85958500}
[13:30:27.954920] [13:30:27.954980] Training epoch 85 for 0:00:30
[13:30:27.955033] [13:30:27.957767] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:30:30.472633] Epoch: [86]  [ 0/42]  eta: 0:01:45  lr: 0.000422  loss: 2.5629 (2.5629)  time: 2.5140  data: 2.0260  max mem: 10046
[13:30:50.670486] Epoch: [86]  [41/42]  eta: 0:00:00  lr: 0.000368  loss: 2.6035 (2.6176)  time: 0.4938  data: 0.0001  max mem: 10046
[13:30:50.892644] Epoch: [86] Total time: 0:00:22 (0.5461 s / it)
[13:30:50.893340] Averaged stats: lr: 0.000368  loss: 2.6035 (2.6175)
[13:30:52.789259] Test:  [ 0/40]  eta: 0:01:15  loss: 4.0771 (4.0771)  acc1: 17.1875 (17.1875)  acc5: 39.0625 (39.0625)  time: 1.8926  data: 1.7140  max mem: 10046
[13:30:58.779077] Test:  [39/40]  eta: 0:00:00  loss: 3.8662 (4.0555)  acc1: 21.8750 (21.6400)  acc5: 43.7500 (44.8800)  time: 0.1501  data: 0.0001  max mem: 10046
[13:30:58.917801] Test: Total time: 0:00:08 (0.2006 s / it)
[13:30:59.008942] * Acc@1 21.810 Acc@5 44.800 loss 4.050
[13:30:59.009126] Accuracy of the network on the 10000 test images: 21.8%
[13:30:59.009349] [13:30:59.009431] Max accuracy: 22.40%
[13:30:59.009489] [13:30:59.010266] {"train_lr": 0.0003984122807310847, "train_loss": 2.6174784983907426, "test_loss": 4.049776269495487, "test_acc1": 21.81, "test_acc5": 44.8, "epoch": 86, "n_parameters": 85958500}
[13:30:59.010337] [13:30:59.010395] Training epoch 86 for 0:00:31
[13:30:59.010447] [13:30:59.013291] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:31:01.412754] Epoch: [87]  [ 0/42]  eta: 0:01:40  lr: 0.000365  loss: 2.8323 (2.8323)  time: 2.3983  data: 1.6770  max mem: 10046
[13:31:21.635122] Epoch: [87]  [41/42]  eta: 0:00:00  lr: 0.000314  loss: 2.6508 (2.6158)  time: 0.4945  data: 0.0001  max mem: 10046
[13:31:21.861010] Epoch: [87] Total time: 0:00:22 (0.5440 s / it)
[13:31:21.862189] Averaged stats: lr: 0.000314  loss: 2.6508 (2.6291)
[13:31:23.852279] Test:  [ 0/40]  eta: 0:01:19  loss: 4.1476 (4.1476)  acc1: 20.3125 (20.3125)  acc5: 40.6250 (40.6250)  time: 1.9864  data: 1.8083  max mem: 10046
[13:31:29.857369] Test:  [39/40]  eta: 0:00:00  loss: 3.8214 (4.0525)  acc1: 23.4375 (22.0400)  acc5: 45.3125 (45.4400)  time: 0.1509  data: 0.0001  max mem: 10046
[13:31:29.990951] Test: Total time: 0:00:08 (0.2032 s / it)
[13:31:29.992267] * Acc@1 21.960 Acc@5 44.830 loss 4.046
[13:31:29.992413] Accuracy of the network on the 10000 test images: 22.0%
[13:31:29.992626] [13:31:29.992700] Max accuracy: 22.40%
[13:31:29.992761] [13:31:29.993535] {"train_lr": 0.0003429284764678335, "train_loss": 2.6290969536418007, "test_loss": 4.045523205399514, "test_acc1": 21.96, "test_acc5": 44.83, "epoch": 87, "n_parameters": 85958500}
[13:31:29.993620] [13:31:29.993680] Training epoch 87 for 0:00:30
[13:31:29.993730] [13:31:29.996577] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:31:32.618839] Epoch: [88]  [ 0/42]  eta: 0:01:50  lr: 0.000312  loss: 2.9423 (2.9423)  time: 2.6214  data: 1.7410  max mem: 10046
[13:31:52.824073] Epoch: [88]  [41/42]  eta: 0:00:00  lr: 0.000265  loss: 2.5442 (2.6636)  time: 0.4942  data: 0.0001  max mem: 10046
[13:31:53.040940] Epoch: [88] Total time: 0:00:23 (0.5487 s / it)
[13:31:53.044948] Averaged stats: lr: 0.000265  loss: 2.5442 (2.6161)
[13:31:54.764926] Test:  [ 0/40]  eta: 0:01:08  loss: 4.1372 (4.1372)  acc1: 18.7500 (18.7500)  acc5: 42.1875 (42.1875)  time: 1.7148  data: 1.5426  max mem: 10046
[13:32:00.879567] Test:  [39/40]  eta: 0:00:00  loss: 3.8968 (4.0502)  acc1: 23.4375 (21.8400)  acc5: 45.3125 (45.4000)  time: 0.1512  data: 0.0001  max mem: 10046
[13:32:01.030389] Test: Total time: 0:00:07 (0.1996 s / it)
[13:32:01.202515] * Acc@1 21.990 Acc@5 44.990 loss 4.047
[13:32:01.202716] Accuracy of the network on the 10000 test images: 22.0%
[13:32:01.202931] [13:32:01.203009] Max accuracy: 22.40%
[13:32:01.203065] [13:32:01.204036] {"train_lr": 0.0002914441729656418, "train_loss": 2.616075818027769, "test_loss": 4.046712984144688, "test_acc1": 21.99, "test_acc5": 44.99, "epoch": 88, "n_parameters": 85958500}
[13:32:01.204114] [13:32:01.204175] Training epoch 88 for 0:00:31
[13:32:01.204226] [13:32:01.207089] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:32:03.748191] Epoch: [89]  [ 0/42]  eta: 0:01:46  lr: 0.000263  loss: 2.4559 (2.4559)  time: 2.5398  data: 2.0644  max mem: 10046
[13:32:23.961028] Epoch: [89]  [41/42]  eta: 0:00:00  lr: 0.000220  loss: 2.6448 (2.5999)  time: 0.4938  data: 0.0001  max mem: 10046
[13:32:24.169514] Epoch: [89] Total time: 0:00:22 (0.5467 s / it)
[13:32:24.170311] Averaged stats: lr: 0.000220  loss: 2.6448 (2.6140)
[13:32:25.950492] Test:  [ 0/40]  eta: 0:01:11  loss: 4.1739 (4.1739)  acc1: 18.7500 (18.7500)  acc5: 37.5000 (37.5000)  time: 1.7765  data: 1.6028  max mem: 10046
[13:32:32.065352] Test:  [39/40]  eta: 0:00:00  loss: 3.8880 (4.0571)  acc1: 21.8750 (21.6000)  acc5: 45.3125 (44.9200)  time: 0.1504  data: 0.0001  max mem: 10046
[13:32:32.213759] Test: Total time: 0:00:08 (0.2010 s / it)
[13:32:32.287919] * Acc@1 22.000 Acc@5 44.930 loss 4.059
[13:32:32.288130] Accuracy of the network on the 10000 test images: 22.0%
[13:32:32.288362] [13:32:32.288447] Max accuracy: 22.40%
[13:32:32.288511] [13:32:32.289421] {"train_lr": 0.0002440156675545032, "train_loss": 2.613984686987741, "test_loss": 4.058838754892349, "test_acc1": 22.0, "test_acc5": 44.93, "epoch": 89, "n_parameters": 85958500}
[13:32:32.289504] [13:32:32.289571] Training epoch 89 for 0:00:31
[13:32:32.289631] [13:32:32.292585] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:32:34.629223] Epoch: [90]  [ 0/42]  eta: 0:01:38  lr: 0.000218  loss: 2.9722 (2.9722)  time: 2.3358  data: 1.6531  max mem: 10046
[13:32:54.980130] Epoch: [90]  [41/42]  eta: 0:00:00  lr: 0.000179  loss: 2.6279 (2.6601)  time: 0.4968  data: 0.0001  max mem: 10046
[13:32:55.204928] Epoch: [90] Total time: 0:00:22 (0.5455 s / it)
[13:32:55.205677] Averaged stats: lr: 0.000179  loss: 2.6279 (2.6372)
[13:32:57.354380] Test:  [ 0/40]  eta: 0:01:25  loss: 4.1831 (4.1831)  acc1: 17.1875 (17.1875)  acc5: 37.5000 (37.5000)  time: 2.1446  data: 1.9711  max mem: 10046
[13:33:03.365443] Test:  [39/40]  eta: 0:00:00  loss: 3.8411 (4.0466)  acc1: 23.4375 (22.3600)  acc5: 45.3125 (45.4800)  time: 0.1510  data: 0.0001  max mem: 10046
[13:33:03.532112] Test: Total time: 0:00:08 (0.2081 s / it)
[13:33:03.533613] * Acc@1 22.370 Acc@5 45.080 loss 4.042
[13:33:03.533775] Accuracy of the network on the 10000 test images: 22.4%
[13:33:03.533994] [13:33:03.534077] Max accuracy: 22.40%
[13:33:03.534136] [13:33:03.535006] {"train_lr": 0.00020069482260874676, "train_loss": 2.6372496769541787, "test_loss": 4.04230011254549, "test_acc1": 22.37, "test_acc5": 45.08, "epoch": 90, "n_parameters": 85958500}
[13:33:03.535091] [13:33:03.535153] Training epoch 90 for 0:00:31
[13:33:03.535209] [13:33:03.538855] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:33:06.178835] Epoch: [91]  [ 0/42]  eta: 0:01:50  lr: 0.000177  loss: 2.6510 (2.6510)  time: 2.6391  data: 2.1555  max mem: 10046
[13:33:26.464447] Epoch: [91]  [41/42]  eta: 0:00:00  lr: 0.000142  loss: 2.6027 (2.6557)  time: 0.4963  data: 0.0001  max mem: 10046
[13:33:26.678330] Epoch: [91] Total time: 0:00:23 (0.5509 s / it)
[13:33:26.689420] Averaged stats: lr: 0.000142  loss: 2.6027 (2.6131)
[13:33:28.430242] Test:  [ 0/40]  eta: 0:01:09  loss: 4.1604 (4.1604)  acc1: 18.7500 (18.7500)  acc5: 37.5000 (37.5000)  time: 1.7366  data: 1.5668  max mem: 10046
[13:33:34.433625] Test:  [39/40]  eta: 0:00:00  loss: 3.8519 (4.0412)  acc1: 23.4375 (22.4400)  acc5: 45.3125 (45.2000)  time: 0.1506  data: 0.0001  max mem: 10046
[13:33:34.569446] Test: Total time: 0:00:07 (0.1969 s / it)
[13:33:34.755090] * Acc@1 22.300 Acc@5 44.870 loss 4.032
[13:33:34.755319] Accuracy of the network on the 10000 test images: 22.3%
[13:33:34.755564] [13:33:34.755641] Max accuracy: 22.40%
[13:33:34.755698] [13:33:34.756521] {"train_lr": 0.00016152900883629017, "train_loss": 2.6130693753560386, "test_loss": 4.031593665480614, "test_acc1": 22.3, "test_acc5": 44.87, "epoch": 91, "n_parameters": 85958500}
[13:33:34.756595] [13:33:34.756656] Training epoch 91 for 0:00:31
[13:33:34.756708] [13:33:34.759425] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:33:37.175502] Epoch: [92]  [ 0/42]  eta: 0:01:41  lr: 0.000140  loss: 2.7852 (2.7852)  time: 2.4151  data: 1.6735  max mem: 10046
[13:33:57.446631] Epoch: [92]  [41/42]  eta: 0:00:00  lr: 0.000109  loss: 2.7643 (2.6788)  time: 0.4957  data: 0.0001  max mem: 10046
[13:33:57.674531] Epoch: [92] Total time: 0:00:22 (0.5456 s / it)
[13:33:57.677228] Averaged stats: lr: 0.000109  loss: 2.7643 (2.6093)
[13:33:59.418501] Test:  [ 0/40]  eta: 0:01:09  loss: 4.1255 (4.1255)  acc1: 18.7500 (18.7500)  acc5: 39.0625 (39.0625)  time: 1.7367  data: 1.5797  max mem: 10046
[13:34:05.619210] Test:  [39/40]  eta: 0:00:00  loss: 3.8194 (4.0380)  acc1: 21.8750 (22.0800)  acc5: 48.4375 (45.8400)  time: 0.1522  data: 0.0001  max mem: 10046
[13:34:05.753378] Test: Total time: 0:00:08 (0.2018 s / it)
[13:34:05.754763] * Acc@1 22.260 Acc@5 45.160 loss 4.037
[13:34:05.754909] Accuracy of the network on the 10000 test images: 22.3%
[13:34:05.755106] [13:34:05.755194] Max accuracy: 22.40%
[13:34:05.755252] [13:34:05.756008] {"train_lr": 0.0001265610534794671, "train_loss": 2.609347414402735, "test_loss": 4.037434859573841, "test_acc1": 22.26, "test_acc5": 45.16, "epoch": 92, "n_parameters": 85958500}
[13:34:05.756088] [13:34:05.756155] Training epoch 92 for 0:00:30
[13:34:05.756210] [13:34:05.759027] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:34:08.180565] Epoch: [93]  [ 0/42]  eta: 0:01:41  lr: 0.000108  loss: 2.8269 (2.8269)  time: 2.4204  data: 1.7092  max mem: 10046
[13:34:28.480985] Epoch: [93]  [41/42]  eta: 0:00:00  lr: 0.000081  loss: 2.4936 (2.6011)  time: 0.4936  data: 0.0001  max mem: 10046
[13:34:28.721396] Epoch: [93] Total time: 0:00:22 (0.5467 s / it)
[13:34:28.722201] Averaged stats: lr: 0.000081  loss: 2.4936 (2.6126)
[13:34:30.416314] Test:  [ 0/40]  eta: 0:01:07  loss: 4.1591 (4.1591)  acc1: 21.8750 (21.8750)  acc5: 39.0625 (39.0625)  time: 1.6901  data: 1.5113  max mem: 10046
[13:34:36.515064] Test:  [39/40]  eta: 0:00:00  loss: 3.8361 (4.0465)  acc1: 23.4375 (22.6400)  acc5: 46.8750 (45.3200)  time: 0.1513  data: 0.0001  max mem: 10046
[13:34:36.678757] Test: Total time: 0:00:07 (0.1989 s / it)
[13:34:36.838740] * Acc@1 22.390 Acc@5 45.190 loss 4.052
[13:34:36.838965] Accuracy of the network on the 10000 test images: 22.4%
[13:34:36.839209] [13:34:36.839290] Max accuracy: 22.40%
[13:34:36.839348] [13:34:36.840259] {"train_lr": 9.582919348406543e-05, "train_loss": 2.6125753748984564, "test_loss": 4.051595063507557, "test_acc1": 22.39, "test_acc5": 45.19, "epoch": 93, "n_parameters": 85958500}
[13:34:36.840330] [13:34:36.840401] Training epoch 93 for 0:00:31
[13:34:36.840480] [13:34:36.843244] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:34:39.396965] Epoch: [94]  [ 0/42]  eta: 0:01:47  lr: 0.000079  loss: 2.6140 (2.6140)  time: 2.5529  data: 1.6875  max mem: 10046
[13:34:59.642280] Epoch: [94]  [41/42]  eta: 0:00:00  lr: 0.000057  loss: 2.6706 (2.6319)  time: 0.4955  data: 0.0001  max mem: 10046
[13:34:59.871143] Epoch: [94] Total time: 0:00:23 (0.5483 s / it)
[13:34:59.871994] Averaged stats: lr: 0.000057  loss: 2.6706 (2.6208)
[13:35:01.832950] Test:  [ 0/40]  eta: 0:01:18  loss: 4.1877 (4.1877)  acc1: 17.1875 (17.1875)  acc5: 34.3750 (34.3750)  time: 1.9574  data: 1.7737  max mem: 10046
[13:35:07.839768] Test:  [39/40]  eta: 0:00:00  loss: 3.8342 (4.0622)  acc1: 21.8750 (21.8400)  acc5: 46.8750 (45.5200)  time: 0.1508  data: 0.0001  max mem: 10046
[13:35:07.990918] Test: Total time: 0:00:08 (0.2029 s / it)
[13:35:08.037232] * Acc@1 22.110 Acc@5 45.180 loss 4.052
[13:35:08.037420] Accuracy of the network on the 10000 test images: 22.1%
[13:35:08.037643] [13:35:08.037724] Max accuracy: 22.40%
[13:35:08.037781] [13:35:08.038576] {"train_lr": 6.936703368779015e-05, "train_loss": 2.620753152029855, "test_loss": 4.0522704988718035, "test_acc1": 22.11, "test_acc5": 45.18, "epoch": 94, "n_parameters": 85958500}
[13:35:08.038649] [13:35:08.038709] Training epoch 94 for 0:00:31
[13:35:08.038761] [13:35:08.041601] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:35:10.501700] Epoch: [95]  [ 0/42]  eta: 0:01:43  lr: 0.000056  loss: 2.5262 (2.5262)  time: 2.4594  data: 1.9829  max mem: 10046
[13:35:30.788795] Epoch: [95]  [41/42]  eta: 0:00:00  lr: 0.000037  loss: 2.5517 (2.6380)  time: 0.4969  data: 0.0001  max mem: 10046
[13:35:31.010719] Epoch: [95] Total time: 0:00:22 (0.5469 s / it)
[13:35:31.011522] Averaged stats: lr: 0.000037  loss: 2.5517 (2.6189)
[13:35:32.969552] Test:  [ 0/40]  eta: 0:01:18  loss: 4.1097 (4.1097)  acc1: 15.6250 (15.6250)  acc5: 42.1875 (42.1875)  time: 1.9542  data: 1.7750  max mem: 10046
[13:35:38.963501] Test:  [39/40]  eta: 0:00:00  loss: 3.8106 (4.0542)  acc1: 23.4375 (22.4000)  acc5: 45.3125 (45.6000)  time: 0.1506  data: 0.0001  max mem: 10046
[13:35:39.116878] Test: Total time: 0:00:08 (0.2026 s / it)
[13:35:39.118202] * Acc@1 22.450 Acc@5 45.170 loss 4.049
[13:35:39.118351] Accuracy of the network on the 10000 test images: 22.4%
[13:35:39.118572] [13:35:42.585971] Max accuracy: 22.45%
[13:35:42.586244] [13:35:42.587149] {"train_lr": 4.720351007386505e-05, "train_loss": 2.6188667444955733, "test_loss": 4.049394129216671, "test_acc1": 22.45, "test_acc5": 45.17, "epoch": 95, "n_parameters": 85958500}
[13:35:42.587266] [13:35:42.587330] Training epoch 95 for 0:00:34
[13:35:42.587383] [13:35:42.590340] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:35:44.538805] Epoch: [96]  [ 0/42]  eta: 0:01:21  lr: 0.000036  loss: 2.4408 (2.4408)  time: 1.9476  data: 1.4695  max mem: 10046
[13:36:04.752446] Epoch: [96]  [41/42]  eta: 0:00:00  lr: 0.000021  loss: 2.6216 (2.6212)  time: 0.4941  data: 0.0001  max mem: 10046
[13:36:04.980890] Epoch: [96] Total time: 0:00:22 (0.5331 s / it)
[13:36:04.981676] Averaged stats: lr: 0.000021  loss: 2.6216 (2.6167)
[13:36:06.973155] Test:  [ 0/40]  eta: 0:01:19  loss: 4.2152 (4.2152)  acc1: 15.6250 (15.6250)  acc5: 35.9375 (35.9375)  time: 1.9881  data: 1.8088  max mem: 10046
[13:36:12.953258] Test:  [39/40]  eta: 0:00:00  loss: 3.8307 (4.0496)  acc1: 21.8750 (22.0800)  acc5: 46.8750 (45.8000)  time: 0.1498  data: 0.0001  max mem: 10046
[13:36:13.094152] Test: Total time: 0:00:08 (0.2028 s / it)
[13:36:13.198173] * Acc@1 22.160 Acc@5 45.220 loss 4.047
[13:36:13.198391] Accuracy of the network on the 10000 test images: 22.2%
[13:36:13.198600] [13:36:13.198679] Max accuracy: 22.45%
[13:36:13.198737] [13:36:13.199599] {"train_lr": 2.936285812996003e-05, "train_loss": 2.616737797146752, "test_loss": 4.047218297421932, "test_acc1": 22.16, "test_acc5": 45.22, "epoch": 96, "n_parameters": 85958500}
[13:36:13.199677] [13:36:13.199743] Training epoch 96 for 0:00:30
[13:36:13.199818] [13:36:13.202578] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:36:15.593841] Epoch: [97]  [ 0/42]  eta: 0:01:40  lr: 0.000021  loss: 2.7899 (2.7899)  time: 2.3904  data: 1.8391  max mem: 10046
[13:36:35.857124] Epoch: [97]  [41/42]  eta: 0:00:00  lr: 0.000010  loss: 2.6224 (2.5961)  time: 0.4942  data: 0.0001  max mem: 10046
[13:36:36.094507] Epoch: [97] Total time: 0:00:22 (0.5450 s / it)
[13:36:36.095228] Averaged stats: lr: 0.000010  loss: 2.6224 (2.5878)
[13:36:37.781974] Test:  [ 0/40]  eta: 0:01:07  loss: 4.0657 (4.0657)  acc1: 18.7500 (18.7500)  acc5: 35.9375 (35.9375)  time: 1.6818  data: 1.5097  max mem: 10046
[13:36:43.956821] Test:  [39/40]  eta: 0:00:00  loss: 3.8955 (4.0547)  acc1: 21.8750 (21.7200)  acc5: 45.3125 (45.0000)  time: 0.1505  data: 0.0001  max mem: 10046
[13:36:44.125514] Test: Total time: 0:00:08 (0.2007 s / it)
[13:36:44.175724] * Acc@1 22.040 Acc@5 44.900 loss 4.048
[13:36:44.175924] Accuracy of the network on the 10000 test images: 22.0%
[13:36:44.176181] [13:36:44.176268] Max accuracy: 22.45%
[13:36:44.176332] [13:36:44.177284] {"train_lr": 1.5864586347041642e-05, "train_loss": 2.5877668233144853, "test_loss": 4.047645990550518, "test_acc1": 22.04, "test_acc5": 44.9, "epoch": 97, "n_parameters": 85958500}
[13:36:44.177359] [13:36:44.177417] Training epoch 97 for 0:00:30
[13:36:44.177471] [13:36:44.180300] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:36:46.646220] Epoch: [98]  [ 0/42]  eta: 0:01:43  lr: 0.000010  loss: 2.5473 (2.5473)  time: 2.4649  data: 1.6601  max mem: 10046
[13:37:06.862921] Epoch: [98]  [41/42]  eta: 0:00:00  lr: 0.000003  loss: 2.6143 (2.6320)  time: 0.4946  data: 0.0001  max mem: 10046
[13:37:07.089403] Epoch: [98] Total time: 0:00:22 (0.5455 s / it)
[13:37:07.090198] Averaged stats: lr: 0.000003  loss: 2.6143 (2.6152)
[13:37:09.155359] Test:  [ 0/40]  eta: 0:01:22  loss: 4.1714 (4.1714)  acc1: 18.7500 (18.7500)  acc5: 37.5000 (37.5000)  time: 2.0617  data: 1.8884  max mem: 10046
[13:37:15.143774] Test:  [39/40]  eta: 0:00:00  loss: 3.8783 (4.0511)  acc1: 25.0000 (22.2800)  acc5: 46.8750 (45.2800)  time: 0.1504  data: 0.0001  max mem: 10046
[13:37:15.291088] Test: Total time: 0:00:08 (0.2050 s / it)
[13:37:15.292368] * Acc@1 22.280 Acc@5 45.190 loss 4.053
[13:37:15.292544] Accuracy of the network on the 10000 test images: 22.3%
[13:37:15.292754] [13:37:15.292833] Max accuracy: 22.45%
[13:37:15.292894] [13:37:15.293754] {"train_lr": 6.723454887125415e-06, "train_loss": 2.6151667052791234, "test_loss": 4.053431776165962, "test_acc1": 22.28, "test_acc5": 45.19, "epoch": 98, "n_parameters": 85958500}
[13:37:15.293837] [13:37:15.293901] Training epoch 98 for 0:00:31
[13:37:15.293975] [13:37:15.296801] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[13:37:17.763501] Epoch: [99]  [ 0/42]  eta: 0:01:43  lr: 0.000003  loss: 2.8568 (2.8568)  time: 2.4657  data: 1.8266  max mem: 10046
[13:37:37.962838] Epoch: [99]  [41/42]  eta: 0:00:00  lr: 0.000001  loss: 2.4659 (2.5658)  time: 0.4939  data: 0.0001  max mem: 10046
[13:37:38.198410] Epoch: [99] Total time: 0:00:22 (0.5453 s / it)
[13:37:38.207174] Averaged stats: lr: 0.000001  loss: 2.4659 (2.6007)
[13:37:40.044514] Test:  [ 0/40]  eta: 0:01:13  loss: 4.1113 (4.1113)  acc1: 20.3125 (20.3125)  acc5: 35.9375 (35.9375)  time: 1.8333  data: 1.6589  max mem: 10046
[13:37:46.044768] Test:  [39/40]  eta: 0:00:00  loss: 3.9445 (4.0573)  acc1: 21.8750 (21.8800)  acc5: 45.3125 (44.9200)  time: 0.1505  data: 0.0001  max mem: 10046
[13:37:46.195006] Test: Total time: 0:00:07 (0.1996 s / it)
[13:37:46.274626] * Acc@1 21.970 Acc@5 44.940 loss 4.053
[13:37:46.274807] Accuracy of the network on the 10000 test images: 22.0%
[13:37:46.275032] [13:37:46.275103] Max accuracy: 22.45%
[13:37:46.275160] [13:37:46.275969] {"train_lr": 1.9494594432538313e-06, "train_loss": 2.6006884773572287, "test_loss": 4.053442139923573, "test_acc1": 21.97, "test_acc5": 44.94, "epoch": 99, "n_parameters": 85958500}
[13:37:46.276039] [13:37:46.276109] Training epoch 99 for 0:00:30
[13:37:46.276178] [13:37:46.276321] Total training time 0:53:47
[13:37:46.276372] /home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 0): env://, gpu 0
[13:49:05.842039] job dir: /home/vision/wonjun/LiVT-main
[13:49:05.842212] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
adamW2=0.95,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt_dir',
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python/ImageNet-LT',
dataset='ImageNet-LT',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=50,
eval=True,
finetune='<bound method Trainer.finetune of <util.trainer.Trainer object at 0x7fd580d76e90>>',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='./exp//ImageNet-LT/',
loss='ce',
lr=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=1000,
num_workers=16,
pin_mem=True,
prit=20,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
Traceback (most recent call last):
  File "./main_finetune.py", line 404, in <module>
    main(args)
  File "./main_finetune.py", line 188, in main
    dataset_train = build_dataset(is_train=True, args=args)
  File "/home/vision/wonjun/LiVT-main/util/datasets.py", line 134, in build_dataset
    dataset = DatasetLT(root, transform=transform)
  File "/home/vision/.local/lib/python3.7/site-packages/torchvision/datasets/folder.py", line 315, in __init__
    is_valid_file=is_valid_file,
  File "/home/vision/.local/lib/python3.7/site-packages/torchvision/datasets/folder.py", line 144, in __init__
    classes, class_to_idx = self.find_classes(self.root)
  File "/home/vision/.local/lib/python3.7/site-packages/torchvision/datasets/folder.py", line 218, in find_classes
    return find_classes(directory)
  File "/home/vision/.local/lib/python3.7/site-packages/torchvision/datasets/folder.py", line 40, in find_classes
    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())
FileNotFoundError: [Errno 2] No such file or directory: './DATA/cifar-100-python/ImageNet-LT/train'
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 7175) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./main_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_13:49:07
  host      : user
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 7175)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 0): env://, gpu 0
[13:50:50.501674] job dir: /home/vision/wonjun/LiVT-main
[13:50:50.501852] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
adamW2=0.95,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt_dir',
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=50,
eval=True,
finetune='<bound method Trainer.finetune of <util.trainer.Trainer object at 0x7fa807511e90>>',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='./exp//cifar100-LT/',
loss='ce',
lr=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=20,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[13:50:50.804523] Files already downloaded and verified
[13:50:51.532908] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[13:50:51.856157] Files already downloaded and verified
[13:50:52.228413] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[13:50:52.228618] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f4c6a1c81d0>
[13:50:52.229015] Train on 10847 Image w.r.t. 100 classes
[13:50:54.007291] Model = vit_base_patch16
[13:50:54.007391] number of params (M): 85.96
[13:50:54.007422] base lr: 1.00e-03
[13:50:54.007443] actual lr: 2.50e-04
[13:50:54.007464] accumulate grad iterations: 1
[13:50:54.007484] effective batch size: 64
[13:50:54.632302] Resume checkpoint ./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth
  0%|          | 0/157 [00:00<?, ?it/s]  1%|          | 1/157 [00:03<08:27,  3.25s/it]  1%|▏         | 2/157 [00:03<03:54,  1.51s/it]  2%|▏         | 3/157 [00:03<02:27,  1.05it/s]  3%|▎         | 4/157 [00:04<01:46,  1.44it/s]  3%|▎         | 5/157 [00:04<01:23,  1.82it/s]  4%|▍         | 6/157 [00:04<01:09,  2.17it/s]  4%|▍         | 7/157 [00:05<01:00,  2.46it/s]  5%|▌         | 8/157 [00:05<00:55,  2.70it/s]  6%|▌         | 9/157 [00:05<00:51,  2.89it/s]  6%|▋         | 10/157 [00:05<00:48,  3.04it/s]  7%|▋         | 11/157 [00:06<00:46,  3.15it/s]  8%|▊         | 12/157 [00:06<00:44,  3.23it/s]  8%|▊         | 13/157 [00:06<00:43,  3.28it/s]  9%|▉         | 14/157 [00:07<00:43,  3.33it/s] 10%|▉         | 15/157 [00:07<00:42,  3.35it/s] 10%|█         | 16/157 [00:07<00:41,  3.37it/s] 11%|█         | 17/157 [00:07<00:41,  3.39it/s] 11%|█▏        | 18/157 [00:08<00:40,  3.40it/s] 12%|█▏        | 19/157 [00:08<00:40,  3.40it/s] 13%|█▎        | 20/157 [00:08<00:40,  3.41it/s] 13%|█▎        | 21/157 [00:09<00:39,  3.42it/s] 14%|█▍        | 22/157 [00:09<00:39,  3.41it/s] 15%|█▍        | 23/157 [00:09<00:39,  3.42it/s] 15%|█▌        | 24/157 [00:09<00:38,  3.42it/s] 16%|█▌        | 25/157 [00:10<00:38,  3.42it/s] 17%|█▋        | 26/157 [00:10<00:38,  3.42it/s] 17%|█▋        | 27/157 [00:10<00:38,  3.42it/s] 18%|█▊        | 28/157 [00:11<00:37,  3.42it/s] 18%|█▊        | 29/157 [00:11<00:37,  3.41it/s] 19%|█▉        | 30/157 [00:11<00:37,  3.41it/s] 20%|█▉        | 31/157 [00:12<00:36,  3.41it/s] 20%|██        | 32/157 [00:12<00:36,  3.41it/s] 21%|██        | 33/157 [00:12<00:36,  3.41it/s] 22%|██▏       | 34/157 [00:12<00:36,  3.41it/s] 22%|██▏       | 35/157 [00:13<00:35,  3.41it/s] 23%|██▎       | 36/157 [00:13<00:35,  3.41it/s] 24%|██▎       | 37/157 [00:13<00:35,  3.41it/s] 24%|██▍       | 38/157 [00:14<00:34,  3.41it/s] 25%|██▍       | 39/157 [00:14<00:34,  3.41it/s] 25%|██▌       | 40/157 [00:14<00:34,  3.41it/s] 26%|██▌       | 41/157 [00:14<00:34,  3.41it/s] 27%|██▋       | 42/157 [00:15<00:33,  3.41it/s] 27%|██▋       | 43/157 [00:15<00:33,  3.41it/s] 28%|██▊       | 44/157 [00:15<00:33,  3.41it/s] 29%|██▊       | 45/157 [00:16<00:32,  3.41it/s] 29%|██▉       | 46/157 [00:16<00:32,  3.41it/s] 30%|██▉       | 47/157 [00:16<00:32,  3.41it/s] 31%|███       | 48/157 [00:17<00:31,  3.41it/s] 31%|███       | 49/157 [00:17<00:31,  3.41it/s] 32%|███▏      | 50/157 [00:17<00:31,  3.41it/s] 32%|███▏      | 51/157 [00:17<00:31,  3.41it/s] 33%|███▎      | 52/157 [00:18<00:30,  3.41it/s] 34%|███▍      | 53/157 [00:18<00:30,  3.41it/s] 34%|███▍      | 54/157 [00:18<00:30,  3.41it/s] 35%|███▌      | 55/157 [00:19<00:29,  3.40it/s] 36%|███▌      | 56/157 [00:19<00:29,  3.40it/s] 36%|███▋      | 57/157 [00:19<00:29,  3.40it/s] 37%|███▋      | 58/157 [00:19<00:29,  3.41it/s] 38%|███▊      | 59/157 [00:20<00:28,  3.40it/s] 38%|███▊      | 60/157 [00:20<00:28,  3.41it/s] 39%|███▉      | 61/157 [00:20<00:28,  3.40it/s] 39%|███▉      | 62/157 [00:21<00:27,  3.41it/s] 40%|████      | 63/157 [00:21<00:27,  3.40it/s] 41%|████      | 64/157 [00:21<00:27,  3.40it/s] 41%|████▏     | 65/157 [00:22<00:27,  3.40it/s] 42%|████▏     | 66/157 [00:22<00:26,  3.40it/s] 43%|████▎     | 67/157 [00:22<00:26,  3.40it/s] 43%|████▎     | 68/157 [00:22<00:26,  3.40it/s] 44%|████▍     | 69/157 [00:23<00:25,  3.40it/s] 45%|████▍     | 70/157 [00:23<00:25,  3.40it/s] 45%|████▌     | 71/157 [00:23<00:25,  3.40it/s] 46%|████▌     | 72/157 [00:24<00:25,  3.40it/s] 46%|████▋     | 73/157 [00:24<00:24,  3.39it/s] 47%|████▋     | 74/157 [00:24<00:24,  3.39it/s] 48%|████▊     | 75/157 [00:24<00:24,  3.39it/s] 48%|████▊     | 76/157 [00:25<00:23,  3.39it/s] 49%|████▉     | 77/157 [00:25<00:23,  3.39it/s] 50%|████▉     | 78/157 [00:25<00:23,  3.39it/s] 50%|█████     | 79/157 [00:26<00:22,  3.39it/s] 51%|█████     | 80/157 [00:26<00:22,  3.39it/s] 52%|█████▏    | 81/157 [00:26<00:22,  3.39it/s] 52%|█████▏    | 82/157 [00:27<00:22,  3.39it/s] 53%|█████▎    | 83/157 [00:27<00:21,  3.39it/s] 54%|█████▎    | 84/157 [00:27<00:21,  3.39it/s] 54%|█████▍    | 85/157 [00:27<00:21,  3.39it/s] 55%|█████▍    | 86/157 [00:28<00:20,  3.39it/s] 55%|█████▌    | 87/157 [00:28<00:20,  3.39it/s] 56%|█████▌    | 88/157 [00:28<00:20,  3.39it/s] 57%|█████▋    | 89/157 [00:29<00:20,  3.39it/s] 57%|█████▋    | 90/157 [00:29<00:19,  3.39it/s] 58%|█████▊    | 91/157 [00:29<00:19,  3.39it/s] 59%|█████▊    | 92/157 [00:29<00:19,  3.39it/s] 59%|█████▉    | 93/157 [00:30<00:18,  3.39it/s] 60%|█████▉    | 94/157 [00:30<00:18,  3.39it/s] 61%|██████    | 95/157 [00:30<00:18,  3.39it/s] 61%|██████    | 96/157 [00:31<00:17,  3.39it/s] 62%|██████▏   | 97/157 [00:31<00:17,  3.39it/s] 62%|██████▏   | 98/157 [00:31<00:17,  3.39it/s] 63%|██████▎   | 99/157 [00:32<00:17,  3.39it/s] 64%|██████▎   | 100/157 [00:32<00:16,  3.39it/s] 64%|██████▍   | 101/157 [00:32<00:16,  3.39it/s] 65%|██████▍   | 102/157 [00:32<00:16,  3.39it/s] 66%|██████▌   | 103/157 [00:33<00:15,  3.39it/s] 66%|██████▌   | 104/157 [00:33<00:15,  3.39it/s] 67%|██████▋   | 105/157 [00:33<00:15,  3.39it/s] 68%|██████▊   | 106/157 [00:34<00:15,  3.39it/s] 68%|██████▊   | 107/157 [00:34<00:14,  3.39it/s] 69%|██████▉   | 108/157 [00:34<00:14,  3.39it/s] 69%|██████▉   | 109/157 [00:34<00:14,  3.39it/s] 70%|███████   | 110/157 [00:35<00:13,  3.39it/s] 71%|███████   | 111/157 [00:35<00:13,  3.39it/s] 71%|███████▏  | 112/157 [00:35<00:13,  3.39it/s] 72%|███████▏  | 113/157 [00:36<00:13,  3.38it/s] 73%|███████▎  | 114/157 [00:36<00:12,  3.38it/s] 73%|███████▎  | 115/157 [00:36<00:12,  3.38it/s] 74%|███████▍  | 116/157 [00:37<00:12,  3.38it/s] 75%|███████▍  | 117/157 [00:37<00:11,  3.37it/s] 75%|███████▌  | 118/157 [00:37<00:11,  3.38it/s] 76%|███████▌  | 119/157 [00:37<00:11,  3.38it/s] 76%|███████▋  | 120/157 [00:38<00:10,  3.38it/s] 77%|███████▋  | 121/157 [00:38<00:10,  3.38it/s] 78%|███████▊  | 122/157 [00:38<00:10,  3.38it/s] 78%|███████▊  | 123/157 [00:39<00:10,  3.38it/s] 79%|███████▉  | 124/157 [00:39<00:09,  3.38it/s] 80%|███████▉  | 125/157 [00:39<00:09,  3.38it/s] 80%|████████  | 126/157 [00:40<00:09,  3.38it/s] 81%|████████  | 127/157 [00:40<00:08,  3.38it/s] 82%|████████▏ | 128/157 [00:40<00:08,  3.38it/s] 82%|████████▏ | 129/157 [00:40<00:08,  3.38it/s] 83%|████████▎ | 130/157 [00:41<00:07,  3.38it/s] 83%|████████▎ | 131/157 [00:41<00:07,  3.38it/s] 84%|████████▍ | 132/157 [00:41<00:07,  3.38it/s] 85%|████████▍ | 133/157 [00:42<00:07,  3.38it/s] 85%|████████▌ | 134/157 [00:42<00:06,  3.38it/s] 86%|████████▌ | 135/157 [00:42<00:06,  3.38it/s] 87%|████████▋ | 136/157 [00:42<00:06,  3.38it/s] 87%|████████▋ | 137/157 [00:43<00:05,  3.38it/s] 88%|████████▊ | 138/157 [00:43<00:05,  3.38it/s] 89%|████████▊ | 139/157 [00:43<00:05,  3.38it/s] 89%|████████▉ | 140/157 [00:44<00:05,  3.38it/s] 90%|████████▉ | 141/157 [00:44<00:04,  3.38it/s] 90%|█████████ | 142/157 [00:44<00:04,  3.38it/s] 91%|█████████ | 143/157 [00:45<00:04,  3.38it/s] 92%|█████████▏| 144/157 [00:45<00:03,  3.38it/s] 92%|█████████▏| 145/157 [00:45<00:03,  3.38it/s] 93%|█████████▎| 146/157 [00:45<00:03,  3.38it/s] 94%|█████████▎| 147/157 [00:46<00:02,  3.38it/s] 94%|█████████▍| 148/157 [00:46<00:02,  3.38it/s] 95%|█████████▍| 149/157 [00:46<00:02,  3.38it/s] 96%|█████████▌| 150/157 [00:47<00:02,  3.38it/s] 96%|█████████▌| 151/157 [00:47<00:01,  3.38it/s] 97%|█████████▋| 152/157 [00:47<00:01,  3.38it/s] 97%|█████████▋| 153/157 [00:47<00:01,  3.38it/s] 98%|█████████▊| 154/157 [00:48<00:00,  3.38it/s] 99%|█████████▊| 155/157 [00:48<00:00,  3.38it/s] 99%|█████████▉| 156/157 [00:48<00:00,  3.38it/s]100%|██████████| 157/157 [00:49<00:00,  3.20it/s]
Traceback (most recent call last):
  File "./main_finetune.py", line 404, in <module>
    main(args)
  File "./main_finetune.py", line 305, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 387, in save_eval_json
    with open(json_pth, 'w') as J:
FileNotFoundError: [Errno 2] No such file or directory: '/ckpt/debug/cifar100-LT/vit_base_patch16/debug/result.json'
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 7686) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./main_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_13:51:47
  host      : user
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 7686)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 3): env://, gpu 3
[13:52:20.574228] job dir: /home/vision/wonjun/LiVT-main
[13:52:20.574436] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
adamW2=0.95,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt_dir',
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=50,
eval=True,
finetune='<bound method Trainer.finetune of <util.trainer.Trainer object at 0x7f8533e7ee90>>',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='./exp//cifar100-LT/',
loss='ce',
lr=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=20,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=4)
[13:52:20.885535] Files already downloaded and verified
[13:52:21.639613] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[13:52:21.959599] Files already downloaded and verified
[13:52:22.326758] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[13:52:22.327020] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f9e52b4d2d0>
[13:52:22.327423] Train on 10847 Image w.r.t. 100 classes
[13:52:24.344530] Model = vit_base_patch16
[13:52:24.344769] number of params (M): 85.96
[13:52:24.344813] base lr: 1.00e-03
[13:52:24.344838] actual lr: 1.00e-03
[13:52:24.344867] accumulate grad iterations: 1
[13:52:24.344889] effective batch size: 256
  0%|          | 0/157 [00:00<?, ?it/s]  0%|          | 0/157 [00:00<?, ?it/s][13:52:25.124102] Resume checkpoint ./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth
  0%|          | 0/157 [00:00<?, ?it/s]  0%|          | 0/157 [00:00<?, ?it/s]  1%|          | 1/157 [00:03<09:32,  3.67s/it]  1%|          | 1/157 [00:03<09:31,  3.66s/it]  1%|          | 1/157 [00:03<09:38,  3.71s/it]  1%|          | 1/157 [00:03<09:44,  3.75s/it]  1%|▏         | 2/157 [00:03<04:20,  1.68s/it]  1%|▏         | 2/157 [00:03<04:20,  1.68s/it]  1%|▏         | 2/157 [00:04<04:23,  1.70s/it]  1%|▏         | 2/157 [00:04<04:26,  1.72s/it]  2%|▏         | 3/157 [00:04<02:41,  1.05s/it]  2%|▏         | 3/157 [00:04<02:41,  1.05s/it]  2%|▏         | 3/157 [00:04<02:43,  1.06s/it]  2%|▏         | 3/157 [00:04<02:44,  1.07s/it]  3%|▎         | 4/157 [00:04<01:54,  1.33it/s]  3%|▎         | 4/157 [00:04<01:55,  1.33it/s]  3%|▎         | 4/157 [00:04<01:56,  1.32it/s]  3%|▎         | 4/157 [00:04<01:56,  1.31it/s]  3%|▎         | 5/157 [00:04<01:28,  1.71it/s]  3%|▎         | 5/157 [00:04<01:29,  1.70it/s]  3%|▎         | 5/157 [00:04<01:30,  1.69it/s]  3%|▎         | 5/157 [00:04<01:30,  1.69it/s]  4%|▍         | 6/157 [00:05<01:13,  2.06it/s]  4%|▍         | 6/157 [00:05<01:13,  2.04it/s]  4%|▍         | 6/157 [00:05<01:14,  2.03it/s]  4%|▍         | 6/157 [00:05<01:14,  2.04it/s]  4%|▍         | 7/157 [00:05<01:03,  2.36it/s]  4%|▍         | 7/157 [00:05<01:04,  2.34it/s]  4%|▍         | 7/157 [00:05<01:04,  2.33it/s]  4%|▍         | 7/157 [00:05<01:03,  2.35it/s]  5%|▌         | 8/157 [00:05<00:56,  2.62it/s]  5%|▌         | 8/157 [00:05<00:57,  2.59it/s]  5%|▌         | 8/157 [00:05<00:57,  2.58it/s]  5%|▌         | 8/157 [00:05<00:57,  2.60it/s]  6%|▌         | 9/157 [00:06<00:52,  2.82it/s]  6%|▌         | 9/157 [00:06<00:52,  2.79it/s]  6%|▌         | 9/157 [00:06<00:53,  2.78it/s]  6%|▌         | 9/157 [00:06<00:52,  2.81it/s]  6%|▋         | 10/157 [00:06<00:49,  2.98it/s]  6%|▋         | 10/157 [00:06<00:49,  2.95it/s]  6%|▋         | 10/157 [00:06<00:50,  2.94it/s]  6%|▋         | 10/157 [00:06<00:49,  2.97it/s]  7%|▋         | 11/157 [00:06<00:47,  3.10it/s]  7%|▋         | 11/157 [00:06<00:47,  3.06it/s]  7%|▋         | 11/157 [00:06<00:47,  3.05it/s]  7%|▋         | 11/157 [00:06<00:47,  3.09it/s]  8%|▊         | 12/157 [00:06<00:45,  3.19it/s]  8%|▊         | 12/157 [00:06<00:46,  3.15it/s]  8%|▊         | 12/157 [00:06<00:46,  3.14it/s]  8%|▊         | 12/157 [00:06<00:45,  3.18it/s]  8%|▊         | 13/157 [00:07<00:44,  3.25it/s]  8%|▊         | 13/157 [00:07<00:44,  3.21it/s]  8%|▊         | 13/157 [00:07<00:44,  3.20it/s]  8%|▊         | 13/157 [00:07<00:44,  3.25it/s]  9%|▉         | 14/157 [00:07<00:43,  3.30it/s]  9%|▉         | 14/157 [00:07<00:43,  3.26it/s]  9%|▉         | 14/157 [00:07<00:44,  3.25it/s]  9%|▉         | 14/157 [00:07<00:43,  3.29it/s] 10%|▉         | 15/157 [00:07<00:42,  3.33it/s] 10%|▉         | 15/157 [00:07<00:43,  3.28it/s] 10%|▉         | 15/157 [00:07<00:43,  3.28it/s] 10%|▉         | 15/157 [00:07<00:42,  3.32it/s] 10%|█         | 16/157 [00:08<00:42,  3.35it/s] 10%|█         | 16/157 [00:08<00:42,  3.31it/s] 10%|█         | 16/157 [00:08<00:42,  3.30it/s] 10%|█         | 16/157 [00:08<00:42,  3.35it/s] 11%|█         | 17/157 [00:08<00:41,  3.37it/s] 11%|█         | 17/157 [00:08<00:42,  3.32it/s] 11%|█         | 17/157 [00:08<00:42,  3.32it/s] 11%|█         | 17/157 [00:08<00:41,  3.37it/s] 11%|█▏        | 18/157 [00:08<00:41,  3.38it/s] 11%|█▏        | 18/157 [00:08<00:41,  3.33it/s] 11%|█▏        | 18/157 [00:08<00:41,  3.33it/s] 11%|█▏        | 18/157 [00:08<00:41,  3.38it/s] 12%|█▏        | 19/157 [00:08<00:40,  3.39it/s] 12%|█▏        | 19/157 [00:09<00:41,  3.34it/s] 12%|█▏        | 19/157 [00:09<00:41,  3.34it/s] 12%|█▏        | 19/157 [00:09<00:40,  3.39it/s] 13%|█▎        | 20/157 [00:09<00:40,  3.39it/s] 13%|█▎        | 20/157 [00:09<00:40,  3.35it/s] 13%|█▎        | 20/157 [00:09<00:41,  3.34it/s] 13%|█▎        | 20/157 [00:09<00:40,  3.39it/s] 13%|█▎        | 21/157 [00:09<00:40,  3.40it/s] 13%|█▎        | 21/157 [00:09<00:40,  3.35it/s] 13%|█▎        | 21/157 [00:09<00:40,  3.34it/s] 13%|█▎        | 21/157 [00:09<00:40,  3.39it/s] 14%|█▍        | 22/157 [00:09<00:39,  3.40it/s] 14%|█▍        | 22/157 [00:09<00:40,  3.35it/s] 14%|█▍        | 22/157 [00:09<00:39,  3.40it/s] 14%|█▍        | 22/157 [00:09<00:40,  3.35it/s] 15%|█▍        | 23/157 [00:10<00:39,  3.40it/s] 15%|█▍        | 23/157 [00:10<00:40,  3.35it/s] 15%|█▍        | 23/157 [00:10<00:39,  3.39it/s] 15%|█▍        | 23/157 [00:10<00:40,  3.34it/s] 15%|█▌        | 24/157 [00:10<00:39,  3.40it/s] 15%|█▌        | 24/157 [00:10<00:39,  3.35it/s] 15%|█▌        | 24/157 [00:10<00:39,  3.39it/s] 15%|█▌        | 24/157 [00:10<00:39,  3.34it/s] 16%|█▌        | 25/157 [00:10<00:38,  3.41it/s] 16%|█▌        | 25/157 [00:10<00:39,  3.35it/s] 16%|█▌        | 25/157 [00:10<00:38,  3.40it/s] 16%|█▌        | 25/157 [00:10<00:39,  3.35it/s] 17%|█▋        | 26/157 [00:11<00:38,  3.40it/s] 17%|█▋        | 26/157 [00:11<00:39,  3.35it/s] 17%|█▋        | 26/157 [00:11<00:38,  3.40it/s] 17%|█▋        | 26/157 [00:11<00:39,  3.35it/s] 17%|█▋        | 27/157 [00:11<00:38,  3.40it/s] 17%|█▋        | 27/157 [00:11<00:38,  3.35it/s] 17%|█▋        | 27/157 [00:11<00:38,  3.40it/s] 17%|█▋        | 27/157 [00:11<00:38,  3.35it/s] 18%|█▊        | 28/157 [00:11<00:37,  3.40it/s] 18%|█▊        | 28/157 [00:11<00:38,  3.35it/s] 18%|█▊        | 28/157 [00:11<00:37,  3.41it/s] 18%|█▊        | 28/157 [00:11<00:38,  3.35it/s] 18%|█▊        | 29/157 [00:11<00:37,  3.40it/s] 18%|█▊        | 29/157 [00:12<00:38,  3.35it/s] 18%|█▊        | 29/157 [00:11<00:37,  3.40it/s] 18%|█▊        | 29/157 [00:12<00:38,  3.35it/s] 19%|█▉        | 30/157 [00:12<00:37,  3.40it/s] 19%|█▉        | 30/157 [00:12<00:37,  3.35it/s] 19%|█▉        | 30/157 [00:12<00:37,  3.40it/s] 19%|█▉        | 30/157 [00:12<00:37,  3.35it/s] 20%|█▉        | 31/157 [00:12<00:37,  3.40it/s] 20%|█▉        | 31/157 [00:12<00:37,  3.35it/s] 20%|█▉        | 31/157 [00:12<00:37,  3.40it/s] 20%|█▉        | 31/157 [00:12<00:37,  3.35it/s] 20%|██        | 32/157 [00:12<00:36,  3.40it/s] 20%|██        | 32/157 [00:12<00:37,  3.35it/s] 20%|██        | 32/157 [00:12<00:36,  3.40it/s] 20%|██        | 32/157 [00:12<00:37,  3.35it/s] 21%|██        | 33/157 [00:13<00:36,  3.40it/s] 21%|██        | 33/157 [00:13<00:37,  3.35it/s] 21%|██        | 33/157 [00:13<00:36,  3.40it/s] 21%|██        | 33/157 [00:13<00:36,  3.35it/s] 22%|██▏       | 34/157 [00:13<00:36,  3.40it/s] 22%|██▏       | 34/157 [00:13<00:36,  3.39it/s] 22%|██▏       | 34/157 [00:13<00:36,  3.34it/s] 22%|██▏       | 34/157 [00:13<00:36,  3.35it/s] 22%|██▏       | 35/157 [00:13<00:35,  3.40it/s] 22%|██▏       | 35/157 [00:13<00:35,  3.39it/s] 22%|██▏       | 35/157 [00:13<00:36,  3.34it/s] 22%|██▏       | 35/157 [00:13<00:36,  3.35it/s] 23%|██▎       | 36/157 [00:13<00:35,  3.40it/s] 23%|██▎       | 36/157 [00:14<00:35,  3.39it/s] 23%|██▎       | 36/157 [00:14<00:36,  3.34it/s] 23%|██▎       | 36/157 [00:14<00:36,  3.35it/s] 24%|██▎       | 37/157 [00:14<00:35,  3.40it/s] 24%|██▎       | 37/157 [00:14<00:35,  3.39it/s] 24%|██▎       | 37/157 [00:14<00:35,  3.34it/s] 24%|██▎       | 37/157 [00:14<00:35,  3.35it/s] 24%|██▍       | 38/157 [00:14<00:35,  3.40it/s] 24%|██▍       | 38/157 [00:14<00:35,  3.39it/s] 24%|██▍       | 38/157 [00:14<00:35,  3.34it/s] 24%|██▍       | 38/157 [00:14<00:35,  3.35it/s] 25%|██▍       | 39/157 [00:14<00:34,  3.40it/s] 25%|██▍       | 39/157 [00:14<00:34,  3.39it/s] 25%|██▍       | 39/157 [00:14<00:35,  3.34it/s] 25%|██▍       | 39/157 [00:15<00:35,  3.35it/s] 25%|██▌       | 40/157 [00:15<00:34,  3.40it/s] 25%|██▌       | 40/157 [00:15<00:34,  3.39it/s] 25%|██▌       | 40/157 [00:15<00:34,  3.34it/s] 25%|██▌       | 40/157 [00:15<00:34,  3.35it/s] 26%|██▌       | 41/157 [00:15<00:34,  3.40it/s] 26%|██▌       | 41/157 [00:15<00:34,  3.39it/s] 26%|██▌       | 41/157 [00:15<00:34,  3.34it/s] 26%|██▌       | 41/157 [00:15<00:34,  3.35it/s] 27%|██▋       | 42/157 [00:15<00:33,  3.40it/s] 27%|██▋       | 42/157 [00:15<00:33,  3.40it/s] 27%|██▋       | 42/157 [00:15<00:34,  3.34it/s] 27%|██▋       | 42/157 [00:15<00:34,  3.35it/s] 27%|██▋       | 43/157 [00:16<00:33,  3.40it/s] 27%|██▋       | 43/157 [00:16<00:33,  3.39it/s] 27%|██▋       | 43/157 [00:16<00:34,  3.34it/s] 27%|██▋       | 43/157 [00:16<00:34,  3.35it/s] 28%|██▊       | 44/157 [00:16<00:33,  3.40it/s] 28%|██▊       | 44/157 [00:16<00:33,  3.39it/s] 28%|██▊       | 44/157 [00:16<00:33,  3.34it/s] 28%|██▊       | 44/157 [00:16<00:33,  3.35it/s] 29%|██▊       | 45/157 [00:16<00:32,  3.40it/s] 29%|██▊       | 45/157 [00:16<00:33,  3.39it/s] 29%|██▊       | 45/157 [00:16<00:33,  3.34it/s] 29%|██▊       | 45/157 [00:16<00:33,  3.35it/s] 29%|██▉       | 46/157 [00:16<00:32,  3.40it/s] 29%|██▉       | 46/157 [00:16<00:32,  3.39it/s] 29%|██▉       | 46/157 [00:17<00:33,  3.34it/s] 29%|██▉       | 46/157 [00:17<00:33,  3.35it/s] 30%|██▉       | 47/157 [00:17<00:32,  3.40it/s] 30%|██▉       | 47/157 [00:17<00:32,  3.39it/s] 30%|██▉       | 47/157 [00:17<00:32,  3.34it/s] 30%|██▉       | 47/157 [00:17<00:32,  3.35it/s] 31%|███       | 48/157 [00:17<00:32,  3.40it/s] 31%|███       | 48/157 [00:17<00:32,  3.39it/s] 31%|███       | 48/157 [00:17<00:32,  3.34it/s] 31%|███       | 48/157 [00:17<00:32,  3.35it/s] 31%|███       | 49/157 [00:17<00:31,  3.40it/s] 31%|███       | 49/157 [00:17<00:31,  3.39it/s] 31%|███       | 49/157 [00:17<00:32,  3.34it/s] 31%|███       | 49/157 [00:18<00:32,  3.35it/s] 32%|███▏      | 50/157 [00:18<00:31,  3.40it/s] 32%|███▏      | 50/157 [00:18<00:31,  3.39it/s] 32%|███▏      | 50/157 [00:18<00:32,  3.34it/s] 32%|███▏      | 50/157 [00:18<00:31,  3.35it/s] 32%|███▏      | 51/157 [00:18<00:31,  3.39it/s] 32%|███▏      | 51/157 [00:18<00:31,  3.39it/s] 32%|███▏      | 51/157 [00:18<00:31,  3.34it/s] 32%|███▏      | 51/157 [00:18<00:31,  3.35it/s] 33%|███▎      | 52/157 [00:18<00:30,  3.39it/s] 33%|███▎      | 52/157 [00:18<00:30,  3.39it/s] 33%|███▎      | 52/157 [00:18<00:31,  3.34it/s] 33%|███▎      | 52/157 [00:18<00:31,  3.35it/s] 34%|███▍      | 53/157 [00:18<00:30,  3.40it/s] 34%|███▍      | 53/157 [00:19<00:30,  3.39it/s] 34%|███▍      | 53/157 [00:19<00:31,  3.34it/s] 34%|███▍      | 53/157 [00:19<00:31,  3.35it/s] 34%|███▍      | 54/157 [00:19<00:30,  3.39it/s] 34%|███▍      | 54/157 [00:19<00:30,  3.39it/s] 34%|███▍      | 54/157 [00:19<00:30,  3.33it/s] 34%|███▍      | 54/157 [00:19<00:30,  3.35it/s] 35%|███▌      | 55/157 [00:19<00:30,  3.40it/s] 35%|███▌      | 55/157 [00:19<00:30,  3.39it/s] 35%|███▌      | 55/157 [00:19<00:30,  3.33it/s] 35%|███▌      | 55/157 [00:19<00:30,  3.35it/s] 36%|███▌      | 56/157 [00:19<00:29,  3.39it/s] 36%|███▌      | 56/157 [00:19<00:29,  3.39it/s] 36%|███▌      | 56/157 [00:20<00:30,  3.33it/s] 36%|███▋      | 57/157 [00:20<00:29,  3.39it/s] 36%|███▌      | 56/157 [00:20<00:30,  3.35it/s] 36%|███▋      | 57/157 [00:20<00:29,  3.39it/s] 36%|███▋      | 57/157 [00:20<00:30,  3.33it/s] 37%|███▋      | 58/157 [00:20<00:29,  3.39it/s] 36%|███▋      | 57/157 [00:20<00:29,  3.34it/s] 37%|███▋      | 58/157 [00:20<00:29,  3.39it/s] 37%|███▋      | 58/157 [00:20<00:29,  3.33it/s] 38%|███▊      | 59/157 [00:20<00:28,  3.39it/s] 37%|███▋      | 58/157 [00:20<00:29,  3.34it/s] 38%|███▊      | 59/157 [00:20<00:28,  3.39it/s] 38%|███▊      | 59/157 [00:20<00:29,  3.33it/s] 38%|███▊      | 60/157 [00:21<00:28,  3.39it/s] 38%|███▊      | 59/157 [00:21<00:29,  3.34it/s] 38%|███▊      | 60/157 [00:21<00:28,  3.39it/s] 38%|███▊      | 60/157 [00:21<00:29,  3.34it/s] 39%|███▉      | 61/157 [00:21<00:28,  3.39it/s] 38%|███▊      | 60/157 [00:21<00:29,  3.34it/s] 39%|███▉      | 61/157 [00:21<00:28,  3.39it/s] 39%|███▉      | 61/157 [00:21<00:28,  3.34it/s] 39%|███▉      | 62/157 [00:21<00:28,  3.39it/s] 39%|███▉      | 61/157 [00:21<00:28,  3.34it/s] 39%|███▉      | 62/157 [00:21<00:28,  3.39it/s] 40%|████      | 63/157 [00:21<00:27,  3.39it/s] 39%|███▉      | 62/157 [00:21<00:28,  3.34it/s] 39%|███▉      | 62/157 [00:21<00:28,  3.34it/s] 40%|████      | 63/157 [00:21<00:27,  3.39it/s] 41%|████      | 64/157 [00:22<00:27,  3.38it/s] 40%|████      | 63/157 [00:22<00:28,  3.32it/s] 40%|████      | 63/157 [00:22<00:28,  3.34it/s] 41%|████      | 64/157 [00:22<00:27,  3.39it/s] 41%|████▏     | 65/157 [00:22<00:27,  3.39it/s] 41%|████      | 64/157 [00:22<00:27,  3.33it/s] 41%|████      | 64/157 [00:22<00:27,  3.34it/s] 41%|████▏     | 65/157 [00:22<00:27,  3.39it/s] 42%|████▏     | 66/157 [00:22<00:26,  3.38it/s] 41%|████▏     | 65/157 [00:22<00:27,  3.33it/s] 41%|████▏     | 65/157 [00:22<00:27,  3.34it/s] 42%|████▏     | 66/157 [00:22<00:26,  3.39it/s] 43%|████▎     | 67/157 [00:23<00:26,  3.38it/s] 42%|████▏     | 66/157 [00:23<00:27,  3.33it/s] 42%|████▏     | 66/157 [00:23<00:27,  3.34it/s] 43%|████▎     | 67/157 [00:23<00:26,  3.39it/s] 43%|████▎     | 68/157 [00:23<00:26,  3.38it/s] 43%|████▎     | 67/157 [00:23<00:27,  3.33it/s] 43%|████▎     | 67/157 [00:23<00:26,  3.34it/s] 43%|████▎     | 68/157 [00:23<00:26,  3.39it/s] 44%|████▍     | 69/157 [00:23<00:25,  3.39it/s] 43%|████▎     | 68/157 [00:23<00:26,  3.33it/s] 43%|████▎     | 68/157 [00:23<00:26,  3.34it/s] 44%|████▍     | 69/157 [00:23<00:25,  3.39it/s] 45%|████▍     | 70/157 [00:23<00:25,  3.39it/s] 44%|████▍     | 69/157 [00:23<00:26,  3.33it/s] 44%|████▍     | 69/157 [00:24<00:26,  3.34it/s] 45%|████▍     | 70/157 [00:24<00:25,  3.39it/s] 45%|████▌     | 71/157 [00:24<00:25,  3.39it/s] 45%|████▍     | 70/157 [00:24<00:26,  3.33it/s] 45%|████▍     | 70/157 [00:24<00:26,  3.34it/s] 45%|████▌     | 71/157 [00:24<00:25,  3.38it/s] 46%|████▌     | 72/157 [00:24<00:25,  3.39it/s] 45%|████▌     | 71/157 [00:24<00:25,  3.33it/s] 45%|████▌     | 71/157 [00:24<00:25,  3.34it/s] 46%|████▌     | 72/157 [00:24<00:25,  3.39it/s] 46%|████▋     | 73/157 [00:24<00:24,  3.39it/s] 46%|████▌     | 72/157 [00:24<00:25,  3.33it/s] 46%|████▌     | 72/157 [00:24<00:25,  3.34it/s] 46%|████▋     | 73/157 [00:24<00:24,  3.38it/s] 47%|████▋     | 74/157 [00:25<00:24,  3.39it/s] 46%|████▋     | 73/157 [00:25<00:25,  3.33it/s] 46%|████▋     | 73/157 [00:25<00:25,  3.34it/s] 47%|████▋     | 74/157 [00:25<00:24,  3.38it/s] 48%|████▊     | 75/157 [00:25<00:24,  3.39it/s] 47%|████▋     | 74/157 [00:25<00:24,  3.33it/s] 47%|████▋     | 74/157 [00:25<00:24,  3.34it/s] 48%|████▊     | 75/157 [00:25<00:24,  3.38it/s] 48%|████▊     | 76/157 [00:25<00:23,  3.38it/s] 48%|████▊     | 75/157 [00:25<00:24,  3.33it/s] 48%|████▊     | 75/157 [00:25<00:24,  3.34it/s] 48%|████▊     | 76/157 [00:25<00:23,  3.38it/s] 49%|████▉     | 77/157 [00:26<00:23,  3.38it/s] 48%|████▊     | 76/157 [00:26<00:24,  3.33it/s] 48%|████▊     | 76/157 [00:26<00:24,  3.34it/s] 49%|████▉     | 77/157 [00:26<00:23,  3.38it/s] 50%|████▉     | 78/157 [00:26<00:23,  3.38it/s] 49%|████▉     | 77/157 [00:26<00:24,  3.33it/s] 49%|████▉     | 77/157 [00:26<00:23,  3.34it/s] 50%|████▉     | 78/157 [00:26<00:23,  3.38it/s] 50%|█████     | 79/157 [00:26<00:23,  3.38it/s] 50%|████▉     | 78/157 [00:26<00:23,  3.33it/s] 50%|████▉     | 78/157 [00:26<00:23,  3.33it/s] 50%|█████     | 79/157 [00:26<00:23,  3.38it/s] 51%|█████     | 80/157 [00:26<00:22,  3.38it/s] 50%|█████     | 79/157 [00:26<00:23,  3.33it/s] 50%|█████     | 79/157 [00:27<00:23,  3.33it/s] 51%|█████     | 80/157 [00:27<00:22,  3.38it/s] 52%|█████▏    | 81/157 [00:27<00:22,  3.38it/s] 51%|█████     | 80/157 [00:27<00:23,  3.33it/s] 51%|█████     | 80/157 [00:27<00:23,  3.33it/s] 52%|█████▏    | 81/157 [00:27<00:22,  3.38it/s] 52%|█████▏    | 82/157 [00:27<00:22,  3.39it/s] 52%|█████▏    | 81/157 [00:27<00:22,  3.33it/s] 52%|█████▏    | 81/157 [00:27<00:22,  3.34it/s] 52%|█████▏    | 82/157 [00:27<00:22,  3.38it/s] 53%|█████▎    | 83/157 [00:27<00:21,  3.38it/s] 52%|█████▏    | 82/157 [00:27<00:22,  3.33it/s] 52%|█████▏    | 82/157 [00:27<00:22,  3.34it/s] 53%|█████▎    | 83/157 [00:27<00:21,  3.38it/s] 54%|█████▎    | 84/157 [00:28<00:21,  3.38it/s] 53%|█████▎    | 83/157 [00:28<00:22,  3.33it/s] 53%|█████▎    | 83/157 [00:28<00:22,  3.33it/s] 54%|█████▎    | 84/157 [00:28<00:21,  3.38it/s] 54%|█████▍    | 85/157 [00:28<00:21,  3.38it/s] 54%|█████▎    | 84/157 [00:28<00:21,  3.33it/s] 54%|█████▎    | 84/157 [00:28<00:21,  3.33it/s] 54%|█████▍    | 85/157 [00:28<00:21,  3.38it/s] 55%|█████▍    | 86/157 [00:28<00:20,  3.38it/s] 54%|█████▍    | 85/157 [00:28<00:21,  3.33it/s] 54%|█████▍    | 85/157 [00:28<00:21,  3.34it/s] 55%|█████▍    | 86/157 [00:28<00:21,  3.38it/s] 55%|█████▌    | 87/157 [00:28<00:20,  3.38it/s] 55%|█████▍    | 86/157 [00:29<00:21,  3.33it/s] 55%|█████▍    | 86/157 [00:29<00:21,  3.34it/s] 55%|█████▌    | 87/157 [00:29<00:20,  3.38it/s] 56%|█████▌    | 88/157 [00:29<00:20,  3.38it/s] 55%|█████▌    | 87/157 [00:29<00:21,  3.33it/s] 55%|█████▌    | 87/157 [00:29<00:20,  3.34it/s] 56%|█████▌    | 88/157 [00:29<00:20,  3.38it/s] 57%|█████▋    | 89/157 [00:29<00:20,  3.38it/s] 56%|█████▌    | 88/157 [00:29<00:20,  3.33it/s] 56%|█████▌    | 88/157 [00:29<00:20,  3.34it/s] 57%|█████▋    | 89/157 [00:29<00:20,  3.38it/s] 57%|█████▋    | 90/157 [00:29<00:19,  3.38it/s] 57%|█████▋    | 89/157 [00:29<00:20,  3.33it/s] 57%|█████▋    | 89/157 [00:30<00:20,  3.34it/s] 57%|█████▋    | 90/157 [00:29<00:19,  3.38it/s] 58%|█████▊    | 91/157 [00:30<00:19,  3.38it/s] 57%|█████▋    | 90/157 [00:30<00:20,  3.33it/s] 57%|█████▋    | 90/157 [00:30<00:20,  3.33it/s] 58%|█████▊    | 91/157 [00:30<00:19,  3.38it/s] 59%|█████▊    | 92/157 [00:30<00:19,  3.38it/s] 58%|█████▊    | 91/157 [00:30<00:19,  3.32it/s] 58%|█████▊    | 91/157 [00:30<00:19,  3.33it/s] 59%|█████▊    | 92/157 [00:30<00:19,  3.38it/s] 59%|█████▉    | 93/157 [00:30<00:18,  3.38it/s] 59%|█████▊    | 92/157 [00:30<00:19,  3.32it/s] 59%|█████▊    | 92/157 [00:30<00:19,  3.33it/s] 59%|█████▉    | 93/157 [00:30<00:18,  3.37it/s] 60%|█████▉    | 94/157 [00:31<00:18,  3.38it/s] 59%|█████▉    | 93/157 [00:31<00:19,  3.32it/s] 59%|█████▉    | 93/157 [00:31<00:19,  3.33it/s] 60%|█████▉    | 94/157 [00:31<00:18,  3.38it/s] 61%|██████    | 95/157 [00:31<00:18,  3.38it/s] 60%|█████▉    | 94/157 [00:31<00:19,  3.31it/s] 60%|█████▉    | 94/157 [00:31<00:18,  3.32it/s] 61%|██████    | 95/157 [00:31<00:18,  3.38it/s] 61%|██████    | 96/157 [00:31<00:18,  3.38it/s] 61%|██████    | 95/157 [00:31<00:18,  3.32it/s] 61%|██████    | 96/157 [00:31<00:18,  3.38it/s] 61%|██████    | 95/157 [00:31<00:18,  3.31it/s] 62%|██████▏   | 97/157 [00:31<00:17,  3.38it/s] 62%|██████▏   | 97/157 [00:32<00:17,  3.35it/s] 61%|██████    | 96/157 [00:32<00:18,  3.30it/s] 61%|██████    | 96/157 [00:32<00:18,  3.29it/s] 62%|██████▏   | 98/157 [00:32<00:17,  3.38it/s] 62%|██████▏   | 98/157 [00:32<00:17,  3.36it/s] 62%|██████▏   | 97/157 [00:32<00:18,  3.30it/s] 62%|██████▏   | 97/157 [00:32<00:18,  3.29it/s] 63%|██████▎   | 99/157 [00:32<00:17,  3.38it/s] 63%|██████▎   | 99/157 [00:32<00:17,  3.36it/s] 62%|██████▏   | 98/157 [00:32<00:17,  3.30it/s] 62%|██████▏   | 98/157 [00:32<00:17,  3.29it/s] 64%|██████▎   | 100/157 [00:32<00:16,  3.38it/s] 64%|██████▎   | 100/157 [00:32<00:16,  3.37it/s] 63%|██████▎   | 99/157 [00:33<00:17,  3.30it/s] 63%|██████▎   | 99/157 [00:33<00:17,  3.29it/s] 64%|██████▍   | 101/157 [00:33<00:16,  3.38it/s] 64%|██████▍   | 101/157 [00:33<00:16,  3.37it/s] 64%|██████▎   | 100/157 [00:33<00:17,  3.31it/s] 64%|██████▎   | 100/157 [00:33<00:17,  3.30it/s] 65%|██████▍   | 102/157 [00:33<00:16,  3.38it/s] 65%|██████▍   | 102/157 [00:33<00:16,  3.37it/s] 64%|██████▍   | 101/157 [00:33<00:16,  3.31it/s] 64%|██████▍   | 101/157 [00:33<00:16,  3.31it/s] 66%|██████▌   | 103/157 [00:33<00:15,  3.38it/s] 66%|██████▌   | 103/157 [00:33<00:16,  3.37it/s] 65%|██████▍   | 102/157 [00:33<00:16,  3.32it/s] 65%|██████▍   | 102/157 [00:33<00:16,  3.31it/s] 66%|██████▌   | 104/157 [00:34<00:15,  3.38it/s] 66%|██████▌   | 104/157 [00:34<00:15,  3.37it/s] 66%|██████▌   | 103/157 [00:34<00:16,  3.32it/s] 66%|██████▌   | 103/157 [00:34<00:16,  3.31it/s] 67%|██████▋   | 105/157 [00:34<00:15,  3.38it/s] 67%|██████▋   | 105/157 [00:34<00:15,  3.37it/s] 66%|██████▌   | 104/157 [00:34<00:15,  3.32it/s] 66%|██████▌   | 104/157 [00:34<00:15,  3.32it/s] 68%|██████▊   | 106/157 [00:34<00:15,  3.38it/s] 68%|██████▊   | 106/157 [00:34<00:15,  3.37it/s] 67%|██████▋   | 105/157 [00:34<00:15,  3.32it/s] 67%|██████▋   | 105/157 [00:34<00:15,  3.32it/s] 68%|██████▊   | 107/157 [00:34<00:14,  3.38it/s] 68%|██████▊   | 107/157 [00:35<00:14,  3.37it/s] 68%|██████▊   | 106/157 [00:35<00:15,  3.32it/s] 68%|██████▊   | 106/157 [00:35<00:15,  3.31it/s] 69%|██████▉   | 108/157 [00:35<00:14,  3.38it/s] 69%|██████▉   | 108/157 [00:35<00:14,  3.37it/s] 68%|██████▊   | 107/157 [00:35<00:15,  3.32it/s] 68%|██████▊   | 107/157 [00:35<00:15,  3.31it/s] 69%|██████▉   | 109/157 [00:35<00:14,  3.38it/s] 69%|██████▉   | 109/157 [00:35<00:14,  3.37it/s] 69%|██████▉   | 108/157 [00:35<00:14,  3.32it/s] 69%|██████▉   | 108/157 [00:35<00:14,  3.31it/s] 70%|███████   | 110/157 [00:35<00:13,  3.38it/s] 70%|███████   | 110/157 [00:35<00:13,  3.37it/s] 69%|██████▉   | 109/157 [00:36<00:14,  3.32it/s] 69%|██████▉   | 109/157 [00:36<00:14,  3.31it/s] 71%|███████   | 111/157 [00:36<00:13,  3.38it/s] 71%|███████   | 111/157 [00:36<00:13,  3.37it/s] 70%|███████   | 110/157 [00:36<00:14,  3.32it/s] 70%|███████   | 110/157 [00:36<00:14,  3.31it/s] 71%|███████▏  | 112/157 [00:36<00:13,  3.37it/s] 71%|███████▏  | 112/157 [00:36<00:13,  3.37it/s] 71%|███████   | 111/157 [00:36<00:13,  3.32it/s] 71%|███████   | 111/157 [00:36<00:13,  3.32it/s] 72%|███████▏  | 113/157 [00:36<00:13,  3.38it/s] 72%|███████▏  | 113/157 [00:36<00:13,  3.37it/s] 71%|███████▏  | 112/157 [00:36<00:13,  3.32it/s] 71%|███████▏  | 112/157 [00:36<00:13,  3.32it/s] 73%|███████▎  | 114/157 [00:36<00:12,  3.37it/s] 73%|███████▎  | 114/157 [00:37<00:12,  3.37it/s] 72%|███████▏  | 113/157 [00:37<00:13,  3.32it/s] 72%|███████▏  | 113/157 [00:37<00:13,  3.32it/s] 73%|███████▎  | 115/157 [00:37<00:12,  3.37it/s] 73%|███████▎  | 115/157 [00:37<00:12,  3.37it/s] 73%|███████▎  | 114/157 [00:37<00:12,  3.32it/s] 73%|███████▎  | 114/157 [00:37<00:12,  3.32it/s] 74%|███████▍  | 116/157 [00:37<00:12,  3.37it/s] 74%|███████▍  | 116/157 [00:37<00:12,  3.37it/s] 73%|███████▎  | 115/157 [00:37<00:12,  3.32it/s] 73%|███████▎  | 115/157 [00:37<00:12,  3.31it/s] 75%|███████▍  | 117/157 [00:37<00:11,  3.37it/s] 75%|███████▍  | 117/157 [00:37<00:11,  3.37it/s] 74%|███████▍  | 116/157 [00:38<00:12,  3.32it/s] 75%|███████▌  | 118/157 [00:38<00:11,  3.37it/s] 74%|███████▍  | 116/157 [00:38<00:12,  3.31it/s] 75%|███████▌  | 118/157 [00:38<00:11,  3.37it/s] 75%|███████▍  | 117/157 [00:38<00:12,  3.32it/s] 76%|███████▌  | 119/157 [00:38<00:11,  3.37it/s] 75%|███████▍  | 117/157 [00:38<00:12,  3.31it/s] 76%|███████▌  | 119/157 [00:38<00:11,  3.37it/s] 75%|███████▌  | 118/157 [00:38<00:11,  3.32it/s] 76%|███████▋  | 120/157 [00:38<00:10,  3.37it/s] 75%|███████▌  | 118/157 [00:38<00:11,  3.31it/s] 76%|███████▋  | 120/157 [00:38<00:10,  3.37it/s] 77%|███████▋  | 121/157 [00:39<00:10,  3.36it/s] 76%|███████▌  | 119/157 [00:39<00:11,  3.31it/s] 76%|███████▌  | 119/157 [00:39<00:11,  3.31it/s] 77%|███████▋  | 121/157 [00:39<00:10,  3.37it/s] 78%|███████▊  | 122/157 [00:39<00:10,  3.36it/s] 76%|███████▋  | 120/157 [00:39<00:11,  3.31it/s] 76%|███████▋  | 120/157 [00:39<00:11,  3.31it/s] 78%|███████▊  | 122/157 [00:39<00:10,  3.36it/s] 78%|███████▊  | 123/157 [00:39<00:10,  3.36it/s] 77%|███████▋  | 121/157 [00:39<00:10,  3.31it/s] 77%|███████▋  | 121/157 [00:39<00:10,  3.31it/s] 78%|███████▊  | 123/157 [00:39<00:10,  3.36it/s] 79%|███████▉  | 124/157 [00:39<00:09,  3.36it/s] 78%|███████▊  | 122/157 [00:39<00:10,  3.31it/s] 78%|███████▊  | 122/157 [00:39<00:10,  3.31it/s] 79%|███████▉  | 124/157 [00:40<00:09,  3.36it/s] 80%|███████▉  | 125/157 [00:40<00:09,  3.37it/s] 78%|███████▊  | 123/157 [00:40<00:10,  3.31it/s] 78%|███████▊  | 123/157 [00:40<00:10,  3.31it/s] 80%|███████▉  | 125/157 [00:40<00:09,  3.36it/s] 80%|████████  | 126/157 [00:40<00:09,  3.37it/s] 79%|███████▉  | 124/157 [00:40<00:09,  3.31it/s] 79%|███████▉  | 124/157 [00:40<00:09,  3.31it/s] 80%|████████  | 126/157 [00:40<00:09,  3.36it/s] 81%|████████  | 127/157 [00:40<00:08,  3.37it/s] 80%|███████▉  | 125/157 [00:40<00:09,  3.31it/s] 80%|███████▉  | 125/157 [00:40<00:09,  3.31it/s] 81%|████████  | 127/157 [00:40<00:08,  3.37it/s] 82%|████████▏ | 128/157 [00:41<00:08,  3.37it/s] 80%|████████  | 126/157 [00:41<00:09,  3.31it/s] 80%|████████  | 126/157 [00:41<00:09,  3.31it/s] 82%|████████▏ | 128/157 [00:41<00:08,  3.36it/s] 82%|████████▏ | 129/157 [00:41<00:08,  3.37it/s] 81%|████████  | 127/157 [00:41<00:09,  3.32it/s] 81%|████████  | 127/157 [00:41<00:09,  3.32it/s] 82%|████████▏ | 129/157 [00:41<00:08,  3.37it/s] 83%|████████▎ | 130/157 [00:41<00:08,  3.37it/s] 82%|████████▏ | 128/157 [00:41<00:08,  3.32it/s] 82%|████████▏ | 128/157 [00:41<00:08,  3.31it/s] 83%|████████▎ | 130/157 [00:41<00:08,  3.37it/s] 83%|████████▎ | 131/157 [00:42<00:07,  3.37it/s] 82%|████████▏ | 129/157 [00:42<00:08,  3.32it/s] 82%|████████▏ | 129/157 [00:42<00:08,  3.31it/s] 83%|████████▎ | 131/157 [00:42<00:07,  3.37it/s] 84%|████████▍ | 132/157 [00:42<00:07,  3.37it/s] 83%|████████▎ | 130/157 [00:42<00:08,  3.32it/s] 83%|████████▎ | 130/157 [00:42<00:08,  3.31it/s] 84%|████████▍ | 132/157 [00:42<00:07,  3.37it/s] 85%|████████▍ | 133/157 [00:42<00:07,  3.37it/s] 83%|████████▎ | 131/157 [00:42<00:07,  3.32it/s] 83%|████████▎ | 131/157 [00:42<00:07,  3.31it/s] 85%|████████▍ | 133/157 [00:42<00:07,  3.36it/s] 85%|████████▌ | 134/157 [00:42<00:06,  3.37it/s] 84%|████████▍ | 132/157 [00:42<00:07,  3.32it/s] 84%|████████▍ | 132/157 [00:42<00:07,  3.31it/s] 85%|████████▌ | 134/157 [00:43<00:06,  3.36it/s] 86%|████████▌ | 135/157 [00:43<00:06,  3.37it/s] 85%|████████▍ | 133/157 [00:43<00:07,  3.32it/s] 85%|████████▍ | 133/157 [00:43<00:07,  3.31it/s] 86%|████████▌ | 135/157 [00:43<00:06,  3.36it/s] 87%|████████▋ | 136/157 [00:43<00:06,  3.37it/s] 85%|████████▌ | 134/157 [00:43<00:06,  3.32it/s] 85%|████████▌ | 134/157 [00:43<00:06,  3.31it/s] 87%|████████▋ | 136/157 [00:43<00:06,  3.36it/s] 87%|████████▋ | 137/157 [00:43<00:05,  3.37it/s] 86%|████████▌ | 135/157 [00:43<00:06,  3.32it/s] 86%|████████▌ | 135/157 [00:43<00:06,  3.31it/s] 87%|████████▋ | 137/157 [00:43<00:05,  3.37it/s] 88%|████████▊ | 138/157 [00:44<00:05,  3.37it/s] 87%|████████▋ | 136/157 [00:44<00:06,  3.32it/s] 87%|████████▋ | 136/157 [00:44<00:06,  3.31it/s] 88%|████████▊ | 138/157 [00:44<00:05,  3.36it/s] 89%|████████▊ | 139/157 [00:44<00:05,  3.37it/s] 87%|████████▋ | 137/157 [00:44<00:06,  3.32it/s] 87%|████████▋ | 137/157 [00:44<00:06,  3.31it/s] 89%|████████▊ | 139/157 [00:44<00:05,  3.36it/s] 89%|████████▉ | 140/157 [00:44<00:05,  3.37it/s] 88%|████████▊ | 138/157 [00:44<00:05,  3.32it/s] 88%|████████▊ | 138/157 [00:44<00:05,  3.31it/s] 89%|████████▉ | 140/157 [00:44<00:05,  3.36it/s] 90%|████████▉ | 141/157 [00:45<00:04,  3.37it/s] 89%|████████▊ | 139/157 [00:45<00:05,  3.32it/s] 89%|████████▊ | 139/157 [00:45<00:05,  3.31it/s] 90%|████████▉ | 141/157 [00:45<00:04,  3.36it/s] 90%|█████████ | 142/157 [00:45<00:04,  3.37it/s] 89%|████████▉ | 140/157 [00:45<00:05,  3.32it/s] 89%|████████▉ | 140/157 [00:45<00:05,  3.31it/s] 90%|█████████ | 142/157 [00:45<00:04,  3.36it/s] 91%|█████████ | 143/157 [00:45<00:04,  3.37it/s] 90%|████████▉ | 141/157 [00:45<00:04,  3.32it/s] 90%|████████▉ | 141/157 [00:45<00:04,  3.31it/s] 91%|█████████ | 143/157 [00:45<00:04,  3.36it/s] 92%|█████████▏| 144/157 [00:45<00:03,  3.37it/s] 90%|█████████ | 142/157 [00:45<00:04,  3.32it/s] 90%|█████████ | 142/157 [00:46<00:04,  3.31it/s] 92%|█████████▏| 144/157 [00:46<00:03,  3.36it/s] 92%|█████████▏| 145/157 [00:46<00:03,  3.37it/s] 91%|█████████ | 143/157 [00:46<00:04,  3.32it/s] 91%|█████████ | 143/157 [00:46<00:04,  3.31it/s] 92%|█████████▏| 145/157 [00:46<00:03,  3.36it/s] 93%|█████████▎| 146/157 [00:46<00:03,  3.37it/s] 92%|█████████▏| 144/157 [00:46<00:03,  3.32it/s] 92%|█████████▏| 144/157 [00:46<00:03,  3.31it/s] 93%|█████████▎| 146/157 [00:46<00:03,  3.36it/s] 94%|█████████▎| 147/157 [00:46<00:02,  3.37it/s] 92%|█████████▏| 145/157 [00:46<00:03,  3.32it/s] 92%|█████████▏| 145/157 [00:46<00:03,  3.31it/s] 94%|█████████▎| 147/157 [00:46<00:02,  3.36it/s] 94%|█████████▍| 148/157 [00:47<00:02,  3.37it/s] 93%|█████████▎| 146/157 [00:47<00:03,  3.32it/s] 93%|█████████▎| 146/157 [00:47<00:03,  3.31it/s] 94%|█████████▍| 148/157 [00:47<00:02,  3.36it/s] 95%|█████████▍| 149/157 [00:47<00:02,  3.37it/s] 94%|█████████▎| 147/157 [00:47<00:03,  3.32it/s] 94%|█████████▎| 147/157 [00:47<00:03,  3.30it/s] 95%|█████████▍| 149/157 [00:47<00:02,  3.36it/s] 96%|█████████▌| 150/157 [00:47<00:02,  3.37it/s] 94%|█████████▍| 148/157 [00:47<00:02,  3.32it/s] 94%|█████████▍| 148/157 [00:47<00:02,  3.31it/s] 96%|█████████▌| 150/157 [00:47<00:02,  3.36it/s] 96%|█████████▌| 151/157 [00:47<00:01,  3.37it/s] 95%|█████████▍| 149/157 [00:48<00:02,  3.32it/s] 95%|█████████▍| 149/157 [00:48<00:02,  3.31it/s] 96%|█████████▌| 151/157 [00:48<00:01,  3.36it/s] 97%|█████████▋| 152/157 [00:48<00:01,  3.37it/s] 96%|█████████▌| 150/157 [00:48<00:02,  3.32it/s] 96%|█████████▌| 150/157 [00:48<00:02,  3.31it/s] 97%|█████████▋| 152/157 [00:48<00:01,  3.36it/s] 97%|█████████▋| 153/157 [00:48<00:01,  3.37it/s] 96%|█████████▌| 151/157 [00:48<00:01,  3.32it/s] 96%|█████████▌| 151/157 [00:48<00:01,  3.31it/s] 97%|█████████▋| 153/157 [00:48<00:01,  3.36it/s] 98%|█████████▊| 154/157 [00:48<00:00,  3.37it/s] 97%|█████████▋| 152/157 [00:49<00:01,  3.32it/s] 97%|█████████▋| 152/157 [00:49<00:01,  3.31it/s] 98%|█████████▊| 154/157 [00:48<00:00,  3.36it/s] 99%|█████████▊| 155/157 [00:49<00:00,  3.37it/s] 97%|█████████▋| 153/157 [00:49<00:01,  3.31it/s] 97%|█████████▋| 153/157 [00:49<00:01,  3.31it/s] 99%|█████████▊| 155/157 [00:49<00:00,  3.36it/s] 99%|█████████▉| 156/157 [00:49<00:00,  3.37it/s] 98%|█████████▊| 154/157 [00:49<00:00,  3.31it/s]100%|██████████| 157/157 [00:49<00:00,  3.16it/s]
Traceback (most recent call last):
  File "./main_finetune.py", line 404, in <module>
    main(args)
  File "./main_finetune.py", line 305, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 387, in save_eval_json
    with open(json_pth, 'w') as J:
FileNotFoundError: [Errno 2] No such file or directory: '/ckpt/debug/cifar100-LT/vit_base_patch16/debug/result.json'
 98%|█████████▊| 154/157 [00:49<00:00,  3.31it/s] 99%|█████████▉| 156/157 [00:49<00:00,  3.36it/s]100%|██████████| 157/157 [00:49<00:00,  3.15it/s]
Traceback (most recent call last):
  File "./main_finetune.py", line 404, in <module>
    main(args)
  File "./main_finetune.py", line 305, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 387, in save_eval_json
    with open(json_pth, 'w') as J:
FileNotFoundError: [Errno 2] No such file or directory: '/ckpt/debug/cifar100-LT/vit_base_patch16/debug/result.json'
 99%|█████████▊| 155/157 [00:49<00:00,  3.31it/s] 99%|█████████▊| 155/157 [00:49<00:00,  3.31it/s] 99%|█████████▉| 156/157 [00:50<00:00,  3.31it/s] 99%|█████████▉| 156/157 [00:50<00:00,  3.31it/s]100%|██████████| 157/157 [00:50<00:00,  3.12it/s]
Traceback (most recent call last):
  File "./main_finetune.py", line 404, in <module>
    main(args)
  File "./main_finetune.py", line 305, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 387, in save_eval_json
    with open(json_pth, 'w') as J:
FileNotFoundError: [Errno 2] No such file or directory: '/ckpt/debug/cifar100-LT/vit_base_patch16/debug/result.json'
100%|██████████| 157/157 [00:50<00:00,  3.11it/s]
Traceback (most recent call last):
  File "./main_finetune.py", line 404, in <module>
    main(args)
  File "./main_finetune.py", line 305, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 387, in save_eval_json
    with open(json_pth, 'w') as J:
FileNotFoundError: [Errno 2] No such file or directory: '/ckpt/debug/cifar100-LT/vit_base_patch16/debug/result.json'
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 8931 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 8933 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 8930) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./main_finetune.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-11-21_13:53:16
  host      : user
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 8932)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_13:53:16
  host      : user
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 8930)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 0): env://, gpu 0
[13:54:02.229218] job dir: /home/vision/wonjun/LiVT-main
[13:54:02.229419] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
adamW2=0.95,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt_dir',
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=50,
eval=True,
finetune='<bound method Trainer.finetune of <util.trainer.Trainer object at 0x7f6a316c0e90>>',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='./exp//cifar100-LT/',
loss='ce',
lr=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=20,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[13:54:02.531881] Files already downloaded and verified
[13:54:03.264130] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[13:54:03.588157] Files already downloaded and verified
[13:54:03.958926] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[13:54:03.959148] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f9f6015f390>
[13:54:03.959525] Train on 10847 Image w.r.t. 100 classes
[13:54:05.725528] Model = vit_base_patch16
[13:54:05.725630] number of params (M): 85.96
[13:54:05.725661] base lr: 1.00e-03
[13:54:05.725682] actual lr: 2.50e-04
[13:54:05.725702] accumulate grad iterations: 1
[13:54:05.725720] effective batch size: 64
[13:54:06.350466] Resume checkpoint ./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth
  0%|          | 0/157 [00:00<?, ?it/s]  1%|          | 1/157 [00:03<08:07,  3.13s/it]  1%|▏         | 2/157 [00:03<03:46,  1.46s/it]  2%|▏         | 3/157 [00:03<02:22,  1.08it/s]  3%|▎         | 4/157 [00:04<01:43,  1.48it/s]  3%|▎         | 5/157 [00:04<01:21,  1.86it/s]  4%|▍         | 6/157 [00:04<01:08,  2.20it/s]  4%|▍         | 7/157 [00:04<01:00,  2.49it/s]  5%|▌         | 8/157 [00:05<00:54,  2.72it/s]  6%|▌         | 9/157 [00:05<00:50,  2.90it/s]  6%|▋         | 10/157 [00:05<00:48,  3.05it/s]  7%|▋         | 11/157 [00:06<00:46,  3.15it/s]  8%|▊         | 12/157 [00:06<00:44,  3.22it/s]  8%|▊         | 13/157 [00:06<00:43,  3.28it/s]  9%|▉         | 14/157 [00:06<00:43,  3.32it/s] 10%|▉         | 15/157 [00:07<00:42,  3.34it/s] 10%|█         | 16/157 [00:07<00:41,  3.36it/s] 11%|█         | 17/157 [00:07<00:41,  3.38it/s] 11%|█▏        | 18/157 [00:08<00:41,  3.39it/s] 12%|█▏        | 19/157 [00:08<00:40,  3.39it/s] 13%|█▎        | 20/157 [00:08<00:40,  3.40it/s] 13%|█▎        | 21/157 [00:08<00:40,  3.40it/s] 14%|█▍        | 22/157 [00:09<00:39,  3.40it/s] 15%|█▍        | 23/157 [00:09<00:39,  3.40it/s] 15%|█▌        | 24/157 [00:09<00:39,  3.41it/s] 16%|█▌        | 25/157 [00:10<00:38,  3.41it/s] 17%|█▋        | 26/157 [00:10<00:38,  3.41it/s] 17%|█▋        | 27/157 [00:10<00:38,  3.41it/s] 18%|█▊        | 28/157 [00:11<00:37,  3.41it/s] 18%|█▊        | 29/157 [00:11<00:37,  3.41it/s] 19%|█▉        | 30/157 [00:11<00:37,  3.41it/s] 20%|█▉        | 31/157 [00:11<00:37,  3.41it/s] 20%|██        | 32/157 [00:12<00:36,  3.41it/s] 21%|██        | 33/157 [00:12<00:36,  3.41it/s] 22%|██▏       | 34/157 [00:12<00:36,  3.41it/s] 22%|██▏       | 35/157 [00:13<00:35,  3.41it/s] 23%|██▎       | 36/157 [00:13<00:35,  3.40it/s] 24%|██▎       | 37/157 [00:13<00:35,  3.40it/s] 24%|██▍       | 38/157 [00:13<00:34,  3.40it/s] 25%|██▍       | 39/157 [00:14<00:34,  3.40it/s] 25%|██▌       | 40/157 [00:14<00:34,  3.40it/s] 26%|██▌       | 41/157 [00:14<00:34,  3.40it/s] 27%|██▋       | 42/157 [00:15<00:33,  3.40it/s] 27%|██▋       | 43/157 [00:15<00:33,  3.40it/s] 28%|██▊       | 44/157 [00:15<00:33,  3.39it/s] 29%|██▊       | 45/157 [00:16<00:33,  3.39it/s] 29%|██▉       | 46/157 [00:16<00:32,  3.39it/s] 30%|██▉       | 47/157 [00:16<00:32,  3.39it/s] 31%|███       | 48/157 [00:16<00:32,  3.39it/s] 31%|███       | 49/157 [00:17<00:31,  3.39it/s] 32%|███▏      | 50/157 [00:17<00:31,  3.39it/s] 32%|███▏      | 51/157 [00:17<00:31,  3.39it/s] 33%|███▎      | 52/157 [00:18<00:30,  3.39it/s] 34%|███▍      | 53/157 [00:18<00:30,  3.39it/s] 34%|███▍      | 54/157 [00:18<00:30,  3.39it/s] 35%|███▌      | 55/157 [00:18<00:30,  3.39it/s] 36%|███▌      | 56/157 [00:19<00:29,  3.39it/s] 36%|███▋      | 57/157 [00:19<00:29,  3.39it/s] 37%|███▋      | 58/157 [00:19<00:29,  3.39it/s] 38%|███▊      | 59/157 [00:20<00:28,  3.39it/s] 38%|███▊      | 60/157 [00:20<00:28,  3.39it/s] 39%|███▉      | 61/157 [00:20<00:28,  3.39it/s] 39%|███▉      | 62/157 [00:21<00:28,  3.39it/s] 40%|████      | 63/157 [00:21<00:27,  3.39it/s] 41%|████      | 64/157 [00:21<00:27,  3.39it/s] 41%|████▏     | 65/157 [00:21<00:27,  3.39it/s] 42%|████▏     | 66/157 [00:22<00:26,  3.39it/s] 43%|████▎     | 67/157 [00:22<00:26,  3.39it/s] 43%|████▎     | 68/157 [00:22<00:26,  3.39it/s] 44%|████▍     | 69/157 [00:23<00:25,  3.39it/s] 45%|████▍     | 70/157 [00:23<00:25,  3.39it/s] 45%|████▌     | 71/157 [00:23<00:25,  3.39it/s] 46%|████▌     | 72/157 [00:24<00:25,  3.39it/s] 46%|████▋     | 73/157 [00:24<00:24,  3.39it/s] 47%|████▋     | 74/157 [00:24<00:24,  3.39it/s] 48%|████▊     | 75/157 [00:24<00:24,  3.39it/s] 48%|████▊     | 76/157 [00:25<00:23,  3.39it/s] 49%|████▉     | 77/157 [00:25<00:23,  3.39it/s] 50%|████▉     | 78/157 [00:25<00:23,  3.39it/s] 50%|█████     | 79/157 [00:26<00:23,  3.38it/s] 51%|█████     | 80/157 [00:26<00:22,  3.38it/s] 52%|█████▏    | 81/157 [00:26<00:22,  3.38it/s] 52%|█████▏    | 82/157 [00:26<00:22,  3.38it/s] 53%|█████▎    | 83/157 [00:27<00:21,  3.38it/s] 54%|█████▎    | 84/157 [00:27<00:21,  3.38it/s] 54%|█████▍    | 85/157 [00:27<00:21,  3.38it/s] 55%|█████▍    | 86/157 [00:28<00:21,  3.38it/s] 55%|█████▌    | 87/157 [00:28<00:20,  3.38it/s] 56%|█████▌    | 88/157 [00:28<00:20,  3.38it/s] 57%|█████▋    | 89/157 [00:29<00:20,  3.38it/s] 57%|█████▋    | 90/157 [00:29<00:19,  3.38it/s] 58%|█████▊    | 91/157 [00:29<00:19,  3.38it/s] 59%|█████▊    | 92/157 [00:29<00:19,  3.38it/s] 59%|█████▉    | 93/157 [00:30<00:18,  3.38it/s] 60%|█████▉    | 94/157 [00:30<00:18,  3.38it/s] 61%|██████    | 95/157 [00:30<00:18,  3.38it/s] 61%|██████    | 96/157 [00:31<00:18,  3.38it/s] 62%|██████▏   | 97/157 [00:31<00:17,  3.38it/s] 62%|██████▏   | 98/157 [00:31<00:17,  3.38it/s] 63%|██████▎   | 99/157 [00:31<00:17,  3.38it/s] 64%|██████▎   | 100/157 [00:32<00:16,  3.38it/s] 64%|██████▍   | 101/157 [00:32<00:16,  3.38it/s] 65%|██████▍   | 102/157 [00:32<00:16,  3.38it/s] 66%|██████▌   | 103/157 [00:33<00:15,  3.38it/s] 66%|██████▌   | 104/157 [00:33<00:15,  3.38it/s] 67%|██████▋   | 105/157 [00:33<00:15,  3.37it/s] 68%|██████▊   | 106/157 [00:34<00:15,  3.38it/s] 68%|██████▊   | 107/157 [00:34<00:14,  3.38it/s] 69%|██████▉   | 108/157 [00:34<00:14,  3.37it/s] 69%|██████▉   | 109/157 [00:34<00:14,  3.37it/s] 70%|███████   | 110/157 [00:35<00:13,  3.37it/s] 71%|███████   | 111/157 [00:35<00:13,  3.37it/s] 71%|███████▏  | 112/157 [00:35<00:13,  3.37it/s] 72%|███████▏  | 113/157 [00:36<00:13,  3.37it/s] 73%|███████▎  | 114/157 [00:36<00:12,  3.37it/s] 73%|███████▎  | 115/157 [00:36<00:12,  3.37it/s] 74%|███████▍  | 116/157 [00:37<00:12,  3.37it/s] 75%|███████▍  | 117/157 [00:37<00:11,  3.38it/s] 75%|███████▌  | 118/157 [00:37<00:11,  3.37it/s] 76%|███████▌  | 119/157 [00:37<00:11,  3.37it/s] 76%|███████▋  | 120/157 [00:38<00:10,  3.38it/s] 77%|███████▋  | 121/157 [00:38<00:10,  3.37it/s] 78%|███████▊  | 122/157 [00:38<00:10,  3.37it/s] 78%|███████▊  | 123/157 [00:39<00:10,  3.37it/s] 79%|███████▉  | 124/157 [00:39<00:09,  3.37it/s] 80%|███████▉  | 125/157 [00:39<00:09,  3.38it/s] 80%|████████  | 126/157 [00:39<00:09,  3.37it/s] 81%|████████  | 127/157 [00:40<00:08,  3.37it/s] 82%|████████▏ | 128/157 [00:40<00:08,  3.37it/s] 82%|████████▏ | 129/157 [00:40<00:08,  3.37it/s] 83%|████████▎ | 130/157 [00:41<00:08,  3.37it/s] 83%|████████▎ | 131/157 [00:41<00:07,  3.37it/s] 84%|████████▍ | 132/157 [00:41<00:07,  3.37it/s] 85%|████████▍ | 133/157 [00:42<00:07,  3.37it/s] 85%|████████▌ | 134/157 [00:42<00:06,  3.37it/s] 86%|████████▌ | 135/157 [00:42<00:06,  3.36it/s] 87%|████████▋ | 136/157 [00:42<00:06,  3.36it/s] 87%|████████▋ | 137/157 [00:43<00:05,  3.37it/s] 88%|████████▊ | 138/157 [00:43<00:05,  3.37it/s] 89%|████████▊ | 139/157 [00:43<00:05,  3.36it/s] 89%|████████▉ | 140/157 [00:44<00:05,  3.37it/s] 90%|████████▉ | 141/157 [00:44<00:04,  3.37it/s] 90%|█████████ | 142/157 [00:44<00:04,  3.37it/s] 91%|█████████ | 143/157 [00:45<00:04,  3.37it/s] 92%|█████████▏| 144/157 [00:45<00:03,  3.36it/s] 92%|█████████▏| 145/157 [00:45<00:03,  3.36it/s] 93%|█████████▎| 146/157 [00:45<00:03,  3.36it/s] 94%|█████████▎| 147/157 [00:46<00:02,  3.36it/s] 94%|█████████▍| 148/157 [00:46<00:02,  3.36it/s] 95%|█████████▍| 149/157 [00:46<00:02,  3.36it/s] 96%|█████████▌| 150/157 [00:47<00:02,  3.36it/s] 96%|█████████▌| 151/157 [00:47<00:01,  3.36it/s] 97%|█████████▋| 152/157 [00:47<00:01,  3.36it/s] 97%|█████████▋| 153/157 [00:48<00:01,  3.36it/s] 98%|█████████▊| 154/157 [00:48<00:00,  3.36it/s] 99%|█████████▊| 155/157 [00:48<00:00,  3.36it/s] 99%|█████████▉| 156/157 [00:48<00:00,  3.36it/s]100%|██████████| 157/157 [00:49<00:00,  3.20it/s]
Traceback (most recent call last):
  File "./main_finetune.py", line 404, in <module>
    main(args)
  File "./main_finetune.py", line 305, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 387, in save_eval_json
    with open(json_pth, 'w') as J:
FileNotFoundError: [Errno 2] No such file or directory: '/ckpt/debug/cifar100-LT/vit_base_patch16/debug/result.json'
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 13145) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./main_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_13:54:59
  host      : user
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 13145)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 0): env://, gpu 0
[13:58:05.920818] job dir: /home/vision/wonjun/LiVT-main
[13:58:05.920993] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
adamW2=0.95,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt_dir',
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=50,
eval=True,
finetune='<bound method Trainer.finetune of <util.trainer.Trainer object at 0x7f4514388e90>>',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='./exp//cifar100-LT/',
loss='ce',
lr=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=20,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[13:58:06.220907] Files already downloaded and verified
[13:58:06.950881] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[13:58:07.275559] Files already downloaded and verified
[13:58:07.645275] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[13:58:07.645491] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f2a68457150>
[13:58:07.645841] Train on 10847 Image w.r.t. 100 classes
[13:58:09.411154] Model = vit_base_patch16
[13:58:09.411245] number of params (M): 85.96
[13:58:09.411277] base lr: 1.00e-03
[13:58:09.411298] actual lr: 2.50e-04
[13:58:09.411317] accumulate grad iterations: 1
[13:58:09.411335] effective batch size: 64
[13:58:10.039143] Resume checkpoint ./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth
  0%|          | 0/157 [00:00<?, ?it/s]  1%|          | 1/157 [00:03<07:55,  3.05s/it]  1%|▏         | 2/157 [00:03<03:41,  1.43s/it]  2%|▏         | 3/157 [00:03<02:19,  1.10it/s]  3%|▎         | 4/157 [00:03<01:41,  1.50it/s]  3%|▎         | 5/157 [00:04<01:20,  1.88it/s]  4%|▍         | 6/157 [00:04<01:07,  2.23it/s]  4%|▍         | 7/157 [00:04<00:59,  2.51it/s]  5%|▌         | 8/157 [00:05<00:54,  2.74it/s]  6%|▌         | 9/157 [00:05<00:50,  2.92it/s]  6%|▋         | 10/157 [00:05<00:48,  3.06it/s]  7%|▋         | 11/157 [00:05<00:46,  3.16it/s]  8%|▊         | 12/157 [00:06<00:44,  3.24it/s]  8%|▊         | 13/157 [00:06<00:43,  3.29it/s]  9%|▉         | 14/157 [00:06<00:42,  3.33it/s] 10%|▉         | 15/157 [00:07<00:42,  3.36it/s] 10%|█         | 16/157 [00:07<00:41,  3.37it/s] 11%|█         | 17/157 [00:07<00:41,  3.39it/s] 11%|█▏        | 18/157 [00:08<00:40,  3.40it/s] 12%|█▏        | 19/157 [00:08<00:40,  3.40it/s] 13%|█▎        | 20/157 [00:08<00:40,  3.41it/s] 13%|█▎        | 21/157 [00:08<00:39,  3.41it/s] 14%|█▍        | 22/157 [00:09<00:39,  3.41it/s] 15%|█▍        | 23/157 [00:09<00:39,  3.41it/s] 15%|█▌        | 24/157 [00:09<00:38,  3.41it/s] 16%|█▌        | 25/157 [00:10<00:38,  3.41it/s] 17%|█▋        | 26/157 [00:10<00:38,  3.41it/s] 17%|█▋        | 27/157 [00:10<00:38,  3.41it/s] 18%|█▊        | 28/157 [00:10<00:37,  3.41it/s] 18%|█▊        | 29/157 [00:11<00:37,  3.41it/s] 19%|█▉        | 30/157 [00:11<00:37,  3.41it/s] 20%|█▉        | 31/157 [00:11<00:36,  3.41it/s] 20%|██        | 32/157 [00:12<00:36,  3.41it/s] 21%|██        | 33/157 [00:12<00:36,  3.41it/s] 22%|██▏       | 34/157 [00:12<00:36,  3.41it/s] 22%|██▏       | 35/157 [00:12<00:35,  3.41it/s] 23%|██▎       | 36/157 [00:13<00:35,  3.41it/s] 24%|██▎       | 37/157 [00:13<00:35,  3.41it/s] 24%|██▍       | 38/157 [00:13<00:34,  3.41it/s] 25%|██▍       | 39/157 [00:14<00:34,  3.41it/s] 25%|██▌       | 40/157 [00:14<00:34,  3.41it/s] 26%|██▌       | 41/157 [00:14<00:34,  3.41it/s] 27%|██▋       | 42/157 [00:15<00:33,  3.41it/s] 27%|██▋       | 43/157 [00:15<00:33,  3.41it/s] 28%|██▊       | 44/157 [00:15<00:33,  3.41it/s] 29%|██▊       | 45/157 [00:15<00:32,  3.41it/s] 29%|██▉       | 46/157 [00:16<00:32,  3.41it/s] 30%|██▉       | 47/157 [00:16<00:32,  3.41it/s] 31%|███       | 48/157 [00:16<00:31,  3.41it/s] 31%|███       | 49/157 [00:17<00:31,  3.41it/s] 32%|███▏      | 50/157 [00:17<00:31,  3.41it/s] 32%|███▏      | 51/157 [00:17<00:31,  3.41it/s] 33%|███▎      | 52/157 [00:17<00:30,  3.41it/s] 34%|███▍      | 53/157 [00:18<00:30,  3.41it/s] 34%|███▍      | 54/157 [00:18<00:30,  3.40it/s] 35%|███▌      | 55/157 [00:18<00:29,  3.40it/s] 36%|███▌      | 56/157 [00:19<00:29,  3.40it/s] 36%|███▋      | 57/157 [00:19<00:29,  3.40it/s] 37%|███▋      | 58/157 [00:19<00:29,  3.40it/s] 38%|███▊      | 59/157 [00:20<00:28,  3.40it/s] 38%|███▊      | 60/157 [00:20<00:28,  3.40it/s] 39%|███▉      | 61/157 [00:20<00:28,  3.40it/s] 39%|███▉      | 62/157 [00:20<00:27,  3.40it/s] 40%|████      | 63/157 [00:21<00:27,  3.40it/s] 41%|████      | 64/157 [00:21<00:27,  3.40it/s] 41%|████▏     | 65/157 [00:21<00:27,  3.40it/s] 42%|████▏     | 66/157 [00:22<00:26,  3.40it/s] 43%|████▎     | 67/157 [00:22<00:26,  3.40it/s] 43%|████▎     | 68/157 [00:22<00:26,  3.40it/s] 44%|████▍     | 69/157 [00:22<00:25,  3.40it/s] 45%|████▍     | 70/157 [00:23<00:25,  3.40it/s] 45%|████▌     | 71/157 [00:23<00:25,  3.40it/s] 46%|████▌     | 72/157 [00:23<00:25,  3.40it/s] 46%|████▋     | 73/157 [00:24<00:24,  3.40it/s] 47%|████▋     | 74/157 [00:24<00:24,  3.40it/s] 48%|████▊     | 75/157 [00:24<00:24,  3.40it/s] 48%|████▊     | 76/157 [00:25<00:23,  3.40it/s] 49%|████▉     | 77/157 [00:25<00:23,  3.40it/s] 50%|████▉     | 78/157 [00:25<00:23,  3.40it/s] 50%|█████     | 79/157 [00:25<00:22,  3.40it/s] 51%|█████     | 80/157 [00:26<00:22,  3.40it/s] 52%|█████▏    | 81/157 [00:26<00:22,  3.40it/s] 52%|█████▏    | 82/157 [00:26<00:22,  3.40it/s] 53%|█████▎    | 83/157 [00:27<00:21,  3.40it/s] 54%|█████▎    | 84/157 [00:27<00:21,  3.40it/s] 54%|█████▍    | 85/157 [00:27<00:21,  3.39it/s] 55%|█████▍    | 86/157 [00:27<00:20,  3.39it/s] 55%|█████▌    | 87/157 [00:28<00:20,  3.39it/s] 56%|█████▌    | 88/157 [00:28<00:20,  3.39it/s] 57%|█████▋    | 89/157 [00:28<00:20,  3.39it/s] 57%|█████▋    | 90/157 [00:29<00:19,  3.39it/s] 58%|█████▊    | 91/157 [00:29<00:19,  3.40it/s] 59%|█████▊    | 92/157 [00:29<00:19,  3.40it/s] 59%|█████▉    | 93/157 [00:30<00:18,  3.40it/s] 60%|█████▉    | 94/157 [00:30<00:18,  3.39it/s] 61%|██████    | 95/157 [00:30<00:18,  3.39it/s] 61%|██████    | 96/157 [00:30<00:18,  3.38it/s] 62%|██████▏   | 97/157 [00:31<00:17,  3.38it/s] 62%|██████▏   | 98/157 [00:31<00:17,  3.38it/s] 63%|██████▎   | 99/157 [00:31<00:17,  3.38it/s] 64%|██████▎   | 100/157 [00:32<00:16,  3.38it/s] 64%|██████▍   | 101/157 [00:32<00:16,  3.38it/s] 65%|██████▍   | 102/157 [00:32<00:16,  3.38it/s] 66%|██████▌   | 103/157 [00:33<00:15,  3.38it/s] 66%|██████▌   | 104/157 [00:33<00:15,  3.38it/s] 67%|██████▋   | 105/157 [00:33<00:15,  3.38it/s] 68%|██████▊   | 106/157 [00:33<00:15,  3.38it/s] 68%|██████▊   | 107/157 [00:34<00:14,  3.37it/s] 69%|██████▉   | 108/157 [00:34<00:14,  3.37it/s] 69%|██████▉   | 109/157 [00:34<00:14,  3.37it/s] 70%|███████   | 110/157 [00:35<00:13,  3.37it/s] 71%|███████   | 111/157 [00:35<00:13,  3.37it/s] 71%|███████▏  | 112/157 [00:35<00:13,  3.38it/s] 72%|███████▏  | 113/157 [00:35<00:13,  3.37it/s] 73%|███████▎  | 114/157 [00:36<00:12,  3.37it/s] 73%|███████▎  | 115/157 [00:36<00:12,  3.37it/s] 74%|███████▍  | 116/157 [00:36<00:12,  3.37it/s] 75%|███████▍  | 117/157 [00:37<00:11,  3.38it/s] 75%|███████▌  | 118/157 [00:37<00:11,  3.38it/s] 76%|███████▌  | 119/157 [00:37<00:11,  3.38it/s] 76%|███████▋  | 120/157 [00:38<00:10,  3.38it/s] 77%|███████▋  | 121/157 [00:38<00:10,  3.38it/s] 78%|███████▊  | 122/157 [00:38<00:10,  3.38it/s] 78%|███████▊  | 123/157 [00:38<00:10,  3.38it/s] 79%|███████▉  | 124/157 [00:39<00:09,  3.38it/s] 80%|███████▉  | 125/157 [00:39<00:09,  3.38it/s] 80%|████████  | 126/157 [00:39<00:09,  3.37it/s] 81%|████████  | 127/157 [00:40<00:08,  3.38it/s] 82%|████████▏ | 128/157 [00:40<00:08,  3.38it/s] 82%|████████▏ | 129/157 [00:40<00:08,  3.38it/s] 83%|████████▎ | 130/157 [00:41<00:07,  3.38it/s] 83%|████████▎ | 131/157 [00:41<00:07,  3.38it/s] 84%|████████▍ | 132/157 [00:41<00:07,  3.38it/s] 85%|████████▍ | 133/157 [00:41<00:07,  3.38it/s] 85%|████████▌ | 134/157 [00:42<00:06,  3.38it/s] 86%|████████▌ | 135/157 [00:42<00:06,  3.38it/s] 87%|████████▋ | 136/157 [00:42<00:06,  3.38it/s] 87%|████████▋ | 137/157 [00:43<00:05,  3.38it/s] 88%|████████▊ | 138/157 [00:43<00:05,  3.38it/s] 89%|████████▊ | 139/157 [00:43<00:05,  3.38it/s] 89%|████████▉ | 140/157 [00:43<00:05,  3.38it/s] 90%|████████▉ | 141/157 [00:44<00:04,  3.38it/s] 90%|█████████ | 142/157 [00:44<00:04,  3.38it/s] 91%|█████████ | 143/157 [00:44<00:04,  3.38it/s] 92%|█████████▏| 144/157 [00:45<00:03,  3.38it/s] 92%|█████████▏| 145/157 [00:45<00:03,  3.38it/s] 93%|█████████▎| 146/157 [00:45<00:03,  3.38it/s] 94%|█████████▎| 147/157 [00:46<00:02,  3.38it/s] 94%|█████████▍| 148/157 [00:46<00:02,  3.38it/s] 95%|█████████▍| 149/157 [00:46<00:02,  3.37it/s] 96%|█████████▌| 150/157 [00:46<00:02,  3.37it/s] 96%|█████████▌| 151/157 [00:47<00:01,  3.37it/s] 97%|█████████▋| 152/157 [00:47<00:01,  3.37it/s] 97%|█████████▋| 153/157 [00:47<00:01,  3.37it/s] 98%|█████████▊| 154/157 [00:48<00:00,  3.37it/s] 99%|█████████▊| 155/157 [00:48<00:00,  3.37it/s] 99%|█████████▉| 156/157 [00:48<00:00,  3.37it/s]100%|██████████| 157/157 [00:48<00:00,  3.21it/s]
Traceback (most recent call last):
  File "./main_finetune.py", line 404, in <module>
    main(args)
  File "./main_finetune.py", line 305, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 385, in save_eval_json
    os.makedirs(path, exist_ok=True)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  [Previous line repeated 1 more time]
  File "/usr/local/lib/python3.7/os.py", line 223, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/ckpt'
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 14552) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./main_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_13:59:02
  host      : user
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 14552)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 0): env://, gpu 0
[14:03:54.892673] job dir: /home/vision/wonjun/LiVT-main
[14:03:54.892853] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
adamW2=0.95,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt_dir',
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=50,
eval=True,
finetune='<bound method Trainer.finetune of <util.trainer.Trainer object at 0x7f8ead304e90>>',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='./exp//cifar100-LT/',
loss='ce',
lr=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=20,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[14:03:55.194279] Files already downloaded and verified
[14:03:55.921485] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[14:03:56.256352] Files already downloaded and verified
[14:03:56.643449] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[14:03:56.643777] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fc2227cd390>
[14:03:56.644426] Train on 10847 Image w.r.t. 100 classes
[14:03:58.428644] Model = vit_base_patch16
[14:03:58.428802] number of params (M): 85.96
[14:03:58.428844] base lr: 1.00e-03
[14:03:58.428867] actual lr: 2.50e-04
[14:03:58.428888] accumulate grad iterations: 1
[14:03:58.428906] effective batch size: 64
[14:03:59.069992] Resume checkpoint ./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth
  0%|          | 0/157 [00:00<?, ?it/s]  1%|          | 1/157 [00:03<08:15,  3.17s/it]  1%|▏         | 2/157 [00:03<03:49,  1.48s/it]  2%|▏         | 3/157 [00:03<02:24,  1.07it/s]  3%|▎         | 4/157 [00:04<01:44,  1.47it/s]  3%|▎         | 5/157 [00:04<01:22,  1.85it/s]  4%|▍         | 6/157 [00:04<01:08,  2.19it/s]  4%|▍         | 7/157 [00:04<01:00,  2.48it/s]  5%|▌         | 8/157 [00:05<00:54,  2.72it/s]  6%|▌         | 9/157 [00:05<00:50,  2.91it/s]  6%|▋         | 10/157 [00:05<00:48,  3.05it/s]  7%|▋         | 11/157 [00:06<00:46,  3.15it/s]  8%|▊         | 12/157 [00:06<00:44,  3.23it/s]  8%|▊         | 13/157 [00:06<00:43,  3.28it/s]  9%|▉         | 14/157 [00:06<00:43,  3.32it/s] 10%|▉         | 15/157 [00:07<00:42,  3.35it/s] 10%|█         | 16/157 [00:07<00:41,  3.37it/s] 11%|█         | 17/157 [00:07<00:41,  3.38it/s] 11%|█▏        | 18/157 [00:08<00:40,  3.39it/s] 12%|█▏        | 19/157 [00:08<00:40,  3.40it/s] 13%|█▎        | 20/157 [00:08<00:40,  3.40it/s] 13%|█▎        | 21/157 [00:09<00:39,  3.41it/s] 14%|█▍        | 22/157 [00:09<00:39,  3.41it/s] 15%|█▍        | 23/157 [00:09<00:39,  3.41it/s] 15%|█▌        | 24/157 [00:09<00:38,  3.41it/s] 16%|█▌        | 25/157 [00:10<00:38,  3.41it/s] 17%|█▋        | 26/157 [00:10<00:38,  3.41it/s] 17%|█▋        | 27/157 [00:10<00:38,  3.41it/s] 18%|█▊        | 28/157 [00:11<00:37,  3.41it/s] 18%|█▊        | 29/157 [00:11<00:37,  3.41it/s] 19%|█▉        | 30/157 [00:11<00:37,  3.41it/s] 20%|█▉        | 31/157 [00:11<00:36,  3.41it/s] 20%|██        | 32/157 [00:12<00:36,  3.41it/s] 21%|██        | 33/157 [00:12<00:36,  3.41it/s] 22%|██▏       | 34/157 [00:12<00:36,  3.41it/s] 22%|██▏       | 35/157 [00:13<00:35,  3.41it/s] 23%|██▎       | 36/157 [00:13<00:35,  3.41it/s] 24%|██▎       | 37/157 [00:13<00:35,  3.41it/s] 24%|██▍       | 38/157 [00:14<00:34,  3.41it/s] 25%|██▍       | 39/157 [00:14<00:34,  3.41it/s] 25%|██▌       | 40/157 [00:14<00:34,  3.41it/s] 26%|██▌       | 41/157 [00:14<00:34,  3.41it/s] 27%|██▋       | 42/157 [00:15<00:33,  3.41it/s] 27%|██▋       | 43/157 [00:15<00:33,  3.41it/s] 28%|██▊       | 44/157 [00:15<00:33,  3.41it/s] 29%|██▊       | 45/157 [00:16<00:32,  3.41it/s] 29%|██▉       | 46/157 [00:16<00:32,  3.41it/s] 30%|██▉       | 47/157 [00:16<00:32,  3.41it/s] 31%|███       | 48/157 [00:16<00:31,  3.41it/s] 31%|███       | 49/157 [00:17<00:31,  3.41it/s] 32%|███▏      | 50/157 [00:17<00:31,  3.40it/s] 32%|███▏      | 51/157 [00:17<00:31,  3.40it/s] 33%|███▎      | 52/157 [00:18<00:30,  3.40it/s] 34%|███▍      | 53/157 [00:18<00:30,  3.40it/s] 34%|███▍      | 54/157 [00:18<00:30,  3.40it/s] 35%|███▌      | 55/157 [00:18<00:29,  3.40it/s] 36%|███▌      | 56/157 [00:19<00:29,  3.40it/s] 36%|███▋      | 57/157 [00:19<00:29,  3.40it/s] 37%|███▋      | 58/157 [00:19<00:29,  3.40it/s] 38%|███▊      | 59/157 [00:20<00:28,  3.40it/s] 38%|███▊      | 60/157 [00:20<00:28,  3.40it/s] 39%|███▉      | 61/157 [00:20<00:28,  3.40it/s] 39%|███▉      | 62/157 [00:21<00:27,  3.40it/s] 40%|████      | 63/157 [00:21<00:27,  3.40it/s] 41%|████      | 64/157 [00:21<00:27,  3.40it/s] 41%|████▏     | 65/157 [00:21<00:27,  3.40it/s] 42%|████▏     | 66/157 [00:22<00:26,  3.40it/s] 43%|████▎     | 67/157 [00:22<00:26,  3.40it/s] 43%|████▎     | 68/157 [00:22<00:26,  3.40it/s] 44%|████▍     | 69/157 [00:23<00:25,  3.40it/s] 45%|████▍     | 70/157 [00:23<00:25,  3.40it/s] 45%|████▌     | 71/157 [00:23<00:25,  3.40it/s] 46%|████▌     | 72/157 [00:23<00:25,  3.40it/s] 46%|████▋     | 73/157 [00:24<00:24,  3.40it/s] 47%|████▋     | 74/157 [00:24<00:24,  3.40it/s] 48%|████▊     | 75/157 [00:24<00:24,  3.40it/s] 48%|████▊     | 76/157 [00:25<00:23,  3.40it/s] 49%|████▉     | 77/157 [00:25<00:23,  3.39it/s] 50%|████▉     | 78/157 [00:25<00:23,  3.40it/s] 50%|█████     | 79/157 [00:26<00:22,  3.40it/s] 51%|█████     | 80/157 [00:26<00:22,  3.39it/s] 52%|█████▏    | 81/157 [00:26<00:22,  3.40it/s] 52%|█████▏    | 82/157 [00:26<00:22,  3.39it/s] 53%|█████▎    | 83/157 [00:27<00:21,  3.39it/s] 54%|█████▎    | 84/157 [00:27<00:21,  3.39it/s] 54%|█████▍    | 85/157 [00:27<00:21,  3.39it/s] 55%|█████▍    | 86/157 [00:28<00:20,  3.39it/s] 55%|█████▌    | 87/157 [00:28<00:20,  3.39it/s] 56%|█████▌    | 88/157 [00:28<00:20,  3.39it/s] 57%|█████▋    | 89/157 [00:29<00:20,  3.39it/s] 57%|█████▋    | 90/157 [00:29<00:19,  3.38it/s] 58%|█████▊    | 91/157 [00:29<00:19,  3.39it/s] 59%|█████▊    | 92/157 [00:29<00:19,  3.39it/s] 59%|█████▉    | 93/157 [00:30<00:18,  3.39it/s] 60%|█████▉    | 94/157 [00:30<00:18,  3.39it/s] 61%|██████    | 95/157 [00:30<00:18,  3.39it/s] 61%|██████    | 96/157 [00:31<00:17,  3.39it/s] 62%|██████▏   | 97/157 [00:31<00:17,  3.39it/s] 62%|██████▏   | 98/157 [00:31<00:17,  3.39it/s] 63%|██████▎   | 99/157 [00:31<00:17,  3.39it/s] 64%|██████▎   | 100/157 [00:32<00:16,  3.39it/s] 64%|██████▍   | 101/157 [00:32<00:16,  3.39it/s] 65%|██████▍   | 102/157 [00:32<00:16,  3.39it/s] 66%|██████▌   | 103/157 [00:33<00:15,  3.38it/s] 66%|██████▌   | 104/157 [00:33<00:15,  3.38it/s] 67%|██████▋   | 105/157 [00:33<00:15,  3.38it/s] 68%|██████▊   | 106/157 [00:34<00:15,  3.38it/s] 68%|██████▊   | 107/157 [00:34<00:14,  3.38it/s] 69%|██████▉   | 108/157 [00:34<00:14,  3.38it/s] 69%|██████▉   | 109/157 [00:34<00:14,  3.38it/s] 70%|███████   | 110/157 [00:35<00:13,  3.38it/s] 71%|███████   | 111/157 [00:35<00:13,  3.38it/s] 71%|███████▏  | 112/157 [00:35<00:13,  3.38it/s] 72%|███████▏  | 113/157 [00:36<00:13,  3.38it/s] 73%|███████▎  | 114/157 [00:36<00:12,  3.38it/s] 73%|███████▎  | 115/157 [00:36<00:12,  3.38it/s] 74%|███████▍  | 116/157 [00:36<00:12,  3.37it/s] 75%|███████▍  | 117/157 [00:37<00:11,  3.38it/s] 75%|███████▌  | 118/157 [00:37<00:11,  3.38it/s] 76%|███████▌  | 119/157 [00:37<00:11,  3.38it/s] 76%|███████▋  | 120/157 [00:38<00:10,  3.38it/s] 77%|███████▋  | 121/157 [00:38<00:10,  3.38it/s] 78%|███████▊  | 122/157 [00:38<00:10,  3.38it/s] 78%|███████▊  | 123/157 [00:39<00:10,  3.38it/s] 79%|███████▉  | 124/157 [00:39<00:09,  3.38it/s] 80%|███████▉  | 125/157 [00:39<00:09,  3.38it/s] 80%|████████  | 126/157 [00:39<00:09,  3.38it/s] 81%|████████  | 127/157 [00:40<00:08,  3.38it/s] 82%|████████▏ | 128/157 [00:40<00:08,  3.38it/s] 82%|████████▏ | 129/157 [00:40<00:08,  3.38it/s] 83%|████████▎ | 130/157 [00:41<00:07,  3.38it/s] 83%|████████▎ | 131/157 [00:41<00:07,  3.38it/s] 84%|████████▍ | 132/157 [00:41<00:07,  3.38it/s] 85%|████████▍ | 133/157 [00:42<00:07,  3.38it/s] 85%|████████▌ | 134/157 [00:42<00:06,  3.38it/s] 86%|████████▌ | 135/157 [00:42<00:06,  3.38it/s] 87%|████████▋ | 136/157 [00:42<00:06,  3.38it/s] 87%|████████▋ | 137/157 [00:43<00:05,  3.38it/s] 88%|████████▊ | 138/157 [00:43<00:05,  3.38it/s] 89%|████████▊ | 139/157 [00:43<00:05,  3.38it/s] 89%|████████▉ | 140/157 [00:44<00:05,  3.38it/s] 90%|████████▉ | 141/157 [00:44<00:04,  3.38it/s] 90%|█████████ | 142/157 [00:44<00:04,  3.38it/s] 91%|█████████ | 143/157 [00:44<00:04,  3.38it/s] 92%|█████████▏| 144/157 [00:45<00:03,  3.38it/s] 92%|█████████▏| 145/157 [00:45<00:03,  3.38it/s] 93%|█████████▎| 146/157 [00:45<00:03,  3.38it/s] 94%|█████████▎| 147/157 [00:46<00:02,  3.38it/s] 94%|█████████▍| 148/157 [00:46<00:02,  3.38it/s] 95%|█████████▍| 149/157 [00:46<00:02,  3.38it/s] 96%|█████████▌| 150/157 [00:47<00:02,  3.38it/s] 96%|█████████▌| 151/157 [00:47<00:01,  3.38it/s] 97%|█████████▋| 152/157 [00:47<00:01,  3.38it/s] 97%|█████████▋| 153/157 [00:47<00:01,  3.38it/s] 98%|█████████▊| 154/157 [00:48<00:00,  3.37it/s] 99%|█████████▊| 155/157 [00:48<00:00,  3.38it/s] 99%|█████████▉| 156/157 [00:48<00:00,  3.38it/s]100%|██████████| 157/157 [00:49<00:00,  3.20it/s]
[14:04:48.143215] {'avg_acc': 22.06, 'ece': 18.65, 'mce': 30.16, 'many': 43.37, 'medium': 17.43, 'few': 2.6, 'pdc': 1.77}
Traceback (most recent call last):
  File "./main_finetune.py", line 405, in <module>
    main(args)
  File "./main_finetune.py", line 306, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 385, in save_eval_json
    os.makedirs(path, exist_ok=True)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  [Previous line repeated 1 more time]
  File "/usr/local/lib/python3.7/os.py", line 223, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/ckpt'
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 15891) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./main_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_14:04:51
  host      : user
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 15891)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 0): env://, gpu 0
[14:10:42.076587] job dir: /home/vision/wonjun/LiVT-main
[14:10:42.076755] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
adamW2=0.95,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt_dir',
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=50,
eval=True,
finetune='<bound method Trainer.finetune of <util.trainer.Trainer object at 0x7fc666a18e90>>',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='./exp//cifar100-LT/',
loss='ce',
lr=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=20,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[14:10:42.377794] Files already downloaded and verified
[14:10:43.107619] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[14:10:43.439577] Files already downloaded and verified
[14:10:43.810083] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[14:10:43.810290] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7f1b61209750>
[14:10:43.810635] Train on 10847 Image w.r.t. 100 classes
[14:10:45.571831] Model = vit_base_patch16
[14:10:45.571948] number of params (M): 85.96
[14:10:45.571980] base lr: 1.00e-03
[14:10:45.572000] actual lr: 2.50e-04
[14:10:45.572020] accumulate grad iterations: 1
[14:10:45.572038] effective batch size: 64
[14:10:46.277489] Resume checkpoint ./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth
  0%|          | 0/157 [00:00<?, ?it/s]  1%|          | 1/157 [00:03<08:08,  3.13s/it]  1%|▏         | 2/157 [00:03<03:46,  1.46s/it]  2%|▏         | 3/157 [00:03<02:22,  1.08it/s]  3%|▎         | 4/157 [00:04<01:43,  1.48it/s]  3%|▎         | 5/157 [00:04<01:21,  1.86it/s]  4%|▍         | 6/157 [00:04<01:08,  2.20it/s]  4%|▍         | 7/157 [00:04<01:00,  2.49it/s]  5%|▌         | 8/157 [00:05<00:54,  2.73it/s]  6%|▌         | 9/157 [00:05<00:50,  2.91it/s]  6%|▋         | 10/157 [00:05<00:48,  3.05it/s]  7%|▋         | 11/157 [00:06<00:46,  3.16it/s]  8%|▊         | 12/157 [00:06<00:44,  3.23it/s]  8%|▊         | 13/157 [00:06<00:43,  3.29it/s]  9%|▉         | 14/157 [00:06<00:43,  3.32it/s] 10%|▉         | 15/157 [00:07<00:42,  3.35it/s] 10%|█         | 16/157 [00:07<00:41,  3.37it/s] 11%|█         | 17/157 [00:07<00:41,  3.39it/s] 11%|█▏        | 18/157 [00:08<00:40,  3.40it/s] 12%|█▏        | 19/157 [00:08<00:40,  3.40it/s] 13%|█▎        | 20/157 [00:08<00:40,  3.41it/s] 13%|█▎        | 21/157 [00:08<00:39,  3.41it/s] 14%|█▍        | 22/157 [00:09<00:39,  3.42it/s] 15%|█▍        | 23/157 [00:09<00:39,  3.42it/s] 15%|█▌        | 24/157 [00:09<00:38,  3.42it/s] 16%|█▌        | 25/157 [00:10<00:38,  3.42it/s] 17%|█▋        | 26/157 [00:10<00:38,  3.42it/s] 17%|█▋        | 27/157 [00:10<00:38,  3.42it/s] 18%|█▊        | 28/157 [00:11<00:37,  3.42it/s] 18%|█▊        | 29/157 [00:11<00:37,  3.42it/s] 19%|█▉        | 30/157 [00:11<00:37,  3.42it/s] 20%|█▉        | 31/157 [00:11<00:36,  3.42it/s] 20%|██        | 32/157 [00:12<00:36,  3.42it/s] 21%|██        | 33/157 [00:12<00:36,  3.42it/s] 22%|██▏       | 34/157 [00:12<00:35,  3.42it/s] 22%|██▏       | 35/157 [00:13<00:35,  3.42it/s] 23%|██▎       | 36/157 [00:13<00:35,  3.42it/s] 24%|██▎       | 37/157 [00:13<00:35,  3.41it/s] 24%|██▍       | 38/157 [00:13<00:34,  3.41it/s] 25%|██▍       | 39/157 [00:14<00:34,  3.41it/s] 25%|██▌       | 40/157 [00:14<00:34,  3.41it/s] 26%|██▌       | 41/157 [00:14<00:33,  3.41it/s] 27%|██▋       | 42/157 [00:15<00:33,  3.41it/s] 27%|██▋       | 43/157 [00:15<00:33,  3.41it/s] 28%|██▊       | 44/157 [00:15<00:33,  3.41it/s] 29%|██▊       | 45/157 [00:15<00:32,  3.41it/s] 29%|██▉       | 46/157 [00:16<00:32,  3.41it/s] 30%|██▉       | 47/157 [00:16<00:32,  3.41it/s] 31%|███       | 48/157 [00:16<00:32,  3.41it/s] 31%|███       | 49/157 [00:17<00:31,  3.40it/s] 32%|███▏      | 50/157 [00:17<00:31,  3.40it/s] 32%|███▏      | 51/157 [00:17<00:31,  3.40it/s] 33%|███▎      | 52/157 [00:18<00:30,  3.40it/s] 34%|███▍      | 53/157 [00:18<00:30,  3.40it/s] 34%|███▍      | 54/157 [00:18<00:30,  3.40it/s] 35%|███▌      | 55/157 [00:18<00:29,  3.40it/s] 36%|███▌      | 56/157 [00:19<00:29,  3.40it/s] 36%|███▋      | 57/157 [00:19<00:29,  3.40it/s] 37%|███▋      | 58/157 [00:19<00:29,  3.40it/s] 38%|███▊      | 59/157 [00:20<00:28,  3.40it/s] 38%|███▊      | 60/157 [00:20<00:28,  3.40it/s] 39%|███▉      | 61/157 [00:20<00:28,  3.40it/s] 39%|███▉      | 62/157 [00:20<00:27,  3.40it/s] 40%|████      | 63/157 [00:21<00:27,  3.40it/s] 41%|████      | 64/157 [00:21<00:27,  3.40it/s] 41%|████▏     | 65/157 [00:21<00:27,  3.40it/s] 42%|████▏     | 66/157 [00:22<00:26,  3.40it/s] 43%|████▎     | 67/157 [00:22<00:26,  3.40it/s] 43%|████▎     | 68/157 [00:22<00:26,  3.40it/s] 44%|████▍     | 69/157 [00:23<00:25,  3.40it/s] 45%|████▍     | 70/157 [00:23<00:25,  3.40it/s] 45%|████▌     | 71/157 [00:23<00:25,  3.40it/s] 46%|████▌     | 72/157 [00:23<00:24,  3.40it/s] 46%|████▋     | 73/157 [00:24<00:24,  3.40it/s] 47%|████▋     | 74/157 [00:24<00:24,  3.40it/s] 48%|████▊     | 75/157 [00:24<00:24,  3.40it/s] 48%|████▊     | 76/157 [00:25<00:23,  3.40it/s] 49%|████▉     | 77/157 [00:25<00:23,  3.40it/s] 50%|████▉     | 78/157 [00:25<00:23,  3.40it/s] 50%|█████     | 79/157 [00:25<00:22,  3.40it/s] 51%|█████     | 80/157 [00:26<00:22,  3.40it/s] 52%|█████▏    | 81/157 [00:26<00:22,  3.40it/s] 52%|█████▏    | 82/157 [00:26<00:22,  3.40it/s] 53%|█████▎    | 83/157 [00:27<00:21,  3.40it/s] 54%|█████▎    | 84/157 [00:27<00:21,  3.39it/s] 54%|█████▍    | 85/157 [00:27<00:21,  3.40it/s] 55%|█████▍    | 86/157 [00:28<00:20,  3.40it/s] 55%|█████▌    | 87/157 [00:28<00:20,  3.39it/s] 56%|█████▌    | 88/157 [00:28<00:20,  3.39it/s] 57%|█████▋    | 89/157 [00:28<00:20,  3.39it/s] 57%|█████▋    | 90/157 [00:29<00:19,  3.39it/s] 58%|█████▊    | 91/157 [00:29<00:19,  3.39it/s] 59%|█████▊    | 92/157 [00:29<00:19,  3.39it/s] 59%|█████▉    | 93/157 [00:30<00:18,  3.39it/s] 60%|█████▉    | 94/157 [00:30<00:18,  3.39it/s] 61%|██████    | 95/157 [00:30<00:18,  3.39it/s] 61%|██████    | 96/157 [00:31<00:17,  3.39it/s] 62%|██████▏   | 97/157 [00:31<00:17,  3.39it/s] 62%|██████▏   | 98/157 [00:31<00:17,  3.39it/s] 63%|██████▎   | 99/157 [00:31<00:17,  3.39it/s] 64%|██████▎   | 100/157 [00:32<00:16,  3.38it/s] 64%|██████▍   | 101/157 [00:32<00:16,  3.39it/s] 65%|██████▍   | 102/157 [00:32<00:16,  3.39it/s] 66%|██████▌   | 103/157 [00:33<00:15,  3.38it/s] 66%|██████▌   | 104/157 [00:33<00:15,  3.39it/s] 67%|██████▋   | 105/157 [00:33<00:15,  3.38it/s] 68%|██████▊   | 106/157 [00:33<00:15,  3.38it/s] 68%|██████▊   | 107/157 [00:34<00:14,  3.38it/s] 69%|██████▉   | 108/157 [00:34<00:14,  3.38it/s] 69%|██████▉   | 109/157 [00:34<00:14,  3.38it/s] 70%|███████   | 110/157 [00:35<00:13,  3.38it/s] 71%|███████   | 111/157 [00:35<00:13,  3.38it/s] 71%|███████▏  | 112/157 [00:35<00:13,  3.38it/s] 72%|███████▏  | 113/157 [00:36<00:13,  3.38it/s] 73%|███████▎  | 114/157 [00:36<00:12,  3.38it/s] 73%|███████▎  | 115/157 [00:36<00:12,  3.38it/s] 74%|███████▍  | 116/157 [00:36<00:12,  3.38it/s] 75%|███████▍  | 117/157 [00:37<00:11,  3.38it/s] 75%|███████▌  | 118/157 [00:37<00:11,  3.38it/s] 76%|███████▌  | 119/157 [00:37<00:11,  3.38it/s] 76%|███████▋  | 120/157 [00:38<00:10,  3.38it/s] 77%|███████▋  | 121/157 [00:38<00:10,  3.38it/s] 78%|███████▊  | 122/157 [00:38<00:10,  3.38it/s] 78%|███████▊  | 123/157 [00:38<00:10,  3.38it/s] 79%|███████▉  | 124/157 [00:39<00:09,  3.38it/s] 80%|███████▉  | 125/157 [00:39<00:09,  3.38it/s] 80%|████████  | 126/157 [00:39<00:09,  3.38it/s] 81%|████████  | 127/157 [00:40<00:08,  3.38it/s] 82%|████████▏ | 128/157 [00:40<00:08,  3.38it/s] 82%|████████▏ | 129/157 [00:40<00:08,  3.38it/s] 83%|████████▎ | 130/157 [00:41<00:07,  3.39it/s] 83%|████████▎ | 131/157 [00:41<00:07,  3.38it/s] 84%|████████▍ | 132/157 [00:41<00:07,  3.39it/s] 85%|████████▍ | 133/157 [00:41<00:07,  3.38it/s] 85%|████████▌ | 134/157 [00:42<00:06,  3.38it/s] 86%|████████▌ | 135/157 [00:42<00:06,  3.39it/s] 87%|████████▋ | 136/157 [00:42<00:06,  3.38it/s] 87%|████████▋ | 137/157 [00:43<00:05,  3.38it/s] 88%|████████▊ | 138/157 [00:43<00:05,  3.38it/s] 89%|████████▊ | 139/157 [00:43<00:05,  3.38it/s] 89%|████████▉ | 140/157 [00:44<00:05,  3.38it/s] 90%|████████▉ | 141/157 [00:44<00:04,  3.38it/s] 90%|█████████ | 142/157 [00:44<00:04,  3.38it/s] 91%|█████████ | 143/157 [00:44<00:04,  3.38it/s] 92%|█████████▏| 144/157 [00:45<00:03,  3.38it/s] 92%|█████████▏| 145/157 [00:45<00:03,  3.38it/s] 93%|█████████▎| 146/157 [00:45<00:03,  3.38it/s] 94%|█████████▎| 147/157 [00:46<00:02,  3.38it/s] 94%|█████████▍| 148/157 [00:46<00:02,  3.38it/s] 95%|█████████▍| 149/157 [00:46<00:02,  3.38it/s] 96%|█████████▌| 150/157 [00:46<00:02,  3.38it/s] 96%|█████████▌| 151/157 [00:47<00:01,  3.37it/s] 97%|█████████▋| 152/157 [00:47<00:01,  3.37it/s] 97%|█████████▋| 153/157 [00:47<00:01,  3.37it/s] 98%|█████████▊| 154/157 [00:48<00:00,  3.37it/s] 99%|█████████▊| 155/157 [00:48<00:00,  3.37it/s] 99%|█████████▉| 156/157 [00:48<00:00,  3.37it/s]100%|██████████| 157/157 [00:48<00:00,  3.21it/s]
[14:11:35.286487] {'avg_acc': 22.06, 'ece': 18.65, 'mce': 30.16, 'many': 49.19, 'medium': 23.29, 'few': 7.36, 'pdc': 1.77}
Traceback (most recent call last):
  File "./main_finetune.py", line 405, in <module>
    main(args)
  File "./main_finetune.py", line 306, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 385, in save_eval_json
    os.makedirs(path, exist_ok=True)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  [Previous line repeated 1 more time]
  File "/usr/local/lib/python3.7/os.py", line 223, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/ckpt'
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 17371) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./main_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_14:11:38
  host      : user
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 17371)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 0): env://, gpu 0
[14:12:32.514317] job dir: /home/vision/wonjun/LiVT-main
[14:12:32.514442] [14:12:32.514625] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=8,
adamW2=0.999,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt/debug/cifar100-LT/vit_base_patch16/debug',
clip_grad=None,
color_jitter=None,
cutmix=1.0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=True,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.2,
epochs=100,
eval=False,
finetune='./ckpt/debug/cifar100-LT/debug/checkpoint.pth',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.65,
local_rank=0,
log_dir='./exp/debug/cifar100-LT/vit_base_patch16/debug',
loss='Bal_CE',
lr=None,
min_lr=1e-06,
mixup=0.8,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=1,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=4)
[14:12:32.514696] [14:12:32.824606] Files already downloaded and verified
[14:12:33.569770] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[14:12:33.903299] Files already downloaded and verified
[14:12:34.283661] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[14:12:34.284645] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7fa65933df50>
[14:12:34.284936] [14:12:34.289395] Train on 10847 Image w.r.t. 100 classes
[14:12:34.289487] [14:12:36.996136] Load pre-trained checkpoint from: ./ckpt/debug/cifar100-LT/debug/checkpoint.pth
[14:12:36.996326] [14:12:37.077049] _IncompatibleKeys(missing_keys=['fc_norm.weight', 'fc_norm.bias', 'head.weight', 'head.bias'], unexpected_keys=['mask_token', 'decoder_pos_embed', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.attn.qkv.weight', 'decoder_blocks.0.attn.qkv.bias', 'decoder_blocks.0.attn.qkv.scaling_factor', 'decoder_blocks.0.attn.proj.weight', 'decoder_blocks.0.attn.proj.bias', 'decoder_blocks.0.attn.proj.scaling_factor', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.0.mlp.fc1.weight', 'decoder_blocks.0.mlp.fc1.bias', 'decoder_blocks.0.mlp.fc1.scaling_factor', 'decoder_blocks.0.mlp.fc2.weight', 'decoder_blocks.0.mlp.fc2.bias', 'decoder_blocks.0.mlp.fc2.scaling_factor', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.attn.qkv.weight', 'decoder_blocks.1.attn.qkv.bias', 'decoder_blocks.1.attn.qkv.scaling_factor', 'decoder_blocks.1.attn.proj.weight', 'decoder_blocks.1.attn.proj.bias', 'decoder_blocks.1.attn.proj.scaling_factor', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_blocks.1.mlp.fc1.weight', 'decoder_blocks.1.mlp.fc1.bias', 'decoder_blocks.1.mlp.fc1.scaling_factor', 'decoder_blocks.1.mlp.fc2.weight', 'decoder_blocks.1.mlp.fc2.bias', 'decoder_blocks.1.mlp.fc2.scaling_factor', 'decoder_blocks.2.norm1.weight', 'decoder_blocks.2.norm1.bias', 'decoder_blocks.2.attn.qkv.weight', 'decoder_blocks.2.attn.qkv.bias', 'decoder_blocks.2.attn.qkv.scaling_factor', 'decoder_blocks.2.attn.proj.weight', 'decoder_blocks.2.attn.proj.bias', 'decoder_blocks.2.attn.proj.scaling_factor', 'decoder_blocks.2.norm2.weight', 'decoder_blocks.2.norm2.bias', 'decoder_blocks.2.mlp.fc1.weight', 'decoder_blocks.2.mlp.fc1.bias', 'decoder_blocks.2.mlp.fc1.scaling_factor', 'decoder_blocks.2.mlp.fc2.weight', 'decoder_blocks.2.mlp.fc2.bias', 'decoder_blocks.2.mlp.fc2.scaling_factor', 'decoder_blocks.3.norm1.weight', 'decoder_blocks.3.norm1.bias', 'decoder_blocks.3.attn.qkv.weight', 'decoder_blocks.3.attn.qkv.bias', 'decoder_blocks.3.attn.qkv.scaling_factor', 'decoder_blocks.3.attn.proj.weight', 'decoder_blocks.3.attn.proj.bias', 'decoder_blocks.3.attn.proj.scaling_factor', 'decoder_blocks.3.norm2.weight', 'decoder_blocks.3.norm2.bias', 'decoder_blocks.3.mlp.fc1.weight', 'decoder_blocks.3.mlp.fc1.bias', 'decoder_blocks.3.mlp.fc1.scaling_factor', 'decoder_blocks.3.mlp.fc2.weight', 'decoder_blocks.3.mlp.fc2.bias', 'decoder_blocks.3.mlp.fc2.scaling_factor', 'decoder_blocks.4.norm1.weight', 'decoder_blocks.4.norm1.bias', 'decoder_blocks.4.attn.qkv.weight', 'decoder_blocks.4.attn.qkv.bias', 'decoder_blocks.4.attn.qkv.scaling_factor', 'decoder_blocks.4.attn.proj.weight', 'decoder_blocks.4.attn.proj.bias', 'decoder_blocks.4.attn.proj.scaling_factor', 'decoder_blocks.4.norm2.weight', 'decoder_blocks.4.norm2.bias', 'decoder_blocks.4.mlp.fc1.weight', 'decoder_blocks.4.mlp.fc1.bias', 'decoder_blocks.4.mlp.fc1.scaling_factor', 'decoder_blocks.4.mlp.fc2.weight', 'decoder_blocks.4.mlp.fc2.bias', 'decoder_blocks.4.mlp.fc2.scaling_factor', 'decoder_blocks.5.norm1.weight', 'decoder_blocks.5.norm1.bias', 'decoder_blocks.5.attn.qkv.weight', 'decoder_blocks.5.attn.qkv.bias', 'decoder_blocks.5.attn.qkv.scaling_factor', 'decoder_blocks.5.attn.proj.weight', 'decoder_blocks.5.attn.proj.bias', 'decoder_blocks.5.attn.proj.scaling_factor', 'decoder_blocks.5.norm2.weight', 'decoder_blocks.5.norm2.bias', 'decoder_blocks.5.mlp.fc1.weight', 'decoder_blocks.5.mlp.fc1.bias', 'decoder_blocks.5.mlp.fc1.scaling_factor', 'decoder_blocks.5.mlp.fc2.weight', 'decoder_blocks.5.mlp.fc2.bias', 'decoder_blocks.5.mlp.fc2.scaling_factor', 'decoder_blocks.6.norm1.weight', 'decoder_blocks.6.norm1.bias', 'decoder_blocks.6.attn.qkv.weight', 'decoder_blocks.6.attn.qkv.bias', 'decoder_blocks.6.attn.qkv.scaling_factor', 'decoder_blocks.6.attn.proj.weight', 'decoder_blocks.6.attn.proj.bias', 'decoder_blocks.6.attn.proj.scaling_factor', 'decoder_blocks.6.norm2.weight', 'decoder_blocks.6.norm2.bias', 'decoder_blocks.6.mlp.fc1.weight', 'decoder_blocks.6.mlp.fc1.bias', 'decoder_blocks.6.mlp.fc1.scaling_factor', 'decoder_blocks.6.mlp.fc2.weight', 'decoder_blocks.6.mlp.fc2.bias', 'decoder_blocks.6.mlp.fc2.scaling_factor', 'decoder_blocks.7.norm1.weight', 'decoder_blocks.7.norm1.bias', 'decoder_blocks.7.attn.qkv.weight', 'decoder_blocks.7.attn.qkv.bias', 'decoder_blocks.7.attn.qkv.scaling_factor', 'decoder_blocks.7.attn.proj.weight', 'decoder_blocks.7.attn.proj.bias', 'decoder_blocks.7.attn.proj.scaling_factor', 'decoder_blocks.7.norm2.weight', 'decoder_blocks.7.norm2.bias', 'decoder_blocks.7.mlp.fc1.weight', 'decoder_blocks.7.mlp.fc1.bias', 'decoder_blocks.7.mlp.fc1.scaling_factor', 'decoder_blocks.7.mlp.fc2.weight', 'decoder_blocks.7.mlp.fc2.bias', 'decoder_blocks.7.mlp.fc2.scaling_factor', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
[14:12:37.077405] [14:12:37.151521] Model = vit_base_patch16
[14:12:37.151742] [14:12:37.151824] number of params (M): 85.96
[14:12:37.151898] [14:12:37.151962] base lr: 1.00e-03
[14:12:37.152046] [14:12:37.152142] actual lr: 8.00e-03
[14:12:37.152213] [14:12:37.152267] accumulate grad iterations: 8
[14:12:37.152333] [14:12:37.152384] effective batch size: 2048
[14:12:37.152479] [14:12:37.348711] Mixup is activated!
[14:12:37.348822] [14:12:37.349551] criterion = Bal_CE_loss()
[14:12:37.349653] [14:12:37.350038] Save config to: ./exp/debug/cifar100-LT/vit_base_patch16/debug/args.txt
[14:12:37.350101] Start training for 100 epochs
[14:12:37.350176] [14:12:37.351860] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:12:42.170004] Epoch: [0]  [ 0/42]  eta: 0:03:22  lr: 0.000000  loss: 3.9413 (3.9413)  time: 4.8171  data: 2.1216  max mem: 9052
[14:13:01.434676] Epoch: [0]  [41/42]  eta: 0:00:00  lr: 0.001524  loss: 3.9515 (3.9735)  time: 0.4706  data: 0.0001  max mem: 10039
[14:13:01.606570] Epoch: [0] Total time: 0:00:24 (0.5775 s / it)
[14:13:01.634746] Averaged stats: lr: 0.001524  loss: 3.9515 (4.0016)
[14:13:03.353461] Test:  [ 0/40]  eta: 0:01:08  loss: 4.5139 (4.5139)  acc1: 7.8125 (7.8125)  acc5: 17.1875 (17.1875)  time: 1.7145  data: 1.5602  max mem: 10039
[14:13:09.329338] Test:  [39/40]  eta: 0:00:00  loss: 4.5212 (4.5327)  acc1: 4.6875 (3.8800)  acc5: 12.5000 (13.3600)  time: 0.1525  data: 0.0001  max mem: 10039
[14:13:09.478930] Test: Total time: 0:00:07 (0.1960 s / it)
[14:13:09.734179] * Acc@1 3.640 Acc@5 13.190 loss 4.534
[14:13:09.734390] Accuracy of the network on the 10000 test images: 3.6%
[14:13:09.734614] [14:13:13.226561] Max accuracy: 3.64%
[14:13:13.226833] [14:13:13.227851] {"train_lr": 0.0006530612244897961, "train_loss": 4.001639211461658, "test_loss": 4.53423843383789, "test_acc1": 3.64, "test_acc5": 13.19, "epoch": 0, "n_parameters": 85958500}
[14:13:13.227971] [14:13:13.228032] Training epoch 0 for 0:00:35
[14:13:13.228095] [14:13:13.230811] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:13:15.007751] Epoch: [1]  [ 0/42]  eta: 0:01:14  lr: 0.001600  loss: 3.9480 (3.9480)  time: 1.7761  data: 1.2881  max mem: 10040
[14:13:34.355248] Epoch: [1]  [41/42]  eta: 0:00:00  lr: 0.003124  loss: 3.9459 (3.9478)  time: 0.4726  data: 0.0001  max mem: 10046
[14:13:34.568995] Epoch: [1] Total time: 0:00:21 (0.5080 s / it)
[14:13:34.594975] Averaged stats: lr: 0.003124  loss: 3.9459 (3.9536)
[14:13:36.368577] Test:  [ 0/40]  eta: 0:01:10  loss: 4.3699 (4.3699)  acc1: 6.2500 (6.2500)  acc5: 20.3125 (20.3125)  time: 1.7700  data: 1.5942  max mem: 10046
[14:13:42.272999] Test:  [39/40]  eta: 0:00:00  loss: 4.3450 (4.3629)  acc1: 3.1250 (4.6800)  acc5: 17.1875 (16.5200)  time: 0.1481  data: 0.0001  max mem: 10046
[14:13:42.382637] Test: Total time: 0:00:07 (0.1946 s / it)
[14:13:42.836038] * Acc@1 4.680 Acc@5 16.420 loss 4.369
[14:13:42.836248] Accuracy of the network on the 10000 test images: 4.7%
[14:13:42.836495] [14:13:46.192054] Max accuracy: 4.68%
[14:13:46.192345] [14:13:46.193270] {"train_lr": 0.0022530612244897968, "train_loss": 3.95357593752089, "test_loss": 4.3691864013671875, "test_acc1": 4.68, "test_acc5": 16.42, "epoch": 1, "n_parameters": 85958500}
[14:13:46.193399] [14:13:46.193464] Training epoch 1 for 0:00:32
[14:13:46.193518] [14:13:46.196353] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:13:48.153788] Epoch: [2]  [ 0/42]  eta: 0:01:22  lr: 0.003200  loss: 3.9635 (3.9635)  time: 1.9562  data: 1.4840  max mem: 10046
[14:14:07.576912] Epoch: [2]  [41/42]  eta: 0:00:00  lr: 0.004724  loss: 3.8700 (3.9127)  time: 0.4751  data: 0.0001  max mem: 10046
[14:14:07.801776] Epoch: [2] Total time: 0:00:21 (0.5144 s / it)
[14:14:07.815255] Averaged stats: lr: 0.004724  loss: 3.8700 (3.9246)
[14:14:09.667817] Test:  [ 0/40]  eta: 0:01:13  loss: 4.3381 (4.3381)  acc1: 4.6875 (4.6875)  acc5: 15.6250 (15.6250)  time: 1.8490  data: 1.6709  max mem: 10046
[14:14:15.599879] Test:  [39/40]  eta: 0:00:00  loss: 4.2643 (4.2615)  acc1: 6.2500 (6.1200)  acc5: 20.3125 (20.0800)  time: 0.1483  data: 0.0001  max mem: 10046
[14:14:15.742931] Test: Total time: 0:00:07 (0.1981 s / it)
[14:14:15.914960] * Acc@1 5.800 Acc@5 19.570 loss 4.284
[14:14:15.915177] Accuracy of the network on the 10000 test images: 5.8%
[14:14:15.915404] [14:14:18.780707] Max accuracy: 5.80%
[14:14:18.780998] [14:14:18.781884] {"train_lr": 0.003853061224489796, "train_loss": 3.924637939248766, "test_loss": 4.283944797515869, "test_acc1": 5.8, "test_acc5": 19.57, "epoch": 2, "n_parameters": 85958500}
[14:14:18.781956] [14:14:18.782018] Training epoch 2 for 0:00:32
[14:14:18.782069] [14:14:18.784913] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:14:20.836831] Epoch: [3]  [ 0/42]  eta: 0:01:26  lr: 0.004800  loss: 4.0026 (4.0026)  time: 2.0505  data: 1.5662  max mem: 10046
[14:14:40.254920] Epoch: [3]  [41/42]  eta: 0:00:00  lr: 0.006324  loss: 3.8716 (3.9115)  time: 0.4736  data: 0.0001  max mem: 10046
[14:14:40.470797] Epoch: [3] Total time: 0:00:21 (0.5163 s / it)
[14:14:40.491643] Averaged stats: lr: 0.006324  loss: 3.8716 (3.9005)
[14:14:42.275092] Test:  [ 0/40]  eta: 0:01:11  loss: 4.2620 (4.2620)  acc1: 6.2500 (6.2500)  acc5: 23.4375 (23.4375)  time: 1.7793  data: 1.6074  max mem: 10046
[14:14:48.213919] Test:  [39/40]  eta: 0:00:00  loss: 4.2018 (4.2014)  acc1: 6.2500 (6.1200)  acc5: 21.8750 (22.5600)  time: 0.1492  data: 0.0001  max mem: 10046
[14:14:48.325845] Test: Total time: 0:00:07 (0.1958 s / it)
[14:14:48.683389] * Acc@1 6.060 Acc@5 22.280 loss 4.231
[14:14:48.683592] Accuracy of the network on the 10000 test images: 6.1%
[14:14:48.683790] [14:14:52.188229] Max accuracy: 6.06%
[14:14:52.188592] [14:14:52.189856] {"train_lr": 0.005453061224489799, "train_loss": 3.900476567801975, "test_loss": 4.2306441307067875, "test_acc1": 6.06, "test_acc5": 22.28, "epoch": 3, "n_parameters": 85958500}
[14:14:52.190002] [14:14:52.190071] Training epoch 3 for 0:00:33
[14:14:52.190125] [14:14:52.193466] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:14:54.120921] Epoch: [4]  [ 0/42]  eta: 0:01:20  lr: 0.006400  loss: 3.8671 (3.8671)  time: 1.9267  data: 1.4424  max mem: 10046
[14:15:13.600403] Epoch: [4]  [41/42]  eta: 0:00:00  lr: 0.007924  loss: 3.8580 (3.8707)  time: 0.4764  data: 0.0001  max mem: 10046
[14:15:13.834334] Epoch: [4] Total time: 0:00:21 (0.5153 s / it)
[14:15:13.835116] Averaged stats: lr: 0.007924  loss: 3.8580 (3.8614)
[14:15:15.462901] Test:  [ 0/40]  eta: 0:01:04  loss: 4.2434 (4.2434)  acc1: 9.3750 (9.3750)  acc5: 23.4375 (23.4375)  time: 1.6236  data: 1.4676  max mem: 10046
[14:15:21.399585] Test:  [39/40]  eta: 0:00:00  loss: 4.1440 (4.1563)  acc1: 6.2500 (7.1600)  acc5: 25.0000 (25.0800)  time: 0.1488  data: 0.0000  max mem: 10046
[14:15:21.513999] Test: Total time: 0:00:07 (0.1919 s / it)
[14:15:21.875170] * Acc@1 7.470 Acc@5 24.630 loss 4.174
[14:15:21.875341] Accuracy of the network on the 10000 test images: 7.5%
[14:15:21.875541] [14:15:25.196992] Max accuracy: 7.47%
[14:15:25.197326] [14:15:25.198441] {"train_lr": 0.007053061224489795, "train_loss": 3.861448351825987, "test_loss": 4.1740546226501465, "test_acc1": 7.47, "test_acc5": 24.63, "epoch": 4, "n_parameters": 85958500}
[14:15:25.198514] [14:15:25.198576] Training epoch 4 for 0:00:33
[14:15:25.198627] [14:15:25.201867] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:15:27.077228] Epoch: [5]  [ 0/42]  eta: 0:01:18  lr: 0.008000  loss: 3.9762 (3.9762)  time: 1.8745  data: 1.4048  max mem: 10046
[14:15:46.539797] Epoch: [5]  [41/42]  eta: 0:00:00  lr: 0.007998  loss: 3.7986 (3.8310)  time: 0.4761  data: 0.0001  max mem: 10046
[14:15:46.767970] Epoch: [5] Total time: 0:00:21 (0.5135 s / it)
[14:15:46.796440] Averaged stats: lr: 0.007998  loss: 3.7986 (3.8482)
[14:15:48.244188] Test:  [ 0/40]  eta: 0:00:57  loss: 4.2005 (4.2005)  acc1: 3.1250 (3.1250)  acc5: 26.5625 (26.5625)  time: 1.4438  data: 1.2884  max mem: 10046
[14:15:54.197746] Test:  [39/40]  eta: 0:00:00  loss: 4.0614 (4.0975)  acc1: 7.8125 (8.2800)  acc5: 25.0000 (25.9200)  time: 0.1494  data: 0.0001  max mem: 10046
[14:15:54.316176] Test: Total time: 0:00:07 (0.1879 s / it)
[14:15:55.003779] * Acc@1 8.120 Acc@5 24.870 loss 4.130
[14:15:55.003950] Accuracy of the network on the 10000 test images: 8.1%
[14:15:55.004161] [14:15:58.134767] Max accuracy: 8.12%
[14:15:58.135055] [14:15:58.135978] {"train_lr": 0.007999452179690645, "train_loss": 3.8481807793889726, "test_loss": 4.130236339569092, "test_acc1": 8.12, "test_acc5": 24.87, "epoch": 5, "n_parameters": 85958500}
[14:15:58.136049] [14:15:58.136134] Training epoch 5 for 0:00:32
[14:15:58.136187] [14:15:58.138874] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:15:59.993611] Epoch: [6]  [ 0/42]  eta: 0:01:17  lr: 0.007998  loss: 3.6844 (3.6844)  time: 1.8540  data: 1.3699  max mem: 10046
[14:16:19.482940] Epoch: [6]  [41/42]  eta: 0:00:00  lr: 0.007992  loss: 3.7396 (3.7994)  time: 0.4764  data: 0.0001  max mem: 10046
[14:16:19.701744] Epoch: [6] Total time: 0:00:21 (0.5134 s / it)
[14:16:19.723104] Averaged stats: lr: 0.007992  loss: 3.7396 (3.8166)
[14:16:21.252831] Test:  [ 0/40]  eta: 0:01:01  loss: 4.1869 (4.1869)  acc1: 10.9375 (10.9375)  acc5: 25.0000 (25.0000)  time: 1.5258  data: 1.3668  max mem: 10046
[14:16:27.199223] Test:  [39/40]  eta: 0:00:00  loss: 4.0124 (4.0306)  acc1: 9.3750 (9.3200)  acc5: 28.1250 (27.6400)  time: 0.1492  data: 0.0001  max mem: 10046
[14:16:27.311175] Test: Total time: 0:00:07 (0.1896 s / it)
[14:16:27.880481] * Acc@1 9.220 Acc@5 27.580 loss 4.058
[14:16:27.880694] Accuracy of the network on the 10000 test images: 9.2%
[14:16:27.880902] [14:16:31.222815] Max accuracy: 9.22%
[14:16:31.223088] [14:16:31.224100] {"train_lr": 0.007995481023328083, "train_loss": 3.816605180501938, "test_loss": 4.057809114456177, "test_acc1": 9.22, "test_acc5": 27.58, "epoch": 6, "n_parameters": 85958500}
[14:16:31.224224] [14:16:31.224288] Training epoch 6 for 0:00:33
[14:16:31.224340] [14:16:31.227055] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:16:33.048518] Epoch: [7]  [ 0/42]  eta: 0:01:16  lr: 0.007991  loss: 3.8245 (3.8245)  time: 1.8204  data: 1.3380  max mem: 10046
[14:16:52.623702] Epoch: [7]  [41/42]  eta: 0:00:00  lr: 0.007981  loss: 3.8259 (3.8061)  time: 0.4775  data: 0.0001  max mem: 10046
[14:16:52.847642] Epoch: [7] Total time: 0:00:21 (0.5148 s / it)
[14:16:52.868677] Averaged stats: lr: 0.007981  loss: 3.8259 (3.8008)
[14:16:54.699011] Test:  [ 0/40]  eta: 0:01:13  loss: 4.1037 (4.1037)  acc1: 9.3750 (9.3750)  acc5: 28.1250 (28.1250)  time: 1.8270  data: 1.6510  max mem: 10046
[14:17:00.652436] Test:  [39/40]  eta: 0:00:00  loss: 3.9527 (3.9885)  acc1: 9.3750 (9.9200)  acc5: 31.2500 (30.2000)  time: 0.1492  data: 0.0001  max mem: 10046
[14:17:00.762123] Test: Total time: 0:00:07 (0.1973 s / it)
[14:17:01.083534] * Acc@1 9.910 Acc@5 29.770 loss 4.020
[14:17:01.083746] Accuracy of the network on the 10000 test images: 9.9%
[14:17:01.083971] [14:17:04.441151] Max accuracy: 9.91%
[14:17:04.441448] [14:17:04.442412] {"train_lr": 0.007987141413870226, "train_loss": 3.800775469768615, "test_loss": 4.019783782958984, "test_acc1": 9.91, "test_acc5": 29.77, "epoch": 7, "n_parameters": 85958500}
[14:17:04.442484] [14:17:04.442545] Training epoch 7 for 0:00:33
[14:17:04.442610] [14:17:04.445359] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:17:06.300296] Epoch: [8]  [ 0/42]  eta: 0:01:17  lr: 0.007980  loss: 3.7376 (3.7376)  time: 1.8539  data: 1.3728  max mem: 10046
[14:17:25.829539] Epoch: [8]  [41/42]  eta: 0:00:00  lr: 0.007966  loss: 3.7606 (3.7939)  time: 0.4776  data: 0.0001  max mem: 10046
[14:17:26.042645] Epoch: [8] Total time: 0:00:21 (0.5142 s / it)
[14:17:26.051452] Averaged stats: lr: 0.007966  loss: 3.7606 (3.7985)
[14:17:28.067564] Test:  [ 0/40]  eta: 0:01:20  loss: 4.0668 (4.0668)  acc1: 7.8125 (7.8125)  acc5: 28.1250 (28.1250)  time: 2.0126  data: 1.8438  max mem: 10046
[14:17:34.030266] Test:  [39/40]  eta: 0:00:00  loss: 3.8956 (3.9515)  acc1: 9.3750 (9.5600)  acc5: 32.8125 (30.6800)  time: 0.1500  data: 0.0001  max mem: 10046
[14:17:34.142304] Test: Total time: 0:00:08 (0.2022 s / it)
[14:17:34.143577] * Acc@1 9.880 Acc@5 30.500 loss 3.985
[14:17:34.143720] Accuracy of the network on the 10000 test images: 9.9%
[14:17:34.143932] [14:17:34.143999] Max accuracy: 9.91%
[14:17:34.144053] [14:17:34.144904] {"train_lr": 0.007974442470557574, "train_loss": 3.798506184702828, "test_loss": 3.9850916385650637, "test_acc1": 9.88, "test_acc5": 30.5, "epoch": 8, "n_parameters": 85958500}
[14:17:34.144974] [14:17:34.145031] Training epoch 8 for 0:00:29
[14:17:34.145081] [14:17:34.147878] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:17:36.671384] Epoch: [9]  [ 0/42]  eta: 0:01:45  lr: 0.007965  loss: 3.7689 (3.7689)  time: 2.5226  data: 1.8409  max mem: 10046
[14:17:56.273166] Epoch: [9]  [41/42]  eta: 0:00:00  lr: 0.007946  loss: 3.7776 (3.7965)  time: 0.4798  data: 0.0001  max mem: 10046
[14:17:56.495222] Epoch: [9] Total time: 0:00:22 (0.5321 s / it)
[14:17:56.506986] Averaged stats: lr: 0.007946  loss: 3.7776 (3.7614)
[14:17:57.892703] Test:  [ 0/40]  eta: 0:00:55  loss: 3.9720 (3.9720)  acc1: 9.3750 (9.3750)  acc5: 31.2500 (31.2500)  time: 1.3815  data: 1.2248  max mem: 10046
[14:18:04.065019] Test:  [39/40]  eta: 0:00:00  loss: 3.8592 (3.9228)  acc1: 9.3750 (9.4400)  acc5: 32.8125 (31.2800)  time: 0.1502  data: 0.0001  max mem: 10046
[14:18:04.193604] Test: Total time: 0:00:07 (0.1921 s / it)
[14:18:04.615321] * Acc@1 9.770 Acc@5 30.450 loss 3.959
[14:18:04.615530] Accuracy of the network on the 10000 test images: 9.8%
[14:18:04.615730] [14:18:04.615799] Max accuracy: 9.91%
[14:18:04.615854] [14:18:04.616668] {"train_lr": 0.007957398079498206, "train_loss": 3.7613574890863326, "test_loss": 3.9586408853530886, "test_acc1": 9.77, "test_acc5": 30.45, "epoch": 9, "n_parameters": 85958500}
[14:18:04.616738] [14:18:04.616795] Training epoch 9 for 0:00:30
[14:18:04.616846] [14:18:04.619483] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:18:07.098099] Epoch: [10]  [ 0/42]  eta: 0:01:44  lr: 0.007945  loss: 3.6896 (3.6896)  time: 2.4776  data: 1.7468  max mem: 10046
[14:18:26.742395] Epoch: [10]  [41/42]  eta: 0:00:00  lr: 0.007923  loss: 3.7459 (3.7546)  time: 0.4807  data: 0.0001  max mem: 10046
[14:18:26.985722] Epoch: [10] Total time: 0:00:22 (0.5325 s / it)
[14:18:26.991011] Averaged stats: lr: 0.007923  loss: 3.7459 (3.7693)
[14:18:28.432030] Test:  [ 0/40]  eta: 0:00:57  loss: 3.9796 (3.9796)  acc1: 4.6875 (4.6875)  acc5: 28.1250 (28.1250)  time: 1.4363  data: 1.2799  max mem: 10046
[14:18:34.671014] Test:  [39/40]  eta: 0:00:00  loss: 3.8742 (3.9019)  acc1: 9.3750 (10.4400)  acc5: 29.6875 (30.4400)  time: 0.1507  data: 0.0001  max mem: 10046
[14:18:34.787569] Test: Total time: 0:00:07 (0.1949 s / it)
[14:18:35.043123] * Acc@1 10.510 Acc@5 31.020 loss 3.922
[14:18:35.043319] Accuracy of the network on the 10000 test images: 10.5%
[14:18:35.043534] [14:18:38.375590] Max accuracy: 10.51%
[14:18:38.375883] [14:18:38.376766] {"train_lr": 0.007936026878483507, "train_loss": 3.769333561261495, "test_loss": 3.9217287063598634, "test_acc1": 10.51, "test_acc5": 31.02, "epoch": 10, "n_parameters": 85958500}
[14:18:38.376850] [14:18:38.376916] Training epoch 10 for 0:00:33
[14:18:38.376984] [14:18:38.379719] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:18:40.390009] Epoch: [11]  [ 0/42]  eta: 0:01:24  lr: 0.007922  loss: 3.7540 (3.7540)  time: 2.0087  data: 1.5207  max mem: 10046
[14:19:00.121439] Epoch: [11]  [41/42]  eta: 0:00:00  lr: 0.007895  loss: 3.7437 (3.7217)  time: 0.4776  data: 0.0001  max mem: 10046
[14:19:00.372943] Epoch: [11] Total time: 0:00:21 (0.5236 s / it)
[14:19:00.382513] Averaged stats: lr: 0.007895  loss: 3.7437 (3.7413)
[14:19:02.309696] Test:  [ 0/40]  eta: 0:01:16  loss: 3.9052 (3.9052)  acc1: 10.9375 (10.9375)  acc5: 35.9375 (35.9375)  time: 1.9229  data: 1.7445  max mem: 10046
[14:19:08.304633] Test:  [39/40]  eta: 0:00:00  loss: 3.7857 (3.8303)  acc1: 12.5000 (12.7600)  acc5: 34.3750 (34.8400)  time: 0.1506  data: 0.0001  max mem: 10046
[14:19:08.433849] Test: Total time: 0:00:08 (0.2012 s / it)
[14:19:08.559443] * Acc@1 12.480 Acc@5 33.610 loss 3.872
[14:19:08.559646] Accuracy of the network on the 10000 test images: 12.5%
[14:19:08.559874] [14:19:11.926076] Max accuracy: 12.48%
[14:19:11.926375] [14:19:11.927253] {"train_lr": 0.007910352236608017, "train_loss": 3.7413445767902194, "test_loss": 3.8715378046035767, "test_acc1": 12.48, "test_acc5": 33.61, "epoch": 11, "n_parameters": 85958500}
[14:19:11.927324] [14:19:11.927384] Training epoch 11 for 0:00:33
[14:19:11.927435] [14:19:11.930337] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:19:13.788681] Epoch: [12]  [ 0/42]  eta: 0:01:18  lr: 0.007893  loss: 3.8763 (3.8763)  time: 1.8573  data: 1.3891  max mem: 10046
[14:19:33.333976] Epoch: [12]  [41/42]  eta: 0:00:00  lr: 0.007862  loss: 3.7446 (3.7686)  time: 0.4786  data: 0.0001  max mem: 10046
[14:19:33.550843] Epoch: [12] Total time: 0:00:21 (0.5148 s / it)
[14:19:33.563523] Averaged stats: lr: 0.007862  loss: 3.7446 (3.7332)
[14:19:35.490993] Test:  [ 0/40]  eta: 0:01:16  loss: 3.8228 (3.8228)  acc1: 10.9375 (10.9375)  acc5: 39.0625 (39.0625)  time: 1.9242  data: 1.7466  max mem: 10046
[14:19:41.476953] Test:  [39/40]  eta: 0:00:00  loss: 3.7799 (3.8278)  acc1: 10.9375 (12.2800)  acc5: 31.2500 (33.8400)  time: 0.1500  data: 0.0001  max mem: 10046
[14:19:41.598377] Test: Total time: 0:00:08 (0.2008 s / it)
[14:19:41.825435] * Acc@1 12.130 Acc@5 34.130 loss 3.855
[14:19:41.825615] Accuracy of the network on the 10000 test images: 12.1%
[14:19:41.825819] [14:19:41.825884] Max accuracy: 12.48%
[14:19:41.825938] [14:19:41.826795] {"train_lr": 0.007880402228715652, "train_loss": 3.733236721583775, "test_loss": 3.8551183938980103, "test_acc1": 12.13, "test_acc5": 34.13, "epoch": 12, "n_parameters": 85958500}
[14:19:41.826873] [14:19:41.826934] Training epoch 12 for 0:00:29
[14:19:41.826985] [14:19:41.829755] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:19:44.322588] Epoch: [13]  [ 0/42]  eta: 0:01:44  lr: 0.007861  loss: 3.6544 (3.6544)  time: 2.4917  data: 1.9949  max mem: 10046
[14:20:04.013994] Epoch: [13]  [41/42]  eta: 0:00:00  lr: 0.007826  loss: 3.7765 (3.7756)  time: 0.4807  data: 0.0001  max mem: 10046
[14:20:04.246898] Epoch: [13] Total time: 0:00:22 (0.5337 s / it)
[14:20:04.247619] Averaged stats: lr: 0.007826  loss: 3.7765 (3.7784)
[14:20:05.929058] Test:  [ 0/40]  eta: 0:01:07  loss: 3.8761 (3.8761)  acc1: 9.3750 (9.3750)  acc5: 35.9375 (35.9375)  time: 1.6776  data: 1.5150  max mem: 10046
[14:20:11.933817] Test:  [39/40]  eta: 0:00:00  loss: 3.8024 (3.8509)  acc1: 12.5000 (12.8000)  acc5: 34.3750 (33.4000)  time: 0.1507  data: 0.0001  max mem: 10046
[14:20:12.045442] Test: Total time: 0:00:07 (0.1949 s / it)
[14:20:12.491548] * Acc@1 12.990 Acc@5 33.940 loss 3.880
[14:20:12.491762] Accuracy of the network on the 10000 test images: 13.0%
[14:20:12.491974] [14:20:15.817857] Max accuracy: 12.99%
[14:20:15.818164] [14:20:15.819080] {"train_lr": 0.00784620960470034, "train_loss": 3.778426100810369, "test_loss": 3.880236768722534, "test_acc1": 12.99, "test_acc5": 33.94, "epoch": 13, "n_parameters": 85958500}
[14:20:15.819154] [14:20:15.819216] Training epoch 13 for 0:00:33
[14:20:15.819268] [14:20:15.822067] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:20:17.734068] Epoch: [14]  [ 0/42]  eta: 0:01:20  lr: 0.007824  loss: 3.4294 (3.4294)  time: 1.9112  data: 1.4229  max mem: 10046
[14:20:37.273991] Epoch: [14]  [41/42]  eta: 0:00:00  lr: 0.007785  loss: 3.7447 (3.7238)  time: 0.4786  data: 0.0001  max mem: 10046
[14:20:37.511119] Epoch: [14] Total time: 0:00:21 (0.5164 s / it)
[14:20:37.517386] Averaged stats: lr: 0.007785  loss: 3.7447 (3.7416)
[14:20:38.892010] Test:  [ 0/40]  eta: 0:00:54  loss: 3.8239 (3.8239)  acc1: 15.6250 (15.6250)  acc5: 42.1875 (42.1875)  time: 1.3704  data: 1.2141  max mem: 10046
[14:20:44.882302] Test:  [39/40]  eta: 0:00:00  loss: 3.7455 (3.7961)  acc1: 12.5000 (13.3600)  acc5: 35.9375 (36.0000)  time: 0.1505  data: 0.0001  max mem: 10046
[14:20:44.993766] Test: Total time: 0:00:07 (0.1868 s / it)
[14:20:45.679475] * Acc@1 12.810 Acc@5 35.720 loss 3.824
[14:20:45.679658] Accuracy of the network on the 10000 test images: 12.8%
[14:20:45.679882] [14:20:45.679954] Max accuracy: 12.99%
[14:20:45.680009] [14:20:45.680818] {"train_lr": 0.007807811753694423, "train_loss": 3.7416359654494693, "test_loss": 3.824039268493652, "test_acc1": 12.81, "test_acc5": 35.72, "epoch": 14, "n_parameters": 85958500}
[14:20:45.680887] [14:20:45.680945] Training epoch 14 for 0:00:29
[14:20:45.680995] [14:20:45.683781] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:20:48.246866] Epoch: [15]  [ 0/42]  eta: 0:01:47  lr: 0.007783  loss: 3.8185 (3.8185)  time: 2.5617  data: 2.0640  max mem: 10046
[14:21:07.855075] Epoch: [15]  [41/42]  eta: 0:00:00  lr: 0.007741  loss: 3.7420 (3.6799)  time: 0.4803  data: 0.0001  max mem: 10046
[14:21:08.091489] Epoch: [15] Total time: 0:00:22 (0.5335 s / it)
[14:21:08.110002] Averaged stats: lr: 0.007741  loss: 3.7420 (3.7021)
[14:21:09.799039] Test:  [ 0/40]  eta: 0:01:07  loss: 3.8223 (3.8223)  acc1: 12.5000 (12.5000)  acc5: 39.0625 (39.0625)  time: 1.6849  data: 1.5278  max mem: 10046
[14:21:15.809263] Test:  [39/40]  eta: 0:00:00  loss: 3.7051 (3.7696)  acc1: 15.6250 (15.0400)  acc5: 37.5000 (37.5200)  time: 0.1511  data: 0.0001  max mem: 10046
[14:21:15.957872] Test: Total time: 0:00:07 (0.1961 s / it)
[14:21:16.137118] * Acc@1 14.240 Acc@5 36.810 loss 3.791
[14:21:16.137342] Accuracy of the network on the 10000 test images: 14.2%
[14:21:16.137581] [14:21:19.552899] Max accuracy: 14.24%
[14:21:19.553179] [14:21:19.554108] {"train_lr": 0.007765250663184244, "train_loss": 3.7020717021964846, "test_loss": 3.7912428855895994, "test_acc1": 14.24, "test_acc5": 36.81, "epoch": 15, "n_parameters": 85958500}
[14:21:19.554235] [14:21:19.554301] Training epoch 15 for 0:00:33
[14:21:19.554355] [14:21:19.559873] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:21:21.577134] Epoch: [16]  [ 0/42]  eta: 0:01:24  lr: 0.007738  loss: 3.8745 (3.8745)  time: 2.0165  data: 1.5483  max mem: 10046
[14:21:41.119985] Epoch: [16]  [41/42]  eta: 0:00:00  lr: 0.007692  loss: 3.6553 (3.6886)  time: 0.4784  data: 0.0001  max mem: 10046
[14:21:41.368765] Epoch: [16] Total time: 0:00:21 (0.5193 s / it)
[14:21:41.369548] Averaged stats: lr: 0.007692  loss: 3.6553 (3.6811)
[14:21:42.754868] Test:  [ 0/40]  eta: 0:00:55  loss: 3.7736 (3.7736)  acc1: 10.9375 (10.9375)  acc5: 31.2500 (31.2500)  time: 1.3815  data: 1.2251  max mem: 10046
[14:21:48.964347] Test:  [39/40]  eta: 0:00:00  loss: 3.6533 (3.7446)  acc1: 14.0625 (13.8000)  acc5: 35.9375 (36.6800)  time: 0.1504  data: 0.0001  max mem: 10046
[14:21:49.075676] Test: Total time: 0:00:07 (0.1926 s / it)
[14:21:49.574981] * Acc@1 13.890 Acc@5 36.490 loss 3.769
[14:21:49.575164] Accuracy of the network on the 10000 test images: 13.9%
[14:21:49.575380] [14:21:49.575453] Max accuracy: 14.24%
[14:21:49.575508] [14:21:49.576340] {"train_lr": 0.0077185728730973764, "train_loss": 3.6811102770623707, "test_loss": 3.769488573074341, "test_acc1": 13.89, "test_acc5": 36.49, "epoch": 16, "n_parameters": 85958500}
[14:21:49.576413] [14:21:49.576471] Training epoch 16 for 0:00:30
[14:21:49.576522] [14:21:49.579256] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:21:52.165793] Epoch: [17]  [ 0/42]  eta: 0:01:48  lr: 0.007689  loss: 3.7552 (3.7552)  time: 2.5857  data: 1.4920  max mem: 10046
[14:22:11.801167] Epoch: [17]  [41/42]  eta: 0:00:00  lr: 0.007639  loss: 3.7126 (3.7180)  time: 0.4808  data: 0.0001  max mem: 10046
[14:22:12.038731] Epoch: [17] Total time: 0:00:22 (0.5347 s / it)
[14:22:12.039520] Averaged stats: lr: 0.007639  loss: 3.7126 (3.7037)
[14:22:13.514636] Test:  [ 0/40]  eta: 0:00:58  loss: 3.8142 (3.8142)  acc1: 9.3750 (9.3750)  acc5: 39.0625 (39.0625)  time: 1.4709  data: 1.3136  max mem: 10046
[14:22:19.644256] Test:  [39/40]  eta: 0:00:00  loss: 3.6637 (3.7480)  acc1: 15.6250 (14.3600)  acc5: 42.1875 (38.6400)  time: 0.1518  data: 0.0001  max mem: 10046
[14:22:19.761103] Test: Total time: 0:00:07 (0.1930 s / it)
[14:22:20.275082] * Acc@1 13.980 Acc@5 37.750 loss 3.781
[14:22:20.275286] Accuracy of the network on the 10000 test images: 14.0%
[14:22:20.275515] [14:22:20.275585] Max accuracy: 14.24%
[14:22:20.275639] [14:22:20.276449] {"train_lr": 0.007667829424911969, "train_loss": 3.7037320264748166, "test_loss": 3.7810503482818603, "test_acc1": 13.98, "test_acc5": 37.75, "epoch": 17, "n_parameters": 85958500}
[14:22:20.276518] [14:22:20.276577] Training epoch 17 for 0:00:30
[14:22:20.276627] [14:22:20.279325] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:22:22.847911] Epoch: [18]  [ 0/42]  eta: 0:01:47  lr: 0.007636  loss: 3.7976 (3.7976)  time: 2.5674  data: 2.0717  max mem: 10046
[14:22:42.452056] Epoch: [18]  [41/42]  eta: 0:00:00  lr: 0.007582  loss: 3.6508 (3.7114)  time: 0.4794  data: 0.0001  max mem: 10046
[14:22:42.687314] Epoch: [18] Total time: 0:00:22 (0.5335 s / it)
[14:22:42.697762] Averaged stats: lr: 0.007582  loss: 3.6508 (3.7100)
[14:22:44.205355] Test:  [ 0/40]  eta: 0:01:00  loss: 3.7943 (3.7943)  acc1: 10.9375 (10.9375)  acc5: 31.2500 (31.2500)  time: 1.5037  data: 1.3465  max mem: 10046
[14:22:50.446495] Test:  [39/40]  eta: 0:00:00  loss: 3.6850 (3.7392)  acc1: 12.5000 (13.4800)  acc5: 37.5000 (36.5200)  time: 0.1511  data: 0.0001  max mem: 10046
[14:22:50.555201] Test: Total time: 0:00:07 (0.1964 s / it)
[14:22:50.974580] * Acc@1 13.180 Acc@5 36.410 loss 3.767
[14:22:50.974797] Accuracy of the network on the 10000 test images: 13.2%
[14:22:50.975009] [14:22:50.975084] Max accuracy: 14.24%
[14:22:50.975140] [14:22:50.975972] {"train_lr": 0.007613075805843609, "train_loss": 3.7100210658141544, "test_loss": 3.7669676780700683, "test_acc1": 13.18, "test_acc5": 36.41, "epoch": 18, "n_parameters": 85958500}
[14:22:50.976041] [14:22:50.976106] Training epoch 18 for 0:00:30
[14:22:50.976160] [14:22:50.978863] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:22:53.401140] Epoch: [19]  [ 0/42]  eta: 0:01:41  lr: 0.007579  loss: 3.7682 (3.7682)  time: 2.4216  data: 1.8824  max mem: 10046
[14:23:12.981967] Epoch: [19]  [41/42]  eta: 0:00:00  lr: 0.007521  loss: 3.6659 (3.6773)  time: 0.4788  data: 0.0001  max mem: 10046
[14:23:13.208564] Epoch: [19] Total time: 0:00:22 (0.5293 s / it)
[14:23:13.223472] Averaged stats: lr: 0.007521  loss: 3.6659 (3.6550)
[14:23:15.059177] Test:  [ 0/40]  eta: 0:01:13  loss: 3.7964 (3.7964)  acc1: 10.9375 (10.9375)  acc5: 35.9375 (35.9375)  time: 1.8323  data: 1.6545  max mem: 10046
[14:23:21.077768] Test:  [39/40]  eta: 0:00:00  loss: 3.6488 (3.6961)  acc1: 14.0625 (14.8400)  acc5: 40.6250 (39.0800)  time: 0.1509  data: 0.0001  max mem: 10046
[14:23:21.184438] Test: Total time: 0:00:07 (0.1990 s / it)
[14:23:21.498523] * Acc@1 14.950 Acc@5 39.000 loss 3.722
[14:23:21.498716] Accuracy of the network on the 10000 test images: 14.9%
[14:23:21.498907] [14:23:24.918921] Max accuracy: 14.95%
[14:23:24.919199] [14:23:24.920095] {"train_lr": 0.007554371888170873, "train_loss": 3.654981722434362, "test_loss": 3.7217852592468263, "test_acc1": 14.95, "test_acc5": 39.0, "epoch": 19, "n_parameters": 85958500}
[14:23:24.920222] [14:23:24.920287] Training epoch 19 for 0:00:33
[14:23:24.920340] [14:23:24.923110] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:23:26.909635] Epoch: [20]  [ 0/42]  eta: 0:01:23  lr: 0.007518  loss: 3.6116 (3.6116)  time: 1.9856  data: 1.5077  max mem: 10046
[14:23:46.441686] Epoch: [20]  [41/42]  eta: 0:00:00  lr: 0.007456  loss: 3.6533 (3.6949)  time: 0.4781  data: 0.0001  max mem: 10046
[14:23:46.687978] Epoch: [20] Total time: 0:00:21 (0.5182 s / it)
[14:23:46.688754] Averaged stats: lr: 0.007456  loss: 3.6533 (3.6779)
[14:23:48.737284] Test:  [ 0/40]  eta: 0:01:21  loss: 3.7294 (3.7294)  acc1: 10.9375 (10.9375)  acc5: 37.5000 (37.5000)  time: 2.0447  data: 1.8665  max mem: 10046
[14:23:54.736139] Test:  [39/40]  eta: 0:00:00  loss: 3.6638 (3.7107)  acc1: 15.6250 (14.0800)  acc5: 35.9375 (36.8800)  time: 0.1507  data: 0.0001  max mem: 10046
[14:23:54.852576] Test: Total time: 0:00:08 (0.2040 s / it)
[14:23:54.910489] * Acc@1 14.140 Acc@5 37.600 loss 3.733
[14:23:54.910664] Accuracy of the network on the 10000 test images: 14.1%
[14:23:54.910877] [14:23:54.910956] Max accuracy: 14.95%
[14:23:54.911011] [14:23:54.911830] {"train_lr": 0.007491781863765832, "train_loss": 3.6778996899014427, "test_loss": 3.73265597820282, "test_acc1": 14.14, "test_acc5": 37.6, "epoch": 20, "n_parameters": 85958500}
[14:23:54.911898] [14:23:54.911962] Training epoch 20 for 0:00:29
[14:23:54.912014] [14:23:54.914858] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:23:57.376987] Epoch: [21]  [ 0/42]  eta: 0:01:43  lr: 0.007453  loss: 3.8186 (3.8186)  time: 2.4610  data: 1.9819  max mem: 10046
[14:24:17.082544] Epoch: [21]  [41/42]  eta: 0:00:00  lr: 0.007388  loss: 3.7905 (3.6895)  time: 0.4834  data: 0.0001  max mem: 10046
[14:24:17.304240] Epoch: [21] Total time: 0:00:22 (0.5331 s / it)
[14:24:17.355156] Averaged stats: lr: 0.007388  loss: 3.7905 (3.6901)
[14:24:18.796913] Test:  [ 0/40]  eta: 0:00:57  loss: 3.6746 (3.6746)  acc1: 17.1875 (17.1875)  acc5: 42.1875 (42.1875)  time: 1.4373  data: 1.2803  max mem: 10046
[14:24:25.041127] Test:  [39/40]  eta: 0:00:00  loss: 3.5791 (3.6560)  acc1: 15.6250 (14.5600)  acc5: 37.5000 (39.5600)  time: 0.1510  data: 0.0001  max mem: 10046
[14:24:25.172249] Test: Total time: 0:00:07 (0.1954 s / it)
[14:24:25.585315] * Acc@1 14.970 Acc@5 39.900 loss 3.683
[14:24:25.585570] Accuracy of the network on the 10000 test images: 15.0%
[14:24:25.585810] [14:24:28.980583] Max accuracy: 14.97%
[14:24:28.980847] [14:24:28.981767] {"train_lr": 0.007425374173901249, "train_loss": 3.69007874528567, "test_loss": 3.682591271400452, "test_acc1": 14.97, "test_acc5": 39.9, "epoch": 21, "n_parameters": 85958500}
[14:24:28.981890] [14:24:28.981955] Training epoch 21 for 0:00:34
[14:24:28.982007] [14:24:28.984912] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:24:30.888896] Epoch: [22]  [ 0/42]  eta: 0:01:19  lr: 0.007384  loss: 3.7807 (3.7807)  time: 1.9029  data: 1.4340  max mem: 10046
[14:24:50.438466] Epoch: [22]  [41/42]  eta: 0:00:00  lr: 0.007316  loss: 3.6220 (3.6531)  time: 0.4787  data: 0.0001  max mem: 10046
[14:24:50.667684] Epoch: [22] Total time: 0:00:21 (0.5163 s / it)
[14:24:50.687350] Averaged stats: lr: 0.007316  loss: 3.6220 (3.6524)
[14:24:52.470569] Test:  [ 0/40]  eta: 0:01:11  loss: 3.7300 (3.7300)  acc1: 18.7500 (18.7500)  acc5: 34.3750 (34.3750)  time: 1.7796  data: 1.6002  max mem: 10046
[14:24:58.455325] Test:  [39/40]  eta: 0:00:00  loss: 3.5926 (3.6542)  acc1: 15.6250 (15.6800)  acc5: 37.5000 (38.7600)  time: 0.1502  data: 0.0001  max mem: 10046
[14:24:58.568172] Test: Total time: 0:00:07 (0.1970 s / it)
[14:24:58.936425] * Acc@1 15.150 Acc@5 38.830 loss 3.680
[14:24:58.936628] Accuracy of the network on the 10000 test images: 15.2%
[14:24:58.936857] [14:25:02.273794] Max accuracy: 15.15%
[14:25:02.274058] [14:25:02.274947] {"train_lr": 0.007355221434411005, "train_loss": 3.652446448802948, "test_loss": 3.679920268058777, "test_acc1": 15.15, "test_acc5": 38.83, "epoch": 22, "n_parameters": 85958500}
[14:25:02.275064] [14:25:02.275126] Training epoch 22 for 0:00:33
[14:25:02.275178] [14:25:02.278058] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:25:04.053406] Epoch: [23]  [ 0/42]  eta: 0:01:14  lr: 0.007312  loss: 3.9503 (3.9503)  time: 1.7744  data: 1.2934  max mem: 10046
[14:25:23.619872] Epoch: [23]  [41/42]  eta: 0:00:00  lr: 0.007240  loss: 3.6589 (3.6982)  time: 0.4787  data: 0.0001  max mem: 10046
[14:25:23.846022] Epoch: [23] Total time: 0:00:21 (0.5135 s / it)
[14:25:23.860455] Averaged stats: lr: 0.007240  loss: 3.6589 (3.6976)
[14:25:25.286422] Test:  [ 0/40]  eta: 0:00:56  loss: 3.7086 (3.7086)  acc1: 15.6250 (15.6250)  acc5: 42.1875 (42.1875)  time: 1.4218  data: 1.2653  max mem: 10046
[14:25:31.395828] Test:  [39/40]  eta: 0:00:00  loss: 3.6601 (3.7067)  acc1: 15.6250 (15.1200)  acc5: 37.5000 (38.8400)  time: 0.1502  data: 0.0001  max mem: 10046
[14:25:31.504254] Test: Total time: 0:00:07 (0.1910 s / it)
[14:25:32.032936] * Acc@1 14.780 Acc@5 38.320 loss 3.724
[14:25:32.033137] Accuracy of the network on the 10000 test images: 14.8%
[14:25:32.033353] [14:25:32.033427] Max accuracy: 15.15%
[14:25:32.033495] [14:25:32.034351] {"train_lr": 0.007281400356285755, "train_loss": 3.6975907584031424, "test_loss": 3.7243002891540526, "test_acc1": 14.78, "test_acc5": 38.32, "epoch": 23, "n_parameters": 85958500}
[14:25:32.034421] [14:25:32.034480] Training epoch 23 for 0:00:29
[14:25:32.034530] [14:25:32.037294] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:25:34.364558] Epoch: [24]  [ 0/42]  eta: 0:01:37  lr: 0.007236  loss: 3.7026 (3.7026)  time: 2.3264  data: 1.5392  max mem: 10046
[14:25:54.003741] Epoch: [24]  [41/42]  eta: 0:00:00  lr: 0.007161  loss: 3.7306 (3.6475)  time: 0.4812  data: 0.0001  max mem: 10046
[14:25:54.248803] Epoch: [24] Total time: 0:00:22 (0.5288 s / it)
[14:25:54.256655] Averaged stats: lr: 0.007161  loss: 3.7306 (3.6655)
[14:25:55.758752] Test:  [ 0/40]  eta: 0:00:59  loss: 3.6175 (3.6175)  acc1: 15.6250 (15.6250)  acc5: 43.7500 (43.7500)  time: 1.4980  data: 1.3408  max mem: 10046
[14:26:01.794036] Test:  [39/40]  eta: 0:00:00  loss: 3.6260 (3.6470)  acc1: 15.6250 (15.8800)  acc5: 40.6250 (40.2000)  time: 0.1511  data: 0.0001  max mem: 10046
[14:26:01.906353] Test: Total time: 0:00:07 (0.1912 s / it)
[14:26:02.434985] * Acc@1 15.390 Acc@5 39.280 loss 3.671
[14:26:02.435159] Accuracy of the network on the 10000 test images: 15.4%
[14:26:02.435360] [14:26:05.768179] Max accuracy: 15.39%
[14:26:05.768416] [14:26:05.769448] {"train_lr": 0.0072039916617906, "train_loss": 3.665477759781338, "test_loss": 3.6706472396850587, "test_acc1": 15.39, "test_acc5": 39.28, "epoch": 24, "n_parameters": 85958500}
[14:26:05.769579] [14:26:05.769644] Training epoch 24 for 0:00:33
[14:26:05.769698] [14:26:05.772520] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:26:07.551763] Epoch: [25]  [ 0/42]  eta: 0:01:14  lr: 0.007157  loss: 3.5062 (3.5062)  time: 1.7784  data: 1.3020  max mem: 10046
[14:26:27.096229] Epoch: [25]  [41/42]  eta: 0:00:00  lr: 0.007078  loss: 3.6930 (3.6550)  time: 0.4784  data: 0.0001  max mem: 10046
[14:26:27.335563] Epoch: [25] Total time: 0:00:21 (0.5134 s / it)
[14:26:27.337708] Averaged stats: lr: 0.007078  loss: 3.6930 (3.6443)
[14:26:29.233355] Test:  [ 0/40]  eta: 0:01:15  loss: 3.6550 (3.6550)  acc1: 21.8750 (21.8750)  acc5: 40.6250 (40.6250)  time: 1.8920  data: 1.7152  max mem: 10046
[14:26:35.219315] Test:  [39/40]  eta: 0:00:00  loss: 3.5492 (3.6227)  acc1: 15.6250 (16.8400)  acc5: 40.6250 (40.8000)  time: 0.1505  data: 0.0001  max mem: 10046
[14:26:35.387479] Test: Total time: 0:00:08 (0.2012 s / it)
[14:26:35.440587] * Acc@1 15.760 Acc@5 40.250 loss 3.649
[14:26:35.440777] Accuracy of the network on the 10000 test images: 15.8%
[14:26:35.440989] [14:26:38.815071] Max accuracy: 15.76%
[14:26:38.815346] [14:26:38.816261] {"train_lr": 0.007123079996196465, "train_loss": 3.644287889912015, "test_loss": 3.6488978981971742, "test_acc1": 15.76, "test_acc5": 40.25, "epoch": 25, "n_parameters": 85958500}
[14:26:38.816384] [14:26:38.816447] Training epoch 25 for 0:00:33
[14:26:38.816500] [14:26:38.819254] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:26:40.656836] Epoch: [26]  [ 0/42]  eta: 0:01:17  lr: 0.007074  loss: 3.6486 (3.6486)  time: 1.8364  data: 1.3687  max mem: 10046
[14:27:00.202405] Epoch: [26]  [41/42]  eta: 0:00:00  lr: 0.006992  loss: 3.6577 (3.6951)  time: 0.4783  data: 0.0001  max mem: 10046
[14:27:00.427580] Epoch: [26] Total time: 0:00:21 (0.5145 s / it)
[14:27:00.438980] Averaged stats: lr: 0.006992  loss: 3.6577 (3.6688)
[14:27:02.412909] Test:  [ 0/40]  eta: 0:01:18  loss: 3.7116 (3.7116)  acc1: 20.3125 (20.3125)  acc5: 40.6250 (40.6250)  time: 1.9705  data: 1.7932  max mem: 10046
[14:27:08.396932] Test:  [39/40]  eta: 0:00:00  loss: 3.5712 (3.6366)  acc1: 14.0625 (15.6800)  acc5: 40.6250 (40.5600)  time: 0.1502  data: 0.0001  max mem: 10046
[14:27:08.544456] Test: Total time: 0:00:08 (0.2026 s / it)
[14:27:08.677109] * Acc@1 15.950 Acc@5 40.490 loss 3.662
[14:27:08.677332] Accuracy of the network on the 10000 test images: 15.9%
[14:27:08.677581] [14:27:10.858449] Max accuracy: 15.95%
[14:27:10.858815] [14:27:10.859811] {"train_lr": 0.0070387538352217486, "train_loss": 3.6688270838487718, "test_loss": 3.6624826908111574, "test_acc1": 15.95, "test_acc5": 40.49, "epoch": 26, "n_parameters": 85958500}
[14:27:10.859896] [14:27:10.859960] Training epoch 26 for 0:00:32
[14:27:10.860012] [14:27:10.863099] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:27:12.786660] Epoch: [27]  [ 0/42]  eta: 0:01:20  lr: 0.006987  loss: 3.6305 (3.6305)  time: 1.9225  data: 1.4384  max mem: 10046
[14:27:32.412814] Epoch: [27]  [41/42]  eta: 0:00:00  lr: 0.006902  loss: 3.6439 (3.6367)  time: 0.4797  data: 0.0001  max mem: 10046
[14:27:32.656361] Epoch: [27] Total time: 0:00:21 (0.5189 s / it)
[14:27:32.662796] Averaged stats: lr: 0.006902  loss: 3.6439 (3.6450)
[14:27:34.199193] Test:  [ 0/40]  eta: 0:01:01  loss: 3.6946 (3.6946)  acc1: 18.7500 (18.7500)  acc5: 43.7500 (43.7500)  time: 1.5321  data: 1.3751  max mem: 10046
[14:27:40.226430] Test:  [39/40]  eta: 0:00:00  loss: 3.5175 (3.6063)  acc1: 17.1875 (16.4800)  acc5: 42.1875 (41.5200)  time: 0.1505  data: 0.0001  max mem: 10046
[14:27:40.354905] Test: Total time: 0:00:07 (0.1922 s / it)
[14:27:40.756167] * Acc@1 15.900 Acc@5 41.030 loss 3.628
[14:27:40.756356] Accuracy of the network on the 10000 test images: 15.9%
[14:27:40.756567] [14:27:40.756636] Max accuracy: 15.95%
[14:27:40.756689] [14:27:40.757523] {"train_lr": 0.006951105388285409, "train_loss": 3.6450370933328355, "test_loss": 3.6284443378448485, "test_acc1": 15.9, "test_acc5": 41.03, "epoch": 27, "n_parameters": 85958500}
[14:27:40.757589] [14:27:40.757646] Training epoch 27 for 0:00:29
[14:27:40.757696] [14:27:40.760479] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:27:43.297229] Epoch: [28]  [ 0/42]  eta: 0:01:46  lr: 0.006898  loss: 4.0405 (4.0405)  time: 2.5359  data: 1.4445  max mem: 10046
[14:28:02.955593] Epoch: [28]  [41/42]  eta: 0:00:00  lr: 0.006810  loss: 3.6676 (3.6751)  time: 0.4806  data: 0.0001  max mem: 10046
[14:28:03.195696] Epoch: [28] Total time: 0:00:22 (0.5342 s / it)
[14:28:03.220589] Averaged stats: lr: 0.006810  loss: 3.6676 (3.6908)
[14:28:05.061878] Test:  [ 0/40]  eta: 0:01:13  loss: 3.7235 (3.7235)  acc1: 20.3125 (20.3125)  acc5: 37.5000 (37.5000)  time: 1.8379  data: 1.6604  max mem: 10046
[14:28:11.098258] Test:  [39/40]  eta: 0:00:00  loss: 3.5942 (3.6530)  acc1: 15.6250 (16.2000)  acc5: 40.6250 (40.8000)  time: 0.1511  data: 0.0001  max mem: 10046
[14:28:11.256989] Test: Total time: 0:00:08 (0.2009 s / it)
[14:28:11.266904] * Acc@1 15.710 Acc@5 40.260 loss 3.679
[14:28:11.267098] Accuracy of the network on the 10000 test images: 15.7%
[14:28:11.267301] [14:28:11.267367] Max accuracy: 15.95%
[14:28:11.267421] [14:28:11.268189] {"train_lr": 0.006860230497677332, "train_loss": 3.690795912629082, "test_loss": 3.6787490367889406, "test_acc1": 15.71, "test_acc5": 40.26, "epoch": 28, "n_parameters": 85958500}
[14:28:11.268258] [14:28:11.268315] Training epoch 28 for 0:00:30
[14:28:11.268366] [14:28:11.271055] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:28:13.737418] Epoch: [29]  [ 0/42]  eta: 0:01:43  lr: 0.006805  loss: 3.4407 (3.4407)  time: 2.4655  data: 1.6603  max mem: 10046
[14:28:33.343944] Epoch: [29]  [41/42]  eta: 0:00:00  lr: 0.006714  loss: 3.7448 (3.6645)  time: 0.4801  data: 0.0001  max mem: 10046
[14:28:33.576611] Epoch: [29] Total time: 0:00:22 (0.5311 s / it)
[14:28:33.600088] Averaged stats: lr: 0.006714  loss: 3.7448 (3.6420)
[14:28:35.088668] Test:  [ 0/40]  eta: 0:00:59  loss: 3.6391 (3.6391)  acc1: 15.6250 (15.6250)  acc5: 42.1875 (42.1875)  time: 1.4843  data: 1.3263  max mem: 10046
[14:28:41.156557] Test:  [39/40]  eta: 0:00:00  loss: 3.5542 (3.6172)  acc1: 15.6250 (16.7200)  acc5: 40.6250 (40.1600)  time: 0.1516  data: 0.0001  max mem: 10046
[14:28:41.271490] Test: Total time: 0:00:07 (0.1917 s / it)
[14:28:41.676916] * Acc@1 15.570 Acc@5 40.020 loss 3.644
[14:28:41.677142] Accuracy of the network on the 10000 test images: 15.6%
[14:28:41.677369] [14:28:41.677439] Max accuracy: 15.95%
[14:28:41.677498] [14:28:41.678351] {"train_lr": 0.0067662285337561886, "train_loss": 3.6419797866117385, "test_loss": 3.643849015235901, "test_acc1": 15.57, "test_acc5": 40.02, "epoch": 29, "n_parameters": 85958500}
[14:28:41.678434] [14:28:41.678490] Training epoch 29 for 0:00:30
[14:28:41.678542] [14:28:41.681293] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:28:44.226649] Epoch: [30]  [ 0/42]  eta: 0:01:46  lr: 0.006709  loss: 3.7066 (3.7066)  time: 2.5444  data: 1.6435  max mem: 10046
[14:29:03.846123] Epoch: [30]  [41/42]  eta: 0:00:00  lr: 0.006615  loss: 3.6619 (3.6725)  time: 0.4803  data: 0.0001  max mem: 10046
[14:29:04.081171] Epoch: [30] Total time: 0:00:22 (0.5333 s / it)
[14:29:04.085931] Averaged stats: lr: 0.006615  loss: 3.6619 (3.6705)
[14:29:05.588686] Test:  [ 0/40]  eta: 0:00:59  loss: 3.6589 (3.6589)  acc1: 12.5000 (12.5000)  acc5: 37.5000 (37.5000)  time: 1.4971  data: 1.3395  max mem: 10046
[14:29:11.678845] Test:  [39/40]  eta: 0:00:00  loss: 3.5570 (3.6272)  acc1: 17.1875 (16.6400)  acc5: 43.7500 (41.0400)  time: 0.1512  data: 0.0001  max mem: 10046
[14:29:11.795585] Test: Total time: 0:00:07 (0.1926 s / it)
[14:29:12.209008] * Acc@1 16.310 Acc@5 40.920 loss 3.657
[14:29:12.209219] Accuracy of the network on the 10000 test images: 16.3%
[14:29:12.209442] [14:29:15.522962] Max accuracy: 16.31%
[14:29:15.523305] [14:29:15.524483] {"train_lr": 0.006669202286289392, "train_loss": 3.6705169564201716, "test_loss": 3.656939458847046, "test_acc1": 16.31, "test_acc5": 40.92, "epoch": 30, "n_parameters": 85958500}
[14:29:15.524601] [14:29:15.524699] Training epoch 30 for 0:00:33
[14:29:15.524821] [14:29:15.528955] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:29:17.352855] Epoch: [31]  [ 0/42]  eta: 0:01:16  lr: 0.006611  loss: 3.9084 (3.9084)  time: 1.8230  data: 1.3441  max mem: 10046
[14:29:36.924059] Epoch: [31]  [41/42]  eta: 0:00:00  lr: 0.006514  loss: 3.6641 (3.6274)  time: 0.4786  data: 0.0001  max mem: 10046
[14:29:37.155595] Epoch: [31] Total time: 0:00:21 (0.5149 s / it)
[14:29:37.165979] Averaged stats: lr: 0.006514  loss: 3.6641 (3.5966)
[14:29:38.533088] Test:  [ 0/40]  eta: 0:00:54  loss: 3.5831 (3.5831)  acc1: 14.0625 (14.0625)  acc5: 46.8750 (46.8750)  time: 1.3620  data: 1.2055  max mem: 10046
[14:29:44.822055] Test:  [39/40]  eta: 0:00:00  loss: 3.5375 (3.5676)  acc1: 17.1875 (16.9600)  acc5: 42.1875 (41.8800)  time: 0.1505  data: 0.0001  max mem: 10046
[14:29:44.933796] Test: Total time: 0:00:07 (0.1941 s / it)
[14:29:45.443890] * Acc@1 16.830 Acc@5 41.930 loss 3.589
[14:29:45.444110] Accuracy of the network on the 10000 test images: 16.8%
[14:29:45.444329] [14:29:48.874634] Max accuracy: 16.83%
[14:29:48.874906] [14:29:48.875818] {"train_lr": 0.0065692578520540335, "train_loss": 3.5965511344728016, "test_loss": 3.5886146068572997, "test_acc1": 16.83, "test_acc5": 41.93, "epoch": 31, "n_parameters": 85958500}
[14:29:48.875938] [14:29:48.876003] Training epoch 31 for 0:00:33
[14:29:48.876056] [14:29:48.878835] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:29:50.822335] Epoch: [32]  [ 0/42]  eta: 0:01:21  lr: 0.006509  loss: 3.4945 (3.4945)  time: 1.9425  data: 1.4537  max mem: 10046
[14:30:10.381634] Epoch: [32]  [41/42]  eta: 0:00:00  lr: 0.006410  loss: 3.6464 (3.7032)  time: 0.4791  data: 0.0001  max mem: 10046
[14:30:10.608203] Epoch: [32] Total time: 0:00:21 (0.5174 s / it)
[14:30:10.615824] Averaged stats: lr: 0.006410  loss: 3.6464 (3.6821)
[14:30:12.300287] Test:  [ 0/40]  eta: 0:01:07  loss: 3.6140 (3.6140)  acc1: 21.8750 (21.8750)  acc5: 43.7500 (43.7500)  time: 1.6794  data: 1.5080  max mem: 10046
[14:30:18.292190] Test:  [39/40]  eta: 0:00:00  loss: 3.5490 (3.6097)  acc1: 18.7500 (17.0800)  acc5: 42.1875 (41.9600)  time: 0.1502  data: 0.0001  max mem: 10046
[14:30:18.424899] Test: Total time: 0:00:07 (0.1952 s / it)
[14:30:18.896553] * Acc@1 16.850 Acc@5 41.950 loss 3.637
[14:30:18.896765] Accuracy of the network on the 10000 test images: 16.9%
[14:30:18.896979] [14:30:22.331024] Max accuracy: 16.85%
[14:30:22.331445] [14:30:22.332344] {"train_lr": 0.006466504518821576, "train_loss": 3.6820831582659768, "test_loss": 3.636892247200012, "test_acc1": 16.85, "test_acc5": 41.95, "epoch": 32, "n_parameters": 85958500}
[14:30:22.332435] [14:30:22.332502] Training epoch 32 for 0:00:33
[14:30:22.332601] [14:30:22.335328] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:30:24.280237] Epoch: [33]  [ 0/42]  eta: 0:01:21  lr: 0.006405  loss: 3.8078 (3.8078)  time: 1.9441  data: 1.4564  max mem: 10046
[14:30:43.898950] Epoch: [33]  [41/42]  eta: 0:00:00  lr: 0.006303  loss: 3.6757 (3.6482)  time: 0.4807  data: 0.0001  max mem: 10046
[14:30:44.136930] Epoch: [33] Total time: 0:00:21 (0.5191 s / it)
[14:30:44.145626] Averaged stats: lr: 0.006303  loss: 3.6757 (3.6403)
[14:30:46.004719] Test:  [ 0/40]  eta: 0:01:14  loss: 3.5866 (3.5866)  acc1: 20.3125 (20.3125)  acc5: 43.7500 (43.7500)  time: 1.8559  data: 1.6769  max mem: 10046
[14:30:52.018932] Test:  [39/40]  eta: 0:00:00  loss: 3.4745 (3.5683)  acc1: 17.1875 (17.5600)  acc5: 45.3125 (43.0800)  time: 0.1504  data: 0.0001  max mem: 10046
[14:30:52.146977] Test: Total time: 0:00:07 (0.2000 s / it)
[14:30:52.228765] * Acc@1 16.840 Acc@5 42.370 loss 3.592
[14:30:52.228976] Accuracy of the network on the 10000 test images: 16.8%
[14:30:52.229195] [14:30:52.229267] Max accuracy: 16.85%
[14:30:52.229331] [14:30:52.230140] {"train_lr": 0.0063610546458533045, "train_loss": 3.640294600100744, "test_loss": 3.591783952713013, "test_acc1": 16.84, "test_acc5": 42.37, "epoch": 33, "n_parameters": 85958500}
[14:30:52.230210] [14:30:52.230273] Training epoch 33 for 0:00:29
[14:30:52.230327] [14:30:52.233203] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:30:54.554447] Epoch: [34]  [ 0/42]  eta: 0:01:37  lr: 0.006298  loss: 3.7050 (3.7050)  time: 2.3203  data: 1.7215  max mem: 10046
[14:31:14.325225] Epoch: [34]  [41/42]  eta: 0:00:00  lr: 0.006193  loss: 3.6485 (3.6115)  time: 0.4828  data: 0.0001  max mem: 10046
[14:31:14.556524] Epoch: [34] Total time: 0:00:22 (0.5315 s / it)
[14:31:14.566861] Averaged stats: lr: 0.006193  loss: 3.6485 (3.6405)
[14:31:16.197158] Test:  [ 0/40]  eta: 0:01:05  loss: 3.5528 (3.5528)  acc1: 21.8750 (21.8750)  acc5: 48.4375 (48.4375)  time: 1.6259  data: 1.4548  max mem: 10046
[14:31:22.218965] Test:  [39/40]  eta: 0:00:00  loss: 3.5179 (3.5773)  acc1: 17.1875 (17.4000)  acc5: 45.3125 (42.3200)  time: 0.1512  data: 0.0001  max mem: 10046
[14:31:22.334990] Test: Total time: 0:00:07 (0.1941 s / it)
[14:31:22.829321] * Acc@1 16.810 Acc@5 41.660 loss 3.598
[14:31:22.829534] Accuracy of the network on the 10000 test images: 16.8%
[14:31:22.829751] [14:31:22.829823] Max accuracy: 16.85%
[14:31:22.829878] [14:31:22.830750] {"train_lr": 0.006253023541037099, "train_loss": 3.6404776374499, "test_loss": 3.598466491699219, "test_acc1": 16.81, "test_acc5": 41.66, "epoch": 34, "n_parameters": 85958500}
[14:31:22.830829] [14:31:22.830904] Training epoch 34 for 0:00:30
[14:31:22.830958] [14:31:22.833748] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:31:25.213901] Epoch: [35]  [ 0/42]  eta: 0:01:39  lr: 0.006188  loss: 3.2252 (3.2252)  time: 2.3787  data: 1.7848  max mem: 10046
[14:31:44.876806] Epoch: [35]  [41/42]  eta: 0:00:00  lr: 0.006081  loss: 3.7206 (3.5636)  time: 0.4789  data: 0.0001  max mem: 10046
[14:31:45.091896] Epoch: [35] Total time: 0:00:22 (0.5300 s / it)
[14:31:45.104698] Averaged stats: lr: 0.006081  loss: 3.7206 (3.5614)
[14:31:46.778575] Test:  [ 0/40]  eta: 0:01:06  loss: 3.5079 (3.5079)  acc1: 18.7500 (18.7500)  acc5: 43.7500 (43.7500)  time: 1.6703  data: 1.5130  max mem: 10046
[14:31:52.882666] Test:  [39/40]  eta: 0:00:00  loss: 3.5107 (3.5470)  acc1: 17.1875 (17.2400)  acc5: 42.1875 (43.0400)  time: 0.1515  data: 0.0001  max mem: 10046
[14:31:52.990991] Test: Total time: 0:00:07 (0.1971 s / it)
[14:31:53.330738] * Acc@1 16.770 Acc@5 42.340 loss 3.568
[14:31:53.330910] Accuracy of the network on the 10000 test images: 16.8%
[14:31:53.331095] [14:31:53.331163] Max accuracy: 16.85%
[14:31:53.331218] [14:31:53.331983] {"train_lr": 0.006142529334799996, "train_loss": 3.5613829763162705, "test_loss": 3.568437385559082, "test_acc1": 16.77, "test_acc5": 42.34, "epoch": 35, "n_parameters": 85958500}
[14:31:53.332065] [14:31:53.332134] Training epoch 35 for 0:00:30
[14:31:53.332187] [14:31:53.334844] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:31:55.622681] Epoch: [36]  [ 0/42]  eta: 0:01:36  lr: 0.006076  loss: 3.6150 (3.6150)  time: 2.2869  data: 1.5857  max mem: 10046
[14:32:15.272136] Epoch: [36]  [41/42]  eta: 0:00:00  lr: 0.005967  loss: 3.6009 (3.6347)  time: 0.4813  data: 0.0001  max mem: 10046
[14:32:15.497431] Epoch: [36] Total time: 0:00:22 (0.5277 s / it)
[14:32:15.500761] Averaged stats: lr: 0.005967  loss: 3.6009 (3.6036)
[14:32:16.938341] Test:  [ 0/40]  eta: 0:00:57  loss: 3.5608 (3.5608)  acc1: 23.4375 (23.4375)  acc5: 40.6250 (40.6250)  time: 1.4333  data: 1.2759  max mem: 10046
[14:32:22.965153] Test:  [39/40]  eta: 0:00:00  loss: 3.5232 (3.5540)  acc1: 18.7500 (18.4400)  acc5: 45.3125 (43.4000)  time: 0.1512  data: 0.0001  max mem: 10046
[14:32:23.076959] Test: Total time: 0:00:07 (0.1893 s / it)
[14:32:23.634292] * Acc@1 17.750 Acc@5 43.100 loss 3.572
[14:32:23.634505] Accuracy of the network on the 10000 test images: 17.8%
[14:32:23.634738] [14:32:26.974650] Max accuracy: 17.75%
[14:32:26.974926] [14:32:26.975833] {"train_lr": 0.0060296928509342455, "train_loss": 3.6036225202537717, "test_loss": 3.57230167388916, "test_acc1": 17.75, "test_acc5": 43.1, "epoch": 36, "n_parameters": 85958500}
[14:32:26.975951] [14:32:26.976014] Training epoch 36 for 0:00:33
[14:32:26.976065] [14:32:26.978797] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:32:28.966661] Epoch: [37]  [ 0/42]  eta: 0:01:23  lr: 0.005962  loss: 4.0161 (4.0161)  time: 1.9871  data: 1.5090  max mem: 10046
[14:32:48.534983] Epoch: [37]  [41/42]  eta: 0:00:00  lr: 0.005851  loss: 3.6278 (3.6030)  time: 0.4794  data: 0.0001  max mem: 10046
[14:32:48.768542] Epoch: [37] Total time: 0:00:21 (0.5188 s / it)
[14:32:48.772104] Averaged stats: lr: 0.005851  loss: 3.6278 (3.5719)
[14:32:50.664294] Test:  [ 0/40]  eta: 0:01:15  loss: 3.5539 (3.5539)  acc1: 23.4375 (23.4375)  acc5: 42.1875 (42.1875)  time: 1.8889  data: 1.7117  max mem: 10046
[14:32:56.654964] Test:  [39/40]  eta: 0:00:00  loss: 3.4412 (3.5230)  acc1: 17.1875 (17.4400)  acc5: 43.7500 (43.2000)  time: 0.1506  data: 0.0001  max mem: 10046
[14:32:56.819445] Test: Total time: 0:00:08 (0.2011 s / it)
[14:32:57.067099] * Acc@1 17.390 Acc@5 42.640 loss 3.540
[14:32:57.067274] Accuracy of the network on the 10000 test images: 17.4%
[14:32:57.067469] [14:32:57.067537] Max accuracy: 17.75%
[14:32:57.067591] [14:32:57.068425] {"train_lr": 0.0059146374744783025, "train_loss": 3.5719256145613536, "test_loss": 3.540070915222168, "test_acc1": 17.39, "test_acc5": 42.64, "epoch": 37, "n_parameters": 85958500}
[14:32:57.068496] [14:32:57.068553] Training epoch 37 for 0:00:30
[14:32:57.068605] [14:32:57.071354] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:32:59.478163] Epoch: [38]  [ 0/42]  eta: 0:01:41  lr: 0.005846  loss: 3.3816 (3.3816)  time: 2.4058  data: 1.6888  max mem: 10046
[14:33:19.163584] Epoch: [38]  [41/42]  eta: 0:00:00  lr: 0.005733  loss: 3.5790 (3.6070)  time: 0.4821  data: 0.0001  max mem: 10046
[14:33:19.394258] Epoch: [38] Total time: 0:00:22 (0.5315 s / it)
[14:33:19.409632] Averaged stats: lr: 0.005733  loss: 3.5790 (3.6391)
[14:33:20.958901] Test:  [ 0/40]  eta: 0:01:01  loss: 3.5651 (3.5651)  acc1: 18.7500 (18.7500)  acc5: 45.3125 (45.3125)  time: 1.5454  data: 1.3883  max mem: 10046
[14:33:27.243304] Test:  [39/40]  eta: 0:00:00  loss: 3.4729 (3.5401)  acc1: 18.7500 (18.3600)  acc5: 43.7500 (44.4800)  time: 0.1516  data: 0.0001  max mem: 10046
[14:33:27.389018] Test: Total time: 0:00:07 (0.1994 s / it)
[14:33:27.544235] * Acc@1 17.860 Acc@5 43.760 loss 3.562
[14:33:27.544447] Accuracy of the network on the 10000 test images: 17.9%
[14:33:27.544677] [14:33:30.929280] Max accuracy: 17.86%
[14:33:30.929545] [14:33:30.930513] {"train_lr": 0.005797489016797103, "train_loss": 3.6391339273679826, "test_loss": 3.561862421035767, "test_acc1": 17.86, "test_acc5": 43.76, "epoch": 38, "n_parameters": 85958500}
[14:33:30.930663] [14:33:30.930728] Training epoch 38 for 0:00:33
[14:33:30.930779] [14:33:30.933563] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:33:32.903384] Epoch: [39]  [ 0/42]  eta: 0:01:22  lr: 0.005727  loss: 3.3581 (3.3581)  time: 1.9688  data: 1.4932  max mem: 10046
[14:33:52.502997] Epoch: [39]  [41/42]  eta: 0:00:00  lr: 0.005613  loss: 3.6248 (3.5212)  time: 0.4786  data: 0.0001  max mem: 10046
[14:33:52.737594] Epoch: [39] Total time: 0:00:21 (0.5191 s / it)
[14:33:52.743813] Averaged stats: lr: 0.005613  loss: 3.6248 (3.5098)
[14:33:54.728663] Test:  [ 0/40]  eta: 0:01:19  loss: 3.4836 (3.4836)  acc1: 23.4375 (23.4375)  acc5: 43.7500 (43.7500)  time: 1.9814  data: 1.8029  max mem: 10046
[14:34:00.731268] Test:  [39/40]  eta: 0:00:00  loss: 3.4133 (3.4727)  acc1: 17.1875 (18.3200)  acc5: 45.3125 (43.9600)  time: 0.1507  data: 0.0001  max mem: 10046
[14:34:00.843176] Test: Total time: 0:00:08 (0.2024 s / it)
[14:34:00.989305] * Acc@1 17.770 Acc@5 43.640 loss 3.490
[14:34:00.989522] Accuracy of the network on the 10000 test images: 17.8%
[14:34:00.989738] [14:34:00.989812] Max accuracy: 17.86%
[14:34:00.989868] [14:34:00.990767] {"train_lr": 0.00567837557800916, "train_loss": 3.5097646557149433, "test_loss": 3.489890730381012, "test_acc1": 17.77, "test_acc5": 43.64, "epoch": 39, "n_parameters": 85958500}
[14:34:00.990835] [14:34:00.990909] Training epoch 39 for 0:00:30
[14:34:00.990960] [14:34:00.993809] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:34:03.434566] Epoch: [40]  [ 0/42]  eta: 0:01:42  lr: 0.005607  loss: 3.9395 (3.9395)  time: 2.4397  data: 1.8135  max mem: 10046
[14:34:23.098606] Epoch: [40]  [41/42]  eta: 0:00:00  lr: 0.005491  loss: 3.6106 (3.6016)  time: 0.4821  data: 0.0001  max mem: 10046
[14:34:23.309932] Epoch: [40] Total time: 0:00:22 (0.5313 s / it)
[14:34:23.322774] Averaged stats: lr: 0.005491  loss: 3.6106 (3.6016)
[14:34:24.797405] Test:  [ 0/40]  eta: 0:00:58  loss: 3.5361 (3.5361)  acc1: 20.3125 (20.3125)  acc5: 42.1875 (42.1875)  time: 1.4702  data: 1.3126  max mem: 10046
[14:34:30.989248] Test:  [39/40]  eta: 0:00:00  loss: 3.4374 (3.4928)  acc1: 18.7500 (18.4800)  acc5: 45.3125 (44.8800)  time: 0.1508  data: 0.0001  max mem: 10046
[14:34:31.101547] Test: Total time: 0:00:07 (0.1944 s / it)
[14:34:31.381354] * Acc@1 18.440 Acc@5 44.680 loss 3.511
[14:34:31.381584] Accuracy of the network on the 10000 test images: 18.4%
[14:34:31.381828] [14:34:34.802476] Max accuracy: 18.44%
[14:34:34.802777] [14:34:34.803697] {"train_lr": 0.005557427406910991, "train_loss": 3.601564783425558, "test_loss": 3.5107876300811767, "test_acc1": 18.44, "test_acc5": 44.68, "epoch": 40, "n_parameters": 85958500}
[14:34:34.803832] [14:34:34.803911] Training epoch 40 for 0:00:33
[14:34:34.803961] [14:34:34.806997] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:34:36.690110] Epoch: [41]  [ 0/42]  eta: 0:01:19  lr: 0.005485  loss: 3.5551 (3.5551)  time: 1.8821  data: 1.3928  max mem: 10046
[14:34:56.229755] Epoch: [41]  [41/42]  eta: 0:00:00  lr: 0.005367  loss: 3.6727 (3.5931)  time: 0.4782  data: 0.0001  max mem: 10046
[14:34:56.458092] Epoch: [41] Total time: 0:00:21 (0.5155 s / it)
[14:34:56.462691] Averaged stats: lr: 0.005367  loss: 3.6727 (3.6007)
[14:34:58.014075] Test:  [ 0/40]  eta: 0:01:01  loss: 3.4978 (3.4978)  acc1: 18.7500 (18.7500)  acc5: 48.4375 (48.4375)  time: 1.5474  data: 1.3904  max mem: 10046
[14:35:04.080962] Test:  [39/40]  eta: 0:00:00  loss: 3.4973 (3.5116)  acc1: 17.1875 (17.6800)  acc5: 43.7500 (43.8000)  time: 0.1505  data: 0.0001  max mem: 10046
[14:35:04.190551] Test: Total time: 0:00:07 (0.1931 s / it)
[14:35:04.542732] * Acc@1 18.070 Acc@5 44.070 loss 3.519
[14:35:04.542952] Accuracy of the network on the 10000 test images: 18.1%
[14:35:04.543186] [14:35:04.543274] Max accuracy: 18.44%
[14:35:04.543368] [14:35:04.544480] {"train_lr": 0.005434776758551934, "train_loss": 3.600687760682333, "test_loss": 3.519113874435425, "test_acc1": 18.07, "test_acc5": 44.07, "epoch": 41, "n_parameters": 85958500}
[14:35:04.544587] [14:35:04.544666] Training epoch 41 for 0:00:29
[14:35:04.544738] [14:35:04.547529] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:35:07.016021] Epoch: [42]  [ 0/42]  eta: 0:01:43  lr: 0.005362  loss: 3.4129 (3.4129)  time: 2.4674  data: 1.4829  max mem: 10046
[14:35:26.715458] Epoch: [42]  [41/42]  eta: 0:00:00  lr: 0.005242  loss: 3.6403 (3.5481)  time: 0.4831  data: 0.0001  max mem: 10046
[14:35:26.928857] Epoch: [42] Total time: 0:00:22 (0.5329 s / it)
[14:35:26.962726] Averaged stats: lr: 0.005242  loss: 3.6403 (3.5889)
[14:35:28.776108] Test:  [ 0/40]  eta: 0:01:12  loss: 3.5007 (3.5007)  acc1: 23.4375 (23.4375)  acc5: 45.3125 (45.3125)  time: 1.8089  data: 1.6345  max mem: 10046
[14:35:34.793537] Test:  [39/40]  eta: 0:00:00  loss: 3.4266 (3.4843)  acc1: 18.7500 (18.6400)  acc5: 48.4375 (44.2400)  time: 0.1508  data: 0.0001  max mem: 10046
[14:35:34.904666] Test: Total time: 0:00:07 (0.1985 s / it)
[14:35:35.187311] * Acc@1 18.220 Acc@5 44.460 loss 3.498
[14:35:35.187505] Accuracy of the network on the 10000 test images: 18.2%
[14:35:35.187691] [14:35:35.187763] Max accuracy: 18.44%
[14:35:35.187820] [14:35:35.188673] {"train_lr": 0.005310557749615243, "train_loss": 3.588910602387928, "test_loss": 3.498447132110596, "test_acc1": 18.22, "test_acc5": 44.46, "epoch": 42, "n_parameters": 85958500}
[14:35:35.188753] [14:35:35.188818] Training epoch 42 for 0:00:30
[14:35:35.188871] [14:35:35.191681] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:35:37.752901] Epoch: [43]  [ 0/42]  eta: 0:01:47  lr: 0.005236  loss: 3.7870 (3.7870)  time: 2.5599  data: 1.9038  max mem: 10046
[14:35:57.353627] Epoch: [43]  [41/42]  eta: 0:00:00  lr: 0.005116  loss: 3.5654 (3.6030)  time: 0.4795  data: 0.0001  max mem: 10046
[14:35:57.588693] Epoch: [43] Total time: 0:00:22 (0.5333 s / it)
[14:35:57.592395] Averaged stats: lr: 0.005116  loss: 3.5654 (3.5984)
[14:35:59.344921] Test:  [ 0/40]  eta: 0:01:09  loss: 3.4373 (3.4373)  acc1: 17.1875 (17.1875)  acc5: 50.0000 (50.0000)  time: 1.7485  data: 1.5701  max mem: 10046
[14:36:05.402245] Test:  [39/40]  eta: 0:00:00  loss: 3.4342 (3.4761)  acc1: 18.7500 (19.1600)  acc5: 46.8750 (44.7200)  time: 0.1512  data: 0.0001  max mem: 10046
[14:36:05.516969] Test: Total time: 0:00:07 (0.1981 s / it)
[14:36:05.824472] * Acc@1 18.610 Acc@5 44.100 loss 3.505
[14:36:05.824665] Accuracy of the network on the 10000 test images: 18.6%
[14:36:05.824878] [14:36:09.221193] Max accuracy: 18.61%
[14:36:09.221467] [14:36:09.222434] {"train_lr": 0.005184906211763425, "train_loss": 3.5983992985316684, "test_loss": 3.5049317598342897, "test_acc1": 18.61, "test_acc5": 44.1, "epoch": 43, "n_parameters": 85958500}
[14:36:09.222558] [14:36:09.222625] Training epoch 43 for 0:00:34
[14:36:09.222677] [14:36:09.226249] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:36:11.146068] Epoch: [44]  [ 0/42]  eta: 0:01:20  lr: 0.005110  loss: 3.7028 (3.7028)  time: 1.9186  data: 1.4444  max mem: 10046
[14:36:30.671554] Epoch: [44]  [41/42]  eta: 0:00:00  lr: 0.004988  loss: 3.6335 (3.5561)  time: 0.4778  data: 0.0001  max mem: 10046
[14:36:30.901675] Epoch: [44] Total time: 0:00:21 (0.5161 s / it)
[14:36:30.911958] Averaged stats: lr: 0.004988  loss: 3.6335 (3.5552)
[14:36:32.657587] Test:  [ 0/40]  eta: 0:01:09  loss: 3.5643 (3.5643)  acc1: 20.3125 (20.3125)  acc5: 37.5000 (37.5000)  time: 1.7414  data: 1.5847  max mem: 10046
[14:36:38.654571] Test:  [39/40]  eta: 0:00:00  loss: 3.4249 (3.4649)  acc1: 18.7500 (18.2400)  acc5: 46.8750 (45.1200)  time: 0.1507  data: 0.0001  max mem: 10046
[14:36:38.801255] Test: Total time: 0:00:07 (0.1972 s / it)
[14:36:39.097384] * Acc@1 18.630 Acc@5 44.910 loss 3.483
[14:36:39.097607] Accuracy of the network on the 10000 test images: 18.6%
[14:36:39.097835] [14:36:42.493309] Max accuracy: 18.63%
[14:36:42.493586] [14:36:42.494513] {"train_lr": 0.005057959543108319, "train_loss": 3.555203754277456, "test_loss": 3.4830915212631224, "test_acc1": 18.63, "test_acc5": 44.91, "epoch": 44, "n_parameters": 85958500}
[14:36:42.494592] [14:36:42.494656] Training epoch 44 for 0:00:33
[14:36:42.494766] [14:36:42.497592] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:36:44.350974] Epoch: [45]  [ 0/42]  eta: 0:01:17  lr: 0.004982  loss: 3.1473 (3.1473)  time: 1.8525  data: 1.3724  max mem: 10046
[14:37:03.915662] Epoch: [45]  [41/42]  eta: 0:00:00  lr: 0.004860  loss: 3.4032 (3.4983)  time: 0.4794  data: 0.0001  max mem: 10046
[14:37:04.138309] Epoch: [45] Total time: 0:00:21 (0.5153 s / it)
[14:37:04.145415] Averaged stats: lr: 0.004860  loss: 3.4032 (3.5032)
[14:37:06.008495] Test:  [ 0/40]  eta: 0:01:14  loss: 3.5032 (3.5032)  acc1: 21.8750 (21.8750)  acc5: 46.8750 (46.8750)  time: 1.8594  data: 1.6788  max mem: 10046
[14:37:11.994652] Test:  [39/40]  eta: 0:00:00  loss: 3.3782 (3.4423)  acc1: 17.1875 (18.6000)  acc5: 46.8750 (45.1200)  time: 0.1502  data: 0.0001  max mem: 10046
[14:37:12.137385] Test: Total time: 0:00:07 (0.1997 s / it)
[14:37:12.277313] * Acc@1 18.380 Acc@5 45.150 loss 3.465
[14:37:12.277526] Accuracy of the network on the 10000 test images: 18.4%
[14:37:12.277746] [14:37:12.277821] Max accuracy: 18.63%
[14:37:12.277876] [14:37:12.278685] {"train_lr": 0.004929856557968283, "train_loss": 3.5031956178801402, "test_loss": 3.4648394703865053, "test_acc1": 18.38, "test_acc5": 45.15, "epoch": 45, "n_parameters": 85958500}
[14:37:12.278754] [14:37:12.278811] Training epoch 45 for 0:00:29
[14:37:12.278863] [14:37:12.281620] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:37:14.959152] Epoch: [46]  [ 0/42]  eta: 0:01:52  lr: 0.004854  loss: 3.2006 (3.2006)  time: 2.6767  data: 1.9211  max mem: 10046
[14:37:34.572289] Epoch: [46]  [41/42]  eta: 0:00:00  lr: 0.004730  loss: 3.5481 (3.4860)  time: 0.4801  data: 0.0001  max mem: 10046
[14:37:34.796206] Epoch: [46] Total time: 0:00:22 (0.5361 s / it)
[14:37:34.808206] Averaged stats: lr: 0.004730  loss: 3.5481 (3.5159)
[14:37:36.440087] Test:  [ 0/40]  eta: 0:01:05  loss: 3.4656 (3.4656)  acc1: 23.4375 (23.4375)  acc5: 43.7500 (43.7500)  time: 1.6279  data: 1.4707  max mem: 10046
[14:37:42.544679] Test:  [39/40]  eta: 0:00:00  loss: 3.3376 (3.4048)  acc1: 18.7500 (19.3600)  acc5: 50.0000 (45.8800)  time: 0.1512  data: 0.0001  max mem: 10046
[14:37:42.671148] Test: Total time: 0:00:07 (0.1965 s / it)
[14:37:43.026231] * Acc@1 19.210 Acc@5 46.030 loss 3.427
[14:37:43.026473] Accuracy of the network on the 10000 test images: 19.2%
[14:37:43.026706] [14:37:46.409287] Max accuracy: 19.21%
[14:37:46.409560] [14:37:46.410470] {"train_lr": 0.004800737335076797, "train_loss": 3.5158936040742055, "test_loss": 3.427270472049713, "test_acc1": 19.21, "test_acc5": 46.03, "epoch": 46, "n_parameters": 85958500}
[14:37:46.410591] [14:37:46.410653] Training epoch 46 for 0:00:34
[14:37:46.410705] [14:37:46.413836] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:37:48.234844] Epoch: [47]  [ 0/42]  eta: 0:01:16  lr: 0.004724  loss: 3.4762 (3.4762)  time: 1.8200  data: 1.3390  max mem: 10046
[14:38:07.803260] Epoch: [47]  [41/42]  eta: 0:00:00  lr: 0.004600  loss: 3.5312 (3.5370)  time: 0.4786  data: 0.0001  max mem: 10046
[14:38:08.039198] Epoch: [47] Total time: 0:00:21 (0.5149 s / it)
[14:38:08.045264] Averaged stats: lr: 0.004600  loss: 3.5312 (3.5476)
[14:38:09.469818] Test:  [ 0/40]  eta: 0:00:56  loss: 3.5089 (3.5089)  acc1: 23.4375 (23.4375)  acc5: 42.1875 (42.1875)  time: 1.4198  data: 1.2633  max mem: 10046
[14:38:15.638354] Test:  [39/40]  eta: 0:00:00  loss: 3.3950 (3.4450)  acc1: 18.7500 (18.5200)  acc5: 48.4375 (46.2000)  time: 0.1506  data: 0.0001  max mem: 10046
[14:38:15.753014] Test: Total time: 0:00:07 (0.1926 s / it)
[14:38:16.182302] * Acc@1 18.900 Acc@5 45.610 loss 3.470
[14:38:16.182526] Accuracy of the network on the 10000 test images: 18.9%
[14:38:16.182772] [14:38:16.182846] Max accuracy: 19.21%
[14:38:16.182906] [14:38:16.183787] {"train_lr": 0.0046707430644083595, "train_loss": 3.5475707536651973, "test_loss": 3.4700949668884276, "test_acc1": 18.9, "test_acc5": 45.61, "epoch": 47, "n_parameters": 85958500}
[14:38:16.183858] [14:38:16.183915] Training epoch 47 for 0:00:29
[14:38:16.183981] [14:38:16.186768] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:38:18.555698] Epoch: [48]  [ 0/42]  eta: 0:01:39  lr: 0.004593  loss: 3.6414 (3.6414)  time: 2.3682  data: 1.8854  max mem: 10046
[14:38:38.191365] Epoch: [48]  [41/42]  eta: 0:00:00  lr: 0.004469  loss: 3.4921 (3.6081)  time: 0.4816  data: 0.0001  max mem: 10046
[14:38:38.411389] Epoch: [48] Total time: 0:00:22 (0.5292 s / it)
[14:38:38.421936] Averaged stats: lr: 0.004469  loss: 3.4921 (3.5959)
[14:38:40.166553] Test:  [ 0/40]  eta: 0:01:09  loss: 3.4919 (3.4919)  acc1: 18.7500 (18.7500)  acc5: 42.1875 (42.1875)  time: 1.7411  data: 1.5611  max mem: 10046
[14:38:46.173219] Test:  [39/40]  eta: 0:00:00  loss: 3.4390 (3.4684)  acc1: 18.7500 (18.7600)  acc5: 46.8750 (44.8800)  time: 0.1508  data: 0.0001  max mem: 10046
[14:38:46.350076] Test: Total time: 0:00:07 (0.1981 s / it)
[14:38:46.451237] * Acc@1 18.770 Acc@5 45.390 loss 3.489
[14:38:46.451422] Accuracy of the network on the 10000 test images: 18.8%
[14:38:46.451643] [14:38:46.451710] Max accuracy: 19.21%
[14:38:46.451765] [14:38:46.452581] {"train_lr": 0.004540015892789405, "train_loss": 3.595857384658995, "test_loss": 3.4894217610359193, "test_acc1": 18.77, "test_acc5": 45.39, "epoch": 48, "n_parameters": 85958500}
[14:38:46.452655] [14:38:46.452711] Training epoch 48 for 0:00:30
[14:38:46.452762] [14:38:46.455420] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:38:48.878886] Epoch: [49]  [ 0/42]  eta: 0:01:41  lr: 0.004462  loss: 3.3973 (3.3973)  time: 2.4225  data: 1.7021  max mem: 10046
[14:39:08.491466] Epoch: [49]  [41/42]  eta: 0:00:00  lr: 0.004337  loss: 3.5419 (3.5673)  time: 0.4800  data: 0.0001  max mem: 10046
[14:39:08.729837] Epoch: [49] Total time: 0:00:22 (0.5303 s / it)
[14:39:08.749614] Averaged stats: lr: 0.004337  loss: 3.5419 (3.5696)
[14:39:10.202933] Test:  [ 0/40]  eta: 0:00:57  loss: 3.4637 (3.4637)  acc1: 23.4375 (23.4375)  acc5: 39.0625 (39.0625)  time: 1.4495  data: 1.2920  max mem: 10046
[14:39:16.448215] Test:  [39/40]  eta: 0:00:00  loss: 3.3884 (3.4377)  acc1: 17.1875 (18.2000)  acc5: 46.8750 (44.6800)  time: 0.1514  data: 0.0001  max mem: 10046
[14:39:16.564060] Test: Total time: 0:00:07 (0.1953 s / it)
[14:39:16.873264] * Acc@1 18.590 Acc@5 45.290 loss 3.455
[14:39:16.873452] Accuracy of the network on the 10000 test images: 18.6%
[14:39:16.873684] [14:39:16.873770] Max accuracy: 19.21%
[14:39:16.873827] [14:39:16.874799] {"train_lr": 0.004408698768462777, "train_loss": 3.5696138867310117, "test_loss": 3.455499196052551, "test_acc1": 18.59, "test_acc5": 45.29, "epoch": 49, "n_parameters": 85958500}
[14:39:16.874873] [14:39:16.874930] Training epoch 49 for 0:00:30
[14:39:16.874981] [14:39:16.877868] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:39:19.498336] Epoch: [50]  [ 0/42]  eta: 0:01:50  lr: 0.004331  loss: 3.4916 (3.4916)  time: 2.6196  data: 1.4189  max mem: 10046
[14:39:39.111837] Epoch: [50]  [41/42]  eta: 0:00:00  lr: 0.004205  loss: 3.5912 (3.5785)  time: 0.4795  data: 0.0001  max mem: 10046
[14:39:39.357266] Epoch: [50] Total time: 0:00:22 (0.5352 s / it)
[14:39:39.385848] Averaged stats: lr: 0.004205  loss: 3.5912 (3.5974)
[14:39:41.133843] Test:  [ 0/40]  eta: 0:01:09  loss: 3.4557 (3.4557)  acc1: 21.8750 (21.8750)  acc5: 43.7500 (43.7500)  time: 1.7436  data: 1.5630  max mem: 10046
[14:39:47.164515] Test:  [39/40]  eta: 0:00:00  loss: 3.4102 (3.4316)  acc1: 20.3125 (19.4800)  acc5: 46.8750 (45.8400)  time: 0.1513  data: 0.0001  max mem: 10046
[14:39:47.271657] Test: Total time: 0:00:07 (0.1971 s / it)
[14:39:47.508123] * Acc@1 18.890 Acc@5 45.750 loss 3.453
[14:39:47.508307] Accuracy of the network on the 10000 test images: 18.9%
[14:39:47.508557] [14:39:47.508630] Max accuracy: 19.21%
[14:39:47.508686] [14:39:47.509542] {"train_lr": 0.00427693528477595, "train_loss": 3.5974223471823192, "test_loss": 3.4530119061470033, "test_acc1": 18.89, "test_acc5": 45.75, "epoch": 50, "n_parameters": 85958500}
[14:39:47.509613] [14:39:47.509673] Training epoch 50 for 0:00:30
[14:39:47.509725] [14:39:47.512570] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:39:50.195780] Epoch: [51]  [ 0/42]  eta: 0:01:52  lr: 0.004199  loss: 3.6742 (3.6742)  time: 2.6824  data: 1.9186  max mem: 10046
[14:40:09.855834] Epoch: [51]  [41/42]  eta: 0:00:00  lr: 0.004073  loss: 3.4986 (3.5377)  time: 0.4816  data: 0.0001  max mem: 10046
[14:40:10.073249] Epoch: [51] Total time: 0:00:22 (0.5372 s / it)
[14:40:10.086519] Averaged stats: lr: 0.004073  loss: 3.4986 (3.5670)
[14:40:11.897920] Test:  [ 0/40]  eta: 0:01:12  loss: 3.5722 (3.5722)  acc1: 18.7500 (18.7500)  acc5: 40.6250 (40.6250)  time: 1.8069  data: 1.6359  max mem: 10046
[14:40:17.924589] Test:  [39/40]  eta: 0:00:00  loss: 3.4207 (3.4526)  acc1: 18.7500 (18.7200)  acc5: 48.4375 (45.4000)  time: 0.1510  data: 0.0001  max mem: 10046
[14:40:18.042342] Test: Total time: 0:00:07 (0.1988 s / it)
[14:40:18.270174] * Acc@1 18.820 Acc@5 45.740 loss 3.458
[14:40:18.270374] Accuracy of the network on the 10000 test images: 18.8%
[14:40:18.270580] [14:40:18.270651] Max accuracy: 19.21%
[14:40:18.270705] [14:40:18.271521] {"train_lr": 0.004144869523163801, "train_loss": 3.5669605263641904, "test_loss": 3.4579317212104796, "test_acc1": 18.82, "test_acc5": 45.74, "epoch": 51, "n_parameters": 85958500}
[14:40:18.271588] [14:40:18.271646] Training epoch 51 for 0:00:30
[14:40:18.271697] [14:40:18.274455] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:40:20.750659] Epoch: [52]  [ 0/42]  eta: 0:01:43  lr: 0.004067  loss: 3.6106 (3.6106)  time: 2.4753  data: 1.6413  max mem: 10046
[14:40:40.348277] Epoch: [52]  [41/42]  eta: 0:00:00  lr: 0.003941  loss: 3.4547 (3.5323)  time: 0.4794  data: 0.0001  max mem: 10046
[14:40:40.562567] Epoch: [52] Total time: 0:00:22 (0.5307 s / it)
[14:40:40.575840] Averaged stats: lr: 0.003941  loss: 3.4547 (3.5348)
[14:40:42.280291] Test:  [ 0/40]  eta: 0:01:08  loss: 3.4656 (3.4656)  acc1: 23.4375 (23.4375)  acc5: 46.8750 (46.8750)  time: 1.7009  data: 1.4907  max mem: 10046
[14:40:48.311106] Test:  [39/40]  eta: 0:00:00  loss: 3.4120 (3.4129)  acc1: 18.7500 (19.5600)  acc5: 45.3125 (47.1200)  time: 0.1510  data: 0.0001  max mem: 10046
[14:40:48.424540] Test: Total time: 0:00:07 (0.1962 s / it)
[14:40:48.647579] * Acc@1 19.750 Acc@5 46.740 loss 3.425
[14:40:48.647828] Accuracy of the network on the 10000 test images: 19.8%
[14:40:48.648195] [14:40:52.024312] Max accuracy: 19.75%
[14:40:52.024626] [14:40:52.025694] {"train_lr": 0.004012645895597684, "train_loss": 3.534774888129461, "test_loss": 3.425266885757446, "test_acc1": 19.75, "test_acc5": 46.74, "epoch": 52, "n_parameters": 85958500}
[14:40:52.025819] [14:40:52.025888] Training epoch 52 for 0:00:33
[14:40:52.025942] [14:40:52.028824] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:40:54.032764] Epoch: [53]  [ 0/42]  eta: 0:01:24  lr: 0.003934  loss: 3.4192 (3.4192)  time: 2.0029  data: 1.5284  max mem: 10046
[14:41:13.578137] Epoch: [53]  [41/42]  eta: 0:00:00  lr: 0.003808  loss: 3.5695 (3.5241)  time: 0.4781  data: 0.0001  max mem: 10046
[14:41:13.824601] Epoch: [53] Total time: 0:00:21 (0.5189 s / it)
[14:41:13.827972] Averaged stats: lr: 0.003808  loss: 3.5695 (3.5248)
[14:41:15.384508] Test:  [ 0/40]  eta: 0:01:02  loss: 3.4581 (3.4581)  acc1: 23.4375 (23.4375)  acc5: 46.8750 (46.8750)  time: 1.5521  data: 1.3866  max mem: 10046
[14:41:21.402896] Test:  [39/40]  eta: 0:00:00  loss: 3.3278 (3.3774)  acc1: 18.7500 (20.6000)  acc5: 48.4375 (47.7200)  time: 0.1505  data: 0.0001  max mem: 10046
[14:41:21.512215] Test: Total time: 0:00:07 (0.1920 s / it)
[14:41:22.053115] * Acc@1 20.350 Acc@5 47.530 loss 3.390
[14:41:22.053335] Accuracy of the network on the 10000 test images: 20.4%
[14:41:22.053569] [14:41:24.900437] Max accuracy: 20.35%
[14:41:24.900782] [14:41:24.901650] {"train_lr": 0.003880408986673056, "train_loss": 3.524752064829781, "test_loss": 3.390100932121277, "test_acc1": 20.35, "test_acc5": 47.53, "epoch": 53, "n_parameters": 85958500}
[14:41:24.901721] [14:41:24.901784] Training epoch 53 for 0:00:32
[14:41:24.901835] [14:41:24.904705] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:41:26.896143] Epoch: [54]  [ 0/42]  eta: 0:01:23  lr: 0.003802  loss: 3.2799 (3.2799)  time: 1.9906  data: 1.5168  max mem: 10046
[14:41:46.448029] Epoch: [54]  [41/42]  eta: 0:00:00  lr: 0.003677  loss: 3.5458 (3.5184)  time: 0.4787  data: 0.0001  max mem: 10046
[14:41:46.679954] Epoch: [54] Total time: 0:00:21 (0.5185 s / it)
[14:41:46.688483] Averaged stats: lr: 0.003677  loss: 3.5458 (3.5178)
[14:41:48.455269] Test:  [ 0/40]  eta: 0:01:10  loss: 3.4907 (3.4907)  acc1: 21.8750 (21.8750)  acc5: 40.6250 (40.6250)  time: 1.7629  data: 1.5916  max mem: 10046
[14:41:54.449135] Test:  [39/40]  eta: 0:00:00  loss: 3.3554 (3.3972)  acc1: 20.3125 (20.2400)  acc5: 46.8750 (47.2800)  time: 0.1501  data: 0.0001  max mem: 10046
[14:41:54.580878] Test: Total time: 0:00:07 (0.1973 s / it)
[14:41:54.762993] * Acc@1 20.260 Acc@5 47.440 loss 3.406
[14:41:54.763188] Accuracy of the network on the 10000 test images: 20.3%
[14:41:54.763434] [14:41:54.763505] Max accuracy: 20.35%
[14:41:54.763561] [14:41:54.764593] {"train_lr": 0.0037483033955083437, "train_loss": 3.5178480375380743, "test_loss": 3.4055009365081785, "test_acc1": 20.26, "test_acc5": 47.44, "epoch": 54, "n_parameters": 85958500}
[14:41:54.764678] [14:41:54.764739] Training epoch 54 for 0:00:29
[14:41:54.764791] [14:41:54.767656] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:41:57.090852] Epoch: [55]  [ 0/42]  eta: 0:01:37  lr: 0.003670  loss: 3.4153 (3.4153)  time: 2.3225  data: 1.4783  max mem: 10046
[14:42:16.793268] Epoch: [55]  [41/42]  eta: 0:00:00  lr: 0.003545  loss: 3.5045 (3.5493)  time: 0.4829  data: 0.0001  max mem: 10046
[14:42:17.043672] Epoch: [55] Total time: 0:00:22 (0.5304 s / it)
[14:42:17.044572] Averaged stats: lr: 0.003545  loss: 3.5045 (3.5545)
[14:42:18.468662] Test:  [ 0/40]  eta: 0:00:56  loss: 3.4505 (3.4505)  acc1: 23.4375 (23.4375)  acc5: 45.3125 (45.3125)  time: 1.4190  data: 1.2619  max mem: 10046
[14:42:24.811433] Test:  [39/40]  eta: 0:00:00  loss: 3.3221 (3.3793)  acc1: 20.3125 (20.3200)  acc5: 50.0000 (47.8800)  time: 0.1508  data: 0.0001  max mem: 10046
[14:42:24.937857] Test: Total time: 0:00:07 (0.1973 s / it)
[14:42:25.020417] * Acc@1 20.480 Acc@5 47.520 loss 3.390
[14:42:25.020643] Accuracy of the network on the 10000 test images: 20.5%
[14:42:25.020904] [14:42:28.367434] Max accuracy: 20.48%
[14:42:28.367702] [14:42:28.368678] {"train_lr": 0.0036164735776279306, "train_loss": 3.5544756140027727, "test_loss": 3.390334129333496, "test_acc1": 20.48, "test_acc5": 47.52, "epoch": 55, "n_parameters": 85958500}
[14:42:28.368804] [14:42:28.368872] Training epoch 55 for 0:00:33
[14:42:28.368924] [14:42:28.371715] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:42:30.367277] Epoch: [56]  [ 0/42]  eta: 0:01:23  lr: 0.003539  loss: 3.8005 (3.8005)  time: 1.9947  data: 1.5211  max mem: 10046
[14:42:49.964401] Epoch: [56]  [41/42]  eta: 0:00:00  lr: 0.003414  loss: 3.5119 (3.5569)  time: 0.4798  data: 0.0001  max mem: 10046
[14:42:50.191252] Epoch: [56] Total time: 0:00:21 (0.5195 s / it)
[14:42:50.200595] Averaged stats: lr: 0.003414  loss: 3.5119 (3.5184)
[14:42:51.842947] Test:  [ 0/40]  eta: 0:01:05  loss: 3.4113 (3.4113)  acc1: 23.4375 (23.4375)  acc5: 42.1875 (42.1875)  time: 1.6380  data: 1.4789  max mem: 10046
[14:42:57.882355] Test:  [39/40]  eta: 0:00:00  loss: 3.3427 (3.4017)  acc1: 20.3125 (19.8000)  acc5: 46.8750 (47.1600)  time: 0.1504  data: 0.0001  max mem: 10046
[14:42:58.043649] Test: Total time: 0:00:07 (0.1960 s / it)
[14:42:58.442050] * Acc@1 19.630 Acc@5 46.850 loss 3.414
[14:42:58.442261] Accuracy of the network on the 10000 test images: 19.6%
[14:42:58.442484] [14:42:58.442552] Max accuracy: 20.48%
[14:42:58.442607] [14:42:58.443436] {"train_lr": 0.003485063687002159, "train_loss": 3.518360733985901, "test_loss": 3.4139618992805483, "test_acc1": 19.63, "test_acc5": 46.85, "epoch": 56, "n_parameters": 85958500}
[14:42:58.443506] [14:42:58.443565] Training epoch 56 for 0:00:30
[14:42:58.443617] [14:42:58.446358] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:43:01.024934] Epoch: [57]  [ 0/42]  eta: 0:01:48  lr: 0.003408  loss: 3.5770 (3.5770)  time: 2.5778  data: 1.7751  max mem: 10046
[14:43:20.645478] Epoch: [57]  [41/42]  eta: 0:00:00  lr: 0.003283  loss: 3.5200 (3.5393)  time: 0.4806  data: 0.0001  max mem: 10046
[14:43:20.857294] Epoch: [57] Total time: 0:00:22 (0.5336 s / it)
[14:43:20.876675] Averaged stats: lr: 0.003283  loss: 3.5200 (3.5495)
[14:43:22.466606] Test:  [ 0/40]  eta: 0:01:03  loss: 3.4231 (3.4231)  acc1: 21.8750 (21.8750)  acc5: 45.3125 (45.3125)  time: 1.5856  data: 1.4146  max mem: 10046
[14:43:28.475549] Test:  [39/40]  eta: 0:00:00  loss: 3.3209 (3.3835)  acc1: 20.3125 (20.2800)  acc5: 50.0000 (47.6400)  time: 0.1508  data: 0.0001  max mem: 10046
[14:43:28.597465] Test: Total time: 0:00:07 (0.1930 s / it)
[14:43:29.139376] * Acc@1 20.210 Acc@5 47.540 loss 3.388
[14:43:29.139573] Accuracy of the network on the 10000 test images: 20.2%
[14:43:29.139785] [14:43:29.139866] Max accuracy: 20.48%
[14:43:29.139927] [14:43:29.140810] {"train_lr": 0.003354217418417092, "train_loss": 3.5494534344900224, "test_loss": 3.3884888768196104, "test_acc1": 20.21, "test_acc5": 47.54, "epoch": 57, "n_parameters": 85958500}
[14:43:29.140906] [14:43:29.140970] Training epoch 57 for 0:00:30
[14:43:29.141042] [14:43:29.143780] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:43:31.517292] Epoch: [58]  [ 0/42]  eta: 0:01:39  lr: 0.003277  loss: 3.1927 (3.1927)  time: 2.3723  data: 1.7966  max mem: 10046
[14:43:51.145867] Epoch: [58]  [41/42]  eta: 0:00:00  lr: 0.003154  loss: 3.4128 (3.5166)  time: 0.4802  data: 0.0001  max mem: 10046
[14:43:51.391630] Epoch: [58] Total time: 0:00:22 (0.5297 s / it)
[14:43:51.399342] Averaged stats: lr: 0.003154  loss: 3.4128 (3.5317)
[14:43:52.916878] Test:  [ 0/40]  eta: 0:01:00  loss: 3.3687 (3.3687)  acc1: 26.5625 (26.5625)  acc5: 45.3125 (45.3125)  time: 1.5135  data: 1.3560  max mem: 10046
[14:43:58.946644] Test:  [39/40]  eta: 0:00:00  loss: 3.3479 (3.3812)  acc1: 18.7500 (20.3600)  acc5: 50.0000 (48.0000)  time: 0.1509  data: 0.0001  max mem: 10046
[14:43:59.057698] Test: Total time: 0:00:07 (0.1914 s / it)
[14:43:59.645853] * Acc@1 20.280 Acc@5 47.550 loss 3.400
[14:43:59.646056] Accuracy of the network on the 10000 test images: 20.3%
[14:43:59.646259] [14:43:59.646329] Max accuracy: 20.48%
[14:43:59.646387] [14:43:59.647376] {"train_lr": 0.003224077850346362, "train_loss": 3.5316522178195773, "test_loss": 3.3996198892593386, "test_acc1": 20.28, "test_acc5": 47.55, "epoch": 58, "n_parameters": 85958500}
[14:43:59.647446] [14:43:59.647502] Training epoch 58 for 0:00:30
[14:43:59.647553] [14:43:59.650300] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:44:02.164882] Epoch: [59]  [ 0/42]  eta: 0:01:45  lr: 0.003147  loss: 3.9471 (3.9471)  time: 2.5134  data: 1.7360  max mem: 10046
[14:44:21.782736] Epoch: [59]  [41/42]  eta: 0:00:00  lr: 0.003025  loss: 3.5690 (3.5460)  time: 0.4807  data: 0.0001  max mem: 10046
[14:44:22.028754] Epoch: [59] Total time: 0:00:22 (0.5328 s / it)
[14:44:22.031116] Averaged stats: lr: 0.003025  loss: 3.5690 (3.5281)
[14:44:23.509513] Test:  [ 0/40]  eta: 0:00:58  loss: 3.3712 (3.3712)  acc1: 21.8750 (21.8750)  acc5: 53.1250 (53.1250)  time: 1.4744  data: 1.3180  max mem: 10046
[14:44:29.797657] Test:  [39/40]  eta: 0:00:00  loss: 3.2964 (3.3518)  acc1: 20.3125 (20.7200)  acc5: 51.5625 (49.0000)  time: 0.1511  data: 0.0001  max mem: 10046
[14:44:29.910012] Test: Total time: 0:00:07 (0.1969 s / it)
[14:44:30.177629] * Acc@1 20.920 Acc@5 48.550 loss 3.366
[14:44:30.177849] Accuracy of the network on the 10000 test images: 20.9%
[14:44:30.178074] [14:44:33.488330] Max accuracy: 20.92%
[14:44:33.488595] [14:44:33.489514] {"train_lr": 0.00309478728849696, "train_loss": 3.528095708006904, "test_loss": 3.3656112790107726, "test_acc1": 20.92, "test_acc5": 48.55, "epoch": 59, "n_parameters": 85958500}
[14:44:33.489636] [14:44:33.489705] Training epoch 59 for 0:00:33
[14:44:33.489757] [14:44:33.492556] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:44:35.313181] Epoch: [60]  [ 0/42]  eta: 0:01:16  lr: 0.003019  loss: 3.6889 (3.6889)  time: 1.8197  data: 1.3387  max mem: 10046
[14:44:54.870729] Epoch: [60]  [41/42]  eta: 0:00:00  lr: 0.002897  loss: 3.5820 (3.5844)  time: 0.4782  data: 0.0001  max mem: 10046
[14:44:55.090870] Epoch: [60] Total time: 0:00:21 (0.5142 s / it)
[14:44:55.091561] Averaged stats: lr: 0.002897  loss: 3.5820 (3.5808)
[14:44:56.856519] Test:  [ 0/40]  eta: 0:01:10  loss: 3.3725 (3.3725)  acc1: 23.4375 (23.4375)  acc5: 48.4375 (48.4375)  time: 1.7600  data: 1.5853  max mem: 10046
[14:45:02.850369] Test:  [39/40]  eta: 0:00:00  loss: 3.2874 (3.3547)  acc1: 21.8750 (20.6000)  acc5: 50.0000 (48.8400)  time: 0.1508  data: 0.0001  max mem: 10046
[14:45:02.964261] Test: Total time: 0:00:07 (0.1968 s / it)
[14:45:03.358560] * Acc@1 20.400 Acc@5 48.170 loss 3.369
[14:45:03.358781] Accuracy of the network on the 10000 test images: 20.4%
[14:45:03.359048] [14:45:03.359124] Max accuracy: 20.92%
[14:45:03.359179] [14:45:03.360051] {"train_lr": 0.0029664871102000495, "train_loss": 3.580773176181884, "test_loss": 3.3692991137504578, "test_acc1": 20.4, "test_acc5": 48.17, "epoch": 60, "n_parameters": 85958500}
[14:45:03.360137] [14:45:03.360197] Training epoch 60 for 0:00:29
[14:45:03.360249] [14:45:03.362984] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:45:05.691259] Epoch: [61]  [ 0/42]  eta: 0:01:37  lr: 0.002891  loss: 3.4973 (3.4973)  time: 2.3272  data: 1.8414  max mem: 10046
[14:45:25.380805] Epoch: [61]  [41/42]  eta: 0:00:00  lr: 0.002771  loss: 3.4675 (3.5160)  time: 0.4826  data: 0.0001  max mem: 10046
[14:45:25.610395] Epoch: [61] Total time: 0:00:22 (0.5297 s / it)
[14:45:25.611189] Averaged stats: lr: 0.002771  loss: 3.4675 (3.5288)
[14:45:27.366667] Test:  [ 0/40]  eta: 0:01:10  loss: 3.3627 (3.3627)  acc1: 23.4375 (23.4375)  acc5: 48.4375 (48.4375)  time: 1.7513  data: 1.5707  max mem: 10046
[14:45:33.385649] Test:  [39/40]  eta: 0:00:00  loss: 3.3077 (3.3556)  acc1: 21.8750 (21.2800)  acc5: 50.0000 (48.3600)  time: 0.1511  data: 0.0001  max mem: 10046
[14:45:33.517614] Test: Total time: 0:00:07 (0.1976 s / it)
[14:45:33.903752] * Acc@1 20.810 Acc@5 48.120 loss 3.369
[14:45:33.904006] Accuracy of the network on the 10000 test images: 20.8%
[14:45:33.904234] [14:45:33.904303] Max accuracy: 20.92%
[14:45:33.904358] [14:45:33.905226] {"train_lr": 0.0028393176098169106, "train_loss": 3.5288297051475164, "test_loss": 3.3691737055778503, "test_acc1": 20.81, "test_acc5": 48.12, "epoch": 61, "n_parameters": 85958500}
[14:45:33.905294] [14:45:33.905354] Training epoch 61 for 0:00:30
[14:45:33.905405] [14:45:33.908047] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:45:36.248584] Epoch: [62]  [ 0/42]  eta: 0:01:38  lr: 0.002765  loss: 3.2637 (3.2637)  time: 2.3395  data: 1.4379  max mem: 10046
[14:45:56.047503] Epoch: [62]  [41/42]  eta: 0:00:00  lr: 0.002645  loss: 3.5584 (3.5778)  time: 0.4807  data: 0.0001  max mem: 10046
[14:45:56.272151] Epoch: [62] Total time: 0:00:22 (0.5325 s / it)
[14:45:56.284931] Averaged stats: lr: 0.002645  loss: 3.5584 (3.5395)
[14:45:57.695393] Test:  [ 0/40]  eta: 0:00:56  loss: 3.3720 (3.3720)  acc1: 25.0000 (25.0000)  acc5: 46.8750 (46.8750)  time: 1.4064  data: 1.2480  max mem: 10046
[14:46:03.853218] Test:  [39/40]  eta: 0:00:00  loss: 3.3011 (3.3578)  acc1: 21.8750 (20.9200)  acc5: 50.0000 (48.1200)  time: 0.1509  data: 0.0001  max mem: 10046
[14:46:03.991115] Test: Total time: 0:00:07 (0.1926 s / it)
[14:46:04.372347] * Acc@1 20.710 Acc@5 48.030 loss 3.367
[14:46:04.372596] Accuracy of the network on the 10000 test images: 20.7%
[14:46:04.372819] [14:46:04.372893] Max accuracy: 20.92%
[14:46:04.372948] [14:46:04.373811] {"train_lr": 0.002713417845329128, "train_loss": 3.5394809274446395, "test_loss": 3.367278349399567, "test_acc1": 20.71, "test_acc5": 48.03, "epoch": 62, "n_parameters": 85958500}
[14:46:04.373880] [14:46:04.373940] Training epoch 62 for 0:00:30
[14:46:04.373992] [14:46:04.376862] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:46:06.811322] Epoch: [63]  [ 0/42]  eta: 0:01:42  lr: 0.002639  loss: 3.7980 (3.7980)  time: 2.4336  data: 1.5053  max mem: 10046
[14:46:26.458583] Epoch: [63]  [41/42]  eta: 0:00:00  lr: 0.002522  loss: 3.6532 (3.5759)  time: 0.4809  data: 0.0001  max mem: 10046
[14:46:26.680983] Epoch: [63] Total time: 0:00:22 (0.5310 s / it)
[14:46:26.687367] Averaged stats: lr: 0.002522  loss: 3.6532 (3.5565)
[14:46:28.442769] Test:  [ 0/40]  eta: 0:01:10  loss: 3.3282 (3.3282)  acc1: 26.5625 (26.5625)  acc5: 43.7500 (43.7500)  time: 1.7516  data: 1.5947  max mem: 10046
[14:46:34.463792] Test:  [39/40]  eta: 0:00:00  loss: 3.2810 (3.3537)  acc1: 20.3125 (21.2400)  acc5: 50.0000 (48.0000)  time: 0.1510  data: 0.0001  max mem: 10046
[14:46:34.618262] Test: Total time: 0:00:07 (0.1982 s / it)
[14:46:34.906177] * Acc@1 20.840 Acc@5 48.170 loss 3.371
[14:46:34.906387] Accuracy of the network on the 10000 test images: 20.8%
[14:46:34.906600] [14:46:34.906674] Max accuracy: 20.92%
[14:46:34.906729] [14:46:34.907542] {"train_lr": 0.00258892548628073, "train_loss": 3.556508311203548, "test_loss": 3.370608079433441, "test_acc1": 20.84, "test_acc5": 48.17, "epoch": 63, "n_parameters": 85958500}
[14:46:34.907611] [14:46:34.907683] Training epoch 63 for 0:00:30
[14:46:34.907735] [14:46:34.910435] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:46:37.285020] Epoch: [64]  [ 0/42]  eta: 0:01:39  lr: 0.002516  loss: 3.6594 (3.6594)  time: 2.3735  data: 1.5898  max mem: 10046
[14:46:56.892915] Epoch: [64]  [41/42]  eta: 0:00:00  lr: 0.002400  loss: 3.5780 (3.5400)  time: 0.4799  data: 0.0001  max mem: 10046
[14:46:57.107934] Epoch: [64] Total time: 0:00:22 (0.5285 s / it)
[14:46:57.128088] Averaged stats: lr: 0.002400  loss: 3.5780 (3.5724)
[14:46:59.000429] Test:  [ 0/40]  eta: 0:01:14  loss: 3.3151 (3.3151)  acc1: 25.0000 (25.0000)  acc5: 51.5625 (51.5625)  time: 1.8684  data: 1.6911  max mem: 10046
[14:47:05.021806] Test:  [39/40]  eta: 0:00:00  loss: 3.3235 (3.3426)  acc1: 20.3125 (21.6000)  acc5: 48.4375 (48.0000)  time: 0.1513  data: 0.0001  max mem: 10046
[14:47:05.141669] Test: Total time: 0:00:08 (0.2003 s / it)
[14:47:05.330478] * Acc@1 21.000 Acc@5 48.280 loss 3.364
[14:47:05.330669] Accuracy of the network on the 10000 test images: 21.0%
[14:47:05.330930] [14:47:08.857096] Max accuracy: 21.00%
[14:47:08.857431] [14:47:08.858342] {"train_lr": 0.0024659766632385543, "train_loss": 3.5723859525862194, "test_loss": 3.3638792455196382, "test_acc1": 21.0, "test_acc5": 48.28, "epoch": 64, "n_parameters": 85958500}
[14:47:08.858482] [14:47:08.858551] Training epoch 64 for 0:00:33
[14:47:08.858604] [14:47:08.861571] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:47:10.737975] Epoch: [65]  [ 0/42]  eta: 0:01:18  lr: 0.002394  loss: 3.3935 (3.3935)  time: 1.8756  data: 1.3887  max mem: 10046
[14:47:30.276059] Epoch: [65]  [41/42]  eta: 0:00:00  lr: 0.002279  loss: 3.5310 (3.5053)  time: 0.4783  data: 0.0001  max mem: 10046
[14:47:30.488298] Epoch: [65] Total time: 0:00:21 (0.5149 s / it)
[14:47:30.514424] Averaged stats: lr: 0.002279  loss: 3.5310 (3.4929)
[14:47:32.020599] Test:  [ 0/40]  eta: 0:01:00  loss: 3.2994 (3.2994)  acc1: 25.0000 (25.0000)  acc5: 53.1250 (53.1250)  time: 1.5020  data: 1.3447  max mem: 10046
[14:47:38.104994] Test:  [39/40]  eta: 0:00:00  loss: 3.2629 (3.3018)  acc1: 21.8750 (21.6400)  acc5: 51.5625 (49.8400)  time: 0.1503  data: 0.0001  max mem: 10046
[14:47:38.219090] Test: Total time: 0:00:07 (0.1926 s / it)
[14:47:38.823432] * Acc@1 21.190 Acc@5 49.490 loss 3.327
[14:47:38.823652] Accuracy of the network on the 10000 test images: 21.2%
[14:47:38.823860] [14:47:42.153631] Max accuracy: 21.19%
[14:47:42.153892] [14:47:42.154819] {"train_lr": 0.0023447058189354796, "train_loss": 3.4928706061272394, "test_loss": 3.327301526069641, "test_acc1": 21.19, "test_acc5": 49.49, "epoch": 65, "n_parameters": 85958500}
[14:47:42.154944] [14:47:42.155009] Training epoch 65 for 0:00:33
[14:47:42.155062] [14:47:42.157871] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:47:43.930943] Epoch: [66]  [ 0/42]  eta: 0:01:14  lr: 0.002274  loss: 3.4188 (3.4188)  time: 1.7721  data: 1.3033  max mem: 10046
[14:48:03.486229] Epoch: [66]  [41/42]  eta: 0:00:00  lr: 0.002161  loss: 3.5243 (3.4686)  time: 0.4786  data: 0.0001  max mem: 10046
[14:48:03.725586] Epoch: [66] Total time: 0:00:21 (0.5135 s / it)
[14:48:03.726501] Averaged stats: lr: 0.002161  loss: 3.5243 (3.4964)
[14:48:05.277148] Test:  [ 0/40]  eta: 0:01:01  loss: 3.2880 (3.2880)  acc1: 21.8750 (21.8750)  acc5: 45.3125 (45.3125)  time: 1.5463  data: 1.3895  max mem: 10046
[14:48:11.378752] Test:  [39/40]  eta: 0:00:00  loss: 3.2881 (3.3162)  acc1: 20.3125 (21.4000)  acc5: 48.4375 (48.8800)  time: 0.1503  data: 0.0001  max mem: 10046
[14:48:11.497892] Test: Total time: 0:00:07 (0.1942 s / it)
[14:48:11.857223] * Acc@1 21.070 Acc@5 48.950 loss 3.330
[14:48:11.857428] Accuracy of the network on the 10000 test images: 21.1%
[14:48:11.857626] [14:48:11.857698] Max accuracy: 21.19%
[14:48:11.857751] [14:48:11.858557] {"train_lr": 0.00222524556125928, "train_loss": 3.496387385186695, "test_loss": 3.330229306221008, "test_acc1": 21.07, "test_acc5": 48.95, "epoch": 66, "n_parameters": 85958500}
[14:48:11.858624] [14:48:11.858681] Training epoch 66 for 0:00:29
[14:48:11.858729] [14:48:11.861534] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:48:14.148432] Epoch: [67]  [ 0/42]  eta: 0:01:36  lr: 0.002155  loss: 3.3217 (3.3217)  time: 2.2860  data: 1.7477  max mem: 10046
[14:48:33.847198] Epoch: [67]  [41/42]  eta: 0:00:00  lr: 0.002045  loss: 3.4504 (3.4953)  time: 0.4816  data: 0.0001  max mem: 10046
[14:48:34.073173] Epoch: [67] Total time: 0:00:22 (0.5288 s / it)
[14:48:34.105631] Averaged stats: lr: 0.002045  loss: 3.4504 (3.4683)
[14:48:35.666523] Test:  [ 0/40]  eta: 0:01:02  loss: 3.3005 (3.3005)  acc1: 25.0000 (25.0000)  acc5: 51.5625 (51.5625)  time: 1.5567  data: 1.3799  max mem: 10046
[14:48:41.675259] Test:  [39/40]  eta: 0:00:00  loss: 3.2707 (3.3186)  acc1: 20.3125 (21.7200)  acc5: 51.5625 (49.5200)  time: 0.1508  data: 0.0001  max mem: 10046
[14:48:41.795200] Test: Total time: 0:00:07 (0.1922 s / it)
[14:48:42.342083] * Acc@1 21.350 Acc@5 49.110 loss 3.334
[14:48:42.342299] Accuracy of the network on the 10000 test images: 21.4%
[14:48:42.342516] [14:48:45.718785] Max accuracy: 21.35%
[14:48:45.719052] [14:48:45.720025] {"train_lr": 0.002107726518247815, "train_loss": 3.468272131113779, "test_loss": 3.3342335700988768, "test_acc1": 21.35, "test_acc5": 49.11, "epoch": 67, "n_parameters": 85958500}
[14:48:45.720153] [14:48:45.720219] Training epoch 67 for 0:00:33
[14:48:45.720272] [14:48:45.722950] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:48:47.566030] Epoch: [68]  [ 0/42]  eta: 0:01:17  lr: 0.002039  loss: 3.4443 (3.4443)  time: 1.8418  data: 1.3662  max mem: 10046
[14:49:07.099491] Epoch: [68]  [41/42]  eta: 0:00:00  lr: 0.001930  loss: 3.4796 (3.4537)  time: 0.4782  data: 0.0001  max mem: 10046
[14:49:07.316376] Epoch: [68] Total time: 0:00:21 (0.5141 s / it)
[14:49:07.336582] Averaged stats: lr: 0.001930  loss: 3.4796 (3.4445)
[14:49:08.736669] Test:  [ 0/40]  eta: 0:00:55  loss: 3.3090 (3.3090)  acc1: 21.8750 (21.8750)  acc5: 48.4375 (48.4375)  time: 1.3955  data: 1.2386  max mem: 10046
[14:49:14.807770] Test:  [39/40]  eta: 0:00:00  loss: 3.2728 (3.3094)  acc1: 18.7500 (21.5200)  acc5: 50.0000 (49.4400)  time: 0.1504  data: 0.0001  max mem: 10046
[14:49:14.925021] Test: Total time: 0:00:07 (0.1897 s / it)
[14:49:15.412411] * Acc@1 20.990 Acc@5 49.280 loss 3.317
[14:49:15.412597] Accuracy of the network on the 10000 test images: 21.0%
[14:49:15.412839] [14:49:15.412909] Max accuracy: 21.35%
[14:49:15.412965] [14:49:15.413804] {"train_lr": 0.0019922771952492306, "train_loss": 3.4444782506851923, "test_loss": 3.3172801792621613, "test_acc1": 20.99, "test_acc5": 49.28, "epoch": 68, "n_parameters": 85958500}
[14:49:15.413875] [14:49:15.413936] Training epoch 68 for 0:00:29
[14:49:15.413988] [14:49:15.416831] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:49:17.727752] Epoch: [69]  [ 0/42]  eta: 0:01:37  lr: 0.001925  loss: 3.6761 (3.6761)  time: 2.3097  data: 1.7943  max mem: 10046
[14:49:37.428596] Epoch: [69]  [41/42]  eta: 0:00:00  lr: 0.001818  loss: 3.5899 (3.5186)  time: 0.4829  data: 0.0001  max mem: 10046
[14:49:37.665223] Epoch: [69] Total time: 0:00:22 (0.5297 s / it)
[14:49:37.702530] Averaged stats: lr: 0.001818  loss: 3.5899 (3.5157)
[14:49:39.404301] Test:  [ 0/40]  eta: 0:01:07  loss: 3.2501 (3.2501)  acc1: 28.1250 (28.1250)  acc5: 50.0000 (50.0000)  time: 1.6961  data: 1.5217  max mem: 10046
[14:49:45.419603] Test:  [39/40]  eta: 0:00:00  loss: 3.3018 (3.3107)  acc1: 21.8750 (22.2000)  acc5: 50.0000 (49.4000)  time: 0.1508  data: 0.0001  max mem: 10046
[14:49:45.529758] Test: Total time: 0:00:07 (0.1956 s / it)
[14:49:45.862089] * Acc@1 21.440 Acc@5 49.170 loss 3.323
[14:49:45.862307] Accuracy of the network on the 10000 test images: 21.4%
[14:49:45.862547] [14:49:49.171294] Max accuracy: 21.44%
[14:49:49.171657] [14:49:49.172753] {"train_lr": 0.0018790238344032094, "train_loss": 3.515698282491593, "test_loss": 3.323208785057068, "test_acc1": 21.44, "test_acc5": 49.17, "epoch": 69, "n_parameters": 85958500}
[14:49:49.172850] [14:49:49.172941] Training epoch 69 for 0:00:33
[14:49:49.173017] [14:49:49.175814] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:49:51.047036] Epoch: [70]  [ 0/42]  eta: 0:01:18  lr: 0.001813  loss: 3.6896 (3.6896)  time: 1.8703  data: 1.3930  max mem: 10046
[14:50:10.573096] Epoch: [70]  [41/42]  eta: 0:00:00  lr: 0.001709  loss: 3.5003 (3.4309)  time: 0.4775  data: 0.0001  max mem: 10046
[14:50:10.800202] Epoch: [70] Total time: 0:00:21 (0.5149 s / it)
[14:50:10.825281] Averaged stats: lr: 0.001709  loss: 3.5003 (3.4667)
[14:50:12.285626] Test:  [ 0/40]  eta: 0:00:58  loss: 3.2737 (3.2737)  acc1: 28.1250 (28.1250)  acc5: 48.4375 (48.4375)  time: 1.4562  data: 1.2998  max mem: 10046
[14:50:18.291108] Test:  [39/40]  eta: 0:00:00  loss: 3.3237 (3.3187)  acc1: 20.3125 (21.8800)  acc5: 50.0000 (49.4800)  time: 0.1506  data: 0.0001  max mem: 10046
[14:50:18.402404] Test: Total time: 0:00:07 (0.1894 s / it)
[14:50:19.058289] * Acc@1 21.520 Acc@5 49.470 loss 3.325
[14:50:19.058496] Accuracy of the network on the 10000 test images: 21.5%
[14:50:19.058707] [14:50:22.372531] Max accuracy: 21.52%
[14:50:22.372851] [14:50:22.373770] {"train_lr": 0.0017680902765970672, "train_loss": 3.4666676748366583, "test_loss": 3.3253323912620543, "test_acc1": 21.52, "test_acc5": 49.47, "epoch": 70, "n_parameters": 85958500}
[14:50:22.373849] [14:50:22.373915] Training epoch 70 for 0:00:33
[14:50:22.373968] [14:50:22.376994] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:50:24.335905] Epoch: [71]  [ 0/42]  eta: 0:01:22  lr: 0.001703  loss: 3.3967 (3.3967)  time: 1.9580  data: 1.4883  max mem: 10046
[14:50:43.892254] Epoch: [71]  [41/42]  eta: 0:00:00  lr: 0.001602  loss: 3.4641 (3.4892)  time: 0.4788  data: 0.0001  max mem: 10046
[14:50:44.151980] Epoch: [71] Total time: 0:00:21 (0.5184 s / it)
[14:50:44.165242] Averaged stats: lr: 0.001602  loss: 3.4641 (3.5022)
[14:50:45.751624] Test:  [ 0/40]  eta: 0:01:03  loss: 3.2457 (3.2457)  acc1: 21.8750 (21.8750)  acc5: 46.8750 (46.8750)  time: 1.5826  data: 1.4258  max mem: 10046
[14:50:51.748727] Test:  [39/40]  eta: 0:00:00  loss: 3.3138 (3.3178)  acc1: 20.3125 (20.8400)  acc5: 51.5625 (49.5200)  time: 0.1501  data: 0.0001  max mem: 10046
[14:50:51.859194] Test: Total time: 0:00:07 (0.1923 s / it)
[14:50:52.299948] * Acc@1 21.020 Acc@5 49.000 loss 3.332
[14:50:52.300175] Accuracy of the network on the 10000 test images: 21.0%
[14:50:52.300393] [14:50:52.300461] Max accuracy: 21.52%
[14:50:52.300517] [14:50:52.301404] {"train_lr": 0.0016595978260475585, "train_loss": 3.5021956250781106, "test_loss": 3.332256680727005, "test_acc1": 21.02, "test_acc5": 49.0, "epoch": 71, "n_parameters": 85958500}
[14:50:52.301487] [14:50:52.301550] Training epoch 71 for 0:00:29
[14:50:52.301603] [14:50:52.304396] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:50:54.937074] Epoch: [72]  [ 0/42]  eta: 0:01:50  lr: 0.001596  loss: 2.9385 (2.9385)  time: 2.6315  data: 1.8097  max mem: 10046
[14:51:14.577077] Epoch: [72]  [41/42]  eta: 0:00:00  lr: 0.001497  loss: 3.5046 (3.4619)  time: 0.4811  data: 0.0001  max mem: 10046
[14:51:14.813545] Epoch: [72] Total time: 0:00:22 (0.5359 s / it)
[14:51:14.824653] Averaged stats: lr: 0.001497  loss: 3.5046 (3.4535)
[14:51:16.368938] Test:  [ 0/40]  eta: 0:01:01  loss: 3.2461 (3.2461)  acc1: 26.5625 (26.5625)  acc5: 48.4375 (48.4375)  time: 1.5266  data: 1.3694  max mem: 10046
[14:51:22.715945] Test:  [39/40]  eta: 0:00:00  loss: 3.2496 (3.2846)  acc1: 20.3125 (21.8400)  acc5: 51.5625 (49.9600)  time: 0.1509  data: 0.0001  max mem: 10046
[14:51:22.838008] Test: Total time: 0:00:08 (0.2003 s / it)
[14:51:22.960528] * Acc@1 21.930 Acc@5 49.680 loss 3.298
[14:51:22.960692] Accuracy of the network on the 10000 test images: 21.9%
[14:51:22.960884] [14:51:26.338277] Max accuracy: 21.93%
[14:51:26.338563] [14:51:26.339430] {"train_lr": 0.0015536651176564875, "train_loss": 3.453451443286169, "test_loss": 3.297909903526306, "test_acc1": 21.93, "test_acc5": 49.68, "epoch": 72, "n_parameters": 85958500}
[14:51:26.339507] [14:51:26.339570] Training epoch 72 for 0:00:34
[14:51:26.339620] [14:51:26.342406] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:51:28.343538] Epoch: [73]  [ 0/42]  eta: 0:01:24  lr: 0.001492  loss: 3.0588 (3.0588)  time: 2.0002  data: 1.5175  max mem: 10046
[14:51:47.876168] Epoch: [73]  [41/42]  eta: 0:00:00  lr: 0.001395  loss: 3.6066 (3.5080)  time: 0.4782  data: 0.0001  max mem: 10046
[14:51:48.119419] Epoch: [73] Total time: 0:00:21 (0.5185 s / it)
[14:51:48.125698] Averaged stats: lr: 0.001395  loss: 3.6066 (3.4885)
[14:51:49.563762] Test:  [ 0/40]  eta: 0:00:57  loss: 3.2160 (3.2160)  acc1: 26.5625 (26.5625)  acc5: 46.8750 (46.8750)  time: 1.4341  data: 1.2773  max mem: 10046
[14:51:55.584926] Test:  [39/40]  eta: 0:00:00  loss: 3.2489 (3.2681)  acc1: 23.4375 (22.9600)  acc5: 51.5625 (50.2800)  time: 0.1504  data: 0.0001  max mem: 10046
[14:51:55.705564] Test: Total time: 0:00:07 (0.1894 s / it)
[14:51:56.345004] * Acc@1 22.440 Acc@5 49.820 loss 3.286
[14:51:56.345208] Accuracy of the network on the 10000 test images: 22.4%
[14:51:56.345432] [14:51:59.697188] Max accuracy: 22.44%
[14:51:59.697465] [14:51:59.698360] {"train_lr": 0.0014504079872851914, "train_loss": 3.488538314898809, "test_loss": 3.2861439108848574, "test_acc1": 22.44, "test_acc5": 49.82, "epoch": 73, "n_parameters": 85958500}
[14:51:59.698438] [14:51:59.698503] Training epoch 73 for 0:00:33
[14:51:59.698555] [14:51:59.701362] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:52:01.553300] Epoch: [74]  [ 0/42]  eta: 0:01:17  lr: 0.001390  loss: 3.3151 (3.3151)  time: 1.8509  data: 1.3657  max mem: 10046
[14:52:21.108365] Epoch: [74]  [41/42]  eta: 0:00:00  lr: 0.001296  loss: 3.4121 (3.4698)  time: 0.4788  data: 0.0001  max mem: 10046
[14:52:21.348824] Epoch: [74] Total time: 0:00:21 (0.5154 s / it)
[14:52:21.352169] Averaged stats: lr: 0.001296  loss: 3.4121 (3.4619)
[14:52:23.318636] Test:  [ 0/40]  eta: 0:01:18  loss: 3.2218 (3.2218)  acc1: 28.1250 (28.1250)  acc5: 50.0000 (50.0000)  time: 1.9632  data: 1.7890  max mem: 10046
[14:52:29.300899] Test:  [39/40]  eta: 0:00:00  loss: 3.2301 (3.2678)  acc1: 21.8750 (23.0000)  acc5: 51.5625 (50.4000)  time: 0.1506  data: 0.0001  max mem: 10046
[14:52:29.412282] Test: Total time: 0:00:08 (0.2014 s / it)
[14:52:29.593647] * Acc@1 22.400 Acc@5 50.280 loss 3.281
[14:52:29.593816] Accuracy of the network on the 10000 test images: 22.4%
[14:52:29.593993] [14:52:29.594060] Max accuracy: 22.44%
[14:52:29.594115] [14:52:29.594936] {"train_lr": 0.0013499393450897242, "train_loss": 3.4619149409589314, "test_loss": 3.28144548535347, "test_acc1": 22.4, "test_acc5": 50.28, "epoch": 74, "n_parameters": 85958500}
[14:52:29.595015] [14:52:29.595076] Training epoch 74 for 0:00:29
[14:52:29.595128] [14:52:29.597840] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:52:32.128767] Epoch: [75]  [ 0/42]  eta: 0:01:46  lr: 0.001292  loss: 2.9903 (2.9903)  time: 2.5297  data: 1.5667  max mem: 10046
[14:52:51.779082] Epoch: [75]  [41/42]  eta: 0:00:00  lr: 0.001200  loss: 3.4645 (3.4221)  time: 0.4820  data: 0.0001  max mem: 10046
[14:52:52.017982] Epoch: [75] Total time: 0:00:22 (0.5338 s / it)
[14:52:52.018690] Averaged stats: lr: 0.001200  loss: 3.4645 (3.4443)
[14:52:53.752336] Test:  [ 0/40]  eta: 0:01:09  loss: 3.2324 (3.2324)  acc1: 31.2500 (31.2500)  acc5: 48.4375 (48.4375)  time: 1.7302  data: 1.5557  max mem: 10046
[14:52:59.774558] Test:  [39/40]  eta: 0:00:00  loss: 3.2226 (3.2747)  acc1: 21.8750 (23.2400)  acc5: 51.5625 (50.1200)  time: 0.1512  data: 0.0001  max mem: 10046
[14:52:59.890979] Test: Total time: 0:00:07 (0.1967 s / it)
[14:53:00.269387] * Acc@1 22.400 Acc@5 49.860 loss 3.290
[14:53:00.269624] Accuracy of the network on the 10000 test images: 22.4%
[14:53:00.269876] [14:53:00.269951] Max accuracy: 22.44%
[14:53:00.270006] [14:53:00.271068] {"train_lr": 0.001252369052055264, "train_loss": 3.4443405199618566, "test_loss": 3.289998298883438, "test_acc1": 22.4, "test_acc5": 49.86, "epoch": 75, "n_parameters": 85958500}
[14:53:00.271139] [14:53:00.271196] Training epoch 75 for 0:00:30
[14:53:00.271245] [14:53:00.274097] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:53:02.632004] Epoch: [76]  [ 0/42]  eta: 0:01:38  lr: 0.001196  loss: 3.3857 (3.3857)  time: 2.3570  data: 1.8000  max mem: 10046
[14:53:22.222464] Epoch: [76]  [41/42]  eta: 0:00:00  lr: 0.001108  loss: 3.4620 (3.5526)  time: 0.4789  data: 0.0001  max mem: 10046
[14:53:22.431364] Epoch: [76] Total time: 0:00:22 (0.5276 s / it)
[14:53:22.454355] Averaged stats: lr: 0.001108  loss: 3.4620 (3.5322)
[14:53:23.875676] Test:  [ 0/40]  eta: 0:00:56  loss: 3.1955 (3.1955)  acc1: 28.1250 (28.1250)  acc5: 51.5625 (51.5625)  time: 1.4156  data: 1.2583  max mem: 10046
[14:53:29.895280] Test:  [39/40]  eta: 0:00:00  loss: 3.2654 (3.2775)  acc1: 23.4375 (23.0400)  acc5: 50.0000 (49.1600)  time: 0.1511  data: 0.0001  max mem: 10046
[14:53:30.009281] Test: Total time: 0:00:07 (0.1888 s / it)
[14:53:30.640790] * Acc@1 22.030 Acc@5 49.460 loss 3.296
[14:53:30.640980] Accuracy of the network on the 10000 test images: 22.0%
[14:53:30.641229] [14:53:30.641297] Max accuracy: 22.44%
[14:53:30.641359] [14:53:30.642175] {"train_lr": 0.001157803799864728, "train_loss": 3.532158533732096, "test_loss": 3.2957287281751633, "test_acc1": 22.03, "test_acc5": 49.46, "epoch": 76, "n_parameters": 85958500}
[14:53:30.642243] [14:53:30.642300] Training epoch 76 for 0:00:30
[14:53:30.642350] [14:53:30.645175] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:53:33.114077] Epoch: [77]  [ 0/42]  eta: 0:01:43  lr: 0.001103  loss: 3.6370 (3.6370)  time: 2.4681  data: 1.6982  max mem: 10046
[14:53:52.702582] Epoch: [77]  [41/42]  eta: 0:00:00  lr: 0.001018  loss: 3.4670 (3.4785)  time: 0.4794  data: 0.0001  max mem: 10046
[14:53:52.931505] Epoch: [77] Total time: 0:00:22 (0.5306 s / it)
[14:53:52.959772] Averaged stats: lr: 0.001018  loss: 3.4670 (3.4919)
[14:53:54.474290] Test:  [ 0/40]  eta: 0:01:00  loss: 3.2449 (3.2449)  acc1: 26.5625 (26.5625)  acc5: 48.4375 (48.4375)  time: 1.5102  data: 1.3529  max mem: 10046
[14:54:00.649846] Test:  [39/40]  eta: 0:00:00  loss: 3.2656 (3.2866)  acc1: 21.8750 (22.8400)  acc5: 51.5625 (50.0400)  time: 0.1512  data: 0.0001  max mem: 10046
[14:54:00.758519] Test: Total time: 0:00:07 (0.1949 s / it)
[14:54:00.960845] * Acc@1 22.300 Acc@5 49.750 loss 3.301
[14:54:00.961063] Accuracy of the network on the 10000 test images: 22.3%
[14:54:00.961304] [14:54:00.961377] Max accuracy: 22.44%
[14:54:00.961437] [14:54:00.962274] {"train_lr": 0.0010663469942329945, "train_loss": 3.49185696102324, "test_loss": 3.3012252867221834, "test_acc1": 22.3, "test_acc5": 49.75, "epoch": 77, "n_parameters": 85958500}
[14:54:00.962343] [14:54:00.962413] Training epoch 77 for 0:00:30
[14:54:00.962467] [14:54:00.965338] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:54:03.276001] Epoch: [78]  [ 0/42]  eta: 0:01:36  lr: 0.001014  loss: 3.6276 (3.6276)  time: 2.3095  data: 1.6240  max mem: 10046
[14:54:22.910639] Epoch: [78]  [41/42]  eta: 0:00:00  lr: 0.000931  loss: 3.5684 (3.5127)  time: 0.4807  data: 0.0001  max mem: 10046
[14:54:23.142273] Epoch: [78] Total time: 0:00:22 (0.5280 s / it)
[14:54:23.161991] Averaged stats: lr: 0.000931  loss: 3.5684 (3.4886)
[14:54:24.771203] Test:  [ 0/40]  eta: 0:01:04  loss: 3.2297 (3.2297)  acc1: 26.5625 (26.5625)  acc5: 51.5625 (51.5625)  time: 1.6050  data: 1.4478  max mem: 10046
[14:54:30.796795] Test:  [39/40]  eta: 0:00:00  loss: 3.2607 (3.2729)  acc1: 23.4375 (22.4000)  acc5: 50.0000 (49.9600)  time: 0.1515  data: 0.0001  max mem: 10046
[14:54:30.911376] Test: Total time: 0:00:07 (0.1937 s / it)
[14:54:31.461872] * Acc@1 22.060 Acc@5 50.120 loss 3.292
[14:54:31.462178] Accuracy of the network on the 10000 test images: 22.1%
[14:54:31.462434] [14:54:31.462513] Max accuracy: 22.44%
[14:54:31.462571] [14:54:31.463607] {"train_lr": 0.0009780986418342695, "train_loss": 3.4886092245578766, "test_loss": 3.2917401790618896, "test_acc1": 22.06, "test_acc5": 50.12, "epoch": 78, "n_parameters": 85958500}
[14:54:31.463675] [14:54:31.463736] Training epoch 78 for 0:00:30
[14:54:31.463788] [14:54:31.466862] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:54:34.022187] Epoch: [79]  [ 0/42]  eta: 0:01:47  lr: 0.000927  loss: 3.5484 (3.5484)  time: 2.5541  data: 1.8675  max mem: 10046
[14:54:53.634204] Epoch: [79]  [41/42]  eta: 0:00:00  lr: 0.000848  loss: 3.4655 (3.3974)  time: 0.4795  data: 0.0001  max mem: 10046
[14:54:53.870338] Epoch: [79] Total time: 0:00:22 (0.5334 s / it)
[14:54:53.900774] Averaged stats: lr: 0.000848  loss: 3.4655 (3.4248)
[14:54:55.814065] Test:  [ 0/40]  eta: 0:01:16  loss: 3.1553 (3.1553)  acc1: 31.2500 (31.2500)  acc5: 53.1250 (53.1250)  time: 1.9100  data: 1.7398  max mem: 10046
[14:55:01.827963] Test:  [39/40]  eta: 0:00:00  loss: 3.2404 (3.2648)  acc1: 21.8750 (22.5200)  acc5: 50.0000 (50.3600)  time: 0.1512  data: 0.0001  max mem: 10046
[14:55:01.942309] Test: Total time: 0:00:08 (0.2010 s / it)
[14:55:02.150017] * Acc@1 22.240 Acc@5 50.200 loss 3.278
[14:55:02.150200] Accuracy of the network on the 10000 test images: 22.2%
[14:55:02.150447] [14:55:02.150518] Max accuracy: 22.44%
[14:55:02.150576] [14:55:02.151387] {"train_lr": 0.0008931552409462468, "train_loss": 3.424840122461319, "test_loss": 3.277746832370758, "test_acc1": 22.24, "test_acc5": 50.2, "epoch": 79, "n_parameters": 85958500}
[14:55:02.151458] [14:55:02.151515] Training epoch 79 for 0:00:30
[14:55:02.151568] [14:55:02.154400] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:55:04.585953] Epoch: [80]  [ 0/42]  eta: 0:01:42  lr: 0.000844  loss: 3.7747 (3.7747)  time: 2.4304  data: 1.9553  max mem: 10046
[14:55:24.210465] Epoch: [80]  [41/42]  eta: 0:00:00  lr: 0.000769  loss: 3.4669 (3.4624)  time: 0.4796  data: 0.0001  max mem: 10046
[14:55:24.430759] Epoch: [80] Total time: 0:00:22 (0.5304 s / it)
[14:55:24.450287] Averaged stats: lr: 0.000769  loss: 3.4669 (3.4733)
[14:55:26.153008] Test:  [ 0/40]  eta: 0:01:07  loss: 3.1871 (3.1871)  acc1: 26.5625 (26.5625)  acc5: 53.1250 (53.1250)  time: 1.6982  data: 1.5242  max mem: 10046
[14:55:32.186399] Test:  [39/40]  eta: 0:00:00  loss: 3.2241 (3.2488)  acc1: 21.8750 (22.8000)  acc5: 51.5625 (50.8400)  time: 0.1511  data: 0.0001  max mem: 10046
[14:55:32.321972] Test: Total time: 0:00:07 (0.1967 s / it)
[14:55:32.661041] * Acc@1 22.440 Acc@5 50.630 loss 3.267
[14:55:32.661258] Accuracy of the network on the 10000 test images: 22.4%
[14:55:32.661485] [14:55:32.661554] Max accuracy: 22.44%
[14:55:32.661610] [14:55:32.662497] {"train_lr": 0.0008116096759306745, "train_loss": 3.473273659036273, "test_loss": 3.2669679790735247, "test_acc1": 22.44, "test_acc5": 50.63, "epoch": 80, "n_parameters": 85958500}
[14:55:32.662577] [14:55:32.662638] Training epoch 80 for 0:00:30
[14:55:32.662691] [14:55:32.665480] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:55:35.170149] Epoch: [81]  [ 0/42]  eta: 0:01:45  lr: 0.000765  loss: 3.7315 (3.7315)  time: 2.5036  data: 1.6679  max mem: 10046
[14:55:54.830636] Epoch: [81]  [41/42]  eta: 0:00:00  lr: 0.000692  loss: 3.4433 (3.4506)  time: 0.4813  data: 0.0001  max mem: 10046
[14:55:55.061128] Epoch: [81] Total time: 0:00:22 (0.5332 s / it)
[14:55:55.066561] Averaged stats: lr: 0.000692  loss: 3.4433 (3.4670)
[14:55:56.898689] Test:  [ 0/40]  eta: 0:01:13  loss: 3.1863 (3.1863)  acc1: 28.1250 (28.1250)  acc5: 54.6875 (54.6875)  time: 1.8285  data: 1.6549  max mem: 10046
[14:56:02.929275] Test:  [39/40]  eta: 0:00:00  loss: 3.2189 (3.2606)  acc1: 21.8750 (22.6400)  acc5: 51.5625 (50.9600)  time: 0.1515  data: 0.0001  max mem: 10046
[14:56:03.039989] Test: Total time: 0:00:07 (0.1993 s / it)
[14:56:03.381399] * Acc@1 22.320 Acc@5 50.410 loss 3.276
[14:56:03.381609] Accuracy of the network on the 10000 test images: 22.3%
[14:56:03.381820] [14:56:03.381893] Max accuracy: 22.44%
[14:56:03.381947] [14:56:03.382860] {"train_lr": 0.000733551115665655, "train_loss": 3.467044146288009, "test_loss": 3.276352173089981, "test_acc1": 22.32, "test_acc5": 50.41, "epoch": 81, "n_parameters": 85958500}
[14:56:03.382954] [14:56:03.383015] Training epoch 81 for 0:00:30
[14:56:03.383066] [14:56:03.385896] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:56:05.710229] Epoch: [82]  [ 0/42]  eta: 0:01:37  lr: 0.000689  loss: 3.5606 (3.5606)  time: 2.3235  data: 1.7320  max mem: 10046
[14:56:25.323096] Epoch: [82]  [41/42]  eta: 0:00:00  lr: 0.000620  loss: 3.3345 (3.4243)  time: 0.4801  data: 0.0001  max mem: 10046
[14:56:25.550330] Epoch: [82] Total time: 0:00:22 (0.5277 s / it)
[14:56:25.554358] Averaged stats: lr: 0.000620  loss: 3.3345 (3.4367)
[14:56:27.313634] Test:  [ 0/40]  eta: 0:01:10  loss: 3.1589 (3.1589)  acc1: 28.1250 (28.1250)  acc5: 56.2500 (56.2500)  time: 1.7554  data: 1.5711  max mem: 10046
[14:56:33.337651] Test:  [39/40]  eta: 0:00:00  loss: 3.2177 (3.2487)  acc1: 21.8750 (22.7200)  acc5: 53.1250 (51.3200)  time: 0.1514  data: 0.0001  max mem: 10046
[14:56:33.455417] Test: Total time: 0:00:07 (0.1975 s / it)
[14:56:33.730857] * Acc@1 22.590 Acc@5 50.730 loss 3.265
[14:56:33.731033] Accuracy of the network on the 10000 test images: 22.6%
[14:56:33.731294] [14:56:37.169753] Max accuracy: 22.59%
[14:56:37.170100] [14:56:37.171075] {"train_lr": 0.0006590649160407968, "train_loss": 3.436716364962714, "test_loss": 3.2648795753717423, "test_acc1": 22.59, "test_acc5": 50.73, "epoch": 82, "n_parameters": 85958500}
[14:56:37.171154] [14:56:37.171226] Training epoch 82 for 0:00:33
[14:56:37.171279] [14:56:37.174289] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:56:39.181255] Epoch: [83]  [ 0/42]  eta: 0:01:24  lr: 0.000617  loss: 3.7998 (3.7998)  time: 2.0061  data: 1.5347  max mem: 10046
[14:56:58.712652] Epoch: [83]  [41/42]  eta: 0:00:00  lr: 0.000551  loss: 3.3586 (3.3999)  time: 0.4779  data: 0.0001  max mem: 10046
[14:56:58.937371] Epoch: [83] Total time: 0:00:21 (0.5182 s / it)
[14:56:58.978712] Averaged stats: lr: 0.000551  loss: 3.3586 (3.4552)
[14:57:00.842624] Test:  [ 0/40]  eta: 0:01:14  loss: 3.1965 (3.1965)  acc1: 29.6875 (29.6875)  acc5: 51.5625 (51.5625)  time: 1.8596  data: 1.6892  max mem: 10046
[14:57:06.891320] Test:  [39/40]  eta: 0:00:00  loss: 3.1980 (3.2409)  acc1: 21.8750 (22.5600)  acc5: 53.1250 (50.6000)  time: 0.1505  data: 0.0001  max mem: 10046
[14:57:07.021976] Test: Total time: 0:00:08 (0.2010 s / it)
[14:57:07.164354] * Acc@1 22.440 Acc@5 50.760 loss 3.257
[14:57:07.164585] Accuracy of the network on the 10000 test images: 22.4%
[14:57:07.164786] [14:57:07.164854] Max accuracy: 22.59%
[14:57:07.164910] [14:57:07.165752] {"train_lr": 0.0005882325266217914, "train_loss": 3.4551738528978255, "test_loss": 3.256823146343231, "test_acc1": 22.44, "test_acc5": 50.76, "epoch": 83, "n_parameters": 85958500}
[14:57:07.165820] [14:57:07.165878] Training epoch 83 for 0:00:29
[14:57:07.165929] [14:57:07.168748] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:57:09.463430] Epoch: [84]  [ 0/42]  eta: 0:01:36  lr: 0.000548  loss: 3.1815 (3.1815)  time: 2.2939  data: 1.5176  max mem: 10046
[14:57:29.118416] Epoch: [84]  [41/42]  eta: 0:00:00  lr: 0.000486  loss: 3.6477 (3.4802)  time: 0.4820  data: 0.0001  max mem: 10046
[14:57:29.364581] Epoch: [84] Total time: 0:00:22 (0.5285 s / it)
[14:57:29.374893] Averaged stats: lr: 0.000486  loss: 3.6477 (3.4568)
[14:57:30.943426] Test:  [ 0/40]  eta: 0:01:02  loss: 3.1697 (3.1697)  acc1: 26.5625 (26.5625)  acc5: 53.1250 (53.1250)  time: 1.5643  data: 1.4072  max mem: 10046
[14:57:37.160857] Test:  [39/40]  eta: 0:00:00  loss: 3.1889 (3.2369)  acc1: 21.8750 (22.6400)  acc5: 53.1250 (50.8000)  time: 0.1508  data: 0.0001  max mem: 10046
[14:57:37.278810] Test: Total time: 0:00:07 (0.1975 s / it)
[14:57:37.458428] * Acc@1 22.750 Acc@5 50.670 loss 3.253
[14:57:37.458658] Accuracy of the network on the 10000 test images: 22.8%
[14:57:37.458900] [14:57:40.889037] Max accuracy: 22.75%
[14:57:40.889296] [14:57:40.890227] {"train_lr": 0.0005211314015865183, "train_loss": 3.4568178298927488, "test_loss": 3.252597576379776, "test_acc1": 22.75, "test_acc5": 50.67, "epoch": 84, "n_parameters": 85958500}
[14:57:40.890346] [14:57:40.890409] Training epoch 84 for 0:00:33
[14:57:40.890462] [14:57:40.893275] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:57:42.824512] Epoch: [85]  [ 0/42]  eta: 0:01:21  lr: 0.000483  loss: 3.4136 (3.4136)  time: 1.9304  data: 1.4556  max mem: 10046
[14:58:02.362256] Epoch: [85]  [41/42]  eta: 0:00:00  lr: 0.000425  loss: 3.5369 (3.5138)  time: 0.4784  data: 0.0001  max mem: 10046
[14:58:02.608332] Epoch: [85] Total time: 0:00:21 (0.5170 s / it)
[14:58:02.611387] Averaged stats: lr: 0.000425  loss: 3.5369 (3.4935)
[14:58:04.457598] Test:  [ 0/40]  eta: 0:01:13  loss: 3.1741 (3.1741)  acc1: 26.5625 (26.5625)  acc5: 54.6875 (54.6875)  time: 1.8425  data: 1.6614  max mem: 10046
[14:58:10.465552] Test:  [39/40]  eta: 0:00:00  loss: 3.1843 (3.2453)  acc1: 21.8750 (22.6000)  acc5: 53.1250 (51.0000)  time: 0.1506  data: 0.0001  max mem: 10046
[14:58:10.612358] Test: Total time: 0:00:07 (0.2000 s / it)
[14:58:10.742694] * Acc@1 22.570 Acc@5 50.800 loss 3.260
[14:58:10.742894] Accuracy of the network on the 10000 test images: 22.6%
[14:58:10.743086] [14:58:10.743160] Max accuracy: 22.75%
[14:58:10.743216] [14:58:10.744011] {"train_lr": 0.00045783491503003623, "train_loss": 3.493493290174575, "test_loss": 3.260312396287918, "test_acc1": 22.57, "test_acc5": 50.8, "epoch": 85, "n_parameters": 85958500}
[14:58:10.744153] [14:58:10.744218] Training epoch 85 for 0:00:29
[14:58:10.744272] [14:58:10.747086] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:58:13.095993] Epoch: [86]  [ 0/42]  eta: 0:01:38  lr: 0.000422  loss: 3.5267 (3.5267)  time: 2.3474  data: 1.7837  max mem: 10046
[14:58:32.742278] Epoch: [86]  [41/42]  eta: 0:00:00  lr: 0.000368  loss: 3.5169 (3.5283)  time: 0.4814  data: 0.0001  max mem: 10046
[14:58:32.993254] Epoch: [86] Total time: 0:00:22 (0.5297 s / it)
[14:58:32.996169] Averaged stats: lr: 0.000368  loss: 3.5169 (3.5207)
[14:58:34.431868] Test:  [ 0/40]  eta: 0:00:57  loss: 3.1505 (3.1505)  acc1: 28.1250 (28.1250)  acc5: 51.5625 (51.5625)  time: 1.4318  data: 1.2743  max mem: 10046
[14:58:40.666790] Test:  [39/40]  eta: 0:00:00  loss: 3.2194 (3.2547)  acc1: 21.8750 (22.8400)  acc5: 51.5625 (50.8800)  time: 0.1509  data: 0.0001  max mem: 10046
[14:58:40.784915] Test: Total time: 0:00:07 (0.1947 s / it)
[14:58:41.041552] * Acc@1 22.300 Acc@5 50.810 loss 3.270
[14:58:41.041761] Accuracy of the network on the 10000 test images: 22.3%
[14:58:41.041980] [14:58:41.042050] Max accuracy: 22.75%
[14:58:41.042104] [14:58:41.042910] {"train_lr": 0.0003984122807310847, "train_loss": 3.5206972459952035, "test_loss": 3.269675225019455, "test_acc1": 22.3, "test_acc5": 50.81, "epoch": 86, "n_parameters": 85958500}
[14:58:41.042979] [14:58:41.043035] Training epoch 86 for 0:00:30
[14:58:41.043086] [14:58:41.045858] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:58:43.700723] Epoch: [87]  [ 0/42]  eta: 0:01:51  lr: 0.000365  loss: 3.2899 (3.2899)  time: 2.6540  data: 1.7023  max mem: 10046
[14:59:03.311789] Epoch: [87]  [41/42]  eta: 0:00:00  lr: 0.000314  loss: 3.4952 (3.4762)  time: 0.4799  data: 0.0001  max mem: 10046
[14:59:03.554839] Epoch: [87] Total time: 0:00:22 (0.5359 s / it)
[14:59:03.555560] Averaged stats: lr: 0.000314  loss: 3.4952 (3.4718)
[14:59:05.375025] Test:  [ 0/40]  eta: 0:01:12  loss: 3.1693 (3.1693)  acc1: 25.0000 (25.0000)  acc5: 54.6875 (54.6875)  time: 1.8148  data: 1.6365  max mem: 10046
[14:59:11.382005] Test:  [39/40]  eta: 0:00:00  loss: 3.2207 (3.2530)  acc1: 23.4375 (23.4400)  acc5: 53.1250 (51.4400)  time: 0.1509  data: 0.0001  max mem: 10046
[14:59:11.495964] Test: Total time: 0:00:07 (0.1984 s / it)
[14:59:11.785799] * Acc@1 22.880 Acc@5 50.820 loss 3.270
[14:59:11.785998] Accuracy of the network on the 10000 test images: 22.9%
[14:59:11.786202] [14:59:15.129247] Max accuracy: 22.88%
[14:59:15.129518] [14:59:15.130409] {"train_lr": 0.0003429284764678335, "train_loss": 3.4717808621270314, "test_loss": 3.269803148508072, "test_acc1": 22.88, "test_acc5": 50.82, "epoch": 87, "n_parameters": 85958500}
[14:59:15.130487] [14:59:15.130553] Training epoch 87 for 0:00:34
[14:59:15.130652] [14:59:15.133465] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:59:16.936052] Epoch: [88]  [ 0/42]  eta: 0:01:15  lr: 0.000312  loss: 3.1719 (3.1719)  time: 1.8016  data: 1.3214  max mem: 10046
[14:59:36.460983] Epoch: [88]  [41/42]  eta: 0:00:00  lr: 0.000265  loss: 3.4952 (3.5115)  time: 0.4780  data: 0.0001  max mem: 10046
[14:59:36.700489] Epoch: [88] Total time: 0:00:21 (0.5135 s / it)
[14:59:36.701238] Averaged stats: lr: 0.000265  loss: 3.4952 (3.4827)
[14:59:38.533883] Test:  [ 0/40]  eta: 0:01:13  loss: 3.1751 (3.1751)  acc1: 26.5625 (26.5625)  acc5: 50.0000 (50.0000)  time: 1.8293  data: 1.6577  max mem: 10046
[14:59:44.546150] Test:  [39/40]  eta: 0:00:00  loss: 3.1710 (3.2435)  acc1: 21.8750 (23.3200)  acc5: 54.6875 (51.0000)  time: 0.1510  data: 0.0001  max mem: 10046
[14:59:44.658494] Test: Total time: 0:00:07 (0.1989 s / it)
[14:59:44.897849] * Acc@1 22.690 Acc@5 50.750 loss 3.263
[14:59:44.898063] Accuracy of the network on the 10000 test images: 22.7%
[14:59:44.898288] [14:59:44.898359] Max accuracy: 22.88%
[14:59:44.898416] [14:59:44.899465] {"train_lr": 0.0002914441729656418, "train_loss": 3.48267299646423, "test_loss": 3.2627032160758973, "test_acc1": 22.69, "test_acc5": 50.75, "epoch": 88, "n_parameters": 85958500}
[14:59:44.899541] [14:59:44.899601] Training epoch 88 for 0:00:29
[14:59:44.899653] [14:59:44.902470] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[14:59:47.258595] Epoch: [89]  [ 0/42]  eta: 0:01:38  lr: 0.000263  loss: 3.5109 (3.5109)  time: 2.3551  data: 1.7961  max mem: 10046
[15:00:06.898757] Epoch: [89]  [41/42]  eta: 0:00:00  lr: 0.000220  loss: 3.5388 (3.4782)  time: 0.4813  data: 0.0001  max mem: 10046
[15:00:07.146248] Epoch: [89] Total time: 0:00:22 (0.5296 s / it)
[15:00:07.157520] Averaged stats: lr: 0.000220  loss: 3.5388 (3.4788)
[15:00:08.601519] Test:  [ 0/40]  eta: 0:00:57  loss: 3.1719 (3.1719)  acc1: 29.6875 (29.6875)  acc5: 53.1250 (53.1250)  time: 1.4398  data: 1.2828  max mem: 10046
[15:00:14.744209] Test:  [39/40]  eta: 0:00:00  loss: 3.1891 (3.2432)  acc1: 21.8750 (23.0400)  acc5: 54.6875 (51.5600)  time: 0.1515  data: 0.0001  max mem: 10046
[15:00:14.852310] Test: Total time: 0:00:07 (0.1923 s / it)
[15:00:15.216334] * Acc@1 22.600 Acc@5 50.740 loss 3.264
[15:00:15.216608] Accuracy of the network on the 10000 test images: 22.6%
[15:00:15.216890] [15:00:15.217027] Max accuracy: 22.88%
[15:00:15.217112] [15:00:15.218115] {"train_lr": 0.0002440156675545032, "train_loss": 3.4788002215680622, "test_loss": 3.263606312870979, "test_acc1": 22.6, "test_acc5": 50.74, "epoch": 89, "n_parameters": 85958500}
[15:00:15.218235] [15:00:15.218334] Training epoch 89 for 0:00:30
[15:00:15.218418] [15:00:15.223291] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:00:17.871202] Epoch: [90]  [ 0/42]  eta: 0:01:51  lr: 0.000218  loss: 3.5458 (3.5458)  time: 2.6469  data: 1.7329  max mem: 10046
[15:00:37.459522] Epoch: [90]  [41/42]  eta: 0:00:00  lr: 0.000179  loss: 3.4786 (3.4867)  time: 0.4792  data: 0.0001  max mem: 10046
[15:00:37.679099] Epoch: [90] Total time: 0:00:22 (0.5347 s / it)
[15:00:37.693710] Averaged stats: lr: 0.000179  loss: 3.4786 (3.4684)
[15:00:39.059636] Test:  [ 0/40]  eta: 0:00:54  loss: 3.1791 (3.1791)  acc1: 29.6875 (29.6875)  acc5: 51.5625 (51.5625)  time: 1.3618  data: 1.2037  max mem: 10046
[15:00:45.116409] Test:  [39/40]  eta: 0:00:00  loss: 3.1705 (3.2431)  acc1: 21.8750 (23.2800)  acc5: 51.5625 (50.9600)  time: 0.1514  data: 0.0001  max mem: 10046
[15:00:45.230808] Test: Total time: 0:00:07 (0.1884 s / it)
[15:00:45.807096] * Acc@1 22.690 Acc@5 50.490 loss 3.260
[15:00:45.807306] Accuracy of the network on the 10000 test images: 22.7%
[15:00:45.807534] [15:00:45.807606] Max accuracy: 22.88%
[15:00:45.807661] [15:00:45.808463] {"train_lr": 0.00020069482260874676, "train_loss": 3.4683709187167033, "test_loss": 3.260281631350517, "test_acc1": 22.69, "test_acc5": 50.49, "epoch": 90, "n_parameters": 85958500}
[15:00:45.808536] [15:00:45.808594] Training epoch 90 for 0:00:30
[15:00:45.808645] [15:00:45.811304] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:00:48.259494] Epoch: [91]  [ 0/42]  eta: 0:01:42  lr: 0.000177  loss: 3.6371 (3.6371)  time: 2.4471  data: 1.6801  max mem: 10046
[15:01:07.890372] Epoch: [91]  [41/42]  eta: 0:00:00  lr: 0.000142  loss: 3.5154 (3.4538)  time: 0.4795  data: 0.0001  max mem: 10046
[15:01:08.089732] Epoch: [91] Total time: 0:00:22 (0.5304 s / it)
[15:01:08.137501] Averaged stats: lr: 0.000142  loss: 3.5154 (3.4538)
[15:01:09.630005] Test:  [ 0/40]  eta: 0:00:59  loss: 3.1554 (3.1554)  acc1: 28.1250 (28.1250)  acc5: 50.0000 (50.0000)  time: 1.4882  data: 1.2996  max mem: 10046
[15:01:15.759411] Test:  [39/40]  eta: 0:00:00  loss: 3.2209 (3.2420)  acc1: 23.4375 (23.0800)  acc5: 53.1250 (51.0000)  time: 0.1514  data: 0.0001  max mem: 10046
[15:01:15.877321] Test: Total time: 0:00:07 (0.1934 s / it)
[15:01:16.242639] * Acc@1 22.980 Acc@5 50.300 loss 3.258
[15:01:16.242815] Accuracy of the network on the 10000 test images: 23.0%
[15:01:16.242976] [15:01:19.669387] Max accuracy: 22.98%
[15:01:19.669750] [15:01:19.670767] {"train_lr": 0.00016152900883629017, "train_loss": 3.4537954983257113, "test_loss": 3.2580694377422335, "test_acc1": 22.98, "test_acc5": 50.3, "epoch": 91, "n_parameters": 85958500}
[15:01:19.670864] [15:01:19.670945] Training epoch 91 for 0:00:33
[15:01:19.671016] [15:01:19.675186] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:01:21.534522] Epoch: [92]  [ 0/42]  eta: 0:01:18  lr: 0.000140  loss: 3.3840 (3.3840)  time: 1.8583  data: 1.3756  max mem: 10046
[15:01:41.138885] Epoch: [92]  [41/42]  eta: 0:00:00  lr: 0.000109  loss: 3.3975 (3.4414)  time: 0.4801  data: 0.0001  max mem: 10046
[15:01:41.361506] Epoch: [92] Total time: 0:00:21 (0.5163 s / it)
[15:01:41.379669] Averaged stats: lr: 0.000109  loss: 3.3975 (3.4071)
[15:01:43.296146] Test:  [ 0/40]  eta: 0:01:16  loss: 3.1471 (3.1471)  acc1: 29.6875 (29.6875)  acc5: 51.5625 (51.5625)  time: 1.9116  data: 1.7417  max mem: 10046
[15:01:49.283053] Test:  [39/40]  eta: 0:00:00  loss: 3.1820 (3.2419)  acc1: 21.8750 (23.1600)  acc5: 53.1250 (51.0800)  time: 0.1506  data: 0.0001  max mem: 10046
[15:01:49.428219] Test: Total time: 0:00:08 (0.2011 s / it)
[15:01:49.513916] * Acc@1 22.780 Acc@5 50.700 loss 3.256
[15:01:49.514120] Accuracy of the network on the 10000 test images: 22.8%
[15:01:49.514317] [15:01:49.514393] Max accuracy: 22.98%
[15:01:49.514449] [15:01:49.515307] {"train_lr": 0.0001265610534794671, "train_loss": 3.4070593870821453, "test_loss": 3.256057557463646, "test_acc1": 22.78, "test_acc5": 50.7, "epoch": 92, "n_parameters": 85958500}
[15:01:49.515382] [15:01:49.515442] Training epoch 92 for 0:00:29
[15:01:49.515496] [15:01:49.518390] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:01:51.933999] Epoch: [93]  [ 0/42]  eta: 0:01:41  lr: 0.000108  loss: 3.6302 (3.6302)  time: 2.4146  data: 1.5916  max mem: 10046
[15:02:11.579927] Epoch: [93]  [41/42]  eta: 0:00:00  lr: 0.000081  loss: 3.3335 (3.3773)  time: 0.4810  data: 0.0001  max mem: 10046
[15:02:11.808925] Epoch: [93] Total time: 0:00:22 (0.5307 s / it)
[15:02:11.819160] Averaged stats: lr: 0.000081  loss: 3.3335 (3.4052)
[15:02:13.309937] Test:  [ 0/40]  eta: 0:00:59  loss: 3.1387 (3.1387)  acc1: 29.6875 (29.6875)  acc5: 54.6875 (54.6875)  time: 1.4868  data: 1.3300  max mem: 10046
[15:02:19.494153] Test:  [39/40]  eta: 0:00:00  loss: 3.1837 (3.2378)  acc1: 21.8750 (23.0400)  acc5: 53.1250 (51.0400)  time: 0.1510  data: 0.0000  max mem: 10046
[15:02:19.613370] Test: Total time: 0:00:07 (0.1948 s / it)
[15:02:20.092064] * Acc@1 22.900 Acc@5 50.780 loss 3.251
[15:02:20.092258] Accuracy of the network on the 10000 test images: 22.9%
[15:02:20.092456] [15:02:20.092528] Max accuracy: 22.98%
[15:02:20.092583] [15:02:20.093377] {"train_lr": 9.582919348406543e-05, "train_loss": 3.4051868674300967, "test_loss": 3.250955265760422, "test_acc1": 22.9, "test_acc5": 50.78, "epoch": 93, "n_parameters": 85958500}
[15:02:20.093444] [15:02:20.093501] Training epoch 93 for 0:00:30
[15:02:20.093552] [15:02:20.096324] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:02:22.643025] Epoch: [94]  [ 0/42]  eta: 0:01:46  lr: 0.000079  loss: 3.1563 (3.1563)  time: 2.5453  data: 1.5259  max mem: 10046
[15:02:42.223154] Epoch: [94]  [41/42]  eta: 0:00:00  lr: 0.000057  loss: 3.4482 (3.4995)  time: 0.4787  data: 0.0001  max mem: 10046
[15:02:42.448144] Epoch: [94] Total time: 0:00:22 (0.5322 s / it)
[15:02:42.452111] Averaged stats: lr: 0.000057  loss: 3.4482 (3.4874)
[15:02:44.214783] Test:  [ 0/40]  eta: 0:01:10  loss: 3.1578 (3.1578)  acc1: 29.6875 (29.6875)  acc5: 54.6875 (54.6875)  time: 1.7590  data: 1.6024  max mem: 10046
[15:02:50.229139] Test:  [39/40]  eta: 0:00:00  loss: 3.1734 (3.2347)  acc1: 21.8750 (22.9600)  acc5: 51.5625 (51.4800)  time: 0.1509  data: 0.0001  max mem: 10046
[15:02:50.438416] Test: Total time: 0:00:07 (0.1996 s / it)
[15:02:50.443327] * Acc@1 22.690 Acc@5 50.960 loss 3.248
[15:02:50.443472] Accuracy of the network on the 10000 test images: 22.7%
[15:02:50.443645] [15:02:50.443710] Max accuracy: 22.98%
[15:02:50.443763] [15:02:50.444522] {"train_lr": 6.936703368779015e-05, "train_loss": 3.4873894424665544, "test_loss": 3.2477897822856905, "test_acc1": 22.69, "test_acc5": 50.96, "epoch": 94, "n_parameters": 85958500}
[15:02:50.444594] [15:02:50.444649] Training epoch 94 for 0:00:30
[15:02:50.444699] [15:02:50.447402] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:02:52.875365] Epoch: [95]  [ 0/42]  eta: 0:01:41  lr: 0.000056  loss: 3.6926 (3.6926)  time: 2.4269  data: 1.7494  max mem: 10046
[15:03:12.471884] Epoch: [95]  [41/42]  eta: 0:00:00  lr: 0.000037  loss: 3.5258 (3.4561)  time: 0.4791  data: 0.0001  max mem: 10046
[15:03:12.699786] Epoch: [95] Total time: 0:00:22 (0.5298 s / it)
[15:03:12.700884] Averaged stats: lr: 0.000037  loss: 3.5258 (3.4388)
[15:03:14.586513] Test:  [ 0/40]  eta: 0:01:15  loss: 3.1595 (3.1595)  acc1: 26.5625 (26.5625)  acc5: 54.6875 (54.6875)  time: 1.8823  data: 1.7040  max mem: 10046
[15:03:20.601658] Test:  [39/40]  eta: 0:00:00  loss: 3.1845 (3.2290)  acc1: 21.8750 (23.4000)  acc5: 54.6875 (51.3200)  time: 0.1512  data: 0.0001  max mem: 10046
[15:03:20.759074] Test: Total time: 0:00:08 (0.2014 s / it)
[15:03:20.760308] * Acc@1 22.860 Acc@5 50.840 loss 3.246
[15:03:20.760476] Accuracy of the network on the 10000 test images: 22.9%
[15:03:20.760695] [15:03:20.760764] Max accuracy: 22.98%
[15:03:20.760820] [15:03:20.761696] {"train_lr": 4.720351007386505e-05, "train_loss": 3.4388486331417445, "test_loss": 3.246431311964989, "test_acc1": 22.86, "test_acc5": 50.84, "epoch": 95, "n_parameters": 85958500}
[15:03:20.761779] [15:03:20.761840] Training epoch 95 for 0:00:30
[15:03:20.761892] [15:03:20.764788] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:03:23.347455] Epoch: [96]  [ 0/42]  eta: 0:01:48  lr: 0.000036  loss: 3.4444 (3.4444)  time: 2.5819  data: 1.6147  max mem: 10046
[15:03:43.076768] Epoch: [96]  [41/42]  eta: 0:00:00  lr: 0.000021  loss: 3.3469 (3.4707)  time: 0.4809  data: 0.0001  max mem: 10046
[15:03:43.301137] Epoch: [96] Total time: 0:00:22 (0.5366 s / it)
[15:03:43.337698] Averaged stats: lr: 0.000021  loss: 3.3469 (3.4905)
[15:03:45.169240] Test:  [ 0/40]  eta: 0:01:13  loss: 3.1384 (3.1384)  acc1: 29.6875 (29.6875)  acc5: 54.6875 (54.6875)  time: 1.8281  data: 1.6501  max mem: 10046
[15:03:51.187900] Test:  [39/40]  eta: 0:00:00  loss: 3.1877 (3.2342)  acc1: 21.8750 (22.9600)  acc5: 53.1250 (51.4400)  time: 0.1510  data: 0.0001  max mem: 10046
[15:03:51.301842] Test: Total time: 0:00:07 (0.1990 s / it)
[15:03:51.540665] * Acc@1 22.940 Acc@5 50.880 loss 3.248
[15:03:51.540958] Accuracy of the network on the 10000 test images: 22.9%
[15:03:51.541225] [15:03:51.541304] Max accuracy: 22.98%
[15:03:51.541362] [15:03:51.542391] {"train_lr": 2.936285812996003e-05, "train_loss": 3.49050443371137, "test_loss": 3.248070764541626, "test_acc1": 22.94, "test_acc5": 50.88, "epoch": 96, "n_parameters": 85958500}
[15:03:51.542462] [15:03:51.542519] Training epoch 96 for 0:00:30
[15:03:51.542571] [15:03:51.545720] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:03:53.939171] Epoch: [97]  [ 0/42]  eta: 0:01:40  lr: 0.000021  loss: 3.8009 (3.8009)  time: 2.3926  data: 1.8168  max mem: 10046
[15:04:13.609174] Epoch: [97]  [41/42]  eta: 0:00:00  lr: 0.000010  loss: 3.4048 (3.3893)  time: 0.4816  data: 0.0001  max mem: 10046
[15:04:13.832593] Epoch: [97] Total time: 0:00:22 (0.5306 s / it)
[15:04:13.864938] Averaged stats: lr: 0.000010  loss: 3.4048 (3.3847)
[15:04:15.387127] Test:  [ 0/40]  eta: 0:01:00  loss: 3.1555 (3.1555)  acc1: 28.1250 (28.1250)  acc5: 53.1250 (53.1250)  time: 1.5184  data: 1.3612  max mem: 10046
[15:04:21.629094] Test:  [39/40]  eta: 0:00:00  loss: 3.1901 (3.2366)  acc1: 23.4375 (22.8800)  acc5: 53.1250 (51.6800)  time: 0.1510  data: 0.0001  max mem: 10046
[15:04:21.751155] Test: Total time: 0:00:07 (0.1971 s / it)
[15:04:22.013621] * Acc@1 22.650 Acc@5 51.030 loss 3.250
[15:04:22.013827] Accuracy of the network on the 10000 test images: 22.6%
[15:04:22.014022] [15:04:22.014095] Max accuracy: 22.98%
[15:04:22.014150] [15:04:22.014955] {"train_lr": 1.5864586347041642e-05, "train_loss": 3.3846747804255712, "test_loss": 3.249568819999695, "test_acc1": 22.65, "test_acc5": 51.03, "epoch": 97, "n_parameters": 85958500}
[15:04:22.015023] [15:04:22.015079] Training epoch 97 for 0:00:30
[15:04:22.015130] [15:04:22.017915] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:04:24.594816] Epoch: [98]  [ 0/42]  eta: 0:01:48  lr: 0.000010  loss: 3.4360 (3.4360)  time: 2.5759  data: 1.6031  max mem: 10046
[15:04:44.201223] Epoch: [98]  [41/42]  eta: 0:00:00  lr: 0.000003  loss: 3.4012 (3.4389)  time: 0.4795  data: 0.0001  max mem: 10046
[15:04:44.423900] Epoch: [98] Total time: 0:00:22 (0.5335 s / it)
[15:04:44.433807] Averaged stats: lr: 0.000003  loss: 3.4012 (3.4273)
[15:04:46.295131] Test:  [ 0/40]  eta: 0:01:14  loss: 3.1284 (3.1284)  acc1: 26.5625 (26.5625)  acc5: 54.6875 (54.6875)  time: 1.8574  data: 1.6792  max mem: 10046
[15:04:52.336310] Test:  [39/40]  eta: 0:00:00  loss: 3.1652 (3.2363)  acc1: 21.8750 (23.0000)  acc5: 51.5625 (51.1200)  time: 0.1514  data: 0.0001  max mem: 10046
[15:04:52.454336] Test: Total time: 0:00:08 (0.2005 s / it)
[15:04:52.802161] * Acc@1 22.690 Acc@5 50.650 loss 3.250
[15:04:52.802345] Accuracy of the network on the 10000 test images: 22.7%
[15:04:52.802548] [15:04:52.802618] Max accuracy: 22.98%
[15:04:52.802674] [15:04:52.803433] {"train_lr": 6.723454887125415e-06, "train_loss": 3.427262933481307, "test_loss": 3.250499564409256, "test_acc1": 22.69, "test_acc5": 50.65, "epoch": 98, "n_parameters": 85958500}
[15:04:52.803501] [15:04:52.803559] Training epoch 98 for 0:00:30
[15:04:52.803609] [15:04:52.806267] log_dir: ./exp/debug/cifar100-LT/vit_base_patch16/debug
[15:04:55.401647] Epoch: [99]  [ 0/42]  eta: 0:01:48  lr: 0.000003  loss: 3.8217 (3.8217)  time: 2.5942  data: 1.9911  max mem: 10046
[15:05:15.020184] Epoch: [99]  [41/42]  eta: 0:00:00  lr: 0.000001  loss: 3.3337 (3.4470)  time: 0.4804  data: 0.0001  max mem: 10046
[15:05:15.238281] Epoch: [99] Total time: 0:00:22 (0.5341 s / it)
[15:05:15.251334] Averaged stats: lr: 0.000001  loss: 3.3337 (3.4669)
[15:05:17.071686] Test:  [ 0/40]  eta: 0:01:12  loss: 3.1200 (3.1200)  acc1: 29.6875 (29.6875)  acc5: 53.1250 (53.1250)  time: 1.8168  data: 1.6442  max mem: 10046
[15:05:23.102856] Test:  [39/40]  eta: 0:00:00  loss: 3.1699 (3.2306)  acc1: 21.8750 (23.0000)  acc5: 51.5625 (51.5600)  time: 0.1517  data: 0.0001  max mem: 10046
[15:05:23.217841] Test: Total time: 0:00:07 (0.1991 s / it)
[15:05:23.459399] * Acc@1 22.800 Acc@5 50.940 loss 3.247
[15:05:23.459618] Accuracy of the network on the 10000 test images: 22.8%
[15:05:23.459852] [15:05:23.459919] Max accuracy: 22.98%
[15:05:23.459973] [15:05:23.460904] {"train_lr": 1.9494594432538313e-06, "train_loss": 3.466875188407444, "test_loss": 3.2472798347473146, "test_acc1": 22.8, "test_acc5": 50.94, "epoch": 99, "n_parameters": 85958500}
[15:05:23.460981] [15:05:23.461045] Training epoch 99 for 0:00:30
[15:05:23.461096] [15:05:23.461235] Total training time 0:52:46
[15:05:23.461283] /home/vision/wonjun/LiVT-main/models/bivit.py:1130: UserWarning: Overwriting vit_tiny_patch16_224 in registry with models.bivit.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1140: UserWarning: Overwriting vit_tiny_patch16_384 in registry with models.bivit.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_tiny_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1150: UserWarning: Overwriting vit_small_patch32_224 in registry with models.bivit.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1160: UserWarning: Overwriting vit_small_patch32_384 in registry with models.bivit.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1170: UserWarning: Overwriting vit_small_patch16_224 in registry with models.bivit.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1180: UserWarning: Overwriting vit_small_patch16_384 in registry with models.bivit.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1190: UserWarning: Overwriting vit_small_patch8_224 in registry with models.bivit.vit_small_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1200: UserWarning: Overwriting vit_base_patch32_224 in registry with models.bivit.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1212: UserWarning: Overwriting vit_base_patch32_384 in registry with models.bivit.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1224: UserWarning: Overwriting vit_base_patch16_224 in registry with models.bivit.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1236: UserWarning: Overwriting vit_base_patch16_384 in registry with models.bivit.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1248: UserWarning: Overwriting vit_base_patch8_224 in registry with models.bivit.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch8_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1260: UserWarning: Overwriting vit_large_patch32_224 in registry with models.bivit.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1270: UserWarning: Overwriting vit_large_patch32_384 in registry with models.bivit.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch32_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1282: UserWarning: Overwriting vit_large_patch16_224 in registry with models.bivit.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1294: UserWarning: Overwriting vit_large_patch16_384 in registry with models.bivit.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch16_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1306: UserWarning: Overwriting vit_large_patch14_224 in registry with models.bivit.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1316: UserWarning: Overwriting vit_huge_patch14_224 in registry with models.bivit.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1326: UserWarning: Overwriting vit_giant_patch14_224 in registry with models.bivit.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1338: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with models.bivit.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1352: UserWarning: Overwriting vit_base_patch16_224_miil in registry with models.bivit.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_224_miil(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1368: UserWarning: Overwriting vit_medium_patch16_gap_240 in registry with models.bivit.vit_medium_patch16_gap_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1390: UserWarning: Overwriting vit_medium_patch16_gap_256 in registry with models.bivit.vit_medium_patch16_gap_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1412: UserWarning: Overwriting vit_medium_patch16_gap_384 in registry with models.bivit.vit_medium_patch16_gap_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_medium_patch16_gap_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1434: UserWarning: Overwriting vit_base_patch16_gap_224 in registry with models.bivit.vit_base_patch16_gap_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_gap_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1454: UserWarning: Overwriting vit_base_patch32_clip_224 in registry with models.bivit.vit_base_patch32_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1473: UserWarning: Overwriting vit_base_patch32_clip_384 in registry with models.bivit.vit_base_patch32_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1492: UserWarning: Overwriting vit_base_patch32_clip_448 in registry with models.bivit.vit_base_patch32_clip_448. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_clip_448(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1511: UserWarning: Overwriting vit_base_patch16_clip_224 in registry with models.bivit.vit_base_patch16_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1530: UserWarning: Overwriting vit_base_patch16_clip_384 in registry with models.bivit.vit_base_patch16_clip_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_clip_384(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1549: UserWarning: Overwriting vit_large_patch14_clip_224 in registry with models.bivit.vit_large_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1568: UserWarning: Overwriting vit_large_patch14_clip_336 in registry with models.bivit.vit_large_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_large_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1587: UserWarning: Overwriting vit_huge_patch14_clip_224 in registry with models.bivit.vit_huge_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1606: UserWarning: Overwriting vit_huge_patch14_clip_336 in registry with models.bivit.vit_huge_patch14_clip_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_huge_patch14_clip_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1625: UserWarning: Overwriting vit_giant_patch14_clip_224 in registry with models.bivit.vit_giant_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_giant_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1647: UserWarning: Overwriting vit_gigantic_patch14_clip_224 in registry with models.bivit.vit_gigantic_patch14_clip_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_gigantic_patch14_clip_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1672: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with models.bivit.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch32_plus_256(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1686: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with models.bivit.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_plus_240(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1700: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with models.bivit.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_rpn_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1722: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with models.bivit.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_36x1_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1739: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with models.bivit.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_small_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1761: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with models.bivit.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def vit_base_patch16_18x2_224(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1782: UserWarning: Overwriting eva_large_patch14_196 in registry with models.bivit.eva_large_patch14_196. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_196(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1794: UserWarning: Overwriting eva_large_patch14_336 in registry with models.bivit.eva_large_patch14_336. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def eva_large_patch14_336(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1806: UserWarning: Overwriting flexivit_small in registry with models.bivit.flexivit_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_small(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1818: UserWarning: Overwriting flexivit_base in registry with models.bivit.flexivit_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_base(pretrained=False, **kwargs):
/home/vision/wonjun/LiVT-main/models/bivit.py:1830: UserWarning: Overwriting flexivit_large in registry with models.bivit.flexivit_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.
  def flexivit_large(pretrained=False, **kwargs):
| distributed init (rank 0): env://, gpu 0
[15:06:32.600517] job dir: /home/vision/wonjun/LiVT-main
[15:06:32.600691] Namespace(aa='rand-m9-mstd0.5-inc1',
accum_iter=1,
adamW2=0.95,
attn_only=False,
bal_tau=1.0,
batch_size=64,
blr=0.001,
ckpt_dir='./ckpt_dir',
clip_grad=None,
color_jitter=None,
cutmix=0,
cutmix_minmax=None,
data_path='./DATA/cifar-100-python',
dataset='cifar100-LT',
device='cuda',
dist_backend='nccl',
dist_eval=False,
dist_on_itp=False,
dist_url='env://',
distributed=True,
drop_path=0.1,
epochs=50,
eval=True,
finetune='<bound method Trainer.finetune of <util.trainer.Trainer object at 0x7f3d719cbe90>>',
global_pool=True,
gpu=0,
imbf=100,
input_size=224,
layer_decay=0.75,
local_rank=0,
log_dir='./exp//cifar100-LT/',
loss='ce',
lr=None,
min_lr=1e-06,
mixup=0,
mixup_mode='batch',
mixup_prob=1.0,
mixup_switch_prob=0.5,
model='vit_base_patch16',
nb_classes=100,
num_workers=16,
pin_mem=True,
prit=20,
rank=0,
recount=1,
remode='pixel',
reprob=0.25,
resplit=False,
resume='./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth',
seed=0,
smoothing=0.0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
world_size=1)
[15:06:32.901558] Files already downloaded and verified
[15:06:33.631926] Dataset CIFAR100_LT
    Number of datapoints: 10847
    Root location: ./DATA/cifar-100-python
    Split: Train
    StandardTransform
Transform: Compose(
               RandomResizedCropAndInterpolation(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               RandAugment(n=2, ops=
           	AugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)
           	AugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))
               ToTensor()
               Normalize(mean=tensor([0.4914, 0.4822, 0.4465]), std=tensor([0.2023, 0.1994, 0.2010]))
               RandomErasing(p=0.25, mode=pixel, count=(1, 1))
           )
[15:06:33.956109] Files already downloaded and verified
[15:06:34.305098] Dataset CIFAR100_LT
    Number of datapoints: 10000
    Root location: ./DATA/cifar-100-python
    Split: Test
    StandardTransform
Transform: Compose(
               Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
               CenterCrop(size=(224, 224))
               ToTensor()
               Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])
           )
[15:06:34.305301] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7ffb2460d3d0>
[15:06:34.305651] Train on 10847 Image w.r.t. 100 classes
[15:06:36.123077] Model = vit_base_patch16
[15:06:36.123180] number of params (M): 85.96
[15:06:36.123216] base lr: 1.00e-03
[15:06:36.123238] actual lr: 2.50e-04
[15:06:36.123260] accumulate grad iterations: 1
[15:06:36.123282] effective batch size: 64
[15:06:36.802158] Resume checkpoint ./ckpt/debug/cifar100-LT/vit_base_patch16/debug/checkpoint.pth
  0%|          | 0/157 [00:00<?, ?it/s]  1%|          | 1/157 [00:03<08:06,  3.12s/it]  1%|▏         | 2/157 [00:03<03:45,  1.46s/it]  2%|▏         | 3/157 [00:03<02:22,  1.08it/s]  3%|▎         | 4/157 [00:04<01:43,  1.48it/s]  3%|▎         | 5/157 [00:04<01:21,  1.86it/s]  4%|▍         | 6/157 [00:04<01:08,  2.20it/s]  4%|▍         | 7/157 [00:04<01:00,  2.49it/s]  5%|▌         | 8/157 [00:05<00:54,  2.73it/s]  6%|▌         | 9/157 [00:05<00:50,  2.91it/s]  6%|▋         | 10/157 [00:05<00:48,  3.05it/s]  7%|▋         | 11/157 [00:06<00:46,  3.15it/s]  8%|▊         | 12/157 [00:06<00:44,  3.23it/s]  8%|▊         | 13/157 [00:06<00:43,  3.28it/s]  9%|▉         | 14/157 [00:06<00:43,  3.32it/s] 10%|▉         | 15/157 [00:07<00:42,  3.35it/s] 10%|█         | 16/157 [00:07<00:41,  3.37it/s] 11%|█         | 17/157 [00:07<00:41,  3.38it/s] 11%|█▏        | 18/157 [00:08<00:41,  3.39it/s] 12%|█▏        | 19/157 [00:08<00:40,  3.40it/s] 13%|█▎        | 20/157 [00:08<00:40,  3.40it/s] 13%|█▎        | 21/157 [00:08<00:39,  3.40it/s] 14%|█▍        | 22/157 [00:09<00:39,  3.40it/s] 15%|█▍        | 23/157 [00:09<00:39,  3.40it/s] 15%|█▌        | 24/157 [00:09<00:39,  3.41it/s] 16%|█▌        | 25/157 [00:10<00:38,  3.40it/s] 17%|█▋        | 26/157 [00:10<00:38,  3.41it/s] 17%|█▋        | 27/157 [00:10<00:38,  3.40it/s] 18%|█▊        | 28/157 [00:11<00:38,  3.39it/s] 18%|█▊        | 29/157 [00:11<00:37,  3.40it/s] 19%|█▉        | 30/157 [00:11<00:37,  3.40it/s] 20%|█▉        | 31/157 [00:11<00:37,  3.40it/s] 20%|██        | 32/157 [00:12<00:36,  3.40it/s] 21%|██        | 33/157 [00:12<00:36,  3.40it/s] 22%|██▏       | 34/157 [00:12<00:36,  3.40it/s] 22%|██▏       | 35/157 [00:13<00:35,  3.40it/s] 23%|██▎       | 36/157 [00:13<00:35,  3.40it/s] 24%|██▎       | 37/157 [00:13<00:35,  3.40it/s] 24%|██▍       | 38/157 [00:13<00:34,  3.40it/s] 25%|██▍       | 39/157 [00:14<00:34,  3.40it/s] 25%|██▌       | 40/157 [00:14<00:34,  3.40it/s] 26%|██▌       | 41/157 [00:14<00:34,  3.40it/s] 27%|██▋       | 42/157 [00:15<00:33,  3.40it/s] 27%|██▋       | 43/157 [00:15<00:33,  3.40it/s] 28%|██▊       | 44/157 [00:15<00:33,  3.40it/s] 29%|██▊       | 45/157 [00:16<00:32,  3.40it/s] 29%|██▉       | 46/157 [00:16<00:32,  3.40it/s] 30%|██▉       | 47/157 [00:16<00:32,  3.40it/s] 31%|███       | 48/157 [00:16<00:32,  3.40it/s] 31%|███       | 49/157 [00:17<00:31,  3.40it/s] 32%|███▏      | 50/157 [00:17<00:31,  3.40it/s] 32%|███▏      | 51/157 [00:17<00:31,  3.40it/s] 33%|███▎      | 52/157 [00:18<00:30,  3.40it/s] 34%|███▍      | 53/157 [00:18<00:30,  3.40it/s] 34%|███▍      | 54/157 [00:18<00:30,  3.40it/s] 35%|███▌      | 55/157 [00:18<00:30,  3.40it/s] 36%|███▌      | 56/157 [00:19<00:29,  3.40it/s] 36%|███▋      | 57/157 [00:19<00:29,  3.40it/s] 37%|███▋      | 58/157 [00:19<00:29,  3.40it/s] 38%|███▊      | 59/157 [00:20<00:28,  3.40it/s] 38%|███▊      | 60/157 [00:20<00:28,  3.39it/s] 39%|███▉      | 61/157 [00:20<00:28,  3.40it/s] 39%|███▉      | 62/157 [00:21<00:27,  3.39it/s] 40%|████      | 63/157 [00:21<00:27,  3.39it/s] 41%|████      | 64/157 [00:21<00:27,  3.39it/s] 41%|████▏     | 65/157 [00:21<00:27,  3.39it/s] 42%|████▏     | 66/157 [00:22<00:26,  3.39it/s] 43%|████▎     | 67/157 [00:22<00:26,  3.39it/s] 43%|████▎     | 68/157 [00:22<00:26,  3.39it/s] 44%|████▍     | 69/157 [00:23<00:25,  3.39it/s] 45%|████▍     | 70/157 [00:23<00:25,  3.39it/s] 45%|████▌     | 71/157 [00:23<00:25,  3.39it/s] 46%|████▌     | 72/157 [00:23<00:25,  3.39it/s] 46%|████▋     | 73/157 [00:24<00:24,  3.39it/s] 47%|████▋     | 74/157 [00:24<00:24,  3.39it/s] 48%|████▊     | 75/157 [00:24<00:24,  3.39it/s] 48%|████▊     | 76/157 [00:25<00:23,  3.39it/s] 49%|████▉     | 77/157 [00:25<00:23,  3.39it/s] 50%|████▉     | 78/157 [00:25<00:23,  3.39it/s] 50%|█████     | 79/157 [00:26<00:23,  3.39it/s] 51%|█████     | 80/157 [00:26<00:22,  3.39it/s] 52%|█████▏    | 81/157 [00:26<00:22,  3.39it/s] 52%|█████▏    | 82/157 [00:26<00:22,  3.39it/s] 53%|█████▎    | 83/157 [00:27<00:21,  3.39it/s] 54%|█████▎    | 84/157 [00:27<00:21,  3.39it/s] 54%|█████▍    | 85/157 [00:27<00:21,  3.39it/s] 55%|█████▍    | 86/157 [00:28<00:20,  3.39it/s] 55%|█████▌    | 87/157 [00:28<00:20,  3.39it/s] 56%|█████▌    | 88/157 [00:28<00:20,  3.39it/s] 57%|█████▋    | 89/157 [00:29<00:20,  3.38it/s] 57%|█████▋    | 90/157 [00:29<00:19,  3.38it/s] 58%|█████▊    | 91/157 [00:29<00:19,  3.38it/s] 59%|█████▊    | 92/157 [00:29<00:19,  3.38it/s] 59%|█████▉    | 93/157 [00:30<00:18,  3.38it/s] 60%|█████▉    | 94/157 [00:30<00:18,  3.38it/s] 61%|██████    | 95/157 [00:30<00:18,  3.38it/s] 61%|██████    | 96/157 [00:31<00:18,  3.39it/s] 62%|██████▏   | 97/157 [00:31<00:17,  3.38it/s] 62%|██████▏   | 98/157 [00:31<00:17,  3.38it/s] 63%|██████▎   | 99/157 [00:31<00:17,  3.37it/s] 64%|██████▎   | 100/157 [00:32<00:16,  3.37it/s] 64%|██████▍   | 101/157 [00:32<00:16,  3.37it/s] 65%|██████▍   | 102/157 [00:32<00:16,  3.38it/s] 66%|██████▌   | 103/157 [00:33<00:15,  3.38it/s] 66%|██████▌   | 104/157 [00:33<00:15,  3.38it/s] 67%|██████▋   | 105/157 [00:33<00:15,  3.38it/s] 68%|██████▊   | 106/157 [00:34<00:15,  3.37it/s] 68%|██████▊   | 107/157 [00:34<00:14,  3.37it/s] 69%|██████▉   | 108/157 [00:34<00:14,  3.37it/s] 69%|██████▉   | 109/157 [00:34<00:14,  3.37it/s] 70%|███████   | 110/157 [00:35<00:13,  3.37it/s] 71%|███████   | 111/157 [00:35<00:13,  3.37it/s] 71%|███████▏  | 112/157 [00:35<00:13,  3.37it/s] 72%|███████▏  | 113/157 [00:36<00:13,  3.37it/s] 73%|███████▎  | 114/157 [00:36<00:12,  3.37it/s] 73%|███████▎  | 115/157 [00:36<00:12,  3.37it/s] 74%|███████▍  | 116/157 [00:37<00:12,  3.37it/s] 75%|███████▍  | 117/157 [00:37<00:11,  3.37it/s] 75%|███████▌  | 118/157 [00:37<00:11,  3.37it/s] 76%|███████▌  | 119/157 [00:37<00:11,  3.37it/s] 76%|███████▋  | 120/157 [00:38<00:10,  3.37it/s] 77%|███████▋  | 121/157 [00:38<00:10,  3.37it/s] 78%|███████▊  | 122/157 [00:38<00:10,  3.37it/s] 78%|███████▊  | 123/157 [00:39<00:10,  3.37it/s] 79%|███████▉  | 124/157 [00:39<00:09,  3.37it/s] 80%|███████▉  | 125/157 [00:39<00:09,  3.37it/s] 80%|████████  | 126/157 [00:39<00:09,  3.37it/s] 81%|████████  | 127/157 [00:40<00:08,  3.37it/s] 82%|████████▏ | 128/157 [00:40<00:08,  3.37it/s] 82%|████████▏ | 129/157 [00:40<00:08,  3.37it/s] 83%|████████▎ | 130/157 [00:41<00:08,  3.37it/s] 83%|████████▎ | 131/157 [00:41<00:07,  3.37it/s] 84%|████████▍ | 132/157 [00:41<00:07,  3.37it/s] 85%|████████▍ | 133/157 [00:42<00:07,  3.37it/s] 85%|████████▌ | 134/157 [00:42<00:06,  3.37it/s] 86%|████████▌ | 135/157 [00:42<00:06,  3.37it/s] 87%|████████▋ | 136/157 [00:42<00:06,  3.37it/s] 87%|████████▋ | 137/157 [00:43<00:05,  3.37it/s] 88%|████████▊ | 138/157 [00:43<00:05,  3.37it/s] 89%|████████▊ | 139/157 [00:43<00:05,  3.37it/s] 89%|████████▉ | 140/157 [00:44<00:05,  3.37it/s] 90%|████████▉ | 141/157 [00:44<00:04,  3.37it/s] 90%|█████████ | 142/157 [00:44<00:04,  3.37it/s] 91%|█████████ | 143/157 [00:45<00:04,  3.37it/s] 92%|█████████▏| 144/157 [00:45<00:03,  3.37it/s] 92%|█████████▏| 145/157 [00:45<00:03,  3.37it/s] 93%|█████████▎| 146/157 [00:45<00:03,  3.37it/s] 94%|█████████▎| 147/157 [00:46<00:02,  3.37it/s] 94%|█████████▍| 148/157 [00:46<00:02,  3.37it/s] 95%|█████████▍| 149/157 [00:46<00:02,  3.37it/s] 96%|█████████▌| 150/157 [00:47<00:02,  3.36it/s] 96%|█████████▌| 151/157 [00:47<00:01,  3.36it/s] 97%|█████████▋| 152/157 [00:47<00:01,  3.36it/s] 97%|█████████▋| 153/157 [00:47<00:01,  3.36it/s] 98%|█████████▊| 154/157 [00:48<00:00,  3.36it/s] 99%|█████████▊| 155/157 [00:48<00:00,  3.36it/s] 99%|█████████▉| 156/157 [00:48<00:00,  3.36it/s]100%|██████████| 157/157 [00:49<00:00,  3.20it/s]
[15:07:25.943040] {'avg_acc': 22.72, 'ece': 4.7, 'mce': 11.93, 'many': 26.77, 'medium': 21.58, 'few': 21.16, 'pdc': 0.39}
Traceback (most recent call last):
  File "./main_finetune.py", line 405, in <module>
    main(args)
  File "./main_finetune.py", line 306, in main
    misc.save_eval_json(result, save_pth)
  File "/home/vision/wonjun/LiVT-main/util/misc.py", line 385, in save_eval_json
    os.makedirs(path, exist_ok=True)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File "/usr/local/lib/python3.7/os.py", line 213, in makedirs
    makedirs(head, exist_ok=exist_ok)
  [Previous line repeated 1 more time]
  File "/usr/local/lib/python3.7/os.py", line 223, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '/ckpt'
/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py:188: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  FutureWarning,
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 21218) of binary: /usr/local/bin/python
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/usr/local/lib/python3.7/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/run.py", line 756, in run
    )(*cmd_args)
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/vision/.local/lib/python3.7/site-packages/torch/distributed/launcher/api.py", line 248, in launch_agent
    failures=result.failures,
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./main_finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-21_15:07:29
  host      : user
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 21218)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
